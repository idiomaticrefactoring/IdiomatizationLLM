repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,chatgpt_real_acc,ridiom_acc,real_acc,double sen,truth_code
InfoGAN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InfoGAN/infogan/models/regularized_gan.py,https://github.com/openai/InfoGAN/tree/master/infogan/models/regularized_gan.py,RegularizedGAN,cont_reg_dist_info$102,"def cont_reg_dist_info(self, reg_dist_info):
    ret = []
    for (dist_i, dist_info_i) in zip(self.reg_latent_dist.dists, self.reg_latent_dist.split_dist_info(reg_dist_info)):
        if isinstance(dist_i, Gaussian):
            ret.append(dist_info_i)
    return self.reg_cont_latent_dist.join_dist_infos(ret)","for (dist_i, dist_info_i) in zip(self.reg_latent_dist.dists, self.reg_latent_dist.split_dist_info(reg_dist_info)):
    if isinstance(dist_i, Gaussian):
        ret.append(dist_info_i)","ret = [dist_info_i for (dist_i, dist_info_i) in zip(self.reg_latent_dist.dists, self.reg_latent_dist.split_dist_info(reg_dist_info)) if isinstance(dist_i, Gaussian)]","ret = [dist_info_i for (dist_i, dist_info_i) in zip(self.reg_latent_dist.dists, self.reg_latent_dist.split_dist_info(reg_dist_info)) if isinstance(dist_i, Gaussian)]",1,,,,,robosuite
easytrader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/easytrader/easytrader/utils/captcha.py,https://github.com/shidenggui/easytrader/tree/master/easytrader/utils/captcha.py,,captcha_recognize$9,"def captcha_recognize(img_path):
    import pytesseract
    im = Image.open(img_path).convert('L')
    threshold = 200
    table = []
    for i in range(256):
        if i < threshold:
            table.append(0)
        else:
            table.append(1)
    out = im.point(table, '1')
    num = pytesseract.image_to_string(out)
    return num","for i in range(256):
    if i < threshold:
        table.append(0)
    else:
        table.append(1)",table = [0 if i < threshold else 1 for i in range(256)],table = [0 if i < threshold else 1 for i in range(256)],1,,,,,robosuite
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/config_validator_util/cv_data_converter.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/config_validator_util/cv_data_converter.py,,generate_ancestors$62,"def generate_ancestors(full_name):
    """"""Generate ancestor list from full_name.

    Args:
        full_name (str): Full name of the resource.

    Returns:
        list: List of ancestors.
    """"""
    ancestors = []
    full_name_items = full_name.split('/')
    for i in reversed(range(0, len(full_name_items) - 1)):
        if full_name_items[i] in SUPPORTED_ANCESTORS:
            ancestors.append(f'{full_name_items[i]}s/{full_name_items[i + 1]}')
    return ancestors","for i in reversed(range(0, len(full_name_items) - 1)):
    if full_name_items[i] in SUPPORTED_ANCESTORS:
        ancestors.append(f'{full_name_items[i]}s/{full_name_items[i + 1]}')","ancestors = [f'{full_name_items[i]}s/{full_name_items[i + 1]}' for i in reversed(range(0, len(full_name_items) - 1)) if full_name_items[i] in SUPPORTED_ANCESTORS]","ancestors = [f'{full_name_items[i]}s/{full_name_items[i + 1]}' for i in reversed(range(0, len(full_name_items) - 1)) if full_name_items[i] in SUPPORTED_ANCESTORS]",1,,,,,robosuite
MLAlgorithms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MLAlgorithms/mla/neuralnet/nnet.py,https://github.com/rushter/MLAlgorithms/tree/master/mla/neuralnet/nnet.py,NeuralNet,_predict$102,"def _predict(self, X=None):
    if not self._initialized:
        self._setup_layers(X.shape)
    y = []
    X_batch = batch_iterator(X, self.batch_size)
    for Xb in X_batch:
        y.append(self.fprop(Xb))
    return np.concatenate(y)","for Xb in X_batch:
    y.append(self.fprop(Xb))",y = [self.fprop(Xb) for Xb in X_batch],y = [self.fprop(Xb) for Xb in X_batch],1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/npu/test_stack_op_npu.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/npu/test_stack_op_npu.py,TestStackOpBase,get_x_names$37,"def get_x_names(self):
    x_names = []
    for i in range(self.num_inputs):
        x_names.append('x{}'.format(i))
    return x_names","for i in range(self.num_inputs):
    x_names.append('x{}'.format(i))",x_names = ['x{}'.format(i) for i in range(self.num_inputs)],x_names = ['x{}'.format(i) for i in range(self.num_inputs)],1,,,,,robosuite
findatapy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/findatapy/findatapy/market/ioengine.py,https://github.com/cuemacro/findatapy/tree/master/findatapy/market/ioengine.py,IOEngine,read_csv_data_frame$751,"def read_csv_data_frame(self, f_name, freq, cutoff=None, dateparse=None, postfix='.close', intraday_tz='UTC', excel_sheet=None):
    """"""Reads CSV/Excel from disk into DataFrame

        Parameters
        ----------
        f_name : str
            CSV/Excel file path to read
        freq : str
            Frequency of data to read (intraday/daily etc)
        cutoff : DateTime (optional)
            end date to read up to
        dateparse : str (optional)
            date parser to use
        postfix : str (optional)
            postfix to add to each columns
        intraday_tz : str (optional)
            timezone of file if uses intraday data
        excel_sheet : str (optional)
            Excel sheet to be read

        Returns
        -------
        DataFrame
        """"""
    if freq == 'intraday':
        if dateparse is None:
            dateparse = lambda x: datetime.datetime(*map(int, [x[6:10], x[3:5], x[0:2], x[11:13], x[14:16], x[17:19]]))
        elif dateparse == 'dukascopy':
            dateparse = lambda x: datetime.datetime(*map(int, [x[0:4], x[5:7], x[8:10], x[11:13], x[14:16], x[17:19]]))
        elif dateparse == 'c':
            import ciso8601
            dateparse = lambda x: ciso8601.parse_datetime(x)
        if excel_sheet is None:
            data_frame = pd.read_csv(f_name, index_col=0, parse_dates=True, date_parser=dateparse)
        else:
            data_frame = pd.read_excel(f_name, excel_sheet, index_col=0, na_values=['NA'])
        data_frame = data_frame.astype('float32')
        data_frame.index.names = ['Date']
        old_cols = data_frame.columns
        new_cols = []
        for col in old_cols:
            new_cols.append(col + postfix)
        data_frame.columns = new_cols
    elif 'events' in f_name:
        data_frame = pd.read_csv(f_name)
        data_frame = data_frame.convert_objects(convert_dates='coerce')
    elif excel_sheet is None:
        try:
            data_frame = pd.read_csv(f_name, index_col=0, parse_dates=['DATE'], date_parser=dateparse)
        except:
            data_frame = pd.read_csv(f_name, index_col=0, parse_dates=['Date'], date_parser=dateparse)
    else:
        data_frame = pd.read_excel(f_name, excel_sheet, index_col=0, na_values=['NA'])
    if freq == 'intraday':
        data_frame = data_frame.tz_localize(intraday_tz)
    if cutoff is not None:
        if isinstance(cutoff, str):
            cutoff = parse(cutoff)
        data_frame = data_frame.loc[data_frame.index < cutoff]
    return data_frame","for col in old_cols:
    new_cols.append(col + postfix)",new_cols = [col + postfix for col in old_cols],new_cols = [col + postfix for col in old_cols],1,,,,,robosuite
keras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras/keras/engine/training_v1.py,https://github.com/keras-team/keras/tree/master/keras/engine/training_v1.py,Model,_process_target_tensor_for_compile$1407,"def _process_target_tensor_for_compile(self, target_tensors):
    if self.run_eagerly:
        return [None for _ in self.output_names]
    if target_tensors is not None and (not (isinstance(target_tensors, list) and target_tensors == [])):
        if isinstance(target_tensors, list):
            if len(target_tensors) != len(self.outputs):
                raise ValueError('When passing a list as `target_tensors`, it should have one entry per model output. The model has %s outputs, but you passed target_tensors=%s' % (len(self.outputs), target_tensors))
        elif isinstance(target_tensors, dict):
            unexpected_target_tensor_names = set(target_tensors.keys()).difference(self.output_names)
            if unexpected_target_tensor_names:
                raise ValueError('Unknown entry in `target_tensors` dictionary: ""{name}"". Only expected the following keys: {keys}'.format(name=unexpected_target_tensor_names, keys=str(self.output_names)))
            tmp_target_tensors = []
            for name in self.output_names:
                tmp_target_tensors.append(target_tensors.get(name, None))
            target_tensors = tmp_target_tensors
        elif tf.is_tensor(target_tensors):
            target_tensors = [target_tensors]
        else:
            raise TypeError('Expected `target_tensors` to be a list or tuple or dict or a single tensor, but got:', target_tensors)
    else:
        target_tensors = [None for _ in self.output_names]
    return target_tensors","for name in self.output_names:
    tmp_target_tensors.append(target_tensors.get(name, None))","tmp_target_tensors = [target_tensors.get(name, None) for name in self.output_names]","tmp_target_tensors = [target_tensors.get(name, None) for name in self.output_names]",1,,,,,robosuite
graph4nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph4nlp/graph4nlp/pytorch/test/graph_embedding/run_gat.py,https://github.com/graph4ai/graph4nlp/tree/master/graph4nlp/pytorch/test/graph_embedding/run_gat.py,,grid_search_main$360,"def grid_search_main(config):
    print_config(config)
    grid_search_hyperparams = []
    for (k, v) in config.items():
        if isinstance(v, list):
            grid_search_hyperparams.append(k)
    logger = Logger(config['save_model_path'], config=config, overwrite=True)
    best_config = None
    best_score = -1
    configs = grid(config)
    for cnf in configs:
        print('\n')
        for k in grid_search_hyperparams:
            cnf['save_model_path'] += '_{}_{}'.format(k, cnf[k])
        print(cnf['save_model_path'])
        logger.write(cnf['save_model_path'])
        score = main(dict_to_namedtuple(cnf), cnf['random_seed'])
        if best_score < score:
            best_score = score
            best_config = cnf
            print('Found a better configuration: {}'.format(best_score))
            logger.write('Found a better configuration: {}'.format(best_score))
    print('\nBest configuration:')
    logger.write('\nBest configuration:')
    for k in grid_search_hyperparams:
        print('{}: {}'.format(k, best_config[k]))
        logger.write('{}: {}'.format(k, best_config[k]))
    print('Best test score: {}'.format(best_score))
    logger.write('Best test score: {}'.format(best_score))
    logger.close()","for (k, v) in config.items():
    if isinstance(v, list):
        grid_search_hyperparams.append(k)","grid_search_hyperparams = [k for (k, v) in config.items() if isinstance(v, list)]","grid_search_hyperparams = [k for (k, v) in config.items() if isinstance(v, list)]",1,,,,,robosuite
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/algorithms.py,https://github.com/animate1978/MB-Lab/tree/master//algorithms.py,,collect_existing_meshes$611,"def collect_existing_meshes():
    existing_mesh_names = []
    for mesh in bpy.data.meshes:
        existing_mesh_names.append(mesh.name)
    return existing_mesh_names","for mesh in bpy.data.meshes:
    existing_mesh_names.append(mesh.name)",existing_mesh_names = [mesh.name for mesh in bpy.data.meshes],existing_mesh_names = [mesh.name for mesh in bpy.data.meshes],1,,,,,robosuite
pool-reference,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pool-reference/tests/test_difficulty.py,https://github.com/Chia-Network/pool-reference/tree/master/tests/test_difficulty.py,TestDifficulty,test_increases_diff$68,"def test_increases_diff(self):
    num_partials = 300
    time_target = 24 * 3600
    partials = []
    current_time = uint64(time.time())
    for i in range(num_partials):
        partials.append((uint64(current_time - i * 200), 20))
    assert get_new_difficulty(partials, num_partials, time_target, 20, current_time, 1) == 28","for i in range(num_partials):
    partials.append((uint64(current_time - i * 200), 20))","partials += [(uint64(current_time - i * 200), 20) for i in range(num_partials)]","partials = [(uint64(current_time - i * 200), 20) for i in range(num_partials)]",0,1,,,,robosuite
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libopengaze.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libopengaze.py,OpenGazeTracker,calibrate$184,"def calibrate(self):
    """"""Calibrates the eye tracking system
        
        arguments
        None
        
        keyword arguments
        None

        returns
        success    -- returns True if calibration succeeded, or False if
                   not; in addition a calibration log is added to the
                   log file and some properties are updated (i.e. the
                   thresholds for detection algorithms)
        """"""
    self.screen.clear()
    self.screen.draw_text(text='Preparing the calibration, please wait...', fontsize=20)
    self.disp.fill(self.screen)
    self.disp.show()
    caldur = {'animation': 1.5, 'point': 1.0, 'timeout': 10.0}
    self.opengaze.calibrate_delay(caldur['animation'])
    self.opengaze.calibrate_timeout(caldur['point'])
    calibpoints = []
    for x in [0.1, 0.5, 0.9]:
        for y in [0.1, 0.5, 0.9]:
            calibpoints.append((x, y))
    random.shuffle(calibpoints)
    self.opengaze.calibrate_clear()
    for (x, y) in calibpoints:
        self.opengaze.calibrate_addpoint(x, y)
    self.screen.clear()
    self.screen.draw_text(text='Press Space to calibrate, S to skip, and Q to quit', fontsize=20)
    self.disp.fill(self.screen)
    self.disp.show()
    (key, keytime) = self.kb.get_key(keylist=['q', 's', 'space'], timeout=None, flush=True)
    self.screen.clear()
    self.disp.fill(self.screen)
    self.disp.show()
    if key == 's':
        return True
    if key == 'q':
        quited = True
    else:
        quited = False
    x = self.dispsize[0] // 2
    y = self.dispsize[1] // 2
    self.screen.draw_circle(colour=(255, 255, 255), pos=(x, y), r=30, fill=True)
    self.screen.draw_circle(colour=(255, 0, 0), pos=(x, y), r=3, fill=True)
    self.disp.fill(self.screen)
    self.disp.show()
    calibrated = False
    while not quited and (not calibrated):
        self.opengaze.clear_calibration_result()
        while True:
            calibpoints = self.opengaze.get_calibration_points()
            if calibpoints is not None:
                break
        self.opengaze.calibrate_start(True)
        pointnr = 0
        n_points = len(calibpoints)
        if self.has_been_calibrated_before:
            n_points += 1
        for i in range(n_points):
            (pointnr, pos) = self.opengaze.wait_for_calibration_point_start(timeout=caldur['timeout'])
            if pointnr is None:
                quited = True
                break
            x = int(pos[0] * self.dispsize[0])
            y = int(pos[1] * self.dispsize[1])
            t1 = clock.get_time()
            t = clock.get_time()
            while t - t1 < caldur['animation'] * 1000:
                if self.kb.get_key(keylist=['q'], timeout=10, flush=False)[0] == 'q':
                    quited = True
                    break
                self.screen.clear(colour=(0, 0, 0))
                p = 1.0 - float(t - t1) / (caldur['animation'] * 1000)
                self.screen.draw_circle(colour=(255, 255, 255), pos=(x, y), r=max(1, int(30 * p)), fill=True)
                self.screen.draw_circle(colour=(255, 0, 0), pos=(x, y), r=3, fill=True)
                self.disp.fill(self.screen)
                t = self.disp.show()
            if self.kb.get_key(keylist=['q'], timeout=1, flush=False)[0] == 'q':
                quited = True
            if quited:
                break
        calibresult = None
        while calibresult is None and (not quited):
            calibresult = self.opengaze.get_calibration_result()
            if self.kb.get_key(keylist=['q'], timeout=100, flush=False)[0] == 'q':
                quited = True
                break
        if quited:
            self.screen.clear()
            self.screen.draw_text(text=""Calibration aborted. Press Space to restart or 'Q' to quit"", fontsize=20)
            self.disp.fill(self.screen)
            self.disp.show()
            (key, keytime) = self.kb.get_key(keylist=['q', 'space'], timeout=None, flush=True)
            if key == 'space':
                quited = False
            continue
        self.disp.fill()
        self.disp.show()
        self.screen.clear()
        if calibresult is not None:
            for p in calibresult:
                for param in ['CALX', 'LX', 'RX']:
                    p[param] *= self.dispsize[0]
                for param in ['CALY', 'LY', 'RY']:
                    p[param] *= self.dispsize[1]
                self.screen.draw_fixation(fixtype='dot', colour=(115, 210, 22), pos=(p['CALX'], p['CALY']))
                col = {'L': (32, 74, 135), 'R': (92, 53, 102)}
                for eye in ['L', 'R']:
                    if p['{}V'.format(eye)]:
                        x = p['{}X'.format(eye)]
                        y = p['{}Y'.format(eye)]
                        c = col[eye]
                    else:
                        x = p['CALX']
                        y = p['CALY']
                        c = (204, 0, 0)
                    if p['{}V'.format(eye)]:
                        self.screen.draw_line(colour=c, spos=(p['CALX'], p['CALY']), epos=(x, y), pw=3)
                    self.screen.draw_fixation(fixtype='dot', pos=(x, y), colour=c)
                    self.screen.draw_text(text=eye, pos=(x + 10, y + 10), colour=c, fontsize=20)
            self.screen.draw_text(text=""Press Space to continue or 'R' to restart"", pos=(int(self.dispsize[0] * 0.5), int(self.dispsize[1] * 0.25 + 60)), fontsize=20)
        else:
            self.screen.draw_text(text=""Calibration failed. Press 'R' to try again."", fontsize=20)
        self.disp.fill(self.screen)
        self.disp.show()
        (key, keytime) = self.kb.get_key(keylist=['space', 'r'], timeout=None, flush=True)
        if key == 'space':
            calibrated = True
        self.has_been_calibrated_before = True
    if quited:
        return False
    err = {'LX': [], 'LY': [], 'RX': [], 'RY': []}
    var = {'LX': [], 'LY': [], 'RX': [], 'RY': []}
    for p in calibresult:
        for eye in ['L', 'R']:
            for dim in ['X', 'Y']:
                if p['{}V'.format(eye)]:
                    d = p['{}{}'.format(eye, dim)] - p['CAL{}'.format(dim)]
                    err['{}{}'.format(eye, dim)].append(abs(d))
                    var['{}{}'.format(eye, dim)].append(d ** 2)
    xnoise = []
    ynoise = []
    if var['LX'] and var['LY']:
        xnoise.append(math.sqrt(mean(var['LX'])))
        ynoise.append(math.sqrt(mean(var['LY'])))
    if var['RX'] and var['RY']:
        xnoise.append(math.sqrt(mean(var['RX'])))
        ynoise.append(math.sqrt(mean(var['RY'])))
    xnoise = mean(xnoise)
    ynoise = mean(ynoise)
    self.pxdsttresh = (xnoise, ynoise)
    pixpercm = (self.dispsize[0] / float(self.screensize[0]) + self.dispsize[1] / float(self.screensize[1])) / 2
    screendist = settings.SCREENDIST
    self.accuracy = []
    self.pxaccuracy = []
    if err['LX'] and err['LY']:
        self.accuracy.append([pix2deg(screendist, mean(err['LX']), pixpercm), pix2deg(screendist, mean(err['LY']), pixpercm)])
        self.pxaccuracy.append([mean(err['LX']), mean(err['LY'])])
    else:
        self.accuracy.append([0, 0])
        self.pxaccuracy.append([0, 0])
    if err['RX'] and err['RY']:
        self.accuracy.append([pix2deg(screendist, mean(err['RX']), pixpercm), pix2deg(screendist, mean(err['RY']), pixpercm)])
        self.pxaccuracy.append([mean(err['RX']), mean(err['RY'])])
    else:
        self.accuracy.append([0, 0])
        self.pxaccuracy.append([0, 0])
    self.pxerrdist = deg2pix(screendist, self.errdist, pixpercm)
    self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
    self.pxspdtresh = deg2pix(screendist, self.spdtresh / 1000.0, pixpercm)
    self.pxacctresh = deg2pix(screendist, self.accthresh / 1000.0, pixpercm)
    self._elog('pygaze calibration report start')
    self._elog('accuracy (degrees): LX={}, LY={}, RX={}, RY={}'.format(self.accuracy[0][0], self.accuracy[0][1], self.accuracy[1][0], self.accuracy[1][1]))
    self._elog('accuracy (in pixels): LX={}, LY={}, RX={}, RY={}'.format(self.pxaccuracy[0][0], self.pxaccuracy[0][1], self.pxaccuracy[1][0], self.pxaccuracy[1][1]))
    self._elog('precision (RMS noise in pixels): X={}, Y={}'.format(self.pxdsttresh[0], self.pxdsttresh[1]))
    self._elog('distance between participant and display: {} cm'.format(screendist))
    self._elog('fixation threshold: {} pixels'.format(self.pxfixtresh))
    self._elog('speed threshold: {} pixels/ms'.format(self.pxspdtresh))
    self._elog('acceleration threshold: {} pixels/ms**2'.format(self.pxacctresh))
    self._elog('pygaze calibration report end')
    return True","for x in [0.1, 0.5, 0.9]:
    for y in [0.1, 0.5, 0.9]:
        calibpoints.append((x, y))","calibpoints = [(x, y) for x in [0.1, 0.5, 0.9] for y in [0.1, 0.5, 0.9]]","calibpoints = [(x, y) for x in [0.1, 0.5, 0.9] for y in [0.1, 0.5, 0.9]]",1,,,,,robosuite
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libopengaze.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libopengaze.py,OpenGazeTracker,calibrate$184,"def calibrate(self):
    """"""Calibrates the eye tracking system
        
        arguments
        None
        
        keyword arguments
        None

        returns
        success    -- returns True if calibration succeeded, or False if
                   not; in addition a calibration log is added to the
                   log file and some properties are updated (i.e. the
                   thresholds for detection algorithms)
        """"""
    self.screen.clear()
    self.screen.draw_text(text='Preparing the calibration, please wait...', fontsize=20)
    self.disp.fill(self.screen)
    self.disp.show()
    caldur = {'animation': 1.5, 'point': 1.0, 'timeout': 10.0}
    self.opengaze.calibrate_delay(caldur['animation'])
    self.opengaze.calibrate_timeout(caldur['point'])
    calibpoints = []
    for x in [0.1, 0.5, 0.9]:
        for y in [0.1, 0.5, 0.9]:
            calibpoints.append((x, y))
    random.shuffle(calibpoints)
    self.opengaze.calibrate_clear()
    for (x, y) in calibpoints:
        self.opengaze.calibrate_addpoint(x, y)
    self.screen.clear()
    self.screen.draw_text(text='Press Space to calibrate, S to skip, and Q to quit', fontsize=20)
    self.disp.fill(self.screen)
    self.disp.show()
    (key, keytime) = self.kb.get_key(keylist=['q', 's', 'space'], timeout=None, flush=True)
    self.screen.clear()
    self.disp.fill(self.screen)
    self.disp.show()
    if key == 's':
        return True
    if key == 'q':
        quited = True
    else:
        quited = False
    x = self.dispsize[0] // 2
    y = self.dispsize[1] // 2
    self.screen.draw_circle(colour=(255, 255, 255), pos=(x, y), r=30, fill=True)
    self.screen.draw_circle(colour=(255, 0, 0), pos=(x, y), r=3, fill=True)
    self.disp.fill(self.screen)
    self.disp.show()
    calibrated = False
    while not quited and (not calibrated):
        self.opengaze.clear_calibration_result()
        while True:
            calibpoints = self.opengaze.get_calibration_points()
            if calibpoints is not None:
                break
        self.opengaze.calibrate_start(True)
        pointnr = 0
        n_points = len(calibpoints)
        if self.has_been_calibrated_before:
            n_points += 1
        for i in range(n_points):
            (pointnr, pos) = self.opengaze.wait_for_calibration_point_start(timeout=caldur['timeout'])
            if pointnr is None:
                quited = True
                break
            x = int(pos[0] * self.dispsize[0])
            y = int(pos[1] * self.dispsize[1])
            t1 = clock.get_time()
            t = clock.get_time()
            while t - t1 < caldur['animation'] * 1000:
                if self.kb.get_key(keylist=['q'], timeout=10, flush=False)[0] == 'q':
                    quited = True
                    break
                self.screen.clear(colour=(0, 0, 0))
                p = 1.0 - float(t - t1) / (caldur['animation'] * 1000)
                self.screen.draw_circle(colour=(255, 255, 255), pos=(x, y), r=max(1, int(30 * p)), fill=True)
                self.screen.draw_circle(colour=(255, 0, 0), pos=(x, y), r=3, fill=True)
                self.disp.fill(self.screen)
                t = self.disp.show()
            if self.kb.get_key(keylist=['q'], timeout=1, flush=False)[0] == 'q':
                quited = True
            if quited:
                break
        calibresult = None
        while calibresult is None and (not quited):
            calibresult = self.opengaze.get_calibration_result()
            if self.kb.get_key(keylist=['q'], timeout=100, flush=False)[0] == 'q':
                quited = True
                break
        if quited:
            self.screen.clear()
            self.screen.draw_text(text=""Calibration aborted. Press Space to restart or 'Q' to quit"", fontsize=20)
            self.disp.fill(self.screen)
            self.disp.show()
            (key, keytime) = self.kb.get_key(keylist=['q', 'space'], timeout=None, flush=True)
            if key == 'space':
                quited = False
            continue
        self.disp.fill()
        self.disp.show()
        self.screen.clear()
        if calibresult is not None:
            for p in calibresult:
                for param in ['CALX', 'LX', 'RX']:
                    p[param] *= self.dispsize[0]
                for param in ['CALY', 'LY', 'RY']:
                    p[param] *= self.dispsize[1]
                self.screen.draw_fixation(fixtype='dot', colour=(115, 210, 22), pos=(p['CALX'], p['CALY']))
                col = {'L': (32, 74, 135), 'R': (92, 53, 102)}
                for eye in ['L', 'R']:
                    if p['{}V'.format(eye)]:
                        x = p['{}X'.format(eye)]
                        y = p['{}Y'.format(eye)]
                        c = col[eye]
                    else:
                        x = p['CALX']
                        y = p['CALY']
                        c = (204, 0, 0)
                    if p['{}V'.format(eye)]:
                        self.screen.draw_line(colour=c, spos=(p['CALX'], p['CALY']), epos=(x, y), pw=3)
                    self.screen.draw_fixation(fixtype='dot', pos=(x, y), colour=c)
                    self.screen.draw_text(text=eye, pos=(x + 10, y + 10), colour=c, fontsize=20)
            self.screen.draw_text(text=""Press Space to continue or 'R' to restart"", pos=(int(self.dispsize[0] * 0.5), int(self.dispsize[1] * 0.25 + 60)), fontsize=20)
        else:
            self.screen.draw_text(text=""Calibration failed. Press 'R' to try again."", fontsize=20)
        self.disp.fill(self.screen)
        self.disp.show()
        (key, keytime) = self.kb.get_key(keylist=['space', 'r'], timeout=None, flush=True)
        if key == 'space':
            calibrated = True
        self.has_been_calibrated_before = True
    if quited:
        return False
    err = {'LX': [], 'LY': [], 'RX': [], 'RY': []}
    var = {'LX': [], 'LY': [], 'RX': [], 'RY': []}
    for p in calibresult:
        for eye in ['L', 'R']:
            for dim in ['X', 'Y']:
                if p['{}V'.format(eye)]:
                    d = p['{}{}'.format(eye, dim)] - p['CAL{}'.format(dim)]
                    err['{}{}'.format(eye, dim)].append(abs(d))
                    var['{}{}'.format(eye, dim)].append(d ** 2)
    xnoise = []
    ynoise = []
    if var['LX'] and var['LY']:
        xnoise.append(math.sqrt(mean(var['LX'])))
        ynoise.append(math.sqrt(mean(var['LY'])))
    if var['RX'] and var['RY']:
        xnoise.append(math.sqrt(mean(var['RX'])))
        ynoise.append(math.sqrt(mean(var['RY'])))
    xnoise = mean(xnoise)
    ynoise = mean(ynoise)
    self.pxdsttresh = (xnoise, ynoise)
    pixpercm = (self.dispsize[0] / float(self.screensize[0]) + self.dispsize[1] / float(self.screensize[1])) / 2
    screendist = settings.SCREENDIST
    self.accuracy = []
    self.pxaccuracy = []
    if err['LX'] and err['LY']:
        self.accuracy.append([pix2deg(screendist, mean(err['LX']), pixpercm), pix2deg(screendist, mean(err['LY']), pixpercm)])
        self.pxaccuracy.append([mean(err['LX']), mean(err['LY'])])
    else:
        self.accuracy.append([0, 0])
        self.pxaccuracy.append([0, 0])
    if err['RX'] and err['RY']:
        self.accuracy.append([pix2deg(screendist, mean(err['RX']), pixpercm), pix2deg(screendist, mean(err['RY']), pixpercm)])
        self.pxaccuracy.append([mean(err['RX']), mean(err['RY'])])
    else:
        self.accuracy.append([0, 0])
        self.pxaccuracy.append([0, 0])
    self.pxerrdist = deg2pix(screendist, self.errdist, pixpercm)
    self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
    self.pxspdtresh = deg2pix(screendist, self.spdtresh / 1000.0, pixpercm)
    self.pxacctresh = deg2pix(screendist, self.accthresh / 1000.0, pixpercm)
    self._elog('pygaze calibration report start')
    self._elog('accuracy (degrees): LX={}, LY={}, RX={}, RY={}'.format(self.accuracy[0][0], self.accuracy[0][1], self.accuracy[1][0], self.accuracy[1][1]))
    self._elog('accuracy (in pixels): LX={}, LY={}, RX={}, RY={}'.format(self.pxaccuracy[0][0], self.pxaccuracy[0][1], self.pxaccuracy[1][0], self.pxaccuracy[1][1]))
    self._elog('precision (RMS noise in pixels): X={}, Y={}'.format(self.pxdsttresh[0], self.pxdsttresh[1]))
    self._elog('distance between participant and display: {} cm'.format(screendist))
    self._elog('fixation threshold: {} pixels'.format(self.pxfixtresh))
    self._elog('speed threshold: {} pixels/ms'.format(self.pxspdtresh))
    self._elog('acceleration threshold: {} pixels/ms**2'.format(self.pxacctresh))
    self._elog('pygaze calibration report end')
    return True","for p in calibresult:
    for eye in ['L', 'R']:
        for dim in ['X', 'Y']:
            if p['{}V'.format(eye)]:
                d = p['{}{}'.format(eye, dim)] - p['CAL{}'.format(dim)]
                err['{}{}'.format(eye, dim)].append(abs(d))
                var['{}{}'.format(eye, dim)].append(d ** 2)","var['{}{}'.format(eye, dim)] = [pow(p[f'{eye}{dim}'] - p[f'CAL{dim}'], 2) for p in calibresult for eye in ['L', 'R'] for dim in ['X', 'Y'] if p[f'{eye}V']]",Cannot refactor,-1,0,,2,1,robosuite
decaNLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/decaNLP/text/torchtext/data/example.py,https://github.com/salesforce/decaNLP/tree/master/text/torchtext/data/example.py,,intern_strings$8,"def intern_strings(x):
    if isinstance(x, (list, tuple)):
        r = []
        for y in x:
            if isinstance(y, str):
                r.append(sys.intern(y))
            else:
                r.append(y)
        return r
    return x","for y in x:
    if isinstance(y, str):
        r.append(sys.intern(y))
    else:
        r.append(y)","r += [sys.intern(y) if isinstance(y, str) else y for y in x]","r = [sys.intern(y) if isinstance(y, str) else y for y in x]",0,1,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/contrib/gsm_um.py,https://github.com/phaethon/kamene/tree/master/kamene/contrib/gsm_um.py,NetworkName,post_build$11060,"def post_build(self, p, pay):
    aList = []
    a = []
    i = 0
    for i in range(0, len(self.fields_desc)):
        aList.append(self.fields_desc[i].name)
    for i in aList:
        a.append(getattr(self, i))
    res = adapt(2, 250, a, self.fields_desc, 1)
    if self.lengthNN is None:
        p = struct.pack('>B', res[1]) + p[1:]
    if res[0] is not 0:
        p = p[:-res[0]]
    return p + pay","for i in range(0, len(self.fields_desc)):
    aList.append(self.fields_desc[i].name)","aList = [self.fields_desc[i].name for i in range(0, len(self.fields_desc))]","aList = [self.fields_desc[i].name for i in range(0, len(self.fields_desc))]",1,,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/contrib/gsm_um.py,https://github.com/phaethon/kamene/tree/master/kamene/contrib/gsm_um.py,NetworkName,post_build$11060,"def post_build(self, p, pay):
    aList = []
    a = []
    i = 0
    for i in range(0, len(self.fields_desc)):
        aList.append(self.fields_desc[i].name)
    for i in aList:
        a.append(getattr(self, i))
    res = adapt(2, 250, a, self.fields_desc, 1)
    if self.lengthNN is None:
        p = struct.pack('>B', res[1]) + p[1:]
    if res[0] is not 0:
        p = p[:-res[0]]
    return p + pay","for i in aList:
    a.append(getattr(self, i))","a += [getattr(self, i) for i in aList]","a = [getattr(self, i) for i in aList]",0,1,,,,robosuite
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/feature_extraction_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/feature_extraction_detr.py,DetrFeatureExtractor,post_process_panoptic$973,"def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):
    """"""
        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.

        Args:
            outputs ([`DetrSegmentationOutput`]):
                Raw outputs of the model.
            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):
                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data
                augmentation but before batching.
            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`, *optional*):
                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction. If left to
                None, it will default to the `processed_sizes`.
            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):
                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.
                If not set, defaults to the `is_thing_map` of COCO panoptic.
            threshold (`float`, *optional*, defaults to 0.85):
                Threshold to use to filter out queries.

        Returns:
            `List[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for
            an image in the batch as predicted by the model.
        """"""
    warnings.warn('`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use `post_process_panoptic_segmentation`.', FutureWarning)
    if target_sizes is None:
        target_sizes = processed_sizes
    if len(processed_sizes) != len(target_sizes):
        raise ValueError('Make sure to pass in as many processed_sizes as target_sizes')
    if is_thing_map is None:
        is_thing_map = {i: i <= 90 for i in range(201)}
    (out_logits, raw_masks, raw_boxes) = (outputs.logits, outputs.pred_masks, outputs.pred_boxes)
    if not len(out_logits) == len(raw_masks) == len(target_sizes):
        raise ValueError('Make sure that you pass in as many target sizes as the batch dimension of the logits and masks')
    preds = []

    def to_tuple(tup):
        if isinstance(tup, tuple):
            return tup
        return tuple(tup.cpu().tolist())
    for (cur_logits, cur_masks, cur_boxes, size, target_size) in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):
        (scores, labels) = cur_logits.softmax(-1).max(-1)
        keep = labels.ne(outputs.logits.shape[-1] - 1) & (scores > threshold)
        (cur_scores, cur_classes) = cur_logits.softmax(-1).max(-1)
        cur_scores = cur_scores[keep]
        cur_classes = cur_classes[keep]
        cur_masks = cur_masks[keep]
        cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode='bilinear').squeeze(1)
        cur_boxes = center_to_corners_format(cur_boxes[keep])
        (h, w) = cur_masks.shape[-2:]
        if len(cur_boxes) != len(cur_classes):
            raise ValueError('Not as many boxes as there are classes')
        cur_masks = cur_masks.flatten(1)
        stuff_equiv_classes = defaultdict(lambda : [])
        for (k, label) in enumerate(cur_classes):
            if not is_thing_map[label.item()]:
                stuff_equiv_classes[label.item()].append(k)

        def get_ids_area(masks, scores, dedup=False):
            m_id = masks.transpose(0, 1).softmax(-1)
            if m_id.shape[-1] == 0:
                m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)
            else:
                m_id = m_id.argmax(-1).view(h, w)
            if dedup:
                for equiv in stuff_equiv_classes.values():
                    if len(equiv) > 1:
                        for eq_id in equiv:
                            m_id.masked_fill_(m_id.eq(eq_id), equiv[0])
            (final_h, final_w) = to_tuple(target_size)
            seg_img = Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))
            seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)
            np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))
            np_seg_img = np_seg_img.view(final_h, final_w, 3)
            np_seg_img = np_seg_img.numpy()
            m_id = torch.from_numpy(rgb_to_id(np_seg_img))
            area = []
            for i in range(len(scores)):
                area.append(m_id.eq(i).sum().item())
            return (area, seg_img)
        (area, seg_img) = get_ids_area(cur_masks, cur_scores, dedup=True)
        if cur_classes.numel() > 0:
            while True:
                filtered_small = torch.as_tensor([area[i] <= 4 for (i, c) in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)
                if filtered_small.any().item():
                    cur_scores = cur_scores[~filtered_small]
                    cur_classes = cur_classes[~filtered_small]
                    cur_masks = cur_masks[~filtered_small]
                    (area, seg_img) = get_ids_area(cur_masks, cur_scores)
                else:
                    break
        else:
            cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)
        segments_info = []
        for (i, a) in enumerate(area):
            cat = cur_classes[i].item()
            segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})
        del cur_classes
        with io.BytesIO() as out:
            seg_img.save(out, format='PNG')
            predictions = {'png_string': out.getvalue(), 'segments_info': segments_info}
        preds.append(predictions)
    return preds","for (k, label) in enumerate(cur_classes):
    if not is_thing_map[label.item()]:
        stuff_equiv_classes[label.item()].append(k)","stuff_equiv_classes[label.item()] = [k for (k, label) in enumerate(cur_classes) if not is_thing_map[label.item()]]",Cannot refactor,-1,0,,,,robosuite
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/feature_extraction_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/feature_extraction_detr.py,DetrFeatureExtractor,post_process_panoptic$973,"def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):
    """"""
        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.

        Args:
            outputs ([`DetrSegmentationOutput`]):
                Raw outputs of the model.
            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):
                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data
                augmentation but before batching.
            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`, *optional*):
                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction. If left to
                None, it will default to the `processed_sizes`.
            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):
                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.
                If not set, defaults to the `is_thing_map` of COCO panoptic.
            threshold (`float`, *optional*, defaults to 0.85):
                Threshold to use to filter out queries.

        Returns:
            `List[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for
            an image in the batch as predicted by the model.
        """"""
    warnings.warn('`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use `post_process_panoptic_segmentation`.', FutureWarning)
    if target_sizes is None:
        target_sizes = processed_sizes
    if len(processed_sizes) != len(target_sizes):
        raise ValueError('Make sure to pass in as many processed_sizes as target_sizes')
    if is_thing_map is None:
        is_thing_map = {i: i <= 90 for i in range(201)}
    (out_logits, raw_masks, raw_boxes) = (outputs.logits, outputs.pred_masks, outputs.pred_boxes)
    if not len(out_logits) == len(raw_masks) == len(target_sizes):
        raise ValueError('Make sure that you pass in as many target sizes as the batch dimension of the logits and masks')
    preds = []

    def to_tuple(tup):
        if isinstance(tup, tuple):
            return tup
        return tuple(tup.cpu().tolist())
    for (cur_logits, cur_masks, cur_boxes, size, target_size) in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):
        (scores, labels) = cur_logits.softmax(-1).max(-1)
        keep = labels.ne(outputs.logits.shape[-1] - 1) & (scores > threshold)
        (cur_scores, cur_classes) = cur_logits.softmax(-1).max(-1)
        cur_scores = cur_scores[keep]
        cur_classes = cur_classes[keep]
        cur_masks = cur_masks[keep]
        cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode='bilinear').squeeze(1)
        cur_boxes = center_to_corners_format(cur_boxes[keep])
        (h, w) = cur_masks.shape[-2:]
        if len(cur_boxes) != len(cur_classes):
            raise ValueError('Not as many boxes as there are classes')
        cur_masks = cur_masks.flatten(1)
        stuff_equiv_classes = defaultdict(lambda : [])
        for (k, label) in enumerate(cur_classes):
            if not is_thing_map[label.item()]:
                stuff_equiv_classes[label.item()].append(k)

        def get_ids_area(masks, scores, dedup=False):
            m_id = masks.transpose(0, 1).softmax(-1)
            if m_id.shape[-1] == 0:
                m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)
            else:
                m_id = m_id.argmax(-1).view(h, w)
            if dedup:
                for equiv in stuff_equiv_classes.values():
                    if len(equiv) > 1:
                        for eq_id in equiv:
                            m_id.masked_fill_(m_id.eq(eq_id), equiv[0])
            (final_h, final_w) = to_tuple(target_size)
            seg_img = Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))
            seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)
            np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))
            np_seg_img = np_seg_img.view(final_h, final_w, 3)
            np_seg_img = np_seg_img.numpy()
            m_id = torch.from_numpy(rgb_to_id(np_seg_img))
            area = []
            for i in range(len(scores)):
                area.append(m_id.eq(i).sum().item())
            return (area, seg_img)
        (area, seg_img) = get_ids_area(cur_masks, cur_scores, dedup=True)
        if cur_classes.numel() > 0:
            while True:
                filtered_small = torch.as_tensor([area[i] <= 4 for (i, c) in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)
                if filtered_small.any().item():
                    cur_scores = cur_scores[~filtered_small]
                    cur_classes = cur_classes[~filtered_small]
                    cur_masks = cur_masks[~filtered_small]
                    (area, seg_img) = get_ids_area(cur_masks, cur_scores)
                else:
                    break
        else:
            cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)
        segments_info = []
        for (i, a) in enumerate(area):
            cat = cur_classes[i].item()
            segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})
        del cur_classes
        with io.BytesIO() as out:
            seg_img.save(out, format='PNG')
            predictions = {'png_string': out.getvalue(), 'segments_info': segments_info}
        preds.append(predictions)
    return preds","for (i, a) in enumerate(area):
    cat = cur_classes[i].item()
    segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})","segments_info = [{'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a} for ((i, a), cat) in zip(enumerate(area), cur_classes)]",Cannot refactor,-1,1,,,,robosuite
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/feature_extraction_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/feature_extraction_detr.py,DetrFeatureExtractor,post_process_panoptic$973,"def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):
    """"""
        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.

        Args:
            outputs ([`DetrSegmentationOutput`]):
                Raw outputs of the model.
            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):
                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data
                augmentation but before batching.
            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`, *optional*):
                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction. If left to
                None, it will default to the `processed_sizes`.
            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):
                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.
                If not set, defaults to the `is_thing_map` of COCO panoptic.
            threshold (`float`, *optional*, defaults to 0.85):
                Threshold to use to filter out queries.

        Returns:
            `List[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for
            an image in the batch as predicted by the model.
        """"""
    warnings.warn('`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use `post_process_panoptic_segmentation`.', FutureWarning)
    if target_sizes is None:
        target_sizes = processed_sizes
    if len(processed_sizes) != len(target_sizes):
        raise ValueError('Make sure to pass in as many processed_sizes as target_sizes')
    if is_thing_map is None:
        is_thing_map = {i: i <= 90 for i in range(201)}
    (out_logits, raw_masks, raw_boxes) = (outputs.logits, outputs.pred_masks, outputs.pred_boxes)
    if not len(out_logits) == len(raw_masks) == len(target_sizes):
        raise ValueError('Make sure that you pass in as many target sizes as the batch dimension of the logits and masks')
    preds = []

    def to_tuple(tup):
        if isinstance(tup, tuple):
            return tup
        return tuple(tup.cpu().tolist())
    for (cur_logits, cur_masks, cur_boxes, size, target_size) in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):
        (scores, labels) = cur_logits.softmax(-1).max(-1)
        keep = labels.ne(outputs.logits.shape[-1] - 1) & (scores > threshold)
        (cur_scores, cur_classes) = cur_logits.softmax(-1).max(-1)
        cur_scores = cur_scores[keep]
        cur_classes = cur_classes[keep]
        cur_masks = cur_masks[keep]
        cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode='bilinear').squeeze(1)
        cur_boxes = center_to_corners_format(cur_boxes[keep])
        (h, w) = cur_masks.shape[-2:]
        if len(cur_boxes) != len(cur_classes):
            raise ValueError('Not as many boxes as there are classes')
        cur_masks = cur_masks.flatten(1)
        stuff_equiv_classes = defaultdict(lambda : [])
        for (k, label) in enumerate(cur_classes):
            if not is_thing_map[label.item()]:
                stuff_equiv_classes[label.item()].append(k)

        def get_ids_area(masks, scores, dedup=False):
            m_id = masks.transpose(0, 1).softmax(-1)
            if m_id.shape[-1] == 0:
                m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)
            else:
                m_id = m_id.argmax(-1).view(h, w)
            if dedup:
                for equiv in stuff_equiv_classes.values():
                    if len(equiv) > 1:
                        for eq_id in equiv:
                            m_id.masked_fill_(m_id.eq(eq_id), equiv[0])
            (final_h, final_w) = to_tuple(target_size)
            seg_img = Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))
            seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)
            np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))
            np_seg_img = np_seg_img.view(final_h, final_w, 3)
            np_seg_img = np_seg_img.numpy()
            m_id = torch.from_numpy(rgb_to_id(np_seg_img))
            area = []
            for i in range(len(scores)):
                area.append(m_id.eq(i).sum().item())
            return (area, seg_img)
        (area, seg_img) = get_ids_area(cur_masks, cur_scores, dedup=True)
        if cur_classes.numel() > 0:
            while True:
                filtered_small = torch.as_tensor([area[i] <= 4 for (i, c) in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)
                if filtered_small.any().item():
                    cur_scores = cur_scores[~filtered_small]
                    cur_classes = cur_classes[~filtered_small]
                    cur_masks = cur_masks[~filtered_small]
                    (area, seg_img) = get_ids_area(cur_masks, cur_scores)
                else:
                    break
        else:
            cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)
        segments_info = []
        for (i, a) in enumerate(area):
            cat = cur_classes[i].item()
            segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})
        del cur_classes
        with io.BytesIO() as out:
            seg_img.save(out, format='PNG')
            predictions = {'png_string': out.getvalue(), 'segments_info': segments_info}
        preds.append(predictions)
    return preds","for i in range(len(scores)):
    area.append(m_id.eq(i).sum().item())",area = [m_id.eq(i).sum().item() for i in range(len(scores))],area = [m_id.eq(i).sum().item() for i in range(len(scores))],1,,,,,robosuite
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/erpnext_integrations/doctype/tally_migration/tally_migration.py,https://github.com/frappe/erpnext/tree/master/erpnext/erpnext_integrations/doctype/tally_migration/tally_migration.py,TallyMigration,_process_master_data$103,"def _process_master_data(self):

    def get_company_name(collection):
        return collection.find_all('REMOTECMPINFO.LIST')[0].REMOTECMPNAME.string.strip()

    def get_coa_customers_suppliers(collection):
        root_type_map = {'Application of Funds (Assets)': 'Asset', 'Expenses': 'Expense', 'Income': 'Income', 'Source of Funds (Liabilities)': 'Liability'}
        roots = set(root_type_map.keys())
        accounts = list(get_groups(collection.find_all('GROUP'))) + list(get_ledgers(collection.find_all('LEDGER')))
        (children, parents) = get_children_and_parent_dict(accounts)
        group_set = [acc[1] for acc in accounts if acc[2]]
        (children, customers, suppliers) = remove_parties(parents, children, group_set)
        try:
            coa = traverse({}, children, roots, roots, group_set)
        except RecursionError:
            self.log(_('Error occured while parsing Chart of Accounts: Please make sure that no two accounts have the same name'))
        for account in coa:
            coa[account]['root_type'] = root_type_map[account]
        return (coa, customers, suppliers)

    def get_groups(accounts):
        for account in accounts:
            if account['NAME'] in (self.tally_creditors_account, self.tally_debtors_account):
                yield (get_parent(account), account['NAME'], 0)
            else:
                yield (get_parent(account), account['NAME'], 1)

    def get_ledgers(accounts):
        for account in accounts:
            if account.PARENT:
                yield (account.PARENT.string.strip(), account['NAME'], 0)

    def get_parent(account):
        if account.PARENT:
            return account.PARENT.string.strip()
        return {('Yes', 'No'): 'Application of Funds (Assets)', ('Yes', 'Yes'): 'Expenses', ('No', 'Yes'): 'Income', ('No', 'No'): 'Source of Funds (Liabilities)'}[account.ISDEEMEDPOSITIVE.string.strip(), account.ISREVENUE.string.strip()]

    def get_children_and_parent_dict(accounts):
        (children, parents) = ({}, {})
        for (parent, account, is_group) in accounts:
            children.setdefault(parent, set()).add(account)
            parents.setdefault(account, set()).add(parent)
            parents[account].update(parents.get(parent, []))
        return (children, parents)

    def remove_parties(parents, children, group_set):
        (customers, suppliers) = (set(), set())
        for account in parents:
            found = False
            if self.tally_creditors_account in parents[account]:
                found = True
                if account not in group_set:
                    suppliers.add(account)
            if self.tally_debtors_account in parents[account]:
                found = True
                if account not in group_set:
                    customers.add(account)
            if found:
                children.pop(account, None)
        return (children, customers, suppliers)

    def traverse(tree, children, accounts, roots, group_set):
        for account in accounts:
            if account in group_set or account in roots:
                if account in children:
                    tree[account] = traverse({}, children, children[account], roots, group_set)
                else:
                    tree[account] = {'is_group': 1}
            else:
                tree[account] = {}
        return tree

    def get_parties_addresses(collection, customers, suppliers):
        (parties, addresses) = ([], [])
        for account in collection.find_all('LEDGER'):
            party_type = None
            links = []
            if account.NAME.string.strip() in customers:
                party_type = 'Customer'
                parties.append({'doctype': party_type, 'customer_name': account.NAME.string.strip(), 'tax_id': account.INCOMETAXNUMBER.string.strip() if account.INCOMETAXNUMBER else None, 'customer_group': 'All Customer Groups', 'territory': 'All Territories', 'customer_type': 'Individual'})
                links.append({'link_doctype': party_type, 'link_name': account['NAME']})
            if account.NAME.string.strip() in suppliers:
                party_type = 'Supplier'
                parties.append({'doctype': party_type, 'supplier_name': account.NAME.string.strip(), 'pan': account.INCOMETAXNUMBER.string.strip() if account.INCOMETAXNUMBER else None, 'supplier_group': 'All Supplier Groups', 'supplier_type': 'Individual'})
                links.append({'link_doctype': party_type, 'link_name': account['NAME']})
            if party_type:
                address = '\n'.join([a.string.strip() for a in account.find_all('ADDRESS')])
                addresses.append({'doctype': 'Address', 'address_line1': address[:140].strip(), 'address_line2': address[140:].strip(), 'country': account.COUNTRYNAME.string.strip() if account.COUNTRYNAME else None, 'state': account.LEDSTATENAME.string.strip() if account.LEDSTATENAME else None, 'gst_state': account.LEDSTATENAME.string.strip() if account.LEDSTATENAME else None, 'pin_code': account.PINCODE.string.strip() if account.PINCODE else None, 'mobile': account.LEDGERPHONE.string.strip() if account.LEDGERPHONE else None, 'phone': account.LEDGERPHONE.string.strip() if account.LEDGERPHONE else None, 'gstin': account.PARTYGSTIN.string.strip() if account.PARTYGSTIN else None, 'links': links})
        return (parties, addresses)

    def get_stock_items_uoms(collection):
        uoms = []
        for uom in collection.find_all('UNIT'):
            uoms.append({'doctype': 'UOM', 'uom_name': uom.NAME.string.strip()})
        items = []
        for item in collection.find_all('STOCKITEM'):
            stock_uom = item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom
            items.append({'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': stock_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]})
        return (items, uoms)
    try:
        self.publish('Process Master Data', _('Reading Uploaded File'), 1, 5)
        collection = self.get_collection(self.master_data)
        company = get_company_name(collection)
        self.tally_company = company
        self.erpnext_company = company
        self.publish('Process Master Data', _('Processing Chart of Accounts and Parties'), 2, 5)
        (chart_of_accounts, customers, suppliers) = get_coa_customers_suppliers(collection)
        self.publish('Process Master Data', _('Processing Party Addresses'), 3, 5)
        (parties, addresses) = get_parties_addresses(collection, customers, suppliers)
        self.publish('Process Master Data', _('Processing Items and UOMs'), 4, 5)
        (items, uoms) = get_stock_items_uoms(collection)
        data = {'chart_of_accounts': chart_of_accounts, 'parties': parties, 'addresses': addresses, 'items': items, 'uoms': uoms}
        self.publish('Process Master Data', _('Done'), 5, 5)
        self.dump_processed_data(data)
        self.is_master_data_processed = 1
    except Exception:
        self.publish('Process Master Data', _('Process Failed'), -1, 5)
        self.log()
    finally:
        self.set_status()","for uom in collection.find_all('UNIT'):
    uoms.append({'doctype': 'UOM', 'uom_name': uom.NAME.string.strip()})","uoms = [{'doctype': 'UOM', 'uom_name': uom.NAME.string.strip()} for uom in collection.find_all('UNIT')]","uoms = [{'doctype': 'UOM', 'uom_name': uom.NAME.string.strip()} for uom in collection.find_all('UNIT')]",1,,,,,robosuite
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/erpnext_integrations/doctype/tally_migration/tally_migration.py,https://github.com/frappe/erpnext/tree/master/erpnext/erpnext_integrations/doctype/tally_migration/tally_migration.py,TallyMigration,_process_master_data$103,"def _process_master_data(self):

    def get_company_name(collection):
        return collection.find_all('REMOTECMPINFO.LIST')[0].REMOTECMPNAME.string.strip()

    def get_coa_customers_suppliers(collection):
        root_type_map = {'Application of Funds (Assets)': 'Asset', 'Expenses': 'Expense', 'Income': 'Income', 'Source of Funds (Liabilities)': 'Liability'}
        roots = set(root_type_map.keys())
        accounts = list(get_groups(collection.find_all('GROUP'))) + list(get_ledgers(collection.find_all('LEDGER')))
        (children, parents) = get_children_and_parent_dict(accounts)
        group_set = [acc[1] for acc in accounts if acc[2]]
        (children, customers, suppliers) = remove_parties(parents, children, group_set)
        try:
            coa = traverse({}, children, roots, roots, group_set)
        except RecursionError:
            self.log(_('Error occured while parsing Chart of Accounts: Please make sure that no two accounts have the same name'))
        for account in coa:
            coa[account]['root_type'] = root_type_map[account]
        return (coa, customers, suppliers)

    def get_groups(accounts):
        for account in accounts:
            if account['NAME'] in (self.tally_creditors_account, self.tally_debtors_account):
                yield (get_parent(account), account['NAME'], 0)
            else:
                yield (get_parent(account), account['NAME'], 1)

    def get_ledgers(accounts):
        for account in accounts:
            if account.PARENT:
                yield (account.PARENT.string.strip(), account['NAME'], 0)

    def get_parent(account):
        if account.PARENT:
            return account.PARENT.string.strip()
        return {('Yes', 'No'): 'Application of Funds (Assets)', ('Yes', 'Yes'): 'Expenses', ('No', 'Yes'): 'Income', ('No', 'No'): 'Source of Funds (Liabilities)'}[account.ISDEEMEDPOSITIVE.string.strip(), account.ISREVENUE.string.strip()]

    def get_children_and_parent_dict(accounts):
        (children, parents) = ({}, {})
        for (parent, account, is_group) in accounts:
            children.setdefault(parent, set()).add(account)
            parents.setdefault(account, set()).add(parent)
            parents[account].update(parents.get(parent, []))
        return (children, parents)

    def remove_parties(parents, children, group_set):
        (customers, suppliers) = (set(), set())
        for account in parents:
            found = False
            if self.tally_creditors_account in parents[account]:
                found = True
                if account not in group_set:
                    suppliers.add(account)
            if self.tally_debtors_account in parents[account]:
                found = True
                if account not in group_set:
                    customers.add(account)
            if found:
                children.pop(account, None)
        return (children, customers, suppliers)

    def traverse(tree, children, accounts, roots, group_set):
        for account in accounts:
            if account in group_set or account in roots:
                if account in children:
                    tree[account] = traverse({}, children, children[account], roots, group_set)
                else:
                    tree[account] = {'is_group': 1}
            else:
                tree[account] = {}
        return tree

    def get_parties_addresses(collection, customers, suppliers):
        (parties, addresses) = ([], [])
        for account in collection.find_all('LEDGER'):
            party_type = None
            links = []
            if account.NAME.string.strip() in customers:
                party_type = 'Customer'
                parties.append({'doctype': party_type, 'customer_name': account.NAME.string.strip(), 'tax_id': account.INCOMETAXNUMBER.string.strip() if account.INCOMETAXNUMBER else None, 'customer_group': 'All Customer Groups', 'territory': 'All Territories', 'customer_type': 'Individual'})
                links.append({'link_doctype': party_type, 'link_name': account['NAME']})
            if account.NAME.string.strip() in suppliers:
                party_type = 'Supplier'
                parties.append({'doctype': party_type, 'supplier_name': account.NAME.string.strip(), 'pan': account.INCOMETAXNUMBER.string.strip() if account.INCOMETAXNUMBER else None, 'supplier_group': 'All Supplier Groups', 'supplier_type': 'Individual'})
                links.append({'link_doctype': party_type, 'link_name': account['NAME']})
            if party_type:
                address = '\n'.join([a.string.strip() for a in account.find_all('ADDRESS')])
                addresses.append({'doctype': 'Address', 'address_line1': address[:140].strip(), 'address_line2': address[140:].strip(), 'country': account.COUNTRYNAME.string.strip() if account.COUNTRYNAME else None, 'state': account.LEDSTATENAME.string.strip() if account.LEDSTATENAME else None, 'gst_state': account.LEDSTATENAME.string.strip() if account.LEDSTATENAME else None, 'pin_code': account.PINCODE.string.strip() if account.PINCODE else None, 'mobile': account.LEDGERPHONE.string.strip() if account.LEDGERPHONE else None, 'phone': account.LEDGERPHONE.string.strip() if account.LEDGERPHONE else None, 'gstin': account.PARTYGSTIN.string.strip() if account.PARTYGSTIN else None, 'links': links})
        return (parties, addresses)

    def get_stock_items_uoms(collection):
        uoms = []
        for uom in collection.find_all('UNIT'):
            uoms.append({'doctype': 'UOM', 'uom_name': uom.NAME.string.strip()})
        items = []
        for item in collection.find_all('STOCKITEM'):
            stock_uom = item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom
            items.append({'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': stock_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]})
        return (items, uoms)
    try:
        self.publish('Process Master Data', _('Reading Uploaded File'), 1, 5)
        collection = self.get_collection(self.master_data)
        company = get_company_name(collection)
        self.tally_company = company
        self.erpnext_company = company
        self.publish('Process Master Data', _('Processing Chart of Accounts and Parties'), 2, 5)
        (chart_of_accounts, customers, suppliers) = get_coa_customers_suppliers(collection)
        self.publish('Process Master Data', _('Processing Party Addresses'), 3, 5)
        (parties, addresses) = get_parties_addresses(collection, customers, suppliers)
        self.publish('Process Master Data', _('Processing Items and UOMs'), 4, 5)
        (items, uoms) = get_stock_items_uoms(collection)
        data = {'chart_of_accounts': chart_of_accounts, 'parties': parties, 'addresses': addresses, 'items': items, 'uoms': uoms}
        self.publish('Process Master Data', _('Done'), 5, 5)
        self.dump_processed_data(data)
        self.is_master_data_processed = 1
    except Exception:
        self.publish('Process Master Data', _('Process Failed'), -1, 5)
        self.log()
    finally:
        self.set_status()","for item in collection.find_all('STOCKITEM'):
    stock_uom = item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom
    items.append({'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': stock_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]})","items = [{'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]} for item in collection.find_all('STOCKITEM')]",Cannot refactor,-1,1,,,,robosuite
nanodet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nanodet/nanodet/data/dataset/xml_dataset.py,https://github.com/RangiLyu/nanodet/tree/master/nanodet/data/dataset/xml_dataset.py,XMLDataset,xml_to_coco$61,"def xml_to_coco(self, ann_path):
    """"""
        convert xml annotations to coco_api
        :param ann_path:
        :return:
        """"""
    logging.info('loading annotations into memory...')
    tic = time.time()
    ann_file_names = get_file_list(ann_path, type='.xml')
    logging.info('Found {} annotation files.'.format(len(ann_file_names)))
    image_info = []
    categories = []
    annotations = []
    for (idx, supercat) in enumerate(self.class_names):
        categories.append({'supercategory': supercat, 'id': idx + 1, 'name': supercat})
    ann_id = 1
    for (idx, xml_name) in enumerate(ann_file_names):
        tree = ET.parse(os.path.join(ann_path, xml_name))
        root = tree.getroot()
        file_name = root.find('filename').text
        width = int(root.find('size').find('width').text)
        height = int(root.find('size').find('height').text)
        info = {'file_name': file_name, 'height': height, 'width': width, 'id': idx + 1}
        image_info.append(info)
        for _object in root.findall('object'):
            category = _object.find('name').text
            if category not in self.class_names:
                logging.warning('WARNING! {} is not in class_names! Pass this box annotation.'.format(category))
                continue
            for cat in categories:
                if category == cat['name']:
                    cat_id = cat['id']
            xmin = int(_object.find('bndbox').find('xmin').text)
            ymin = int(_object.find('bndbox').find('ymin').text)
            xmax = int(_object.find('bndbox').find('xmax').text)
            ymax = int(_object.find('bndbox').find('ymax').text)
            w = xmax - xmin
            h = ymax - ymin
            if w < 0 or h < 0:
                logging.warning('WARNING! Find error data in file {}! Box w and h should > 0. Pass this box annotation.'.format(xml_name))
                continue
            coco_box = [max(xmin, 0), max(ymin, 0), min(w, width), min(h, height)]
            ann = {'image_id': idx + 1, 'bbox': coco_box, 'category_id': cat_id, 'iscrowd': 0, 'id': ann_id, 'area': coco_box[2] * coco_box[3]}
            annotations.append(ann)
            ann_id += 1
    coco_dict = {'images': image_info, 'categories': categories, 'annotations': annotations}
    logging.info('Load {} xml files and {} boxes'.format(len(image_info), len(annotations)))
    logging.info('Done (t={:0.2f}s)'.format(time.time() - tic))
    return coco_dict","for (idx, supercat) in enumerate(self.class_names):
    categories.append({'supercategory': supercat, 'id': idx + 1, 'name': supercat})","categories = [{'supercategory': supercat, 'id': idx + 1, 'name': supercat} for (idx, supercat) in enumerate(self.class_names)]","categories = [{'supercategory': supercat, 'id': idx + 1, 'name': supercat} for (idx, supercat) in enumerate(self.class_names)]",1,,,,,robosuite
searx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/searx/searx_extra/update/update_osm_keys_tags.py,https://github.com/searx/searx/tree/master/searx_extra/update/update_osm_keys_tags.py,,optimize_data_lang$160,"def optimize_data_lang(translations):
    language_to_delete = []
    for language in translations:
        if '-' in language:
            base_language = language.split('-')[0]
            if translations.get(base_language) == translations.get(language):
                language_to_delete.append(language)
    for language in language_to_delete:
        del translations[language]
    language_to_delete = []
    value_en = translations.get('en')
    if value_en:
        for (language, value) in translations.items():
            if language != 'en' and value == value_en:
                language_to_delete.append(language)
    for language in language_to_delete:
        del translations[language]","for language in translations:
    if '-' in language:
        base_language = language.split('-')[0]
        if translations.get(base_language) == translations.get(language):
            language_to_delete.append(language)",language_to_delete = [language for language in translations if '-' in language and translations.get(language.split('-')[0]) == translations.get(language)],Cannot refactor,-1,1,,,,robosuite
searx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/searx/searx_extra/update/update_osm_keys_tags.py,https://github.com/searx/searx/tree/master/searx_extra/update/update_osm_keys_tags.py,,optimize_data_lang$160,"def optimize_data_lang(translations):
    language_to_delete = []
    for language in translations:
        if '-' in language:
            base_language = language.split('-')[0]
            if translations.get(base_language) == translations.get(language):
                language_to_delete.append(language)
    for language in language_to_delete:
        del translations[language]
    language_to_delete = []
    value_en = translations.get('en')
    if value_en:
        for (language, value) in translations.items():
            if language != 'en' and value == value_en:
                language_to_delete.append(language)
    for language in language_to_delete:
        del translations[language]","for (language, value) in translations.items():
    if language != 'en' and value == value_en:
        language_to_delete.append(language)","language_to_delete += [language for (language, value) in translations.items() if language != 'en' and value == value_en]","language_to_delete = [language for (language, value) in translations.items() if language != 'en' and value == value_en]",0,1,,,,robosuite
primerpython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/tools/tex_complex.py,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/tex_complex.py,TexComplex,add_annotation$145,"def add_annotation(self, targets=None, alignment='top', labels=None, angle=0, length=1, label_scale=0.67, gest_scale=1):
    gesture_series = []
    tex_bobj = self.tex_bobjects[targets[0]]
    for (j, target) in enumerate(targets[1]):
        bobjs = []
        path = tex_bobj.paths[target[0]]
        for i in range(target[1], target[2] + 1):
            try:
                bobjs.append(tex_bobj.imported_svg_data[path]['curves'][i])
            except:
                print(i)
                print(tex_bobj.imported_svg_data[path]['curves'])
                raise ()
        left_most = math.inf
        right_most = -math.inf
        y = 0
        for bobj in bobjs:
            cur = bobj.objects[0]
            for spline in cur.data.splines:
                for point in spline.bezier_points:
                    candidatex = cur.location[0] * cur.parent.scale[0] + cur.parent.location[0] * cur.parent.parent.scale[0] + point.co[0] * cur.scale[0]
                    if right_most < candidatex:
                        right_most = candidatex
                    if left_most > candidatex:
                        left_most = candidatex
                for point in spline.bezier_points:
                    candidatey = cur.location[1] * cur.parent.scale[1] + cur.parent.location[1] * cur.parent.parent.scale[1] + point.co[1] * cur.scale[1]
                    if alignment == 'top':
                        if y < candidatey:
                            y = candidatey
                    elif alignment == 'bottom':
                        if y > candidatey:
                            y = candidatey
        if isinstance(angle, (float, int)):
            this_angle = angle
        elif isinstance(angle, list):
            this_angle = angle[j]
        if len(target) > 3 and target[3] == None:
            if alignment == 'top':
                head = ((right_most + left_most) / 2 / gest_scale, y + length, 0)
                rot = (0, 0, 0)
            elif alignment == 'bottom':
                head = ((right_most + left_most) / 2 / gest_scale, y - length, 0)
                rot = (0, 0, math.pi)
            gesture_series.append({'type': None, 'points': {'location': head, 'rotation': rot}})
        elif len(target) > 3 and target[3] == 'bracket' or (len(target) == 3 and len(bobjs) > 1):
            if alignment == 'top':
                y += 0.2 * self.ref_obj.scale[1] * tex_bobj.ref_obj.scale[1]
                annotation_point = ((right_most + left_most) / 2 / gest_scale, y + length, 0)
                left_point = (left_most / gest_scale, y, 0)
                right_point = (right_most / gest_scale, y, 0)
            elif alignment == 'bottom':
                y -= 0.2 * self.ref_obj.scale[1] * tex_bobj.ref_obj.scale[1]
                annotation_point = ((right_most + left_most) / 2 / gest_scale, y - length, 0)
                left_point = [right_most / gest_scale, y, 0]
                right_point = [left_most / gest_scale, y, 0]
            gesture_series.append({'type': 'bracket', 'points': {'annotation_point': annotation_point, 'left_point': left_point, 'right_point': right_point}})
        elif len(target) > 3 and target[3] == 'arrow' or (len(target) == 3 and len(bobjs) == 1):
            if alignment == 'top':
                y += 0.3 * tex_bobj.ref_obj.scale[1]
                head = ((right_most + left_most) / 2 / gest_scale + math.tan(this_angle) * 0.4, y / gest_scale, 0)
                tail = ((right_most + left_most) / 2 / gest_scale + math.tan(this_angle) * length, (y + length) / gest_scale, 0)
            elif alignment == 'bottom':
                y -= 0.3 * tex_bobj.ref_obj.scale[1]
                head = ((right_most + left_most) / 2 / gest_scale - math.tan(this_angle) * 0.4, y / gest_scale, 0)
                tail = ((right_most + left_most) / 2 / gest_scale - math.tan(this_angle) * length, (y - length) / gest_scale, 0)
            gesture_series.append({'type': 'arrow', 'points': {'head': head, 'tail': tail}})
        else:
            raise Warning('Something is wrong with the gesture targets.')
    container = bobject.Bobject(name='annotation')
    gest = gesture.Gesture(gesture_series=gesture_series, color='color2', scale=gest_scale)
    container.add_subbobject(gest)
    tex_bobj.annotations.append([container, targets[1], alignment])
    self.annotations.append([container, targets[0]])
    t_bobj_count = 0
    for label in labels:
        t_bobj_count = max(len(label), t_bobj_count)
    for label in labels:
        while len(label) < t_bobj_count:
            label.append(None)
    t_bobjs = []
    for i in range(t_bobj_count):
        strings = []
        for label in labels:
            strings.append(label[i])
        t_bobj = tex_bobject.TexBobject(*strings, centered=True, color='color2')
        t_bobjs.append(t_bobj)
    if alignment == 'top':
        dy = (t_bobj_count / 2 + 1 / 2) * self.line_height
    if alignment == 'bottom':
        dy = t_bobj_count / 2 * self.line_height
    if alignment == 'top':
        for t_bobj in t_bobjs:
            if t_bobj.paths[0] == None:
                dy -= self.line_height
    label_text = TexComplex(*t_bobjs, multiline=True, centered=True, align_y='center', scale=label_scale, name='label', location=(0, dy, 0), rotation_euler=[0, 0, -gest.subbobjects[0].ref_obj.rotation_euler[2]])
    gest.subbobjects[0].add_subbobject(label_text)
    self.add_subbobject(container)","for i in range(t_bobj_count):
    strings = []
    for label in labels:
        strings.append(label[i])
    t_bobj = tex_bobject.TexBobject(*strings, centered=True, color='color2')
    t_bobjs.append(t_bobj)","t_bobjs = [tex_bobject.TexBobject(*[label[i] for label in labels], centered=True, color='color2') for i in range(t_bobj_count)]",Cannot refactor,-1,1,,,,robosuite
primerpython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/tools/tex_complex.py,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/tex_complex.py,TexComplex,add_annotation$145,"def add_annotation(self, targets=None, alignment='top', labels=None, angle=0, length=1, label_scale=0.67, gest_scale=1):
    gesture_series = []
    tex_bobj = self.tex_bobjects[targets[0]]
    for (j, target) in enumerate(targets[1]):
        bobjs = []
        path = tex_bobj.paths[target[0]]
        for i in range(target[1], target[2] + 1):
            try:
                bobjs.append(tex_bobj.imported_svg_data[path]['curves'][i])
            except:
                print(i)
                print(tex_bobj.imported_svg_data[path]['curves'])
                raise ()
        left_most = math.inf
        right_most = -math.inf
        y = 0
        for bobj in bobjs:
            cur = bobj.objects[0]
            for spline in cur.data.splines:
                for point in spline.bezier_points:
                    candidatex = cur.location[0] * cur.parent.scale[0] + cur.parent.location[0] * cur.parent.parent.scale[0] + point.co[0] * cur.scale[0]
                    if right_most < candidatex:
                        right_most = candidatex
                    if left_most > candidatex:
                        left_most = candidatex
                for point in spline.bezier_points:
                    candidatey = cur.location[1] * cur.parent.scale[1] + cur.parent.location[1] * cur.parent.parent.scale[1] + point.co[1] * cur.scale[1]
                    if alignment == 'top':
                        if y < candidatey:
                            y = candidatey
                    elif alignment == 'bottom':
                        if y > candidatey:
                            y = candidatey
        if isinstance(angle, (float, int)):
            this_angle = angle
        elif isinstance(angle, list):
            this_angle = angle[j]
        if len(target) > 3 and target[3] == None:
            if alignment == 'top':
                head = ((right_most + left_most) / 2 / gest_scale, y + length, 0)
                rot = (0, 0, 0)
            elif alignment == 'bottom':
                head = ((right_most + left_most) / 2 / gest_scale, y - length, 0)
                rot = (0, 0, math.pi)
            gesture_series.append({'type': None, 'points': {'location': head, 'rotation': rot}})
        elif len(target) > 3 and target[3] == 'bracket' or (len(target) == 3 and len(bobjs) > 1):
            if alignment == 'top':
                y += 0.2 * self.ref_obj.scale[1] * tex_bobj.ref_obj.scale[1]
                annotation_point = ((right_most + left_most) / 2 / gest_scale, y + length, 0)
                left_point = (left_most / gest_scale, y, 0)
                right_point = (right_most / gest_scale, y, 0)
            elif alignment == 'bottom':
                y -= 0.2 * self.ref_obj.scale[1] * tex_bobj.ref_obj.scale[1]
                annotation_point = ((right_most + left_most) / 2 / gest_scale, y - length, 0)
                left_point = [right_most / gest_scale, y, 0]
                right_point = [left_most / gest_scale, y, 0]
            gesture_series.append({'type': 'bracket', 'points': {'annotation_point': annotation_point, 'left_point': left_point, 'right_point': right_point}})
        elif len(target) > 3 and target[3] == 'arrow' or (len(target) == 3 and len(bobjs) == 1):
            if alignment == 'top':
                y += 0.3 * tex_bobj.ref_obj.scale[1]
                head = ((right_most + left_most) / 2 / gest_scale + math.tan(this_angle) * 0.4, y / gest_scale, 0)
                tail = ((right_most + left_most) / 2 / gest_scale + math.tan(this_angle) * length, (y + length) / gest_scale, 0)
            elif alignment == 'bottom':
                y -= 0.3 * tex_bobj.ref_obj.scale[1]
                head = ((right_most + left_most) / 2 / gest_scale - math.tan(this_angle) * 0.4, y / gest_scale, 0)
                tail = ((right_most + left_most) / 2 / gest_scale - math.tan(this_angle) * length, (y - length) / gest_scale, 0)
            gesture_series.append({'type': 'arrow', 'points': {'head': head, 'tail': tail}})
        else:
            raise Warning('Something is wrong with the gesture targets.')
    container = bobject.Bobject(name='annotation')
    gest = gesture.Gesture(gesture_series=gesture_series, color='color2', scale=gest_scale)
    container.add_subbobject(gest)
    tex_bobj.annotations.append([container, targets[1], alignment])
    self.annotations.append([container, targets[0]])
    t_bobj_count = 0
    for label in labels:
        t_bobj_count = max(len(label), t_bobj_count)
    for label in labels:
        while len(label) < t_bobj_count:
            label.append(None)
    t_bobjs = []
    for i in range(t_bobj_count):
        strings = []
        for label in labels:
            strings.append(label[i])
        t_bobj = tex_bobject.TexBobject(*strings, centered=True, color='color2')
        t_bobjs.append(t_bobj)
    if alignment == 'top':
        dy = (t_bobj_count / 2 + 1 / 2) * self.line_height
    if alignment == 'bottom':
        dy = t_bobj_count / 2 * self.line_height
    if alignment == 'top':
        for t_bobj in t_bobjs:
            if t_bobj.paths[0] == None:
                dy -= self.line_height
    label_text = TexComplex(*t_bobjs, multiline=True, centered=True, align_y='center', scale=label_scale, name='label', location=(0, dy, 0), rotation_euler=[0, 0, -gest.subbobjects[0].ref_obj.rotation_euler[2]])
    gest.subbobjects[0].add_subbobject(label_text)
    self.add_subbobject(container)","for label in labels:
    strings.append(label[i])",strings = [label[i] for label in labels],strings = [label[i] for label in labels],1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/toolbox/views/sources.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/toolbox/views/sources.py,StaticToolBoxViewSources,get_definitions$23,"def get_definitions(self) -> List[StaticToolBoxView]:
    view_definitions = []
    for view_dict in self.view_dicts:
        view_definitions.append(StaticToolBoxView.from_dict(view_dict))
    for view_directory in self.view_directories:
        if not os.path.exists(view_directory):
            log.warning(f'Failed to find toolbox view directory {view_directory}')
        for filename in os.listdir(view_directory):
            if not looks_like_view_source_filename(filename):
                continue
            view_path = os.path.join(view_directory, filename)
            with open(view_path) as f:
                view_dict = yaml.safe_load(f)
            if 'id' not in view_dict:
                file_id = os.path.splitext(filename)[0]
                view_dict['id'] = file_id
            view_definitions.append(StaticToolBoxView.from_dict(view_dict))
    return view_definitions","for view_dict in self.view_dicts:
    view_definitions.append(StaticToolBoxView.from_dict(view_dict))",view_definitions = [StaticToolBoxView.from_dict(view_dict) for view_dict in self.view_dicts],view_definitions = [StaticToolBoxView.from_dict(view_dict) for view_dict in self.view_dicts],1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/toolbox/views/sources.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/toolbox/views/sources.py,StaticToolBoxViewSources,get_definitions$23,"def get_definitions(self) -> List[StaticToolBoxView]:
    view_definitions = []
    for view_dict in self.view_dicts:
        view_definitions.append(StaticToolBoxView.from_dict(view_dict))
    for view_directory in self.view_directories:
        if not os.path.exists(view_directory):
            log.warning(f'Failed to find toolbox view directory {view_directory}')
        for filename in os.listdir(view_directory):
            if not looks_like_view_source_filename(filename):
                continue
            view_path = os.path.join(view_directory, filename)
            with open(view_path) as f:
                view_dict = yaml.safe_load(f)
            if 'id' not in view_dict:
                file_id = os.path.splitext(filename)[0]
                view_dict['id'] = file_id
            view_definitions.append(StaticToolBoxView.from_dict(view_dict))
    return view_definitions","for view_directory in self.view_directories:
    if not os.path.exists(view_directory):
        log.warning(f'Failed to find toolbox view directory {view_directory}')
    for filename in os.listdir(view_directory):
        if not looks_like_view_source_filename(filename):
            continue
        view_path = os.path.join(view_directory, filename)
        with open(view_path) as f:
            view_dict = yaml.safe_load(f)
        if 'id' not in view_dict:
            file_id = os.path.splitext(filename)[0]
            view_dict['id'] = file_id
        view_definitions.append(StaticToolBoxView.from_dict(view_dict))","view_definitions += [StaticToolBoxView.from_dict(view_dict) for view_directory in self.view_directories if os.path.exists(view_directory) for filename in os.listdir(view_directory) if looks_like_view_source_filename(filename) for view_path in [os.path.join(view_directory, filename)] for file_id in [os.path.splitext(filename)[0]] for view_dict in [{'id': file_id, **yaml.safe_load(open(view_path))} if 'id' not in yaml.safe_load(open(view_path)) else yaml.safe_load(open(view_path))]]",Cannot refactor,-1,0,,,,robosuite
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/tests/unit/scheduler/test_filters.py,https://github.com/openstack/nova/tree/master/nova/tests/unit/scheduler/test_filters.py,FiltersTestCase,test_filter_all_recursive_yields$59,"def test_filter_all_recursive_yields(self, mock_filter_one):
    filter_obj_list = ['obj1', 'obj2', 'obj3']
    spec_obj = objects.RequestSpec()
    base_filter = filters.BaseFilter()
    mock_results = []
    total_iterations = 200
    for x in range(total_iterations):
        mock_results.append(True)
    mock_results.append(False)
    for x in range(total_iterations):
        mock_results.append(True)
    mock_filter_one.side_effect = mock_results
    objs = iter(filter_obj_list)
    for x in range(total_iterations):
        objs = base_filter.filter_all(objs, spec_obj)
    self.assertTrue(inspect.isgenerator(objs))
    self.assertEqual(['obj1', 'obj3'], list(objs))","for x in range(total_iterations):
    mock_results.append(True)",mock_results = [True for x in range(total_iterations)],mock_results = [True for x in range(total_iterations)],1,,,,,robosuite
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/tests/unit/scheduler/test_filters.py,https://github.com/openstack/nova/tree/master/nova/tests/unit/scheduler/test_filters.py,FiltersTestCase,test_filter_all_recursive_yields$59,"def test_filter_all_recursive_yields(self, mock_filter_one):
    filter_obj_list = ['obj1', 'obj2', 'obj3']
    spec_obj = objects.RequestSpec()
    base_filter = filters.BaseFilter()
    mock_results = []
    total_iterations = 200
    for x in range(total_iterations):
        mock_results.append(True)
    mock_results.append(False)
    for x in range(total_iterations):
        mock_results.append(True)
    mock_filter_one.side_effect = mock_results
    objs = iter(filter_obj_list)
    for x in range(total_iterations):
        objs = base_filter.filter_all(objs, spec_obj)
    self.assertTrue(inspect.isgenerator(objs))
    self.assertEqual(['obj1', 'obj3'], list(objs))","for x in range(total_iterations):
    mock_results.append(True)",mock_results += [True for x in range(total_iterations)],Cannot refactor,-1,1,,,,robosuite
SeleniumLibrary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SeleniumLibrary/utest/test/keywords/test_javascript.py,https://github.com/robotframework/SeleniumLibrary/tree/master/utest/test/keywords/test_javascript.py,JavaScriptKeywordsTest,test_indexing$66,"def test_indexing(self):
    all_results = []
    for code in self.code_examples:
        all_results.append(self.js._get_marker_index(code))
    verify_all('index', all_results, reporter=self.reporter)","for code in self.code_examples:
    all_results.append(self.js._get_marker_index(code))",all_results = [self.js._get_marker_index(code) for code in self.code_examples],all_results = [self.js._get_marker_index(code) for code in self.code_examples],1,,,,,robosuite
Pyrebase,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyrebase/pyrebase/pyrebase.py,https://github.com/thisbejim/Pyrebase/tree/master/pyrebase/pyrebase.py,Database,sort$352,"def sort(self, origin, by_key):
    pyres = origin.each()
    new_list = []
    for pyre in pyres:
        new_list.append(pyre.item)
    data = sorted(dict(new_list).items(), key=lambda item: item[1][by_key])
    return PyreResponse(convert_to_pyre(data), origin.key())","for pyre in pyres:
    new_list.append(pyre.item)",new_list = [pyre.item for pyre in pyres],new_list = [pyre.item for pyre in pyres],1,,,,,robosuite
python-driver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-driver/cassandra/cqlengine/query.py,https://github.com/datastax/python-driver/tree/master/cassandra/cqlengine/query.py,AbstractQuerySet,order_by$787,"def order_by(self, *colnames):
    """"""
        Sets the column(s) to be used for ordering

        Default order is ascending, prepend a '-' to any column name for descending

        *Note: column names must be a clustering key*

        .. code-block:: python

            from uuid import uuid1,uuid4

            class Comment(Model):
                photo_id = UUID(primary_key=True)
                comment_id = TimeUUID(primary_key=True, default=uuid1) # second primary key component is a clustering key
                comment = Text()

            sync_table(Comment)

            u = uuid4()
            for x in range(5):
                Comment.create(photo_id=u, comment=""test %d"" % x)

            print(""Normal"")
            for comment in Comment.objects(photo_id=u):
                print comment.comment_id

            print(""Reversed"")
            for comment in Comment.objects(photo_id=u).order_by(""-comment_id""):
                print comment.comment_id
        """"""
    if len(colnames) == 0:
        clone = copy.deepcopy(self)
        clone._order = []
        return clone
    conditions = []
    for colname in colnames:
        conditions.append('""{0}"" {1}'.format(*self._get_ordering_condition(colname)))
    clone = copy.deepcopy(self)
    clone._order.extend(conditions)
    return clone","for colname in colnames:
    conditions.append('""{0}"" {1}'.format(*self._get_ordering_condition(colname)))",conditions = ['"{0}" {1}'.format(*self._get_ordering_condition(colname)) for colname in colnames],conditions = ['"{0}" {1}'.format(*self._get_ordering_condition(colname)) for colname in colnames],1,,,,,robosuite
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/openedx/core/djangoapps/xblock/runtime/blockstore_field_data.py,https://github.com/edx/edx-platform/tree/master/openedx/core/djangoapps/xblock/runtime/blockstore_field_data.py,BlockstoreChildrenData,get$291,"def get(self, block, name):
    """"""
        Get the ""children' field value.

        We do this by reading the parsed <xblock-include /> values from
        the regular authored data store and then transforming them to usage IDs.
        """"""
    self._check_field(block, name)
    children_includes = self.get_includes(block)
    if not children_includes:
        return []
    learning_context = get_learning_context_impl(block.scope_ids.usage_id)
    child_usages = []
    for parsed_include in children_includes:
        child_usages.append(learning_context.usage_for_child_include(block.scope_ids.usage_id, block.scope_ids.def_id, parsed_include))
    return child_usages","for parsed_include in children_includes:
    child_usages.append(learning_context.usage_for_child_include(block.scope_ids.usage_id, block.scope_ids.def_id, parsed_include))","child_usages = [learning_context.usage_for_child_include(block.scope_ids.usage_id, block.scope_ids.def_id, parsed_include) for parsed_include in children_includes]","child_usages = [learning_context.usage_for_child_include(block.scope_ids.usage_id, block.scope_ids.def_id, parsed_include) for parsed_include in children_includes]",1,,,,,robosuite
apex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/apex/tests/L0/run_optimizers/test_fused_optimizer.py,https://github.com/NVIDIA/apex/tree/master/tests/L0/run_optimizers/test_fused_optimizer.py,TestFusedAdam,test_multi_params$111,"def test_multi_params(self):
    sizes = [[4096, 1024], [4096], [4096, 2048], [32320, 1024], [1]]
    tensors = []
    for size in sizes:
        tensors.append(torch.rand(size, dtype=torch.float, device='cuda'))
    (ref_param, tst_param, ref_optim, tst_optim) = self.gen_param_optim(tensors, self.options)
    for i in range(self.iters):
        self.gen_grad(ref_param, tst_param)
        ref_optim.step()
        tst_optim.step()
        (max_abs_diff, max_rel_diff) = self.get_max_diff(ref_param, tst_param)
        self.assertLessEqual(max_abs_diff, self.max_abs_diff)
        self.assertLessEqual(max_rel_diff, self.max_rel_diff)","for size in sizes:
    tensors.append(torch.rand(size, dtype=torch.float, device='cuda'))","tensors = [torch.rand(size, dtype=torch.float, device='cuda') for size in sizes]","tensors = [torch.rand(size, dtype=torch.float, device='cuda') for size in sizes]",1,,,,,robosuite
kivent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivent/examples/17_joints/main.py,https://github.com/kivy/kivent/tree/master/examples/17_joints/main.py,TestGame,create_roundrect2body$83,"def create_roundrect2body(self, pos):
    w = 100
    mass = 50
    col_shapes = []
    for x in (-w / 4, w / 4):
        for y in (-w / 4, w / 4):
            col_shapes.append({'shape_type': 'circle', 'elasticity': 0.0, 'collision_type': 1, 'shape_info': {'inner_radius': 0, 'outer_radius': w / 4, 'mass': mass / 4, 'offset': (x, y)}, 'friction': 1.0})
    physics_component = {'main_shape': 'circle', 'velocity': (0, 0), 'position': pos, 'angle': 0, 'angular_velocity': 0, 'vel_limit': 250, 'ang_vel_limit': radians(200), 'mass': mass, 'col_shapes': col_shapes}
    create_component_dict = {'cymunk_physics': physics_component, 'rotate_renderer': {'texture': 'roundrect', 'size': (w, w), 'render': True}, 'position': pos, 'rotate': 0}
    component_order = ['position', 'rotate', 'rotate_renderer', 'cymunk_physics']
    eid = self.gameworld.init_entity(create_component_dict, component_order)
    (posx, posy) = pos
    pos2 = (posx, posy - w)
    e2 = self.create_body(pos2, 20, 'redcircle', mass=100, collision_type=0)
    body1 = self.gameworld.entities[eid].cymunk_physics.body
    body2 = self.gameworld.entities[e2].cymunk_physics.body
    pivot = PivotJoint(body1, body2, pos2)
    self.gameworld.physics.space.add(pivot)","for x in (-w / 4, w / 4):
    for y in (-w / 4, w / 4):
        col_shapes.append({'shape_type': 'circle', 'elasticity': 0.0, 'collision_type': 1, 'shape_info': {'inner_radius': 0, 'outer_radius': w / 4, 'mass': mass / 4, 'offset': (x, y)}, 'friction': 1.0})","col_shapes = [{'shape_type': 'circle', 'elasticity': 0.0, 'collision_type': 1, 'shape_info': {'inner_radius': 0, 'outer_radius': w / 4, 'mass': mass / 4, 'offset': (x, y)}, 'friction': 1.0} for x in (-w / 4, w / 4) for y in (-w / 4, w / 4)]","col_shapes = [{'shape_type': 'circle', 'elasticity': 0.0, 'collision_type': 1, 'shape_info': {'inner_radius': 0, 'outer_radius': w / 4, 'mass': mass / 4, 'offset': (x, y)}, 'friction': 1.0} for x in (-w / 4, w / 4) for y in (-w / 4, w / 4)]",1,,,,,robosuite
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-ranks.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-ranks.py,,visualize_relative_info$37,"def visualize_relative_info(vis_save_dir, search_space, indicator, topk):
    vis_save_dir = vis_save_dir.resolve()
    print('{:} start to visualize {:} with top-{:} information'.format(time_string(), search_space, topk))
    vis_save_dir.mkdir(parents=True, exist_ok=True)
    cache_file_path = vis_save_dir / 'cache-{:}-info.pth'.format(search_space)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']
    if not cache_file_path.exists():
        api = create(None, search_space, fast_mode=False, verbose=False)
        all_infos = OrderedDict()
        for index in range(len(api)):
            all_info = OrderedDict()
            for dataset in datasets:
                info_less = api.get_more_info(index, dataset, hp='12', is_random=False)
                info_more = api.get_more_info(index, dataset, hp=api.full_train_epochs, is_random=False)
                all_info[dataset] = dict(less=info_less['test-accuracy'], more=info_more['test-accuracy'])
            all_infos[index] = all_info
        torch.save(all_infos, cache_file_path)
        print('{:} save all cache data into {:}'.format(time_string(), cache_file_path))
    else:
        api = create(None, search_space, fast_mode=True, verbose=False)
        all_infos = torch.load(cache_file_path)
    (dpi, width, height) = (250, 5000, 1300)
    figsize = (width / float(dpi), height / float(dpi))
    (LabelSize, LegendFontsize) = (16, 16)
    (fig, axs) = plt.subplots(1, 3, figsize=figsize)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']

    def sub_plot_fn(ax, dataset, indicator):
        performances = []
        for _index in range(len(api)):
            performances.append((all_infos[_index][dataset][indicator], _index))
        performances = sorted(performances, reverse=True)
        performances = performances[:int(len(api) * topk * 0.01)]
        selected_indexes = [x[1] for x in performances]
        print('{:} plot {:10s} with {:}, {:} architectures'.format(time_string(), dataset, indicator, len(selected_indexes)))
        standard_scores = []
        random_scores = []
        for idx in selected_indexes:
            standard_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'])
            random_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy'])
        indexes = list(range(len(selected_indexes)))
        standard_indexes = sorted(indexes, key=lambda i: standard_scores[i])
        random_indexes = sorted(indexes, key=lambda i: random_scores[i])
        random_labels = []
        for idx in standard_indexes:
            random_labels.append(random_indexes.index(idx))
        for tick in ax.get_xticklabels():
            tick.set_fontsize(LabelSize - 3)
        for tick in ax.get_yticklabels():
            tick.set_rotation(25)
            tick.set_fontsize(LabelSize - 3)
        ax.set_xlim(0, len(indexes))
        ax.set_ylim(0, len(indexes))
        ax.set_yticks(np.arange(min(indexes), max(indexes), max(indexes) // 3))
        ax.set_xticks(np.arange(min(indexes), max(indexes), max(indexes) // 5))
        ax.scatter(indexes, random_labels, marker='^', s=0.5, c='tab:green', alpha=0.8)
        ax.scatter(indexes, indexes, marker='o', s=0.5, c='tab:blue', alpha=0.8)
        ax.scatter([-1], [-1], marker='o', s=100, c='tab:blue', label='Average Over Multi-Trials')
        ax.scatter([-1], [-1], marker='^', s=100, c='tab:green', label='Randomly Selected Trial')
        (coef, p) = scipy.stats.kendalltau(standard_scores, random_scores)
        ax.set_xlabel('architecture ranking in {:}'.format(name2label[dataset]), fontsize=LabelSize)
        if dataset == 'cifar10':
            ax.set_ylabel('architecture ranking', fontsize=LabelSize)
        ax.legend(loc=4, fontsize=LegendFontsize)
        return coef
    for (dataset, ax) in zip(datasets, axs):
        rank_coef = sub_plot_fn(ax, dataset, indicator)
        print('sub-plot {:} on {:} done, the ranking coefficient is {:.4f}.'.format(dataset, search_space, rank_coef))
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.pdf'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='pdf')
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.png'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='png')
    print('Save into {:}'.format(save_path))","for _index in range(len(api)):
    performances.append((all_infos[_index][dataset][indicator], _index))","performances = [(all_infos[_index][dataset][indicator], _index) for _index in range(len(api))]","performances = [(all_infos[_index][dataset][indicator], _index) for _index in range(len(api))]",1,,,,,robosuite
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-ranks.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-ranks.py,,visualize_relative_info$37,"def visualize_relative_info(vis_save_dir, search_space, indicator, topk):
    vis_save_dir = vis_save_dir.resolve()
    print('{:} start to visualize {:} with top-{:} information'.format(time_string(), search_space, topk))
    vis_save_dir.mkdir(parents=True, exist_ok=True)
    cache_file_path = vis_save_dir / 'cache-{:}-info.pth'.format(search_space)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']
    if not cache_file_path.exists():
        api = create(None, search_space, fast_mode=False, verbose=False)
        all_infos = OrderedDict()
        for index in range(len(api)):
            all_info = OrderedDict()
            for dataset in datasets:
                info_less = api.get_more_info(index, dataset, hp='12', is_random=False)
                info_more = api.get_more_info(index, dataset, hp=api.full_train_epochs, is_random=False)
                all_info[dataset] = dict(less=info_less['test-accuracy'], more=info_more['test-accuracy'])
            all_infos[index] = all_info
        torch.save(all_infos, cache_file_path)
        print('{:} save all cache data into {:}'.format(time_string(), cache_file_path))
    else:
        api = create(None, search_space, fast_mode=True, verbose=False)
        all_infos = torch.load(cache_file_path)
    (dpi, width, height) = (250, 5000, 1300)
    figsize = (width / float(dpi), height / float(dpi))
    (LabelSize, LegendFontsize) = (16, 16)
    (fig, axs) = plt.subplots(1, 3, figsize=figsize)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']

    def sub_plot_fn(ax, dataset, indicator):
        performances = []
        for _index in range(len(api)):
            performances.append((all_infos[_index][dataset][indicator], _index))
        performances = sorted(performances, reverse=True)
        performances = performances[:int(len(api) * topk * 0.01)]
        selected_indexes = [x[1] for x in performances]
        print('{:} plot {:10s} with {:}, {:} architectures'.format(time_string(), dataset, indicator, len(selected_indexes)))
        standard_scores = []
        random_scores = []
        for idx in selected_indexes:
            standard_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'])
            random_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy'])
        indexes = list(range(len(selected_indexes)))
        standard_indexes = sorted(indexes, key=lambda i: standard_scores[i])
        random_indexes = sorted(indexes, key=lambda i: random_scores[i])
        random_labels = []
        for idx in standard_indexes:
            random_labels.append(random_indexes.index(idx))
        for tick in ax.get_xticklabels():
            tick.set_fontsize(LabelSize - 3)
        for tick in ax.get_yticklabels():
            tick.set_rotation(25)
            tick.set_fontsize(LabelSize - 3)
        ax.set_xlim(0, len(indexes))
        ax.set_ylim(0, len(indexes))
        ax.set_yticks(np.arange(min(indexes), max(indexes), max(indexes) // 3))
        ax.set_xticks(np.arange(min(indexes), max(indexes), max(indexes) // 5))
        ax.scatter(indexes, random_labels, marker='^', s=0.5, c='tab:green', alpha=0.8)
        ax.scatter(indexes, indexes, marker='o', s=0.5, c='tab:blue', alpha=0.8)
        ax.scatter([-1], [-1], marker='o', s=100, c='tab:blue', label='Average Over Multi-Trials')
        ax.scatter([-1], [-1], marker='^', s=100, c='tab:green', label='Randomly Selected Trial')
        (coef, p) = scipy.stats.kendalltau(standard_scores, random_scores)
        ax.set_xlabel('architecture ranking in {:}'.format(name2label[dataset]), fontsize=LabelSize)
        if dataset == 'cifar10':
            ax.set_ylabel('architecture ranking', fontsize=LabelSize)
        ax.legend(loc=4, fontsize=LegendFontsize)
        return coef
    for (dataset, ax) in zip(datasets, axs):
        rank_coef = sub_plot_fn(ax, dataset, indicator)
        print('sub-plot {:} on {:} done, the ranking coefficient is {:.4f}.'.format(dataset, search_space, rank_coef))
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.pdf'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='pdf')
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.png'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='png')
    print('Save into {:}'.format(save_path))","for idx in selected_indexes:
    standard_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'])
    random_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy'])","(standard_scores, random_scores) = [(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'], api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy']) for idx in selected_indexes]",Cannot refactor,-1,0,,2,1,robosuite
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-ranks.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-ranks.py,,visualize_relative_info$37,"def visualize_relative_info(vis_save_dir, search_space, indicator, topk):
    vis_save_dir = vis_save_dir.resolve()
    print('{:} start to visualize {:} with top-{:} information'.format(time_string(), search_space, topk))
    vis_save_dir.mkdir(parents=True, exist_ok=True)
    cache_file_path = vis_save_dir / 'cache-{:}-info.pth'.format(search_space)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']
    if not cache_file_path.exists():
        api = create(None, search_space, fast_mode=False, verbose=False)
        all_infos = OrderedDict()
        for index in range(len(api)):
            all_info = OrderedDict()
            for dataset in datasets:
                info_less = api.get_more_info(index, dataset, hp='12', is_random=False)
                info_more = api.get_more_info(index, dataset, hp=api.full_train_epochs, is_random=False)
                all_info[dataset] = dict(less=info_less['test-accuracy'], more=info_more['test-accuracy'])
            all_infos[index] = all_info
        torch.save(all_infos, cache_file_path)
        print('{:} save all cache data into {:}'.format(time_string(), cache_file_path))
    else:
        api = create(None, search_space, fast_mode=True, verbose=False)
        all_infos = torch.load(cache_file_path)
    (dpi, width, height) = (250, 5000, 1300)
    figsize = (width / float(dpi), height / float(dpi))
    (LabelSize, LegendFontsize) = (16, 16)
    (fig, axs) = plt.subplots(1, 3, figsize=figsize)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']

    def sub_plot_fn(ax, dataset, indicator):
        performances = []
        for _index in range(len(api)):
            performances.append((all_infos[_index][dataset][indicator], _index))
        performances = sorted(performances, reverse=True)
        performances = performances[:int(len(api) * topk * 0.01)]
        selected_indexes = [x[1] for x in performances]
        print('{:} plot {:10s} with {:}, {:} architectures'.format(time_string(), dataset, indicator, len(selected_indexes)))
        standard_scores = []
        random_scores = []
        for idx in selected_indexes:
            standard_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'])
            random_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy'])
        indexes = list(range(len(selected_indexes)))
        standard_indexes = sorted(indexes, key=lambda i: standard_scores[i])
        random_indexes = sorted(indexes, key=lambda i: random_scores[i])
        random_labels = []
        for idx in standard_indexes:
            random_labels.append(random_indexes.index(idx))
        for tick in ax.get_xticklabels():
            tick.set_fontsize(LabelSize - 3)
        for tick in ax.get_yticklabels():
            tick.set_rotation(25)
            tick.set_fontsize(LabelSize - 3)
        ax.set_xlim(0, len(indexes))
        ax.set_ylim(0, len(indexes))
        ax.set_yticks(np.arange(min(indexes), max(indexes), max(indexes) // 3))
        ax.set_xticks(np.arange(min(indexes), max(indexes), max(indexes) // 5))
        ax.scatter(indexes, random_labels, marker='^', s=0.5, c='tab:green', alpha=0.8)
        ax.scatter(indexes, indexes, marker='o', s=0.5, c='tab:blue', alpha=0.8)
        ax.scatter([-1], [-1], marker='o', s=100, c='tab:blue', label='Average Over Multi-Trials')
        ax.scatter([-1], [-1], marker='^', s=100, c='tab:green', label='Randomly Selected Trial')
        (coef, p) = scipy.stats.kendalltau(standard_scores, random_scores)
        ax.set_xlabel('architecture ranking in {:}'.format(name2label[dataset]), fontsize=LabelSize)
        if dataset == 'cifar10':
            ax.set_ylabel('architecture ranking', fontsize=LabelSize)
        ax.legend(loc=4, fontsize=LegendFontsize)
        return coef
    for (dataset, ax) in zip(datasets, axs):
        rank_coef = sub_plot_fn(ax, dataset, indicator)
        print('sub-plot {:} on {:} done, the ranking coefficient is {:.4f}.'.format(dataset, search_space, rank_coef))
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.pdf'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='pdf')
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.png'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='png')
    print('Save into {:}'.format(save_path))","for idx in standard_indexes:
    random_labels.append(random_indexes.index(idx))",random_labels = [random_indexes.index(idx) for idx in standard_indexes],random_labels = [random_indexes.index(idx) for idx in standard_indexes],1,,,,,robosuite
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/dataimport/fits_hdf5_helpers.py,https://github.com/veusz/veusz/tree/master/veusz/dataimport/fits_hdf5_helpers.py,,convertSliceToText$96,"def convertSliceToText(slice):
    """"""Convert tuple slice into text.""""""
    if slice is None:
        return ''
    out = []
    for spart in slice:
        if isinstance(spart, int):
            out.append(str(spart))
            continue
        sparttxt = []
        for p in spart:
            if p is not None:
                sparttxt.append(str(p))
            else:
                sparttxt.append('')
        if sparttxt[-1] == '':
            del sparttxt[-1]
        out.append(':'.join(sparttxt))
    return ', '.join(out)","for spart in slice:
    if isinstance(spart, int):
        out.append(str(spart))
        continue
    sparttxt = []
    for p in spart:
        if p is not None:
            sparttxt.append(str(p))
        else:
            sparttxt.append('')
    if sparttxt[-1] == '':
        del sparttxt[-1]
    out.append(':'.join(sparttxt))","out = [str(spart) if isinstance(spart, int) else ':'.join([str(p) if p is not None else '' for p in spart][:-1]) for spart in slice]",Cannot refactor,-1,0,,2,,robosuite
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/dataimport/fits_hdf5_helpers.py,https://github.com/veusz/veusz/tree/master/veusz/dataimport/fits_hdf5_helpers.py,,convertSliceToText$96,"def convertSliceToText(slice):
    """"""Convert tuple slice into text.""""""
    if slice is None:
        return ''
    out = []
    for spart in slice:
        if isinstance(spart, int):
            out.append(str(spart))
            continue
        sparttxt = []
        for p in spart:
            if p is not None:
                sparttxt.append(str(p))
            else:
                sparttxt.append('')
        if sparttxt[-1] == '':
            del sparttxt[-1]
        out.append(':'.join(sparttxt))
    return ', '.join(out)","for p in spart:
    if p is not None:
        sparttxt.append(str(p))
    else:
        sparttxt.append('')",sparttxt = [str(p) if p is not None else '' for p in spart],sparttxt = [str(p) if p is not None else '' for p in spart],1,,,,,robosuite
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/ctc/corpus/libriphone.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/ctc/corpus/libriphone.py,LibriPhoneDataset,__init__$35,"def __init__(self, split, tokenizer, bucket_size, path, lexicon, ascending=False, **kwargs):
    self.path = path
    self.bucket_size = bucket_size
    word2phonemes_all = defaultdict(list)
    for lexicon_file in lexicon:
        with open(lexicon_file, 'r') as file:
            lines = [line.strip() for line in file.readlines()]
            for line in lines:
                (word, phonemes) = parse_lexicon(line, tokenizer)
                word2phonemes_all[word].append(phonemes)
    word2phonemes = {}
    for (word, phonemes_all) in word2phonemes_all.items():
        if len(phonemes_all) > 1:
            print(f'[LibriPhone] - {len(phonemes_all)} of phoneme sequences found for {word}.')
            for (idx, phonemes) in enumerate(phonemes_all):
                print(f'{idx}. {phonemes}')
        word2phonemes[word] = phonemes_all[0]
    print(f'[LibriPhone] - Taking the first phoneme sequences for a deterministic behavior.')
    file_list = []
    for s in split:
        split_list = list(Path(join(path, s)).rglob('*.flac'))
        assert len(split_list) > 0, 'No data found @ {}'.format(join(path, s))
        file_list += split_list
    text = []
    for f in tqdm(file_list, desc='word -> phonemes'):
        text.append(read_text(str(f), word2phonemes, tokenizer))
    (self.file_list, self.text) = zip(*[(f_name, txt) for (f_name, txt) in sorted(zip(file_list, text), reverse=not ascending, key=lambda x: len(x[1]))])","for lexicon_file in lexicon:
    with open(lexicon_file, 'r') as file:
        lines = [line.strip() for line in file.readlines()]
        for line in lines:
            (word, phonemes) = parse_lexicon(line, tokenizer)
            word2phonemes_all[word].append(phonemes)","word2phonemes_all[word] = [phonemes for lexicon_file in lexicon for line in [line.strip() for line in open(lexicon_file, 'r').readlines()] for (word, phonemes) in [parse_lexicon(line, tokenizer)]]",Cannot refactor,-1,0,,,,robosuite
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/ctc/corpus/libriphone.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/ctc/corpus/libriphone.py,LibriPhoneDataset,__init__$35,"def __init__(self, split, tokenizer, bucket_size, path, lexicon, ascending=False, **kwargs):
    self.path = path
    self.bucket_size = bucket_size
    word2phonemes_all = defaultdict(list)
    for lexicon_file in lexicon:
        with open(lexicon_file, 'r') as file:
            lines = [line.strip() for line in file.readlines()]
            for line in lines:
                (word, phonemes) = parse_lexicon(line, tokenizer)
                word2phonemes_all[word].append(phonemes)
    word2phonemes = {}
    for (word, phonemes_all) in word2phonemes_all.items():
        if len(phonemes_all) > 1:
            print(f'[LibriPhone] - {len(phonemes_all)} of phoneme sequences found for {word}.')
            for (idx, phonemes) in enumerate(phonemes_all):
                print(f'{idx}. {phonemes}')
        word2phonemes[word] = phonemes_all[0]
    print(f'[LibriPhone] - Taking the first phoneme sequences for a deterministic behavior.')
    file_list = []
    for s in split:
        split_list = list(Path(join(path, s)).rglob('*.flac'))
        assert len(split_list) > 0, 'No data found @ {}'.format(join(path, s))
        file_list += split_list
    text = []
    for f in tqdm(file_list, desc='word -> phonemes'):
        text.append(read_text(str(f), word2phonemes, tokenizer))
    (self.file_list, self.text) = zip(*[(f_name, txt) for (f_name, txt) in sorted(zip(file_list, text), reverse=not ascending, key=lambda x: len(x[1]))])","for f in tqdm(file_list, desc='word -> phonemes'):
    text.append(read_text(str(f), word2phonemes, tokenizer))","text = [read_text(str(f), word2phonemes, tokenizer) for f in tqdm(file_list, desc='word -> phonemes')]","text = [read_text(str(f), word2phonemes, tokenizer) for f in tqdm(file_list, desc='word -> phonemes')]",1,,,,,robosuite
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/server/src/search.py,https://github.com/nlplab/brat/tree/master/server/src/search.py,,search_anns_for_event$938,"def search_anns_for_event(ann_objs, trigger_text, args, restrict_types=None, ignore_types=None, text_match='word', match_case=False):
    """"""Searches the given Annotations objects for Event annotations matching
    the given specification.

    Returns a SearchMatchSet object.
    """"""
    global REPORT_SEARCH_TIMINGS
    if REPORT_SEARCH_TIMINGS:
        process_start = datetime.now()
    restrict_types = [] if restrict_types is None else restrict_types
    ignore_types = [] if ignore_types is None else ignore_types
    description = ""Event triggered by text containing '%s'"" % trigger_text
    if restrict_types != []:
        description = description + ' (of type %s)' % ','.join(restrict_types)
    matches = SearchMatchSet(description)
    if trigger_text is not None:
        trigger_match_regex = _get_match_regex(trigger_text, text_match, match_case)
        if trigger_match_regex is None:
            return matches
    for ann_obj in ann_objs:
        ann_matches = []
        for e in ann_obj.get_events():
            if e.type in ignore_types:
                continue
            if restrict_types != [] and e.type not in restrict_types:
                continue
            try:
                t_ann = ann_obj.get_ann_by_id(e.trigger)
            except BaseException:
                Messager.error('Failed to retrieve trigger annotation %s, skipping event %s in search' % (e.trigger, e.id))
            if trigger_text is not None and trigger_text != '' and (trigger_text != DEFAULT_EMPTY_STRING) and (not trigger_match_regex.search(t_ann.text)):
                continue
            arg_constraints = []
            for arg in args:
                if arg['role'] != '' or arg['type'] != '' or arg['text'] != '':
                    arg_constraints.append(arg)
            args = arg_constraints
            if len(args) > 0:
                missing_match = False
                for arg in args:
                    for s in ('role', 'type', 'text'):
                        assert s in arg, ""Error: missing mandatory field '%s' in event search"" % s
                    found_match = False
                    for (role, aid) in e.args:
                        if arg['role'] is not None and arg['role'] != '' and (arg['role'] != role):
                            continue
                        arg_ent = ann_obj.get_ann_by_id(aid)
                        if arg['type'] is not None and arg['type'] != '' and (arg['type'] != arg_ent.type):
                            continue
                        if arg['text'] is not None and arg['text'] != '':
                            match_regex = _get_match_regex(arg['text'], text_match, match_case)
                            if match_regex is None:
                                return matches
                            if isinstance(arg_ent, annotation.EventAnnotation):
                                text_ent = ann_obj.get_ann_by_id(ann_ent.trigger)
                            else:
                                text_ent = arg_ent
                            if not match_regex.search(text_ent.get_text()):
                                continue
                        found_match = True
                        break
                    if not found_match:
                        missing_match = True
                        break
                if missing_match:
                    continue
            ann_matches.append((t_ann, e))
        ann_matches.sort(key=lambda a: (a[0].first_start(), -a[0].last_end()))
        for (t_obj, e) in ann_matches:
            matches.add_match(ann_obj, e)
        if len(matches) > MAX_SEARCH_RESULT_NUMBER and MAX_SEARCH_RESULT_NUMBER > 0:
            Messager.warning('Search result limit (%d) exceeded, stopping search.' % MAX_SEARCH_RESULT_NUMBER)
            break
    matches.limit_to(MAX_SEARCH_RESULT_NUMBER)
    matches.sort_matches()
    if REPORT_SEARCH_TIMINGS:
        process_delta = datetime.now() - process_start
        print('search_anns_for_event: processed in', str(process_delta.seconds) + '.' + str(process_delta.microseconds / 10000), 'seconds', file=stderr)
    return matches","for arg in args:
    if arg['role'] != '' or arg['type'] != '' or arg['text'] != '':
        arg_constraints.append(arg)",arg_constraints = [arg for arg in args if arg['role'] != '' or arg['type'] != '' or arg['text'] != ''],arg_constraints = [arg for arg in args if arg['role'] != '' or arg['type'] != '' or arg['text'] != ''],1,,,,,robosuite
stumpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stumpy/stumpy/aamped.py,https://github.com/TDAmeritrade/stumpy/tree/master/stumpy/aamped.py,,aamped$15,"def aamped(dask_client, T_A, m, T_B=None, ignore_trivial=True):
    """"""
    Compute the non-normalized (i.e., without z-normalization) matrix profile

    This is a highly distributed implementation around the Numba JIT-compiled
    parallelized `_aamp` function which computes the non-normalized matrix profile
    according to AAMP.

    Parameters
    ----------
    dask_client : client
        A Dask Distributed client that is connected to a Dask scheduler and
        Dask workers. Setting up a Dask distributed cluster is beyond the
        scope of this library. Please refer to the Dask Distributed
        documentation.

    T_A : numpy.ndarray
        The time series or sequence for which to compute the matrix profile

    m : int
        Window size

    T_B : numpy.ndarray, default None
        The time series or sequence that will be used to annotate T_A. For every
        subsequence in T_A, its nearest neighbor in T_B will be recorded. Default is
        `None` which corresponds to a self-join.

    ignore_trivial : bool, default True
        Set to `True` if this is a self-join. Otherwise, for AB-join, set this
        to `False`. Default is `True`.

    Returns
    -------
    out : numpy.ndarray
        The first column consists of the matrix profile, the second column
        consists of the matrix profile indices.

    Notes
    -----
    `arXiv:1901.05708     <https://arxiv.org/pdf/1901.05708.pdf>`__

    See Algorithm 1

    Note that we have extended this algorithm for AB-joins as well.
    """"""
    if T_B is None:
        T_B = T_A.copy()
        ignore_trivial = True
    (T_A, T_A_subseq_isfinite) = core.preprocess_non_normalized(T_A, m)
    (T_B, T_B_subseq_isfinite) = core.preprocess_non_normalized(T_B, m)
    if T_A.ndim != 1:
        raise ValueError(f'T_A is {T_A.ndim}-dimensional and must be 1-dimensional. ')
    if T_B.ndim != 1:
        raise ValueError(f'T_B is {T_B.ndim}-dimensional and must be 1-dimensional. ')
    core.check_window_size(m, max_size=min(T_A.shape[0], T_B.shape[0]))
    if ignore_trivial is False and core.are_arrays_equal(T_A, T_B):
        logger.warning('Arrays T_A, T_B are equal, which implies a self-join.')
        logger.warning('Try setting `ignore_trivial = True`.')
    if ignore_trivial and core.are_arrays_equal(T_A, T_B) is False:
        logger.warning('Arrays T_A, T_B are not equal, which implies an AB-join.')
        logger.warning('Try setting `ignore_trivial = False`.')
    n_A = T_A.shape[0]
    n_B = T_B.shape[0]
    l = n_A - m + 1
    excl_zone = int(np.ceil(m / config.STUMPY_EXCL_ZONE_DENOM))
    out = np.empty((l, 4), dtype=object)
    hosts = list(dask_client.ncores().keys())
    nworkers = len(hosts)
    if ignore_trivial:
        diags = np.arange(excl_zone + 1, n_A - m + 1, dtype=np.int64)
    else:
        diags = np.arange(-(n_A - m + 1) + 1, n_B - m + 1, dtype=np.int64)
    ndist_counts = core._count_diagonal_ndist(diags, m, n_A, n_B)
    diags_ranges = core._get_array_ranges(ndist_counts, nworkers, False)
    diags_ranges += diags[0]
    T_A_future = dask_client.scatter(T_A, broadcast=True, hash=False)
    T_B_future = dask_client.scatter(T_B, broadcast=True, hash=False)
    T_A_subseq_isfinite_future = dask_client.scatter(T_A_subseq_isfinite, broadcast=True, hash=False)
    T_B_subseq_isfinite_future = dask_client.scatter(T_B_subseq_isfinite, broadcast=True, hash=False)
    diags_futures = []
    for (i, host) in enumerate(hosts):
        diags_future = dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False)
        diags_futures.append(diags_future)
    futures = []
    for i in range(len(hosts)):
        futures.append(dask_client.submit(_aamp, T_A_future, T_B_future, m, T_A_subseq_isfinite_future, T_B_subseq_isfinite_future, diags_futures[i], ignore_trivial))
    results = dask_client.gather(futures)
    (profile, indices) = results[0]
    for i in range(1, len(hosts)):
        (P, I) = results[i]
        for col in range(P.shape[1]):
            cond = P[:, col] < profile[:, col]
            profile[:, col] = np.where(cond, P[:, col], profile[:, col])
            indices[:, col] = np.where(cond, I[:, col], indices[:, col])
    out[:, 0] = profile[:, 0]
    out[:, 1:4] = indices
    dask_client.cancel(T_A_future)
    dask_client.cancel(T_B_future)
    dask_client.cancel(T_A_subseq_isfinite_future)
    dask_client.cancel(T_B_subseq_isfinite_future)
    for diags_future in diags_futures:
        dask_client.cancel(diags_future)
    for future in futures:
        dask_client.cancel(future)
    threshold = 1e-05
    if core.are_distances_too_small(out[:, 0], threshold=threshold):
        logger.warning(f'A large number of values are smaller than {threshold}.')
        logger.warning('For a self-join, try setting `ignore_trivial = True`.')
    return out","for (i, host) in enumerate(hosts):
    diags_future = dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False)
    diags_futures.append(diags_future)","diags_futures = [dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False) for (i, host) in enumerate(hosts)]",Cannot refactor,-1,1,,,,robosuite
stumpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stumpy/stumpy/aamped.py,https://github.com/TDAmeritrade/stumpy/tree/master/stumpy/aamped.py,,aamped$15,"def aamped(dask_client, T_A, m, T_B=None, ignore_trivial=True):
    """"""
    Compute the non-normalized (i.e., without z-normalization) matrix profile

    This is a highly distributed implementation around the Numba JIT-compiled
    parallelized `_aamp` function which computes the non-normalized matrix profile
    according to AAMP.

    Parameters
    ----------
    dask_client : client
        A Dask Distributed client that is connected to a Dask scheduler and
        Dask workers. Setting up a Dask distributed cluster is beyond the
        scope of this library. Please refer to the Dask Distributed
        documentation.

    T_A : numpy.ndarray
        The time series or sequence for which to compute the matrix profile

    m : int
        Window size

    T_B : numpy.ndarray, default None
        The time series or sequence that will be used to annotate T_A. For every
        subsequence in T_A, its nearest neighbor in T_B will be recorded. Default is
        `None` which corresponds to a self-join.

    ignore_trivial : bool, default True
        Set to `True` if this is a self-join. Otherwise, for AB-join, set this
        to `False`. Default is `True`.

    Returns
    -------
    out : numpy.ndarray
        The first column consists of the matrix profile, the second column
        consists of the matrix profile indices.

    Notes
    -----
    `arXiv:1901.05708     <https://arxiv.org/pdf/1901.05708.pdf>`__

    See Algorithm 1

    Note that we have extended this algorithm for AB-joins as well.
    """"""
    if T_B is None:
        T_B = T_A.copy()
        ignore_trivial = True
    (T_A, T_A_subseq_isfinite) = core.preprocess_non_normalized(T_A, m)
    (T_B, T_B_subseq_isfinite) = core.preprocess_non_normalized(T_B, m)
    if T_A.ndim != 1:
        raise ValueError(f'T_A is {T_A.ndim}-dimensional and must be 1-dimensional. ')
    if T_B.ndim != 1:
        raise ValueError(f'T_B is {T_B.ndim}-dimensional and must be 1-dimensional. ')
    core.check_window_size(m, max_size=min(T_A.shape[0], T_B.shape[0]))
    if ignore_trivial is False and core.are_arrays_equal(T_A, T_B):
        logger.warning('Arrays T_A, T_B are equal, which implies a self-join.')
        logger.warning('Try setting `ignore_trivial = True`.')
    if ignore_trivial and core.are_arrays_equal(T_A, T_B) is False:
        logger.warning('Arrays T_A, T_B are not equal, which implies an AB-join.')
        logger.warning('Try setting `ignore_trivial = False`.')
    n_A = T_A.shape[0]
    n_B = T_B.shape[0]
    l = n_A - m + 1
    excl_zone = int(np.ceil(m / config.STUMPY_EXCL_ZONE_DENOM))
    out = np.empty((l, 4), dtype=object)
    hosts = list(dask_client.ncores().keys())
    nworkers = len(hosts)
    if ignore_trivial:
        diags = np.arange(excl_zone + 1, n_A - m + 1, dtype=np.int64)
    else:
        diags = np.arange(-(n_A - m + 1) + 1, n_B - m + 1, dtype=np.int64)
    ndist_counts = core._count_diagonal_ndist(diags, m, n_A, n_B)
    diags_ranges = core._get_array_ranges(ndist_counts, nworkers, False)
    diags_ranges += diags[0]
    T_A_future = dask_client.scatter(T_A, broadcast=True, hash=False)
    T_B_future = dask_client.scatter(T_B, broadcast=True, hash=False)
    T_A_subseq_isfinite_future = dask_client.scatter(T_A_subseq_isfinite, broadcast=True, hash=False)
    T_B_subseq_isfinite_future = dask_client.scatter(T_B_subseq_isfinite, broadcast=True, hash=False)
    diags_futures = []
    for (i, host) in enumerate(hosts):
        diags_future = dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False)
        diags_futures.append(diags_future)
    futures = []
    for i in range(len(hosts)):
        futures.append(dask_client.submit(_aamp, T_A_future, T_B_future, m, T_A_subseq_isfinite_future, T_B_subseq_isfinite_future, diags_futures[i], ignore_trivial))
    results = dask_client.gather(futures)
    (profile, indices) = results[0]
    for i in range(1, len(hosts)):
        (P, I) = results[i]
        for col in range(P.shape[1]):
            cond = P[:, col] < profile[:, col]
            profile[:, col] = np.where(cond, P[:, col], profile[:, col])
            indices[:, col] = np.where(cond, I[:, col], indices[:, col])
    out[:, 0] = profile[:, 0]
    out[:, 1:4] = indices
    dask_client.cancel(T_A_future)
    dask_client.cancel(T_B_future)
    dask_client.cancel(T_A_subseq_isfinite_future)
    dask_client.cancel(T_B_subseq_isfinite_future)
    for diags_future in diags_futures:
        dask_client.cancel(diags_future)
    for future in futures:
        dask_client.cancel(future)
    threshold = 1e-05
    if core.are_distances_too_small(out[:, 0], threshold=threshold):
        logger.warning(f'A large number of values are smaller than {threshold}.')
        logger.warning('For a self-join, try setting `ignore_trivial = True`.')
    return out","for i in range(len(hosts)):
    futures.append(dask_client.submit(_aamp, T_A_future, T_B_future, m, T_A_subseq_isfinite_future, T_B_subseq_isfinite_future, diags_futures[i], ignore_trivial))","futures += [dask_client.submit(_aamp, T_A_future, T_B_future, m, T_A_subseq_isfinite_future, T_B_subseq_isfinite_future, diags_futures[i], ignore_trivial) for i in range(len(hosts))]","futures = [dask_client.submit(_aamp, T_A_future, T_B_future, m, T_A_subseq_isfinite_future, T_B_subseq_isfinite_future, diags_futures[i], ignore_trivial) for i in range(len(hosts))]",0,1,,,,robosuite
evennia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evennia/evennia/scripts/tickerhandler.py,https://github.com/evennia/evennia/tree/master/evennia/scripts/tickerhandler.py,TickerHandler,all_display$626,"def all_display(self):
    """"""
        Get all tickers on an easily displayable form.

        Returns:
            tickers (dict): A list of all storekeys

        """"""
    store_keys = []
    for ticker in self.ticker_pool.tickers.values():
        for ((objtup, callfunc, path, interval, idstring, persistent), (args, kwargs)) in ticker.subscriptions.items():
            store_keys.append((kwargs.get('_obj', None), callfunc, path, interval, idstring, persistent))
    return store_keys","for ticker in self.ticker_pool.tickers.values():
    for ((objtup, callfunc, path, interval, idstring, persistent), (args, kwargs)) in ticker.subscriptions.items():
        store_keys.append((kwargs.get('_obj', None), callfunc, path, interval, idstring, persistent))","store_keys = [(kwargs.get('_obj', None), callfunc, path, interval, idstring, persistent) for ticker in self.ticker_pool.tickers.values() for ((objtup, callfunc, path, interval, idstring, persistent), (args, kwargs)) in ticker.subscriptions.items()]","store_keys = [(kwargs.get('_obj', None), callfunc, path, interval, idstring, persistent) for ticker in self.ticker_pool.tickers.values() for ((objtup, callfunc, path, interval, idstring, persistent), (args, kwargs)) in ticker.subscriptions.items()]",1,,,,,robosuite
HigherHRNet-Human-Pose-Estimation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HigherHRNet-Human-Pose-Estimation/lib/dataset/CrowdPoseDataset.py,https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation/tree/master/lib/dataset/CrowdPoseDataset.py,CrowdPoseDataset,_do_python_keypoint_eval$281,"def _do_python_keypoint_eval(self, res_file, res_folder):
    coco_dt = self.coco.loadRes(res_file)
    coco_eval = COCOeval(self.coco, coco_dt, 'keypoints')
    coco_eval.params.useSegm = None
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    stats_names = ['AP', 'Ap .5', 'AP .75', 'AR', 'AR .5', 'AR .75', 'AP (easy)', 'AP (medium)', 'AP (hard)']
    stats_index = [0, 1, 2, 5, 6, 7, 8, 9, 10]
    info_str = []
    for (ind, name) in enumerate(stats_names):
        info_str.append((name, coco_eval.stats[stats_index[ind]]))
    return info_str","for (ind, name) in enumerate(stats_names):
    info_str.append((name, coco_eval.stats[stats_index[ind]]))","info_str = [(name, coco_eval.stats[stats_index[ind]]) for (ind, name) in enumerate(stats_names)]","info_str = [(name, coco_eval.stats[stats_index[ind]]) for (ind, name) in enumerate(stats_names)]",1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/tensor/search.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/tensor/search.py,,nonzero$383,"def nonzero(x, as_tuple=False):
    """"""
    Return a tensor containing the indices of all non-zero elements of the `input`
    tensor. If as_tuple is True, return a tuple of 1-D tensors, one for each dimension
    in `input`, each containing the indices (in that dimension) of all non-zero elements
    of `input`. Given a n-Dimensional `input` tensor with shape [x_1, x_2, ..., x_n], If
    as_tuple is False, we can get a output tensor with shape [z, n], where `z` is the
    number of all non-zero elements in the `input` tensor. If as_tuple is True, we can get
    a 1-D tensor tuple of length `n`, and the shape of each 1-D tensor is [z, 1].

    Args:
        x (Tensor): The input tensor variable.
        as_tuple (bool, optional): Return type, Tensor or tuple of Tensor.

    Returns:
        Tensor. The data type is int64.

    Examples:

        .. code-block:: python

            import paddle

            x1 = paddle.to_tensor([[1.0, 0.0, 0.0],
                                   [0.0, 2.0, 0.0],
                                   [0.0, 0.0, 3.0]])
            x2 = paddle.to_tensor([0.0, 1.0, 0.0, 3.0])
            out_z1 = paddle.nonzero(x1)
            print(out_z1)
            #[[0 0]
            # [1 1]
            # [2 2]]
            out_z1_tuple = paddle.nonzero(x1, as_tuple=True)
            for out in out_z1_tuple:
                print(out)
            #[[0]
            # [1]
            # [2]]
            #[[0]
            # [1]
            # [2]]
            out_z2 = paddle.nonzero(x2)
            print(out_z2)
            #[[1]
            # [3]]
            out_z2_tuple = paddle.nonzero(x2, as_tuple=True)
            for out in out_z2_tuple:
                print(out)
            #[[1]
            # [3]]

    """"""
    list_out = []
    shape = x.shape
    rank = len(shape)
    if in_dygraph_mode():
        outs = _C_ops.nonzero(x)
    elif paddle.in_dynamic_mode():
        outs = _legacy_C_ops.where_index(x)
    else:
        helper = LayerHelper('where_index', **locals())
        outs = helper.create_variable_for_type_inference(dtype=core.VarDesc.VarType.INT64)
        helper.append_op(type='where_index', inputs={'Condition': x}, outputs={'Out': [outs]})
    if not as_tuple:
        return outs
    elif rank == 1:
        return tuple([outs])
    else:
        for i in range(rank):
            list_out.append(paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]))
        return tuple(list_out)","for i in range(rank):
    list_out.append(paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]))","list_out = [paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]) for i in range(rank)]","list_out = [paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]) for i in range(rank)]",1,,,,,robosuite
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/file_ops.py,https://github.com/animate1978/MB-Lab/tree/master//file_ops.py,,load_vertices_database$195,"def load_vertices_database(vertices_path):
    vertices = []
    verts = load_json_data(vertices_path, 'Vertices data')
    if verts:
        for vert_co in verts:
            vertices.append(mathutils.Vector(vert_co))
    return vertices","for vert_co in verts:
    vertices.append(mathutils.Vector(vert_co))",vertices += [mathutils.Vector(vert_co) for vert_co in verts],vertices = [mathutils.Vector(vert_co) for vert_co in verts],0,1,,,,robosuite
lemur,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lemur/lemur/certificates/utils.py,https://github.com/Netflix/lemur/tree/master/lemur/certificates/utils.py,,get_sans_from_csr$19,"def get_sans_from_csr(data):
    """"""
    Fetches SubjectAlternativeNames from CSR.
    Works with any kind of SubjectAlternativeName
    :param data: PEM-encoded string with CSR
    :return: List of LemurAPI-compatible subAltNames
    """"""
    sub_alt_names = []
    try:
        request = x509.load_pem_x509_csr(data.encode('utf-8'), default_backend())
    except Exception:
        raise ValidationError('CSR presented is not valid.')
    try:
        alt_names = request.extensions.get_extension_for_class(x509.SubjectAlternativeName)
        for alt_name in alt_names.value:
            sub_alt_names.append({'nameType': type(alt_name).__name__, 'value': alt_name.value})
    except x509.ExtensionNotFound:
        pass
    return sub_alt_names","for alt_name in alt_names.value:
    sub_alt_names.append({'nameType': type(alt_name).__name__, 'value': alt_name.value})","sub_alt_names = [{'nameType': type(alt_name).__name__, 'value': alt_name.value} for alt_name in alt_names.value]","sub_alt_names = [{'nameType': type(alt_name).__name__, 'value': alt_name.value} for alt_name in alt_names.value]",1,,,,,robosuite
FSL-Mate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSL-Mate/PaddleFSL/examples/FewCLUE/p-tuning/model.py,https://github.com/tata1661/FSL-Mate/tree/master/PaddleFSL/examples/FewCLUE/p-tuning/model.py,ErnieForPretraining,forward$33,"def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None, masked_positions=None):
    with paddle.static.amp.fp16_guard():
        outputs = self.ernie(input_ids, token_type_ids=token_type_ids, position_ids=position_ids, attention_mask=attention_mask)
        (sequence_output, pooled_output) = outputs[:2]
        max_len = input_ids.shape[1]
        new_masked_positions = []
        for (bs_index, mask_pos) in enumerate(masked_positions.numpy()):
            for pos in mask_pos:
                new_masked_positions.append(bs_index * max_len + pos)
        new_masked_positions = np.array(new_masked_positions).astype('int32')
        new_masked_positions = paddle.to_tensor(new_masked_positions)
        (prediction_scores, seq_relationship_score) = self.cls(sequence_output, pooled_output, new_masked_positions)
        return prediction_scores","for (bs_index, mask_pos) in enumerate(masked_positions.numpy()):
    for pos in mask_pos:
        new_masked_positions.append(bs_index * max_len + pos)","new_masked_positions = [bs_index * max_len + pos for (bs_index, mask_pos) in enumerate(masked_positions.numpy()) for pos in mask_pos]","new_masked_positions = [bs_index * max_len + pos for (bs_index, mask_pos) in enumerate(masked_positions.numpy()) for pos in mask_pos]",1,,,,,robosuite
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/python_utils/formatters.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/python_utils/formatters.py,,timesince$42,"def timesince(dt, default='just now'):
    """"""
    Returns string representing 'time since' e.g.
    3 days ago, 5 hours ago etc.

    >>> now = datetime.datetime.now()
    >>> timesince(now)
    'just now'
    >>> timesince(now - datetime.timedelta(seconds=1))
    '1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=2))
    '2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=60))
    '1 minute ago'
    >>> timesince(now - datetime.timedelta(seconds=61))
    '1 minute and 1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=62))
    '1 minute and 2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=120))
    '2 minutes ago'
    >>> timesince(now - datetime.timedelta(seconds=121))
    '2 minutes and 1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=122))
    '2 minutes and 2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=3599))
    '59 minutes and 59 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=3600))
    '1 hour ago'
    >>> timesince(now - datetime.timedelta(seconds=3601))
    '1 hour and 1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=3602))
    '1 hour and 2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=3660))
    '1 hour and 1 minute ago'
    >>> timesince(now - datetime.timedelta(seconds=3661))
    '1 hour and 1 minute ago'
    >>> timesince(now - datetime.timedelta(seconds=3720))
    '1 hour and 2 minutes ago'
    >>> timesince(now - datetime.timedelta(seconds=3721))
    '1 hour and 2 minutes ago'
    >>> timesince(datetime.timedelta(seconds=3721))
    '1 hour and 2 minutes ago'
    """"""
    if isinstance(dt, datetime.timedelta):
        diff = dt
    else:
        now = datetime.datetime.now()
        diff = abs(now - dt)
    periods = ((diff.days / 365, 'year', 'years'), (diff.days % 365 / 30, 'month', 'months'), (diff.days % 30 / 7, 'week', 'weeks'), (diff.days % 7, 'day', 'days'), (diff.seconds / 3600, 'hour', 'hours'), (diff.seconds % 3600 / 60, 'minute', 'minutes'), (diff.seconds % 60, 'second', 'seconds'))
    output = []
    for (period, singular, plural) in periods:
        if int(period):
            if int(period) == 1:
                output.append(f'{int(period)} {singular}')
            else:
                output.append(f'{int(period)} {plural}')
    if output:
        return f""{' and '.join(output[:2])} ago""
    return default","for (period, singular, plural) in periods:
    if int(period):
        if int(period) == 1:
            output.append(f'{int(period)} {singular}')
        else:
            output.append(f'{int(period)} {plural}')","output = [f'{int(period)} {singular}' if int(period) == 1 else f'{int(period)} {plural}' for (period, singular, plural) in periods if int(period)]","output = [f'{int(period)} {singular}' if int(period) == 1 else f'{int(period)} {plural}' for (period, singular, plural) in periods if int(period)]",1,,,,,robosuite
football,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/football/gfootball/env/football_env_core.py,https://github.com/google-research/football/tree/master/gfootball/env/football_env_core.py,FootballEnvCore,sticky_actions_state$378,"def sticky_actions_state(self, left_team, player_id):
    result = []
    for a in self._sticky_actions:
        result.append(self._env.sticky_action_state(a._backend_action, left_team, player_id))
    return np.uint8(result)","for a in self._sticky_actions:
    result.append(self._env.sticky_action_state(a._backend_action, left_team, player_id))","result = [self._env.sticky_action_state(a._backend_action, left_team, player_id) for a in self._sticky_actions]","result = [self._env.sticky_action_state(a._backend_action, left_team, player_id) for a in self._sticky_actions]",1,,,,,robosuite
SpanBERT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SpanBERT/code/run_squad.py,https://github.com/facebookresearch/SpanBERT/tree/master/code/run_squad.py,,_compute_softmax$595,"def _compute_softmax(scores):
    """"""Compute softmax probability over raw logits.""""""
    if not scores:
        return []
    max_score = None
    for score in scores:
        if max_score is None or score > max_score:
            max_score = score
    exp_scores = []
    total_sum = 0.0
    for score in scores:
        x = math.exp(score - max_score)
        exp_scores.append(x)
        total_sum += x
    probs = []
    for score in exp_scores:
        probs.append(score / total_sum)
    return probs","for score in exp_scores:
    probs.append(score / total_sum)",probs = [score / total_sum for score in exp_scores],probs = [score / total_sum for score in exp_scores],1,,,,,robosuite
Sorcar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Sorcar/nodes/arrays/ScMakeArray.py,https://github.com/aachman98/Sorcar/tree/master/nodes/arrays/ScMakeArray.py,ScMakeArray,post_execute$26,"def post_execute(self):
    arr = []
    for i in self.inputs:
        if not i.bl_rna.name == 'ScNodeSocketArrayPlaceholder':
            if i.bl_rna.name == 'ScNodeSocketVector':
                arr.append(list(i.default_value))
            else:
                arr.append(i.default_value)
    return {'Array': repr(arr)}","for i in self.inputs:
    if not i.bl_rna.name == 'ScNodeSocketArrayPlaceholder':
        if i.bl_rna.name == 'ScNodeSocketVector':
            arr.append(list(i.default_value))
        else:
            arr.append(i.default_value)",arr = [list(i.default_value) if i.bl_rna.name == 'ScNodeSocketVector' else i.default_value for i in self.inputs if not i.bl_rna.name == 'ScNodeSocketArrayPlaceholder'],arr = [list(i.default_value) if i.bl_rna.name == 'ScNodeSocketVector' else i.default_value for i in self.inputs if not i.bl_rna.name == 'ScNodeSocketArrayPlaceholder'],1,,,,,robosuite
PaddleViT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleViT/object_detection/PVTv2/coco_eval.py,https://github.com/BR-IDL/PaddleViT/tree/master/object_detection/PVTv2/coco_eval.py,,merge$178,"def merge(img_ids, eval_imgs):
    all_img_ids = all_gather(img_ids)
    all_eval_imgs = all_gather(eval_imgs)
    merged_img_ids = []
    for p in all_img_ids:
        merged_img_ids.extend(p)
    merged_eval_imgs = []
    for p in all_eval_imgs:
        merged_eval_imgs.append(p)
    merged_img_ids = np.array(merged_img_ids)
    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)
    (merged_img_ids, idx) = np.unique(merged_img_ids, return_index=True)
    merged_eval_imgs = merged_eval_imgs[..., idx]
    return (merged_img_ids, merged_eval_imgs)","for p in all_eval_imgs:
    merged_eval_imgs.append(p)",merged_eval_imgs = [p for p in all_eval_imgs],merged_eval_imgs = [p for p in all_eval_imgs],1,,,,,robosuite
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,on_data_dir_move_down_button_clicked$28367,"def on_data_dir_move_down_button_clicked(self, button, treeview, liststore, button2):
    """"""Called from callback in self.setup_files_database_tab().

        Moves the selected data directory down one position in the list of
        alternative data directories.

        Args:

            button (Gtk.Button): The widget that was clicked (the down button)

            treeview (Gtk.TreeView): The widget in which a line was selected

            liststore (Gtk.ListStore): The treeview's liststore

            button2 (Gtk.Button): The up button

        """"""
    selection = treeview.get_selection()
    (model, path_list) = selection.get_selected_rows()
    if not path_list:
        return
    first_item = None
    last_item = None
    path_list.reverse()
    for path in path_list:
        this_iter = model.get_iter(path)
        last_item = model[this_iter][0]
        if first_item is None:
            first_item = model[this_iter][0]
        if model.iter_next(this_iter):
            liststore.move_after(this_iter, model.iter_next(this_iter))
        else:
            break
    dir_list = []
    for row in liststore:
        dir_list.append(row[0])
    self.app_obj.set_data_dir_alt_list(dir_list)
    if dir_list.index(first_item) == 0:
        button2.set_sensitive(False)
    else:
        button2.set_sensitive(True)
    if dir_list.index(last_item) == len(dir_list) - 1:
        button.set_sensitive(False)
    else:
        button.set_sensitive(True)","for row in liststore:
    dir_list.append(row[0])",dir_list = [row[0] for row in liststore],dir_list = [row[0] for row in liststore],1,,,,,robosuite
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/scripts/release/generate_history_notes.py,https://github.com/Azure/azure-cli/tree/master/scripts/release/generate_history_notes.py,,construct_core_history$133,"def construct_core_history(component: str):
    history = []
    for note in history_notes[component]:
        history.append('* {}'.format(note))
    history.append('\n')
    return '\n'.join(history)","for note in history_notes[component]:
    history.append('* {}'.format(note))",history += ['* {}'.format(note) for note in history_notes[component]],history = ['* {}'.format(note) for note in history_notes[component]],0,1,,,,robosuite
PaddleHub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleHub/modules/text/language_model/lda_news/module.py,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/text/language_model/lda_news/module.py,TopicModel,infer_doc_topic_distribution$150,"def infer_doc_topic_distribution(self, document):
    """"""
        This interface infers the topic distribution of document.

        Args:
            document(str): the input document text.

        Returns:
            results(list): returns the topic distribution of document.
        """"""
    tokens = self.__tokenizer.tokenize(document)
    if tokens == []:
        return []
    results = []
    doc = LDADoc()
    self.__engine.infer(tokens, doc)
    topics = doc.sparse_topic_dist()
    for topic in topics:
        results.append({'topic id': topic.tid, 'distribution': topic.prob})
    return results","for topic in topics:
    results.append({'topic id': topic.tid, 'distribution': topic.prob})","results = [{'topic id': topic.tid, 'distribution': topic.prob} for topic in topics]","results = [{'topic id': topic.tid, 'distribution': topic.prob} for topic in topics]",1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/xpu/test_fill_constant_op_xpu.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/xpu/test_fill_constant_op_xpu.py,TestFillConstantOp1_ShapeTensorList,set_data$146,"def set_data(self):
    shape_tensor_list = []
    for (index, ele) in enumerate(self.shape):
        shape_tensor_list.append(('x' + str(index), np.ones(1).astype('int32') * ele))
    self.inputs = {'ShapeTensorList': shape_tensor_list}
    self.attrs = {'shape': self.infer_shape, 'dtype': self.index, 'value': self.value}
    self.outputs = {'Out': np.full(self.shape, self.value)}
    if self.index == 22:
        self.outputs = {'Out': np.full(self.shape, convert_float_to_uint16(np.array([self.value]).astype('float32')))}","for (index, ele) in enumerate(self.shape):
    shape_tensor_list.append(('x' + str(index), np.ones(1).astype('int32') * ele))","shape_tensor_list = [('x' + str(index), np.ones(1).astype('int32') * ele) for (index, ele) in enumerate(self.shape)]","shape_tensor_list = [('x' + str(index), np.ones(1).astype('int32') * ele) for (index, ele) in enumerate(self.shape)]",1,,,,,robosuite
aws-data-wrangler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-data-wrangler/awswrangler/timestream.py,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/timestream.py,,_paginate_query$149,"def _paginate_query(sql: str, chunked: bool, pagination_config: Optional[Dict[str, Any]], boto3_session: Optional[boto3.Session]=None) -> Iterator[pd.DataFrame]:
    client: boto3.client = _utils.client(service_name='timestream-query', session=boto3_session, botocore_config=Config(read_timeout=60, retries={'max_attempts': 10}))
    paginator = client.get_paginator('query')
    rows: List[List[Any]] = []
    schema: List[Dict[str, str]] = []
    page_iterator = paginator.paginate(QueryString=sql, PaginationConfig=pagination_config or {})
    for page in page_iterator:
        if not schema:
            schema = _process_schema(page=page)
            _logger.debug('schema: %s', schema)
        for row in page['Rows']:
            rows.append(_process_row(schema=schema, row=row))
        if len(rows) > 0:
            df_metadata = {}
            if chunked:
                if 'NextToken' in page:
                    df_metadata['NextToken'] = page['NextToken']
                df_metadata['QueryId'] = page['QueryId']
            yield _rows_to_df(rows, schema, df_metadata)
        rows = []","for row in page['Rows']:
    rows.append(_process_row(schema=schema, row=row))","rows = [_process_row(schema=schema, row=row) for row in page['Rows']]","rows = [_process_row(schema=schema, row=row) for row in page['Rows']]",1,,,,,robosuite
Deep-Reinforcement-Learning-Algorithms-with-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tests/Test_Four_Rooms_Environment.py,https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tree/master/tests/Test_Four_Rooms_Environment.py,,test_randomness_of_moves$101,"def test_randomness_of_moves():
    """"""Test that determine_which_action_will_actually_occur correctly implements stochastic_actions_probability""""""
    env = Four_Rooms_Environment(stochastic_actions_probability=0.0)
    env.reset()
    for _ in range(10):
        for move in env.actions:
            assert move == env.determine_which_action_will_actually_occur(move)
    env = Four_Rooms_Environment(stochastic_actions_probability=1.0)
    num_iterations = 10000
    for move in env.actions:
        moves = []
        for _ in range(num_iterations):
            moves.append(env.determine_which_action_will_actually_occur(move))
        count = Counter(moves)
        for move_test in env.actions:
            if move != move_test:
                assert abs(num_iterations / (len(env.actions) - 1) - count[move_test]) < num_iterations / 20.0, '{}'.format(count)
    env = Four_Rooms_Environment(stochastic_actions_probability=0.75)
    num_iterations = 10000
    for move in env.actions:
        moves = []
        for _ in range(num_iterations):
            moves.append(env.determine_which_action_will_actually_occur(move))
        count = Counter(moves)
        for move_test in env.actions:
            assert abs(num_iterations / len(env.actions) - count[move_test]) < num_iterations / 20.0, '{}'.format(count)","for _ in range(num_iterations):
    moves.append(env.determine_which_action_will_actually_occur(move))",moves = [env.determine_which_action_will_actually_occur(move) for _ in range(num_iterations)],moves = [env.determine_which_action_will_actually_occur(move) for _ in range(num_iterations)],1,,,,,robosuite
Deep-Reinforcement-Learning-Algorithms-with-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tests/Test_Four_Rooms_Environment.py,https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tree/master/tests/Test_Four_Rooms_Environment.py,,test_randomness_of_moves$101,"def test_randomness_of_moves():
    """"""Test that determine_which_action_will_actually_occur correctly implements stochastic_actions_probability""""""
    env = Four_Rooms_Environment(stochastic_actions_probability=0.0)
    env.reset()
    for _ in range(10):
        for move in env.actions:
            assert move == env.determine_which_action_will_actually_occur(move)
    env = Four_Rooms_Environment(stochastic_actions_probability=1.0)
    num_iterations = 10000
    for move in env.actions:
        moves = []
        for _ in range(num_iterations):
            moves.append(env.determine_which_action_will_actually_occur(move))
        count = Counter(moves)
        for move_test in env.actions:
            if move != move_test:
                assert abs(num_iterations / (len(env.actions) - 1) - count[move_test]) < num_iterations / 20.0, '{}'.format(count)
    env = Four_Rooms_Environment(stochastic_actions_probability=0.75)
    num_iterations = 10000
    for move in env.actions:
        moves = []
        for _ in range(num_iterations):
            moves.append(env.determine_which_action_will_actually_occur(move))
        count = Counter(moves)
        for move_test in env.actions:
            assert abs(num_iterations / len(env.actions) - count[move_test]) < num_iterations / 20.0, '{}'.format(count)","for _ in range(num_iterations):
    moves.append(env.determine_which_action_will_actually_occur(move))",moves += [env.determine_which_action_will_actually_occur(move) for _ in range(num_iterations)],moves = [env.determine_which_action_will_actually_occur(move) for _ in range(num_iterations)],0,1,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,TestReorderLoDTensor,reorder$112,"def reorder(self):
    level = 0
    ref_lod = self.data[self.data_desc[1][0]][1][level]
    rank_table = []
    for i in range(len(ref_lod)):
        rank_table.append((i, ref_lod[i]))
    rank_table = sorted(rank_table, key=functools.cmp_to_key(lambda x, y: y[1] - x[1]))
    (input_value, input_lod) = self.data[self.data_desc[0][0]]
    offset_lod = convert_to_offset(input_lod)
    input_table = []
    if offset_lod:
        for i in range(len(offset_lod[level]) - 1):
            start_idx = i
            end_idx = i + 1
            sub_lod = []
            for lod_level_i in offset_lod[level:]:
                sub_lod_i = []
                for idx in range(start_idx, end_idx):
                    sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])
                sub_lod.append(sub_lod_i)
                start_idx = lod_level_i[start_idx]
                end_idx = lod_level_i[end_idx]
            input_table.append((start_idx, end_idx - start_idx, sub_lod))
    else:
        input_table = [(i, 1, []) for i in range(len(rank_table))]
    output_value = np.zeros_like(input_value)
    output_lod = []
    offset = 0
    for (index, length) in rank_table:
        input_seq_start = input_table[index][0]
        input_seq_len = input_table[index][1]
        input_seq_end = input_seq_start + input_seq_len
        output_value[offset:offset + input_seq_len] = input_value[input_seq_start:input_seq_end]
        offset += input_seq_len
        input_seq_sub_lod = input_table[index][2]
        if len(output_lod) == 0:
            output_lod = [[] for i in input_seq_sub_lod]
        for (i, level) in enumerate(input_seq_sub_lod):
            output_lod[i].extend(level)
    return (output_value, output_lod)","for i in range(len(ref_lod)):
    rank_table.append((i, ref_lod[i]))","rank_table = [(i, ref_lod[i]) for i in range(len(ref_lod))]","rank_table = [(i, ref_lod[i]) for i in range(len(ref_lod))]",1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,TestReorderLoDTensor,reorder$112,"def reorder(self):
    level = 0
    ref_lod = self.data[self.data_desc[1][0]][1][level]
    rank_table = []
    for i in range(len(ref_lod)):
        rank_table.append((i, ref_lod[i]))
    rank_table = sorted(rank_table, key=functools.cmp_to_key(lambda x, y: y[1] - x[1]))
    (input_value, input_lod) = self.data[self.data_desc[0][0]]
    offset_lod = convert_to_offset(input_lod)
    input_table = []
    if offset_lod:
        for i in range(len(offset_lod[level]) - 1):
            start_idx = i
            end_idx = i + 1
            sub_lod = []
            for lod_level_i in offset_lod[level:]:
                sub_lod_i = []
                for idx in range(start_idx, end_idx):
                    sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])
                sub_lod.append(sub_lod_i)
                start_idx = lod_level_i[start_idx]
                end_idx = lod_level_i[end_idx]
            input_table.append((start_idx, end_idx - start_idx, sub_lod))
    else:
        input_table = [(i, 1, []) for i in range(len(rank_table))]
    output_value = np.zeros_like(input_value)
    output_lod = []
    offset = 0
    for (index, length) in rank_table:
        input_seq_start = input_table[index][0]
        input_seq_len = input_table[index][1]
        input_seq_end = input_seq_start + input_seq_len
        output_value[offset:offset + input_seq_len] = input_value[input_seq_start:input_seq_end]
        offset += input_seq_len
        input_seq_sub_lod = input_table[index][2]
        if len(output_lod) == 0:
            output_lod = [[] for i in input_seq_sub_lod]
        for (i, level) in enumerate(input_seq_sub_lod):
            output_lod[i].extend(level)
    return (output_value, output_lod)","for i in range(len(offset_lod[level]) - 1):
    start_idx = i
    end_idx = i + 1
    sub_lod = []
    for lod_level_i in offset_lod[level:]:
        sub_lod_i = []
        for idx in range(start_idx, end_idx):
            sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])
        sub_lod.append(sub_lod_i)
        start_idx = lod_level_i[start_idx]
        end_idx = lod_level_i[end_idx]
    input_table.append((start_idx, end_idx - start_idx, sub_lod))",,Cannot refactor,-1,0,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,TestReorderLoDTensor,reorder$112,"def reorder(self):
    level = 0
    ref_lod = self.data[self.data_desc[1][0]][1][level]
    rank_table = []
    for i in range(len(ref_lod)):
        rank_table.append((i, ref_lod[i]))
    rank_table = sorted(rank_table, key=functools.cmp_to_key(lambda x, y: y[1] - x[1]))
    (input_value, input_lod) = self.data[self.data_desc[0][0]]
    offset_lod = convert_to_offset(input_lod)
    input_table = []
    if offset_lod:
        for i in range(len(offset_lod[level]) - 1):
            start_idx = i
            end_idx = i + 1
            sub_lod = []
            for lod_level_i in offset_lod[level:]:
                sub_lod_i = []
                for idx in range(start_idx, end_idx):
                    sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])
                sub_lod.append(sub_lod_i)
                start_idx = lod_level_i[start_idx]
                end_idx = lod_level_i[end_idx]
            input_table.append((start_idx, end_idx - start_idx, sub_lod))
    else:
        input_table = [(i, 1, []) for i in range(len(rank_table))]
    output_value = np.zeros_like(input_value)
    output_lod = []
    offset = 0
    for (index, length) in rank_table:
        input_seq_start = input_table[index][0]
        input_seq_len = input_table[index][1]
        input_seq_end = input_seq_start + input_seq_len
        output_value[offset:offset + input_seq_len] = input_value[input_seq_start:input_seq_end]
        offset += input_seq_len
        input_seq_sub_lod = input_table[index][2]
        if len(output_lod) == 0:
            output_lod = [[] for i in input_seq_sub_lod]
        for (i, level) in enumerate(input_seq_sub_lod):
            output_lod[i].extend(level)
    return (output_value, output_lod)","for idx in range(start_idx, end_idx):
    sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])","sub_lod_i += [lod_level_i[idx + 1] - lod_level_i[idx] for idx in range(start_idx, end_idx)]","sub_lod_i = [lod_level_i[idx + 1] - lod_level_i[idx] for idx in range(start_idx, end_idx)]",0,1,,,,robosuite
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/Malt/Pipeline.py,https://github.com/bnpr/Malt/tree/master/Malt/Pipeline.py,Pipeline,preprocess_shaders_from_source$111,"def preprocess_shaders_from_source(self, shader_source, include_paths=[], defines_lists=[]):
    shader_source = Pipeline.GLSL_HEADER + shader_source
    include_paths = include_paths + Pipeline.SHADER_INCLUDE_PATHS

    def preprocess(params):
        return shader_preprocessor(*params)
    params = []
    for defines in defines_lists:
        params.append((shader_source, include_paths, defines))
    return self.pool.map(preprocess, params)","for defines in defines_lists:
    params.append((shader_source, include_paths, defines))","params = [(shader_source, include_paths, defines) for defines in defines_lists]","params = [(shader_source, include_paths, defines) for defines in defines_lists]",1,,,,,robosuite
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_empty_track$13,"def remove_empty_track(midi_file):
    """"""
    1. read pretty midi data
    2. remove empty track,
    also remove track with fewer than 10% notes of the track
    with most notes
    ********
    Return: pretty_midi object, pypianoroll object
    """"""
    try:
        pretty_midi_data = pretty_midi.PrettyMIDI(midi_file)
    except Exception as e:
        print(f'exceptions in reading the file {midi_file}')
        return (None, None)
    pypiano_data = pypianoroll.Multitrack()
    try:
        pypiano_data.parse_pretty_midi(pretty_midi_data, skip_empty_tracks=False)
    except Exception as e:
        print(f'exceptions for pypianoroll in reading the file {midi_file}')
        return (None, None)
    drum_idx = []
    for (i, instrument) in enumerate(pretty_midi_data.instruments):
        if instrument.is_drum:
            drum_idx.append(i)
    note_count = [np.count_nonzero(np.any(track.pianoroll, axis=1)) for track in pypiano_data.tracks]
    empty_indices = np.array(note_count) < 10
    remove_indices = np.arange(len(pypiano_data.tracks))[empty_indices]
    for index in sorted(remove_indices, reverse=True):
        del pypiano_data.tracks[index]
        del pretty_midi_data.instruments[index]
    return (pretty_midi_data, pypiano_data)","for (i, instrument) in enumerate(pretty_midi_data.instruments):
    if instrument.is_drum:
        drum_idx.append(i)","drum_idx = [i for (i, instrument) in enumerate(pretty_midi_data.instruments) if instrument.is_drum]","drum_idx = [i for (i, instrument) in enumerate(pretty_midi_data.instruments) if instrument.is_drum]",1,,,,,robosuite
pixelsort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pixelsort/pixelsort/sorter.py,https://github.com/satyarth/pixelsort/tree/master/pixelsort/sorter.py,,sort_image$4,"def sort_image(size, image_data, mask_data, intervals, randomness, sorting_function):
    sorted_pixels = []
    for y in range(size[1]):
        row = []
        x_min = 0
        for x_max in intervals[y] + [size[0]]:
            interval = []
            for x in range(x_min, x_max):
                if mask_data[x, y]:
                    interval.append(image_data[x, y])
            if random.random() < randomness / 100:
                row += interval
            else:
                row += sort_interval(interval, sorting_function)
            x_min = x_max
        sorted_pixels.append(row)
    return sorted_pixels","for y in range(size[1]):
    row = []
    x_min = 0
    for x_max in intervals[y] + [size[0]]:
        interval = []
        for x in range(x_min, x_max):
            if mask_data[x, y]:
                interval.append(image_data[x, y])
        if random.random() < randomness / 100:
            row += interval
        else:
            row += sort_interval(interval, sorting_function)
        x_min = x_max
    sorted_pixels.append(row)","sorted_pixels = [[image_data[x, y] for x in range(x_min, x_max) if mask_data[x, y]] if random.random() < randomness / 100 else sort_interval([image_data[x, y] for x in range(x_min, x_max) if mask_data[x, y]], sorting_function) for y in range(size[1]) for (x_min, x_max) in zip([0] + intervals[y], intervals[y] + [size[0]])]",Cannot refactor,-1,0,,,,robosuite
PyFlow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyFlow/PyFlow/Core/PinBase.py,https://github.com/wonderworks-software/PyFlow/tree/master/PyFlow/Core/PinBase.py,PinBase,supportDictElement$897,"def supportDictElement(self, checked=[], can=True, selfCheck=True):
    """"""Iterative functions that search in all connected pins to see if they support DictElement nodes.

        :param checked: Already visited pins, defaults to []
        :type checked: list, optional
        :param can: this is the variable that will be actualized during the recursive function, defaults to False
        :type can: bool, optional
        :param selfCheck: Define if look itself or no, defaults to True
        :type selfCheck: bool, optional
        :returns: True if can connect DictElement nodes to this pin
        :rtype: bool
        """"""
    if not self.optionEnabled(PinOptions.DictElementSupported):
        return False
    con = []
    neis = []
    if selfCheck:
        if self.hasConnections() and self.direction == PinDirection.Input:
            for c in getConnectedPins(self):
                if c not in checked:
                    con.append(c)
    else:
        checked.append(self)
    if self.constraint and self.owningNode().__class__.__name__ != 'makeDictElement':
        neis = self.owningNode().constraints[self.constraint]
    for port in neis + con:
        if port not in checked and can:
            checked.append(port)
            can = port.supportDictElement(checked, can, selfCheck=True)
    return can","for c in getConnectedPins(self):
    if c not in checked:
        con.append(c)",con = [c for c in getConnectedPins(self) if c not in checked],con = [c for c in getConnectedPins(self) if c not in checked],1,,,,,robosuite
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-fig2_5.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-fig2_5.py,,calculate_correlation$517,"def calculate_correlation(*vectors):
    matrix = []
    for (i, vectori) in enumerate(vectors):
        x = []
        for (j, vectorj) in enumerate(vectors):
            x.append(compute_kendalltau(vectori, vectorj))
        matrix.append(x)
    return np.array(matrix)","for (i, vectori) in enumerate(vectors):
    x = []
    for (j, vectorj) in enumerate(vectors):
        x.append(compute_kendalltau(vectori, vectorj))
    matrix.append(x)","matrix = [[compute_kendalltau(vectori, vectorj) for (j, vectorj) in enumerate(vectors)] for (i, vectori) in enumerate(vectors)]",Cannot refactor,-1,1,,,,robosuite
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-fig2_5.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-fig2_5.py,,calculate_correlation$517,"def calculate_correlation(*vectors):
    matrix = []
    for (i, vectori) in enumerate(vectors):
        x = []
        for (j, vectorj) in enumerate(vectors):
            x.append(compute_kendalltau(vectori, vectorj))
        matrix.append(x)
    return np.array(matrix)","for (j, vectorj) in enumerate(vectors):
    x.append(compute_kendalltau(vectori, vectorj))","x += [compute_kendalltau(vectori, vectorj) for (j, vectorj) in enumerate(vectors)]","x = [compute_kendalltau(vectori, vectorj) for (j, vectorj) in enumerate(vectors)]",0,1,,,,robosuite
termgraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/termgraph/termgraph/termgraph.py,https://github.com/mkaz/termgraph/tree/master/termgraph/termgraph.py,,normalize$153,"def normalize(data: List, width: int) -> List:
    """"""Normalize the data and return it.""""""
    data_offset = []
    min_datum = find_min(data)
    if min_datum < 0:
        min_datum = abs(min_datum)
        for datum in data:
            data_offset.append([d + min_datum for d in datum])
    else:
        data_offset = data
    min_datum = find_min(data_offset)
    max_datum = find_max(data_offset)
    if min_datum == max_datum:
        return data_offset
    norm_factor = width / float(max_datum)
    normal_data = []
    for datum in data_offset:
        normal_data.append([v * norm_factor for v in datum])
    return normal_data","for datum in data_offset:
    normal_data.append([v * norm_factor for v in datum])",normal_data = [[v * norm_factor for v in datum] for datum in data_offset],normal_data = [[v * norm_factor for v in datum] for datum in data_offset],1,,,,,robosuite
termgraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/termgraph/termgraph/termgraph.py,https://github.com/mkaz/termgraph/tree/master/termgraph/termgraph.py,,normalize$153,"def normalize(data: List, width: int) -> List:
    """"""Normalize the data and return it.""""""
    data_offset = []
    min_datum = find_min(data)
    if min_datum < 0:
        min_datum = abs(min_datum)
        for datum in data:
            data_offset.append([d + min_datum for d in datum])
    else:
        data_offset = data
    min_datum = find_min(data_offset)
    max_datum = find_max(data_offset)
    if min_datum == max_datum:
        return data_offset
    norm_factor = width / float(max_datum)
    normal_data = []
    for datum in data_offset:
        normal_data.append([v * norm_factor for v in datum])
    return normal_data","for datum in data:
    data_offset.append([d + min_datum for d in datum])",data_offset = [[d + min_datum for d in datum] for datum in data],data_offset = [[d + min_datum for d in datum] for datum in data],1,,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_unit_list_data)):
    goods_unit_list.append(goods_unit_list_data[i].goods_unit)",goods_unit_list += [goods_unit_list_data[i].goods_unit for i in range(len(goods_unit_list_data))],goods_unit_list = [goods_unit_list_data[i].goods_unit for i in range(len(goods_unit_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_class_list_data)):
    goods_class_list.append(goods_class_list_data[i].goods_class)",goods_class_list += [goods_class_list_data[i].goods_class for i in range(len(goods_class_list_data))],goods_class_list = [goods_class_list_data[i].goods_class for i in range(len(goods_class_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_brand_list_data)):
    goods_brand_list.append(goods_brand_list_data[i].goods_brand)",goods_brand_list += [data.goods_brand for data in goods_brand_list_data],goods_brand_list = [goods_brand_list_data[i].goods_brand for i in range(len(goods_brand_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_color_list_data)):
    goods_color_list.append(goods_color_list_data[i].goods_color)",goods_color_list += [goods_color_list_data[i].goods_color for i in range(len(goods_color_list_data))],goods_color_list = [goods_color_list_data[i].goods_color for i in range(len(goods_color_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_shape_list_data)):
    goods_shape_list.append(goods_shape_list_data[i].goods_shape)",goods_shape_list += [goods_shape_list_data[i].goods_shape for i in range(len(goods_shape_list_data))],goods_shape_list = [goods_shape_list_data[i].goods_shape for i in range(len(goods_shape_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_specs_list_data)):
    goods_specs_list.append(goods_specs_list_data[i].goods_specs)",goods_specs_list += [goods_specs_list_data[i].goods_specs for i in range(len(goods_specs_list_data))],goods_specs_list = [goods_specs_list_data[i].goods_specs for i in range(len(goods_specs_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(goods_origin_list_data)):
    goods_origin_list.append(goods_origin_list_data[i].goods_origin)",goods_origin_list += [data.goods_origin for data in goods_origin_list_data],goods_origin_list = [goods_origin_list_data[i].goods_origin for i in range(len(goods_origin_list_data))],0,1,,,,robosuite
GreaterWMS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GreaterWMS/goods/page.py,https://github.com/Singosgu/GreaterWMS/tree/master/goods/page.py,MyPageNumberPagination,get_paginated_response$21,"def get_paginated_response(self, data):
    goods_unit_list_data = goods_unit.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_unit_list = []
    for i in range(len(goods_unit_list_data)):
        goods_unit_list.append(goods_unit_list_data[i].goods_unit)
    goods_class_list_data = goods_class.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_class_list = []
    for i in range(len(goods_class_list_data)):
        goods_class_list.append(goods_class_list_data[i].goods_class)
    goods_brand_list_data = goods_brand.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_brand_list = []
    for i in range(len(goods_brand_list_data)):
        goods_brand_list.append(goods_brand_list_data[i].goods_brand)
    goods_color_list_data = goods_color.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_color_list = []
    for i in range(len(goods_color_list_data)):
        goods_color_list.append(goods_color_list_data[i].goods_color)
    goods_shape_list_data = goods_shape.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_shape_list = []
    for i in range(len(goods_shape_list_data)):
        goods_shape_list.append(goods_shape_list_data[i].goods_shape)
    goods_specs_list_data = goods_specs.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_specs_list = []
    for i in range(len(goods_specs_list_data)):
        goods_specs_list.append(goods_specs_list_data[i].goods_specs)
    goods_origin_list_data = goods_origin.objects.filter(openid=self.request.auth.openid, is_delete=False)
    goods_origin_list = []
    for i in range(len(goods_origin_list_data)):
        goods_origin_list.append(goods_origin_list_data[i].goods_origin)
    supplier_list_data = supplier.objects.filter(openid=self.request.auth.openid, is_delete=False)
    supplier_list = []
    for i in range(len(supplier_list_data)):
        supplier_list.append(supplier_list_data[i].supplier_name)
    return Response(OrderedDict([('goods_unit_list', goods_unit_list), ('goods_class_list', goods_class_list), ('goods_brand_list', goods_brand_list), ('goods_color_list', goods_color_list), ('goods_shape_list', goods_shape_list), ('goods_specs_list', goods_specs_list), ('goods_origin_list', goods_origin_list), ('supplier_list', supplier_list), ('count', self.page.paginator.count), ('next', self.get_next_link()), ('previous', self.get_previous_link()), ('results', data)]))","for i in range(len(supplier_list_data)):
    supplier_list.append(supplier_list_data[i].supplier_name)",supplier_list += [supplier_list_data[i].supplier_name for i in range(len(supplier_list_data))],supplier_list = [supplier_list_data[i].supplier_name for i in range(len(supplier_list_data))],0,1,,,,robosuite
OpsManage,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpsManage/dao/account.py,https://github.com/welliamcao/OpsManage/tree/master/dao/account.py,RolesManage,get_roles$245,"def get_roles(self):
    dataList = []
    for ds in Role.objects.all():
        dataList.append(ds.to_json())
    return dataList","for ds in Role.objects.all():
    dataList.append(ds.to_json())",dataList = [ds.to_json() for ds in Role.objects.all()],dataList = [ds.to_json() for ds in Role.objects.all()],1,,,,,robosuite
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/fis/autogate_grda_s1_trainer_callback.py,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/fis/autogate_grda_s1_trainer_callback.py,AutoGateGrdaS1TrainerCallback,after_valid$119,"def after_valid(self, logs=None):
    """"""Call after_valid of the managed callbacks.""""""
    self.model = self.trainer.model
    feature_interaction_score = self.model.get_feature_interaction_score()
    print('get feature_interaction_score', feature_interaction_score)
    feature_interaction = []
    for feature in feature_interaction_score:
        if abs(feature_interaction_score[feature]) > 0:
            feature_interaction.append(feature)
    print('get feature_interaction', feature_interaction)
    curr_auc = float(self.trainer.valid_metrics.results['auc'])
    if curr_auc > self.best_score:
        best_config = {'score': curr_auc, 'feature_interaction': feature_interaction}
        logging.info('BEST CONFIG IS\n{}'.format(best_config))
        pickle_result_file = FileOps.join_path(self.trainer.local_output_path, 'best_config.pickle')
        logging.info('Saved to {}'.format(pickle_result_file))
        FileOps.dump_pickle(best_config, pickle_result_file)
        self.best_score = curr_auc","for feature in feature_interaction_score:
    if abs(feature_interaction_score[feature]) > 0:
        feature_interaction.append(feature)",feature_interaction += [feature for feature in feature_interaction_score if abs(feature_interaction_score[feature]) > 0],feature_interaction = [feature for feature in feature_interaction_score if abs(feature_interaction_score[feature]) > 0],0,1,,,,robosuite
butterflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/butterflow/butterflow/mux.py,https://github.com/dthpham/butterflow/tree/master/butterflow/mux.py,,extract_audio$62,"def extract_audio(vid, dest, ss, to, speed=1.0):
    filename = os.path.splitext(os.path.basename(dest))[0]
    tempfile1 = os.path.join(settings['tempdir'], '{}.{}'.format(filename, settings['v_container']).lower())
    call = [settings['avutil'], '-loglevel', settings['av_loglevel'], '-y', '-i', vid, '-ss', str(ss / 1000.0), '-to', str(to / 1000.0), '-map_metadata', '-1', '-map_chapters', '-1', '-vn', '-sn']
    if settings['ca'] == 'aac':
        call.extend(['-strict', '-2'])
    call.extend(['-c:a', settings['ca'], '-b:a', settings['ba'], tempfile1])
    log.info('[Subprocess] Video chunk extraction')
    log.debug('Call: {}'.format(' '.join(call)))
    log.info('Extracting to:\t%s', os.path.basename(tempfile1))
    if subprocess.call(call) == 1:
        raise RuntimeError
    tempfile2 = os.path.join(settings['tempdir'], '{}.{}x.{}'.format(filename, speed, settings['a_container']))

    def solve_atempo_chain(speed):
        if speed >= 0.5 and speed <= 2.0:
            return [speed]

        def solve(speed, limit):
            vals = []
            x = int(math.log(speed) / math.log(limit))
            for i in range(x):
                vals.append(limit)
            y = float(speed) / math.pow(limit, x)
            vals.append(y)
            return vals
        if speed < 0.5:
            return solve(speed, 0.5)
        else:
            return solve(speed, 2.0)
    atempo_chain = solve_atempo_chain(speed)
    chain_string = ''
    chain = []
    for (i, tempo) in enumerate(atempo_chain):
        chain.append('atempo={}'.format(tempo))
        chain_string += str(tempo)
        if i < len(atempo_chain) - 1:
            chain_string += '*'
    log.info('Solved tempo chain for speed ({}x): {}'.format(speed, chain_string))
    call = [settings['avutil'], '-loglevel', settings['av_loglevel'], '-y', '-i', tempfile1, '-filter:a', ','.join(chain)]
    if settings['ca'] == 'aac':
        call.extend(['-strict', '-2'])
    call.extend(['-c:a', settings['ca'], '-b:a', settings['ba'], tempfile2])
    log.info('[Subprocess] Altering audio tempo')
    log.debug('Call: {}'.format(' '.join(call)))
    log.info('Writing to:\t%s', os.path.basename(tempfile2))
    if subprocess.call(call) == 1:
        raise RuntimeError
    log.info('Delete:\t%s', os.path.basename(tempfile1))
    os.remove(tempfile1)
    log.info('Moving:\t%s -> %s', os.path.basename(tempfile2), os.path.basename(dest))
    shutil.move(tempfile2, dest)","for i in range(x):
    vals.append(limit)",vals = [limit for i in range(x)],vals = [limit for i in range(x)],1,,,,,robosuite
WireViz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WireViz/src/wireviz/wv_bom.py,https://github.com/formatc1702/WireViz/tree/master/src/wireviz/wv_bom.py,,get_additional_component_bom$47,"def get_additional_component_bom(component: Union[Connector, Cable]) -> List[BOMEntry]:
    """"""Return a list of BOM entries with additional components.""""""
    bom_entries = []
    for part in component.additional_components:
        bom_entries.append({'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)})
    return bom_entries","for part in component.additional_components:
    bom_entries.append({'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)})","bom_entries = [{'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)} for part in component.additional_components]","bom_entries = [{'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)} for part in component.additional_components]",1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for cd in composite_data:
    composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))","composite_data_resolved = [abs_path_or_uri(cd, test_data_directory) for cd in composite_data]","composite_data_resolved = [abs_path_or_uri(cd, test_data_directory) for cd in composite_data]",1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for (i, item) in enumerate(value):
    dataset = replacement_item(item, force_to_file=True)
    collection_element = dataset.copy()
    collection_element['name'] = str(i)
    collection_element_identifiers.append(collection_element)","collection_element_identifiers = [{'name': str(i), **replacement_item(item, force_to_file=True)} for (i, item) in enumerate(value)]",Cannot refactor,-1,0,0,2,1,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for element in elements:
    if not is_nested_collection:
        dataset = replacement_item(element, force_to_file=True)
        collection_element = dataset.copy()
        collection_element['name'] = element['identifier']
        collection_element_identifiers.append(collection_element)
    else:
        sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
        collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
        collection_element_identifiers.append(collection_element)","collection_element_identifiers += [{'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)} if is_nested_collection else replacement_item(element, force_to_file=True).copy() | {'name': element['identifier']} for element in elements]",Cannot refactor,-1,1,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for entry in composite_data_raw:
    path = None
    if isinstance(entry, dict):
        path = entry.get('location', None) or entry.get('path', None)
    else:
        path = entry
    composite_data.append(path)","composite_data += [entry.get('location', None) or entry.get('path', None) if isinstance(entry, dict) else entry for entry in composite_data_raw]",Cannot refactor,-1,1,,,,robosuite
GAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GAM/src/gam/gapi/vault.py,https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/vault.py,,printExports$851,"def printExports():
    v = buildGAPIObject()
    todrive = False
    csvRows = []
    initialTitles = ['matterId', 'id', 'name', 'createTime', 'status']
    titles = initialTitles[:]
    matters = []
    matterIds = []
    i = 3
    while i < len(sys.argv):
        myarg = sys.argv[i].lower().replace('_', '')
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg in ['matter', 'matters']:
            matters = sys.argv[i + 1].split(',')
            i += 2
        else:
            controlflow.invalid_argument_exit(myarg, 'gam print exports')
    if not matters:
        fields = 'matters(matterId),nextPageToken'
        matters_results = gapi.get_all_pages(v.matters(), 'list', 'matters', view='BASIC', state='OPEN', fields=fields)
        for matter in matters_results:
            matterIds.append(matter['matterId'])
    else:
        for matter in matters:
            matterIds.append(getMatterItem(v, matter))
    for matterId in matterIds:
        sys.stderr.write(f'Retrieving exports for matter {matterId}\n')
        exports = gapi.get_all_pages(v.matters().exports(), 'list', 'exports', matterId=matterId)
        for export in exports:
            display.add_row_titles_to_csv_file(utils.flatten_json(export, flattened={'matterId': matterId}), csvRows, titles)
    display.sort_csv_titles(initialTitles, titles)
    display.write_csv_file(csvRows, titles, 'Vault Exports', todrive)","for matter in matters_results:
    matterIds.append(matter['matterId'])",matterIds = [matter['matterId'] for matter in matters_results],matterIds = [matter['matterId'] for matter in matters_results],1,,,,,robosuite
GAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GAM/src/gam/gapi/vault.py,https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/vault.py,,printExports$851,"def printExports():
    v = buildGAPIObject()
    todrive = False
    csvRows = []
    initialTitles = ['matterId', 'id', 'name', 'createTime', 'status']
    titles = initialTitles[:]
    matters = []
    matterIds = []
    i = 3
    while i < len(sys.argv):
        myarg = sys.argv[i].lower().replace('_', '')
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg in ['matter', 'matters']:
            matters = sys.argv[i + 1].split(',')
            i += 2
        else:
            controlflow.invalid_argument_exit(myarg, 'gam print exports')
    if not matters:
        fields = 'matters(matterId),nextPageToken'
        matters_results = gapi.get_all_pages(v.matters(), 'list', 'matters', view='BASIC', state='OPEN', fields=fields)
        for matter in matters_results:
            matterIds.append(matter['matterId'])
    else:
        for matter in matters:
            matterIds.append(getMatterItem(v, matter))
    for matterId in matterIds:
        sys.stderr.write(f'Retrieving exports for matter {matterId}\n')
        exports = gapi.get_all_pages(v.matters().exports(), 'list', 'exports', matterId=matterId)
        for export in exports:
            display.add_row_titles_to_csv_file(utils.flatten_json(export, flattened={'matterId': matterId}), csvRows, titles)
    display.sort_csv_titles(initialTitles, titles)
    display.write_csv_file(csvRows, titles, 'Vault Exports', todrive)","for matter in matters:
    matterIds.append(getMatterItem(v, matter))","matterIds += [getMatterItem(v, matter) for matter in matters]",Cannot refactor,-1,1,,,,robosuite
neural_sp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_sp/neural_sp/datasets/token_converter/character.py,https://github.com/hirofumi0810/neural_sp/tree/master/neural_sp/datasets/token_converter/character.py,Char2idx,__init__$19,"def __init__(self, dict_path, nlsyms=False, remove_space=False, remove_list=[]):
    self.remove_space = remove_space
    self.remove_list = remove_list
    self.token2idx = {'<blank>': 0}
    with codecs.open(dict_path, 'r', encoding='utf-8') as f:
        for line in f:
            (c, idx) = line.strip().split(' ')
            if c in remove_list:
                continue
            self.token2idx[c] = int(idx)
    self.vocab = len(self.token2idx.keys())
    self.nlsyms_list = []
    if nlsyms and os.path.isfile(nlsyms):
        with codecs.open(nlsyms, 'r', encoding='utf-8') as f:
            for line in f:
                self.nlsyms_list.append(line.strip())","for line in f:
    self.nlsyms_list.append(line.strip())",self.nlsyms_list = [line.strip() for line in f],self.nlsyms_list = [line.strip() for line in f],1,,,,,robosuite
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/lookups/handlers/dynamodb.py,https://github.com/cloudtools/stacker/tree/master/stacker/lookups/handlers/dynamodb.py,,_convert_ddb_list_to_list$180,"def _convert_ddb_list_to_list(conversion_list):
    """"""Given a dynamodb list, it will return a python list without the dynamodb
        datatypes

    Args:
        conversion_list (dict): a dynamodb list which includes the
            datatypes

    Returns:
        list: Returns a sanitized list without the dynamodb datatypes
    """"""
    ret_list = []
    for v in conversion_list:
        for v1 in v:
            ret_list.append(v[v1])
    return ret_list","for v in conversion_list:
    for v1 in v:
        ret_list.append(v[v1])",ret_list = [v[v1] for v in conversion_list for v1 in v],ret_list = [v[v1] for v in conversion_list for v1 in v],1,,,,,robosuite
GyoiThon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GyoiThon/modules/Gyoi_CloudChecker.py,https://github.com/gyoisamurai/GyoiThon/tree/master/modules/Gyoi_CloudChecker.py,CloudChecker,check_azure$101,"def check_azure(self, ip_addr):
    self.utility.print_message(NOTE, 'Check Azure IP range.')
    msg = self.utility.make_log_msg(self.utility.log_in, self.utility.log_dis, self.file_name, action=self.action_name, note='Check Azure IP range.', dest=self.utility.target_host)
    self.utility.write_log(20, msg)
    self.utility.write_log(20, 'Accessing : {}'.format(self.azure_ip_range))
    (res, _, _, _, encoding) = self.utility.send_request('GET', self.azure_ip_range)
    soup = BeautifulSoup(res.data.decode(encoding).lower(), 'lxml')
    regions = soup.find_all('region')
    target_ip = ipaddress.ip_address(ip_addr)
    for (idx, region) in enumerate(regions):
        azure_nw_addres = []
        region_name = region.attrs['name']
        for content in region.contents:
            if content.name == 'iprange':
                azure_nw_addres.append(content['subnet'])
        for azure_nw_addr in azure_nw_addres:
            if target_ip in ipaddress.ip_network(azure_nw_addr):
                msg = 'Detect : service=Azure target={} prefix={} region={}'.format(target_ip, azure_nw_addr, region_name)
                self.utility.print_message(OK, msg)
                msg = self.utility.make_log_msg(self.utility.log_mid, self.utility.log_dis, self.file_name, action=self.action_name, note=msg, dest=self.utility.target_host)
                self.utility.write_log(20, msg)
                msg = self.utility.make_log_msg(self.utility.log_out, self.utility.log_dis, self.file_name, action=self.action_name, note='Check Azure IP range', dest=self.utility.target_host)
                self.utility.write_log(20, msg)
                return True
            else:
                self.utility.print_message(FAIL, 'Not include : service=Azure target={} prefix={}'.format(target_ip, azure_nw_addr))
    msg = self.utility.make_log_msg(self.utility.log_out, self.utility.log_dis, self.file_name, action=self.action_name, note='Check Azure IP range', dest=self.utility.target_host)
    self.utility.write_log(20, msg)
    return False","for content in region.contents:
    if content.name == 'iprange':
        azure_nw_addres.append(content['subnet'])",azure_nw_addres = [content['subnet'] for content in region.contents if content.name == 'iprange'],azure_nw_addres = [content['subnet'] for content in region.contents if content.name == 'iprange'],1,,,,,robosuite
textfsm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textfsm/textfsm/texttable.py,https://github.com/google/textfsm/tree/master/textfsm/texttable.py,Row,_ToStr$241,"def _ToStr(value):
    """"""Convert individul list entries to string.""""""
    if isinstance(value, (list, tuple)):
        result = []
        for val in value:
            result.append(str(val))
        return result
    else:
        return str(value)","for val in value:
    result.append(str(val))",result = [str(val) for val in value],result = [str(val) for val in value],1,,,,,robosuite
ludwig,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ludwig/ludwig/data/dataset_synthesizer.py,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/data/dataset_synthesizer.py,,generate_text$231,"def generate_text(feature):
    length = feature.get('max_len', 10)
    text = []
    for _ in range(random.randint(length - int(length * 0.2), length)):
        text.append(random.choice(feature['idx2str']))
    return ' '.join(text)","for _ in range(random.randint(length - int(length * 0.2), length)):
    text.append(random.choice(feature['idx2str']))","text = [random.choice(feature['idx2str']) for _ in range(random.randint(length - int(length * 0.2), length))]","text = [random.choice(feature['idx2str']) for _ in range(random.randint(length - int(length * 0.2), length))]",1,,,,,robosuite
novelWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/novelWriter/novelwriter/dialogs/docmerge.py,https://github.com/vkbo/novelWriter/tree/master/novelwriter/dialogs/docmerge.py,GuiDocMerge,_doMerge$93,"def _doMerge(self):
    """"""Perform the merge of the files in the selected folder, and
        create a new file in the same parent folder. The old files are
        not removed in the merge process, and must be deleted manually.
        """"""
    logger.verbose('GuiDocMerge merge button clicked')
    finalOrder = []
    for i in range(self.listBox.count()):
        finalOrder.append(self.listBox.item(i).data(Qt.UserRole))
    if len(finalOrder) == 0:
        self.theParent.makeAlert(self.tr('No source documents found. Nothing to do.'), nwAlert.ERROR)
        return False
    theText = ''
    for tHandle in finalOrder:
        inDoc = NWDoc(self.theProject, tHandle)
        docText = inDoc.readDocument()
        docErr = inDoc.getError()
        if docText is None and docErr:
            self.theParent.makeAlert([self.tr('Failed to open document file.'), docErr], nwAlert.ERROR)
        if docText:
            theText += docText.rstrip('\n') + '\n\n'
    if self.sourceItem is None:
        self.theParent.makeAlert(self.tr('No source folder selected. Nothing to do.'), nwAlert.ERROR)
        return False
    srcItem = self.theProject.projTree[self.sourceItem]
    if srcItem is None:
        self.theParent.makeAlert(self.tr('Internal error.'), nwAlert.ERROR)
        return False
    nHandle = self.theProject.newFile(srcItem.itemName, srcItem.itemClass, srcItem.itemParent)
    newItem = self.theProject.projTree[nHandle]
    newItem.setStatus(srcItem.itemStatus)
    outDoc = NWDoc(self.theProject, nHandle)
    if not outDoc.writeDocument(theText):
        self.theParent.makeAlert([self.tr('Could not save document.'), outDoc.getError()], nwAlert.ERROR)
        return False
    self.theParent.treeView.revealNewTreeItem(nHandle)
    self.theParent.openDocument(nHandle, doScroll=True)
    self._doClose()
    return True","for i in range(self.listBox.count()):
    finalOrder.append(self.listBox.item(i).data(Qt.UserRole))",finalOrder = [self.listBox.item(i).data(Qt.UserRole) for i in range(self.listBox.count())],finalOrder = [self.listBox.item(i).data(Qt.UserRole) for i in range(self.listBox.count())],1,,,,,robosuite
BBN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BBN/lib/dataset/imbalance_cifar.py,https://github.com/Megvii-Nanjing/BBN/tree/master/lib/dataset/imbalance_cifar.py,IMBALANCECIFAR10,get_cls_num_list$171,"def get_cls_num_list(self):
    cls_num_list = []
    for i in range(self.cls_num):
        cls_num_list.append(self.num_per_cls_dict[i])
    return cls_num_list","for i in range(self.cls_num):
    cls_num_list.append(self.num_per_cls_dict[i])",cls_num_list = [self.num_per_cls_dict[i] for i in range(self.cls_num)],cls_num_list = [self.num_per_cls_dict[i] for i in range(self.cls_num)],1,,,,,robosuite
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/utils/dist/dist_common.py,https://github.com/poodarchu/Det3D/tree/master/det3d/utils/dist/dist_common.py,,all_gather$48,"def all_gather(data):
    """"""
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    """"""
    world_size = get_world_size()
    if world_size == 1:
        return [data]
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to('cuda')
    local_size = torch.IntTensor([tensor.numel()]).to('cuda')
    size_list = [torch.IntTensor([0]).to('cuda') for _ in range(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.ByteTensor(size=(max_size,)).to('cuda'))
    if local_size != max_size:
        padding = torch.ByteTensor(size=(max_size - local_size,)).to('cuda')
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list","for _ in size_list:
    tensor_list.append(torch.ByteTensor(size=(max_size,)).to('cuda'))","tensor_list = [torch.ByteTensor(size=(max_size,)).to('cuda') for _ in size_list]","tensor_list = [torch.ByteTensor(size=(max_size,)).to('cuda') for _ in size_list]",1,,,,,robosuite
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/utils/dist/dist_common.py,https://github.com/poodarchu/Det3D/tree/master/det3d/utils/dist/dist_common.py,,all_gather$48,"def all_gather(data):
    """"""
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    """"""
    world_size = get_world_size()
    if world_size == 1:
        return [data]
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to('cuda')
    local_size = torch.IntTensor([tensor.numel()]).to('cuda')
    size_list = [torch.IntTensor([0]).to('cuda') for _ in range(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.ByteTensor(size=(max_size,)).to('cuda'))
    if local_size != max_size:
        padding = torch.ByteTensor(size=(max_size - local_size,)).to('cuda')
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list","for (size, tensor) in zip(size_list, tensor_list):
    buffer = tensor.cpu().numpy().tobytes()[:size]
    data_list.append(pickle.loads(buffer))","data_list = [pickle.loads(tensor.cpu().numpy().tobytes()[:size]) for (size, tensor) in zip(size_list, tensor_list)]",Cannot refactor,-1,1,,,,robosuite
Minecraft-Overviewer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Minecraft-Overviewer/overviewer.py,https://github.com/overviewer/Minecraft-Overviewer/tree/master//overviewer.py,,main$52,"def main():
    logger.configure()
    if os.name == 'posix':
        if os.geteuid() == 0:
            logging.warning('You are running Overviewer as root. It is recommended that you never do this, as it is dangerous for your system. If you are running into permission errors, fix your file/directory permissions instead. Overviewer does not need access to critical system resources and therefore does not require root access.')
        try:
            with open('/etc/redhat-release', 'r') as release_f:
                rel_contents = release_f.read()
                try:
                    major_rel = re.search('\\d(\\.\\d+)?', rel_contents).group(0).split('.')[0]
                    if major_rel == '6':
                        logging.warning('We will be dropping support for this release of your distribution soon. Please upgrade as soon as possible, or you will not receive future Overviewer updates.')
                except AttributeError:
                    pass
        except IOError:
            pass
    try:
        cpus = multiprocessing.cpu_count()
    except NotImplementedError:
        cpus = 1
    avail_north_dirs = ['lower-left', 'upper-left', 'upper-right', 'lower-right', 'auto']
    parser = ArgumentParser(usage=helptext)
    parser.add_argument('-c', '--config', dest='config', action='store', help='Specify the config file to use.')
    parser.add_argument('-p', '--processes', dest='procs', action='store', type=int, help='The number of local worker processes to spawn. Defaults to the number of CPU cores your computer has.')
    parser.add_argument('--pid', dest='pid', action='store', help='Specify the pid file to use.')
    parser.add_argument('--rendermodes', dest='rendermodes', action='store', help=""If you're not using a config file, specify which rendermodes to render with this option. This is a comma-separated list."")
    parser.add_argument('world', nargs='?', help='Path or name of the world you want to render.')
    parser.add_argument('output', nargs='?', help='Output directory for the rendered map.')
    render_modifiers = parser.add_mutually_exclusive_group()
    render_modifiers.add_argument('--forcerender', dest='forcerender', action='store_true', help='Force re-render the entire map.')
    render_modifiers.add_argument('--check-tiles', dest='checktiles', action='store_true', help='Check each tile on disk and re-render old tiles.')
    render_modifiers.add_argument('--no-tile-checks', dest='notilechecks', action='store_true', help='Only render tiles that come from chunks that have changed since the last render (the default).')
    parser.add_argument('--check-terrain', dest='check_terrain', action='store_true', help='Try to locate the texture files. Useful for debugging texture problems.')
    parser.add_argument('-V', '--version', dest='version', help='Display version information and then exits.', action='store_true')
    parser.add_argument('--check-version', dest='checkversion', help='Fetch information about the latest version of Overviewer.', action='store_true')
    parser.add_argument('--update-web-assets', dest='update_web_assets', action='store_true', help='Update web assets. Will *not* render tiles or update overviewerConfig.js.')
    parser.add_argument('-q', '--quiet', dest='quiet', action='count', default=0, help='Print less output. You can specify this option multiple times.')
    parser.add_argument('-v', '--verbose', dest='verbose', action='count', default=0, help='Print more output. You can specify this option multiple times.')
    parser.add_argument('--simple-output', dest='simple', action='store_true', default=False, help='Use a simple output format, with no colors or progress bars.')
    exegroup = parser.add_argument_group('Other Scripts', 'These scripts may accept different arguments than the ones listed above.')
    exegroup.add_argument('--genpoi', dest='genpoi', action='store_true', help='Run the genPOI script.')
    exegroup.add_argument('--skip-scan', dest='skipscan', action='store_true', help=""When running GenPOI, don't scan for entities."")
    exegroup.add_argument('--skip-players', dest='skipplayers', action='store_true', help=""When running GenPOI, don't scan player data."")
    (args, unknowns) = parser.parse_known_args()
    if len(unknowns) > 0 and args.world and args.output:
        possible_mistakes = []
        for i in range(len(unknowns) + 1):
            possible_mistakes.append(' '.join([args.world, args.output] + unknowns[:i]))
            possible_mistakes.append(' '.join([args.output] + unknowns[:i]))
        for mistake in possible_mistakes:
            if os.path.exists(mistake):
                logging.warning('Looks like you tried to make me use {0} as an argument, but forgot to quote the argument correctly. Try using ""{0}"" instead if the spaces are part of the path.'.format(mistake))
                parser.error('Too many arguments.')
        parser.error('Too many arguments.')
    if args.genpoi:
        sys.argv.remove('--genpoi')
        g = __import__('overviewer_core.aux_files', {}, {}, ['genPOI'])
        g.genPOI.main()
        return 0
    logger.configure(logging.INFO + 10 * args.quiet - 10 * args.verbose, verbose=args.verbose > 0, simple=args.simple)
    if args.version:
        print('Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            import overviewer_core.overviewer_version as overviewer_version
            print('built on %s' % overviewer_version.BUILD_DATE)
            if args.verbose > 0:
                print('Build machine: %s %s' % (overviewer_version.BUILD_PLATFORM, overviewer_version.BUILD_OS))
                print('Read version information from %r' % overviewer_version.__file__)
        except ImportError:
            print('(build info not found)')
        if args.verbose > 0:
            print('Python executable: %r' % sys.executable)
            print(sys.version)
        if not args.checkversion:
            return 0
    if args.checkversion:
        print('Currently running Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            from urllib import request
            import json
            latest_ver = json.loads(request.urlopen('http://overviewer.org/download.json').read())['src']
            print('Latest version of Minecraft Overviewer %s (%s)' % (latest_ver['version'], latest_ver['commit'][:7]))
            print('See https://overviewer.org/downloads for more information.')
        except Exception:
            print('Failed to fetch latest version info.')
            if args.verbose > 0:
                import traceback
                traceback.print_exc()
            else:
                print('Re-run with --verbose for more details.')
            return 1
        return 0
    if args.pid:
        if os.path.exists(args.pid):
            try:
                with open(args.pid, 'r') as fpid:
                    pid = int(fpid.read())
                    if util.pid_exists(pid):
                        print('Overviewer is already running (pid exists) - exiting.')
                        return 0
            except (IOError, ValueError):
                pass
        with open(args.pid, 'w') as f:
            f.write(str(os.getpid()))
    if args.check_terrain and (not args.config):
        import hashlib
        from overviewer_core.textures import Textures
        tex = Textures()
        logging.info('Looking for a few common texture files...')
        try:
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/acacia_planks.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/ancient_debris_top.png', verbose=True)
        except IOError:
            logging.error('Could not find any texture files.')
            return 1
        return 0
    if not (args.world and args.output) and (not args.config):
        if util.is_bare_console():
            print('\n')
            print('The Overviewer is a console program.  Please open a Windows command prompt')
            print('first and run Overviewer from there.   Further documentation is available at')
            print('http://docs.overviewer.org/\n')
            print('\n')
            print('For a quick-start guide on Windows, visit the following URL:\n')
            print('http://docs.overviewer.org/en/latest/win_tut/windowsguide/\n')
        else:
            logging.error('You must either specify --config or give me a world directory and output directory.')
            parser.print_help()
            list_worlds()
        return 1
    if args.config and (args.world and args.output):
        print()
        print('If you specify --config, you need to specify the world to render as well as the destination in the config file, not on the command line.')
        print('Put something like this in your config file:')
        print(""worlds['myworld'] = %r"" % args[0])
        print('outputdir = %r' % (args[1] if len(args) > 1 else '/path/to/output'))
        print()
        logging.error('You cannot specify both --config AND a world + output directory on the command line.')
        parser.print_help()
        return 1
    if not args.config and (args.world or args.output) and (not (args.world and args.output)):
        logging.error('You must specify both the world directory and an output directory')
        parser.print_help()
        return 1
    mw_parser = config_parser.MultiWorldParser()
    if not args.config:
        (worldpath, destdir) = map(os.path.expanduser, [args.world, args.output])
        logging.debug('Using %r as the world directory', worldpath)
        logging.debug('Using %r as the output directory', destdir)
        mw_parser.set_config_item('worlds', {'world': worldpath})
        mw_parser.set_config_item('outputdir', destdir)
        rendermodes = ['lighting']
        if args.rendermodes:
            rendermodes = args.rendermodes.replace('-', '_').split(',')
        renders = OrderedDict()
        for rm in rendermodes:
            renders['world-' + rm] = {'world': 'world', 'title': 'Overviewer Render (%s)' % rm, 'rendermode': rm}
        mw_parser.set_config_item('renders', renders)
    else:
        if args.rendermodes:
            logging.error('You cannot specify --rendermodes if you give a config file. Configure your rendermodes in the config file instead.')
            parser.print_help()
            return 1
        try:
            mw_parser.parse(os.path.expanduser(args.config))
        except config_parser.MissingConfigException as e:
            logging.error(str(e))
            util.nice_exit(1)
    if args.procs:
        mw_parser.set_config_item('processes', args.procs)
    try:
        config = mw_parser.get_validated_config()
    except Exception as ex:
        if args.verbose:
            logging.exception('An error was encountered with your configuration. See the information below.')
        else:
            logging.error('An error was encountered with your configuration.')
            logging.error(str(ex))
        return 1
    if args.check_terrain:
        logging.info('Looking for a few common texture files...')
        for (render_name, render) in config['renders'].items():
            logging.info('Looking at render %r.', render_name)
            texopts = util.dict_subset(render, ['texturepath'])
            tex = textures.Textures(**texopts)
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/oak_planks.png', verbose=True)
        return 0
    logging.info('Welcome to Minecraft Overviewer version %s (%s)!' % (util.findGitVersion(), util.findGitHash()[:7]))
    logging.debug('Current log level: {0}.'.format(logging.getLogger().level))

    def set_renderchecks(checkname, num):
        for (name, render) in config['renders'].items():
            if render.get('renderchecks', 0) == 3:
                logging.warning(checkname + ' ignoring render ' + repr(name) + ' since it\'s marked as ""don\'t render"".')
            else:
                render['renderchecks'] = num
    if args.forcerender:
        logging.info('Forcerender mode activated. ALL tiles will be rendered.')
        set_renderchecks('forcerender', 2)
    elif args.checktiles:
        logging.info('Checking all tiles for updates manually.')
        set_renderchecks('checktiles', 1)
    elif args.notilechecks:
        logging.info('Disabling all tile mtime checks. Only rendering tiles that need updating since last render.')
        set_renderchecks('notilechecks', 0)
    if not config['renders']:
        logging.error(""You must specify at least one render in your config file. Check the documentation at http://docs.overviewer.org if you're having trouble."")
        return 1
    for (rname, render) in config['renders'].items():
        try:
            worldpath = config['worlds'][render['world']]
        except KeyError:
            logging.error(""Render %s's world is '%s', but I could not find a corresponding entry in the worlds dictionary."", rname, render['world'])
            return 1
        render['worldname_orig'] = render['world']
        render['world'] = worldpath
        if render.get('forcerender', False):
            render['renderchecks'] = 2
        if render.get('overlay', []) != []:
            for x in render.get('overlay'):
                if x != rname:
                    try:
                        renderLink = config['renders'][x]
                    except KeyError:
                        logging.error(""Render %s's overlay is '%s', but I could not find a corresponding entry in the renders dictionary."", rname, x)
                        return 1
                else:
                    logging.error(""Render %s's overlay contains itself."", rname)
                    return 1
    destdir = config['outputdir']
    if not destdir:
        logging.error('You must specify the output directory in your config file.')
        logging.error(""e.g. outputdir = '/path/to/outputdir'"")
        return 1
    if not os.path.exists(destdir):
        try:
            os.mkdir(destdir)
        except OSError:
            logging.exception('Could not create the output directory.')
            return 1
    assetMrg = assetmanager.AssetManager(destdir, config.get('customwebassets', None))
    if args.update_web_assets:
        assetMrg.output_noconfig()
        logging.info('Web assets have been updated.')
        return 0
    changelists = {}
    for render in config['renders'].values():
        if 'changelist' in render:
            path = render['changelist']
            if path not in changelists:
                out = open(path, 'w')
                logging.debug('Opening changelist %s (%s).', out, out.fileno())
                changelists[path] = out
            else:
                out = changelists[path]
            render['changelist'] = out.fileno()
    tilesets = []
    worldcache = {}
    texcache = {}
    caches = []
    caches.append(cache.LRUCache(size=100))
    renders = config['renders']
    for (render_name, render) in renders.items():
        logging.debug('Found the following render thing: %r', render)
        try:
            w = worldcache[render['world']]
        except KeyError:
            try:
                w = world.World(render['world'])
            except CorruptNBTError as e:
                logging.error('Failed to open world %r.', render['world'])
                raise e
            except world.UnsupportedVersion as e:
                for ln in str(e).split('\n'):
                    logging.error(ln)
                sys.exit(1)
            worldcache[render['world']] = w
        texopts = util.dict_subset(render, ['texturepath', 'bgcolor', 'northdirection'])
        texopts_key = tuple(texopts.items())
        if texopts_key not in texcache:
            tex = textures.Textures(**texopts)
            logging.info('Generating textures...')
            tex.generate()
            logging.debug('Finished generating textures.')
            texcache[texopts_key] = tex
        else:
            tex = texcache[texopts_key]
        try:
            logging.debug('Asking for regionset %r.' % render['dimension'][1])
            rset = w.get_regionset(render['dimension'][1])
        except IndexError:
            logging.error(""Sorry, I can't find anything to render!  Are you sure there are .mca files in the world directory of %s?"" % render['world'])
            return 1
        if rset is None:
            logging.warning(""Sorry, you requested dimension '%s' for %s, but I couldn't find it."", render['dimension'][0], render_name)
            continue
        rset = world.CachedRegionSet(rset, caches)
        if 'crop' in render:
            rsets = []
            for zone in render['crop']:
                rsets.append(world.CroppedRegionSet(rset, *zone))
        else:
            rsets = [rset]
        if render['northdirection'] > 0:
            newrsets = []
            for r in rsets:
                r = world.RotatedRegionSet(r, render['northdirection'])
                newrsets.append(r)
            rsets = newrsets
        tileset_dir = os.path.abspath(os.path.join(destdir, render_name))
        render['name'] = render_name
        tileSetOpts = util.dict_subset(render, ['name', 'imgformat', 'renderchecks', 'rerenderprob', 'bgcolor', 'defaultzoom', 'imgquality', 'imglossless', 'optimizeimg', 'rendermode', 'worldname_orig', 'title', 'dimension', 'changelist', 'showspawn', 'overlay', 'base', 'poititle', 'maxzoom', 'showlocationmarker', 'minzoom', 'center'])
        tileSetOpts.update({'spawn': w.find_true_spawn()})
        for rset in rsets:
            tset = tileset.TileSet(w, rset, assetMrg, tex, tileSetOpts, tileset_dir)
            tilesets.append(tset)
    if not tilesets:
        logging.error(""There are no tilesets to render! There's nothing to do, so exiting."")
        return 1
    logging.info('Preprocessing...')
    for ts in tilesets:
        ts.do_preprocessing()
    assetMrg.initialize(tilesets)
    if config['processes'] == 1:
        dispatch = dispatcher.Dispatcher()
    else:
        dispatch = dispatcher.MultiprocessingDispatcher(local_procs=config['processes'])
    dispatch.render_all(tilesets, config['observer'])
    dispatch.close()
    assetMrg.finalize(tilesets)
    for out in changelists.values():
        logging.debug('Closing %s (%s).', out, out.fileno())
        out.close()
    if config['processes'] == 1:
        logging.debug('Final cache stats:')
        for c in caches:
            logging.debug('\t%s: %s hits, %s misses', c.__class__.__name__, c.hits, c.misses)
    if args.pid:
        os.remove(args.pid)
    logging.info(""Your render has been written to '%s', open index.html to view it."" % destdir)
    return 0","for i in range(len(unknowns) + 1):
    possible_mistakes.append(' '.join([args.world, args.output] + unknowns[:i]))
    possible_mistakes.append(' '.join([args.output] + unknowns[:i]))","possible_mistakes = [' '.join([args.world, args.output] + unknowns[:i]) for i in range(len(unknowns) + 1)] + [' '.join([args.output] + unknowns[:i]) for i in range(len(unknowns) + 1)]",Cannot refactor,-1,0,,2,1,robosuite
cfn-lint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cfn-lint/scripts/update_specs_services_from_ssm.py,https://github.com/aws-cloudformation/cfn-lint/tree/master/scripts/update_specs_services_from_ssm.py,,get_regions_for_service$127,"def get_regions_for_service(service):
    """""" get regions for a service """"""
    LOGGER.info('Get the regions for service %s', service)
    results = []
    paginator = client.get_paginator('get_parameters_by_path')
    page_iterator = paginator.paginate(Path='/aws/service/global-infrastructure/services/{}/regions'.format(service))
    for page in page_iterator:
        for region in page.get('Parameters'):
            results.append(region.get('Value'))
    return results","for page in page_iterator:
    for region in page.get('Parameters'):
        results.append(region.get('Value'))",results = [region.get('Value') for page in page_iterator for region in page.get('Parameters')],results = [region.get('Value') for page in page_iterator for region in page.get('Parameters')],1,,,,,robosuite
inthe.am,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/inthe.am/inthe_am/taskmanager/models/taskstore.py,https://github.com/coddingtonbear/inthe.am/tree/master/inthe_am/taskmanager/models/taskstore.py,TaskStore,get_blocks_for_task$208,"def get_blocks_for_task(self, task):
    if not hasattr(self, '_blocks'):
        self._blocks = self.client.filter_tasks({'depends.not': '', 'or': [('status', 'pending'), ('status', 'waiting')]})
    blocks = []
    for other in self._blocks:
        if task['uuid'] in other.get('depends', ''):
            blocks.append(other['uuid'])
    return blocks","for other in self._blocks:
    if task['uuid'] in other.get('depends', ''):
        blocks.append(other['uuid'])","blocks += [other['uuid'] for other in self._blocks if task['uuid'] in other.get('depends', '')]","blocks = [other['uuid'] for other in self._blocks if task['uuid'] in other.get('depends', '')]",0,1,,,,robosuite
crnn_ctc_ocr_tf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crnn_ctc_ocr_tf/tools/train_crnn_ctc.py,https://github.com/bai-shang/crnn_ctc_ocr_tf/tree/master/tools/train_crnn_ctc.py,,_sparse_matrix_to_list$63,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict=None):
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    if char_map_dict is None:
        char_map_dict = json.load(open(FLAGS.char_map_json_file, 'r'))
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","for row in dense_matrix:
    string = []
    for val in row:
        string.append(_int_to_string(val, char_map_dict))
    string_list.append(''.join((s for s in string if s != '*')))","string_list = [''.join([_int_to_string(val, char_map_dict) for val in row if _int_to_string(val, char_map_dict) != '*']) for row in dense_matrix]",Cannot refactor,-1,1,,,,robosuite
crnn_ctc_ocr_tf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crnn_ctc_ocr_tf/tools/train_crnn_ctc.py,https://github.com/bai-shang/crnn_ctc_ocr_tf/tree/master/tools/train_crnn_ctc.py,,_sparse_matrix_to_list$63,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict=None):
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    if char_map_dict is None:
        char_map_dict = json.load(open(FLAGS.char_map_json_file, 'r'))
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","for val in row:
    string.append(_int_to_string(val, char_map_dict))","string += [_int_to_string(val, char_map_dict) for val in row]","string = [_int_to_string(val, char_map_dict) for val in row]",0,1,,,,robosuite
trankit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trankit/trankit/adapter_transformers/tokenization_bert.py,https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/tokenization_bert.py,BertTokenizer,_tokenize$207,"def _tokenize(self, text):
    split_tokens = []
    if self.do_basic_tokenize:
        for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):
            for sub_token in self.wordpiece_tokenizer.tokenize(token):
                split_tokens.append(sub_token)
    else:
        split_tokens = self.wordpiece_tokenizer.tokenize(text)
    return split_tokens","for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):
    for sub_token in self.wordpiece_tokenizer.tokenize(token):
        split_tokens.append(sub_token)","split_tokens = [sub_token for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens) for sub_token in self.wordpiece_tokenizer.tokenize(token)]","split_tokens = [sub_token for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens) for sub_token in self.wordpiece_tokenizer.tokenize(token)]",1,,,,,robosuite
stable-baselines3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/logger.py,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/logger.py,,read_json$663,"def read_json(filename: str) -> pandas.DataFrame:
    """"""
    read a json file using pandas

    :param filename: the file path to read
    :return: the data in the json
    """"""
    data = []
    with open(filename) as file_handler:
        for line in file_handler:
            data.append(json.loads(line))
    return pandas.DataFrame(data)","for line in file_handler:
    data.append(json.loads(line))",data = [json.loads(line) for line in file_handler],data = [json.loads(line) for line in file_handler],1,,,,,robosuite
platform-espressif32,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/platform-espressif32/builder/frameworks/espidf.py,https://github.com/platformio/platform-espressif32/tree/master/builder/frameworks/espidf.py,,load_component_paths$422,"def load_component_paths(framework_components_dir, ignored_component_prefixes=None):

    def _scan_components_from_framework():
        result = []
        for component in os.listdir(framework_components_dir):
            component_path = os.path.join(framework_components_dir, component)
            if component.startswith(ignored_component_prefixes) or not os.path.isdir(component_path):
                continue
            result.append(component_path)
        return result
    components = []
    ignored_component_prefixes = ignored_component_prefixes or []
    project_description_file = os.path.join(BUILD_DIR, 'project_description.json')
    if os.path.isfile(project_description_file):
        with open(project_description_file) as fp:
            try:
                data = json.load(fp)
                for path in data.get('build_component_paths', []):
                    if not os.path.basename(path).startswith(ignored_component_prefixes):
                        components.append(path)
            except:
                print('Warning: Could not find load components from project description!\n')
    return components or _scan_components_from_framework()","for component in os.listdir(framework_components_dir):
    component_path = os.path.join(framework_components_dir, component)
    if component.startswith(ignored_component_prefixes) or not os.path.isdir(component_path):
        continue
    result.append(component_path)","result = [os.path.join(framework_components_dir, component) for component in os.listdir(framework_components_dir) if not component.startswith(ignored_component_prefixes) and os.path.isdir(os.path.join(framework_components_dir, component))]",Cannot refactor,-1,1,,,,robosuite
platform-espressif32,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/platform-espressif32/builder/frameworks/espidf.py,https://github.com/platformio/platform-espressif32/tree/master/builder/frameworks/espidf.py,,load_component_paths$422,"def load_component_paths(framework_components_dir, ignored_component_prefixes=None):

    def _scan_components_from_framework():
        result = []
        for component in os.listdir(framework_components_dir):
            component_path = os.path.join(framework_components_dir, component)
            if component.startswith(ignored_component_prefixes) or not os.path.isdir(component_path):
                continue
            result.append(component_path)
        return result
    components = []
    ignored_component_prefixes = ignored_component_prefixes or []
    project_description_file = os.path.join(BUILD_DIR, 'project_description.json')
    if os.path.isfile(project_description_file):
        with open(project_description_file) as fp:
            try:
                data = json.load(fp)
                for path in data.get('build_component_paths', []):
                    if not os.path.basename(path).startswith(ignored_component_prefixes):
                        components.append(path)
            except:
                print('Warning: Could not find load components from project description!\n')
    return components or _scan_components_from_framework()","for path in data.get('build_component_paths', []):
    if not os.path.basename(path).startswith(ignored_component_prefixes):
        components.append(path)","components = [path for path in data.get('build_component_paths', []) if not os.path.basename(path).startswith(ignored_component_prefixes)]","components = [path for path in data.get('build_component_paths', []) if not os.path.basename(path).startswith(ignored_component_prefixes)]",1,,,,,robosuite
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/scanner/scanners/forwarding_rule_rules_scanner_test.py,https://github.com/forseti-security/forseti-security/tree/master/tests/scanner/scanners/forwarding_rule_rules_scanner_test.py,ForwardingRuleScannerTest,test_forwarding_rules_scanner_all_match$31,"def test_forwarding_rules_scanner_all_match(self):
    rules_local_path = get_datafile_path(__file__, 'forward_rule_test_1.yaml')
    scanner = forwarding_rule_scanner.ForwardingRuleScanner({}, {}, mock.MagicMock(), '', '', rules_local_path)
    project_id = 'abc-123'
    gcp_forwarding_rules_resource_data = [{'id': '46', 'creationTimestamp': '2017-06-01 04:19:37', 'name': 'abc-123', 'description': '', 'region': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1', 'IPAddress': '198.51.100.99', 'IPProtocol': 'UDP', 'portRange': '4500-4500', 'ports': [], 'target': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123', 'loadBalancingScheme': 'EXTERNAL'}, {'id': '23', 'creationTimestamp': '2017-06-01 04:19:37', 'name': 'abc-123', 'description': '', 'region': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1', 'IPAddress': '198.51.100.23', 'IPProtocol': 'TCP', 'ports': [8080], 'target': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123', 'loadBalancingScheme': 'INTERNAL'}, {'id': '46', 'creationTimestamp': '2017-06-01 04:19:37', 'name': 'abc-123', 'description': '', 'region': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1', 'IPAddress': '198.51.100.46', 'IPProtocol': 'ESP', 'ports': [], 'target': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123', 'loadBalancingScheme': 'EXTERNAL'}, {'id': '46', 'creationTimestamp': '2017-06-01 04:19:37', 'name': 'abc-123', 'description': '', 'region': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1', 'IPAddress': '198.51.100.35', 'IPProtocol': 'TCP', 'portRange': '4500-4500', 'target': 'https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123', 'loadBalancingScheme': 'EXTERNAL'}]
    gcp_forwarding_rules_resource_objs = []
    for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data:
        gcp_forwarding_rules_resource_objs.append(fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data))
    violations = scanner._find_violations(gcp_forwarding_rules_resource_objs)
    self.assertEqual(0, len(violations))","for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data:
    gcp_forwarding_rules_resource_objs.append(fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data))","gcp_forwarding_rules_resource_objs = [fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data) for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data]","gcp_forwarding_rules_resource_objs = [fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data) for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data]",1,,,,,robosuite
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/bindiff.py,https://github.com/angr/angr/tree/master/angr/analyses/bindiff.py,FunctionDiff,identical_blocks$380,"def identical_blocks(self):
    """"""
        :returns: A list of block matches which appear to be identical
        """"""
    identical_blocks = []
    for (block_a, block_b) in self._block_matches:
        if self.blocks_probably_identical(block_a, block_b):
            identical_blocks.append((block_a, block_b))
    return identical_blocks","for (block_a, block_b) in self._block_matches:
    if self.blocks_probably_identical(block_a, block_b):
        identical_blocks.append((block_a, block_b))","identical_blocks = [(block_a, block_b) for (block_a, block_b) in self._block_matches if self.blocks_probably_identical(block_a, block_b)]","identical_blocks = [(block_a, block_b) for (block_a, block_b) in self._block_matches if self.blocks_probably_identical(block_a, block_b)]",1,,,,,robosuite
appdaemon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/appdaemon/conf/example_apps/secure.py,https://github.com/AppDaemon/appdaemon/tree/master/conf/example_apps/secure.py,Secure,list_entities$94,"def list_entities(self):
    entities = []
    for zone in self.args['zones']:
        for entity in self.args['zones'][zone]:
            entities.append(entity)
    return entities","for zone in self.args['zones']:
    for entity in self.args['zones'][zone]:
        entities.append(entity)",entities = [entity for zone in self.args['zones'] for entity in self.args['zones'][zone]],entities = [entity for zone in self.args['zones'] for entity in self.args['zones'][zone]],1,,,,,robosuite
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/vimeo.py,VimeoExtractor,extract$75,"def extract(self, **kwargs):
    for s in self.streams:
        self.streams[s]['size'] = urls_size(self.streams[s]['src'])
    master_m3u8s = []
    for m in self.master_m3u8:
        master_m3u8s.append(self.master_m3u8[m]['url'])
    master_content = None
    master_url = None
    for master_u in master_m3u8s:
        try:
            master_content = get_content(master_u).split('\n')
        except urllib.error.URLError:
            continue
        else:
            master_url = master_u
    if master_content is None:
        return
    lines = []
    for line in master_content:
        if len(line.strip()) > 0:
            lines.append(line.strip())
    pos = 0
    while pos < len(lines):
        if lines[pos].startswith('#EXT-X-STREAM-INF'):
            patt = 'RESOLUTION=(\\d+)x(\\d+)'
            hit = re.search(patt, lines[pos])
            if hit is None:
                continue
            width = hit.group(1)
            height = hit.group(2)
            if height in ('2160', '1440'):
                m3u8_url = urllib.parse.urljoin(master_url, lines[pos + 1])
                meta = dict(m3u8_url=m3u8_url, container='m3u8')
                if height == '1440':
                    meta['video_profile'] = '2560x1440'
                else:
                    meta['video_profile'] = '3840x2160'
                meta['size'] = 0
                meta['src'] = general_m3u8_extractor(m3u8_url)
                self.streams[height + 'p'] = meta
            pos += 2
        else:
            pos += 1
    self.streams_sorted = []
    for stream_type in self.stream_types:
        if stream_type['id'] in self.streams:
            item = [('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())
            self.streams_sorted.append(dict(item))","for m in self.master_m3u8:
    master_m3u8s.append(self.master_m3u8[m]['url'])",master_m3u8s = [self.master_m3u8[m]['url'] for m in self.master_m3u8],master_m3u8s = [self.master_m3u8[m]['url'] for m in self.master_m3u8],1,,,,,robosuite
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/vimeo.py,VimeoExtractor,extract$75,"def extract(self, **kwargs):
    for s in self.streams:
        self.streams[s]['size'] = urls_size(self.streams[s]['src'])
    master_m3u8s = []
    for m in self.master_m3u8:
        master_m3u8s.append(self.master_m3u8[m]['url'])
    master_content = None
    master_url = None
    for master_u in master_m3u8s:
        try:
            master_content = get_content(master_u).split('\n')
        except urllib.error.URLError:
            continue
        else:
            master_url = master_u
    if master_content is None:
        return
    lines = []
    for line in master_content:
        if len(line.strip()) > 0:
            lines.append(line.strip())
    pos = 0
    while pos < len(lines):
        if lines[pos].startswith('#EXT-X-STREAM-INF'):
            patt = 'RESOLUTION=(\\d+)x(\\d+)'
            hit = re.search(patt, lines[pos])
            if hit is None:
                continue
            width = hit.group(1)
            height = hit.group(2)
            if height in ('2160', '1440'):
                m3u8_url = urllib.parse.urljoin(master_url, lines[pos + 1])
                meta = dict(m3u8_url=m3u8_url, container='m3u8')
                if height == '1440':
                    meta['video_profile'] = '2560x1440'
                else:
                    meta['video_profile'] = '3840x2160'
                meta['size'] = 0
                meta['src'] = general_m3u8_extractor(m3u8_url)
                self.streams[height + 'p'] = meta
            pos += 2
        else:
            pos += 1
    self.streams_sorted = []
    for stream_type in self.stream_types:
        if stream_type['id'] in self.streams:
            item = [('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())
            self.streams_sorted.append(dict(item))","for line in master_content:
    if len(line.strip()) > 0:
        lines.append(line.strip())",lines = [line.strip() for line in master_content if len(line.strip()) > 0],lines = [line.strip() for line in master_content if len(line.strip()) > 0],1,,,,,robosuite
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/vimeo.py,VimeoExtractor,extract$75,"def extract(self, **kwargs):
    for s in self.streams:
        self.streams[s]['size'] = urls_size(self.streams[s]['src'])
    master_m3u8s = []
    for m in self.master_m3u8:
        master_m3u8s.append(self.master_m3u8[m]['url'])
    master_content = None
    master_url = None
    for master_u in master_m3u8s:
        try:
            master_content = get_content(master_u).split('\n')
        except urllib.error.URLError:
            continue
        else:
            master_url = master_u
    if master_content is None:
        return
    lines = []
    for line in master_content:
        if len(line.strip()) > 0:
            lines.append(line.strip())
    pos = 0
    while pos < len(lines):
        if lines[pos].startswith('#EXT-X-STREAM-INF'):
            patt = 'RESOLUTION=(\\d+)x(\\d+)'
            hit = re.search(patt, lines[pos])
            if hit is None:
                continue
            width = hit.group(1)
            height = hit.group(2)
            if height in ('2160', '1440'):
                m3u8_url = urllib.parse.urljoin(master_url, lines[pos + 1])
                meta = dict(m3u8_url=m3u8_url, container='m3u8')
                if height == '1440':
                    meta['video_profile'] = '2560x1440'
                else:
                    meta['video_profile'] = '3840x2160'
                meta['size'] = 0
                meta['src'] = general_m3u8_extractor(m3u8_url)
                self.streams[height + 'p'] = meta
            pos += 2
        else:
            pos += 1
    self.streams_sorted = []
    for stream_type in self.stream_types:
        if stream_type['id'] in self.streams:
            item = [('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())
            self.streams_sorted.append(dict(item))","for stream_type in self.stream_types:
    if stream_type['id'] in self.streams:
        item = [('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())
        self.streams_sorted.append(dict(item))","self.streams_sorted = [dict([('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())) for stream_type in self.stream_types if stream_type['id'] in self.streams]",Cannot refactor,-1,1,,,,robosuite
faust,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/unit/transport/test_utils.py,https://github.com/robinhood/faust/tree/master/t/unit/transport/test_utils.py,test_TopicBuffer,test_iter$23,"def test_iter(self):
    buffer = TopicBuffer()
    buffer.add(TP1, BUF1)
    buffer.add(TP2, BUF2)
    buffer.add(TP3, BUF3)
    buffer.add(TP4, BUF4)
    buffer.add(TP5, BUF5)
    consumed = []
    for (tp, item) in buffer:
        consumed.append((tp, item))
    assert consumed == [(TP1, 0), (TP2, 5), (TP3, 9), (TP4, 11), (TP5, 14), (TP1, 1), (TP2, 6), (TP3, 10), (TP4, 12), (TP5, 15), (TP1, 2), (TP2, 7), (TP4, 13), (TP1, 3), (TP2, 8), (TP1, 4)]","for (tp, item) in buffer:
    consumed.append((tp, item))","consumed = [(tp, item) for (tp, item) in buffer]","consumed = [(tp, item) for (tp, item) in buffer]",1,,,,,robosuite
FSL-Mate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FSL-Mate/PaddleFSL/examples/FewCLUE/pet/evaluate.py,https://github.com/tata1661/FSL-Mate/tree/master/PaddleFSL/examples/FewCLUE/pet/evaluate.py,,do_evaluate$21,"def do_evaluate(model, tokenizer, data_loader, label_normalize_dict):
    model.eval()
    total_num = 0
    correct_num = 0
    normed_labels = [normalized_lable for (origin_lable, normalized_lable) in label_normalize_dict.items()]
    label_length = len(normed_labels[0])
    for batch in data_loader:
        (src_ids, token_type_ids, masked_positions, masked_lm_labels) = batch
        max_len = src_ids.shape[1]
        new_masked_positions = []
        for (bs_index, mask_pos) in enumerate(masked_positions.numpy()):
            for pos in mask_pos:
                new_masked_positions.append(bs_index * max_len + pos)
        new_masked_positions = paddle.to_tensor(np.array(new_masked_positions).astype('int32'))
        prediction_scores = model(input_ids=src_ids, token_type_ids=token_type_ids, masked_positions=new_masked_positions)
        softmax_fn = paddle.nn.Softmax()
        prediction_probs = softmax_fn(prediction_scores)
        batch_size = len(src_ids)
        vocab_size = prediction_probs.shape[1]
        prediction_probs = paddle.reshape(prediction_probs, shape=[batch_size, -1, vocab_size]).numpy()
        label_ids = np.array([tokenizer(label)['input_ids'][1:-1] for label in normed_labels])
        y_pred = np.ones(shape=[batch_size, len(label_ids)])
        for index in range(label_length):
            y_pred *= prediction_probs[:, index, label_ids[:, index]]
        y_pred_index = np.argmax(y_pred, axis=-1)
        y_true_index = []
        for masked_lm_label in masked_lm_labels.numpy():
            label_text = ''.join(tokenizer.convert_ids_to_tokens(list(masked_lm_label)))
            label_index = normed_labels.index(label_text)
            y_true_index.append(label_index)
        y_true_index = np.array(y_true_index)
        total_num += len(y_true_index)
        correct_num += (y_true_index == y_pred_index).sum()
    return (100 * correct_num / total_num, total_num)","for (bs_index, mask_pos) in enumerate(masked_positions.numpy()):
    for pos in mask_pos:
        new_masked_positions.append(bs_index * max_len + pos)","new_masked_positions = [bs_index * max_len + pos for (bs_index, mask_pos) in enumerate(masked_positions.numpy()) for pos in mask_pos]","new_masked_positions = [bs_index * max_len + pos for (bs_index, mask_pos) in enumerate(masked_positions.numpy()) for pos in mask_pos]",1,,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_elb.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_elb.py,,_listeners_present$804,"def _listeners_present(name, listeners, region, key, keyid, profile):
    ret = {'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
    if not lb:
        ret['comment'] = '{} ELB configuration could not be retrieved.'.format(name)
        ret['result'] = False
        return ret
    if not listeners:
        listeners = []
    expected_listeners_by_tuple = {}
    for l in listeners:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        expected_listeners_by_tuple[l_key] = l
    actual_listeners_by_tuple = {}
    for l in lb['listeners']:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        actual_listeners_by_tuple[l_key] = l
    to_delete = []
    to_create = []
    for (t, l) in expected_listeners_by_tuple.items():
        if t not in actual_listeners_by_tuple:
            to_create.append(l)
    for (t, l) in actual_listeners_by_tuple.items():
        if t not in expected_listeners_by_tuple:
            to_delete.append(l)
    if __opts__['test']:
        msg = []
        if to_create or to_delete:
            msg.append('ELB {} set to have listeners modified:'.format(name))
            for listener in to_create:
                msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            for listener in to_delete:
                msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            ret['result'] = None
        else:
            msg.append('Listeners already set on ELB {}.'.format(name))
        ret['comment'] = '  '.join(msg)
        return ret
    if to_delete:
        ports = [l['elb_port'] for l in to_delete]
        deleted = __salt__['boto_elb.delete_listeners'](name, ports, region, key, keyid, profile)
        if deleted:
            ret['comment'] = 'Deleted listeners on {} ELB.'.format(name)
        else:
            ret['comment'] = 'Failed to delete listeners on {} ELB.'.format(name)
            ret['result'] = False
    if to_create:
        created = __salt__['boto_elb.create_listeners'](name, to_create, region, key, keyid, profile)
        if created:
            msg = 'Created listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
        else:
            msg = 'Failed to create listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
            ret['result'] = False
    if to_create or to_delete:
        ret['changes']['listeners'] = {}
        ret['changes']['listeners']['old'] = lb['listeners']
        lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
        ret['changes']['listeners']['new'] = lb['listeners']
    else:
        ret['comment'] = 'Listeners already set on ELB {}.'.format(name)
    return ret","for (t, l) in expected_listeners_by_tuple.items():
    if t not in actual_listeners_by_tuple:
        to_create.append(l)","to_create = [l for (t, l) in expected_listeners_by_tuple.items() if t not in actual_listeners_by_tuple]","to_create = [l for (t, l) in expected_listeners_by_tuple.items() if t not in actual_listeners_by_tuple]",1,,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_elb.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_elb.py,,_listeners_present$804,"def _listeners_present(name, listeners, region, key, keyid, profile):
    ret = {'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
    if not lb:
        ret['comment'] = '{} ELB configuration could not be retrieved.'.format(name)
        ret['result'] = False
        return ret
    if not listeners:
        listeners = []
    expected_listeners_by_tuple = {}
    for l in listeners:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        expected_listeners_by_tuple[l_key] = l
    actual_listeners_by_tuple = {}
    for l in lb['listeners']:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        actual_listeners_by_tuple[l_key] = l
    to_delete = []
    to_create = []
    for (t, l) in expected_listeners_by_tuple.items():
        if t not in actual_listeners_by_tuple:
            to_create.append(l)
    for (t, l) in actual_listeners_by_tuple.items():
        if t not in expected_listeners_by_tuple:
            to_delete.append(l)
    if __opts__['test']:
        msg = []
        if to_create or to_delete:
            msg.append('ELB {} set to have listeners modified:'.format(name))
            for listener in to_create:
                msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            for listener in to_delete:
                msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            ret['result'] = None
        else:
            msg.append('Listeners already set on ELB {}.'.format(name))
        ret['comment'] = '  '.join(msg)
        return ret
    if to_delete:
        ports = [l['elb_port'] for l in to_delete]
        deleted = __salt__['boto_elb.delete_listeners'](name, ports, region, key, keyid, profile)
        if deleted:
            ret['comment'] = 'Deleted listeners on {} ELB.'.format(name)
        else:
            ret['comment'] = 'Failed to delete listeners on {} ELB.'.format(name)
            ret['result'] = False
    if to_create:
        created = __salt__['boto_elb.create_listeners'](name, to_create, region, key, keyid, profile)
        if created:
            msg = 'Created listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
        else:
            msg = 'Failed to create listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
            ret['result'] = False
    if to_create or to_delete:
        ret['changes']['listeners'] = {}
        ret['changes']['listeners']['old'] = lb['listeners']
        lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
        ret['changes']['listeners']['new'] = lb['listeners']
    else:
        ret['comment'] = 'Listeners already set on ELB {}.'.format(name)
    return ret","for (t, l) in actual_listeners_by_tuple.items():
    if t not in expected_listeners_by_tuple:
        to_delete.append(l)","to_delete = [l for (t, l) in actual_listeners_by_tuple.items() if t not in expected_listeners_by_tuple]","to_delete = [l for (t, l) in actual_listeners_by_tuple.items() if t not in expected_listeners_by_tuple]",1,,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_elb.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_elb.py,,_listeners_present$804,"def _listeners_present(name, listeners, region, key, keyid, profile):
    ret = {'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
    if not lb:
        ret['comment'] = '{} ELB configuration could not be retrieved.'.format(name)
        ret['result'] = False
        return ret
    if not listeners:
        listeners = []
    expected_listeners_by_tuple = {}
    for l in listeners:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        expected_listeners_by_tuple[l_key] = l
    actual_listeners_by_tuple = {}
    for l in lb['listeners']:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        actual_listeners_by_tuple[l_key] = l
    to_delete = []
    to_create = []
    for (t, l) in expected_listeners_by_tuple.items():
        if t not in actual_listeners_by_tuple:
            to_create.append(l)
    for (t, l) in actual_listeners_by_tuple.items():
        if t not in expected_listeners_by_tuple:
            to_delete.append(l)
    if __opts__['test']:
        msg = []
        if to_create or to_delete:
            msg.append('ELB {} set to have listeners modified:'.format(name))
            for listener in to_create:
                msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            for listener in to_delete:
                msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            ret['result'] = None
        else:
            msg.append('Listeners already set on ELB {}.'.format(name))
        ret['comment'] = '  '.join(msg)
        return ret
    if to_delete:
        ports = [l['elb_port'] for l in to_delete]
        deleted = __salt__['boto_elb.delete_listeners'](name, ports, region, key, keyid, profile)
        if deleted:
            ret['comment'] = 'Deleted listeners on {} ELB.'.format(name)
        else:
            ret['comment'] = 'Failed to delete listeners on {} ELB.'.format(name)
            ret['result'] = False
    if to_create:
        created = __salt__['boto_elb.create_listeners'](name, to_create, region, key, keyid, profile)
        if created:
            msg = 'Created listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
        else:
            msg = 'Failed to create listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
            ret['result'] = False
    if to_create or to_delete:
        ret['changes']['listeners'] = {}
        ret['changes']['listeners']['old'] = lb['listeners']
        lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
        ret['changes']['listeners']['new'] = lb['listeners']
    else:
        ret['comment'] = 'Listeners already set on ELB {}.'.format(name)
    return ret","for listener in to_create:
    msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))",msg += ['Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)) for listener in to_create],Cannot refactor,-1,1,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_elb.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_elb.py,,_listeners_present$804,"def _listeners_present(name, listeners, region, key, keyid, profile):
    ret = {'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
    if not lb:
        ret['comment'] = '{} ELB configuration could not be retrieved.'.format(name)
        ret['result'] = False
        return ret
    if not listeners:
        listeners = []
    expected_listeners_by_tuple = {}
    for l in listeners:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        expected_listeners_by_tuple[l_key] = l
    actual_listeners_by_tuple = {}
    for l in lb['listeners']:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        actual_listeners_by_tuple[l_key] = l
    to_delete = []
    to_create = []
    for (t, l) in expected_listeners_by_tuple.items():
        if t not in actual_listeners_by_tuple:
            to_create.append(l)
    for (t, l) in actual_listeners_by_tuple.items():
        if t not in expected_listeners_by_tuple:
            to_delete.append(l)
    if __opts__['test']:
        msg = []
        if to_create or to_delete:
            msg.append('ELB {} set to have listeners modified:'.format(name))
            for listener in to_create:
                msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            for listener in to_delete:
                msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            ret['result'] = None
        else:
            msg.append('Listeners already set on ELB {}.'.format(name))
        ret['comment'] = '  '.join(msg)
        return ret
    if to_delete:
        ports = [l['elb_port'] for l in to_delete]
        deleted = __salt__['boto_elb.delete_listeners'](name, ports, region, key, keyid, profile)
        if deleted:
            ret['comment'] = 'Deleted listeners on {} ELB.'.format(name)
        else:
            ret['comment'] = 'Failed to delete listeners on {} ELB.'.format(name)
            ret['result'] = False
    if to_create:
        created = __salt__['boto_elb.create_listeners'](name, to_create, region, key, keyid, profile)
        if created:
            msg = 'Created listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
        else:
            msg = 'Failed to create listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
            ret['result'] = False
    if to_create or to_delete:
        ret['changes']['listeners'] = {}
        ret['changes']['listeners']['old'] = lb['listeners']
        lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
        ret['changes']['listeners']['new'] = lb['listeners']
    else:
        ret['comment'] = 'Listeners already set on ELB {}.'.format(name)
    return ret","for listener in to_delete:
    msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))",msg += ['Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)) for listener in to_delete],Cannot refactor,-1,1,,,,robosuite
FairMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FairMOT/src/lib/models/networks/pose_hrnet.py,https://github.com/ifzhang/FairMOT/tree/master/src/lib/models/networks/pose_hrnet.py,PoseHighResolutionNet,forward$462,"def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    x = self.layer1(x)
    x_list = []
    for i in range(self.stage2_cfg['NUM_BRANCHES']):
        if self.transition1[i] is not None:
            x_list.append(self.transition1[i](x))
        else:
            x_list.append(x)
    y_list = self.stage2(x_list)
    x_list = []
    for i in range(self.stage3_cfg['NUM_BRANCHES']):
        if self.transition2[i] is not None:
            if i < self.stage2_cfg['NUM_BRANCHES']:
                x_list.append(self.transition2[i](y_list[i]))
            else:
                x_list.append(self.transition2[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    y_list = self.stage3(x_list)
    x_list = []
    for i in range(self.stage4_cfg['NUM_BRANCHES']):
        if self.transition3[i] is not None:
            if i < self.stage3_cfg['NUM_BRANCHES']:
                x_list.append(self.transition3[i](y_list[i]))
            else:
                x_list.append(self.transition3[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    x = self.stage4(x_list)
    (x0_h, x0_w) = (x[0].size(2), x[0].size(3))
    x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')
    x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')
    x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')
    x = torch.cat([x[0], x1, x2, x3], 1)
    z = {}
    for head in self.heads:
        z[head] = self.__getattr__(head)(x)
    return [z]","for i in range(self.stage2_cfg['NUM_BRANCHES']):
    if self.transition1[i] is not None:
        x_list.append(self.transition1[i](x))
    else:
        x_list.append(x)",x_list = [self.transition1[i](x) if self.transition1[i] is not None else x for i in range(self.stage2_cfg['NUM_BRANCHES'])],x_list = [self.transition1[i](x) if self.transition1[i] is not None else x for i in range(self.stage2_cfg['NUM_BRANCHES'])],1,,,,,robosuite
FairMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FairMOT/src/lib/models/networks/pose_hrnet.py,https://github.com/ifzhang/FairMOT/tree/master/src/lib/models/networks/pose_hrnet.py,PoseHighResolutionNet,forward$462,"def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    x = self.layer1(x)
    x_list = []
    for i in range(self.stage2_cfg['NUM_BRANCHES']):
        if self.transition1[i] is not None:
            x_list.append(self.transition1[i](x))
        else:
            x_list.append(x)
    y_list = self.stage2(x_list)
    x_list = []
    for i in range(self.stage3_cfg['NUM_BRANCHES']):
        if self.transition2[i] is not None:
            if i < self.stage2_cfg['NUM_BRANCHES']:
                x_list.append(self.transition2[i](y_list[i]))
            else:
                x_list.append(self.transition2[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    y_list = self.stage3(x_list)
    x_list = []
    for i in range(self.stage4_cfg['NUM_BRANCHES']):
        if self.transition3[i] is not None:
            if i < self.stage3_cfg['NUM_BRANCHES']:
                x_list.append(self.transition3[i](y_list[i]))
            else:
                x_list.append(self.transition3[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    x = self.stage4(x_list)
    (x0_h, x0_w) = (x[0].size(2), x[0].size(3))
    x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')
    x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')
    x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')
    x = torch.cat([x[0], x1, x2, x3], 1)
    z = {}
    for head in self.heads:
        z[head] = self.__getattr__(head)(x)
    return [z]","for i in range(self.stage3_cfg['NUM_BRANCHES']):
    if self.transition2[i] is not None:
        if i < self.stage2_cfg['NUM_BRANCHES']:
            x_list.append(self.transition2[i](y_list[i]))
        else:
            x_list.append(self.transition2[i](y_list[-1]))
    else:
        x_list.append(y_list[i])",x_list += [self.transition2[i](y_list[i]) if self.transition2[i] is not None and i < self.stage2_cfg['NUM_BRANCHES'] else self.transition2[i](y_list[-1]) if self.transition2[i] is not None else y_list[i] for i in range(self.stage3_cfg['NUM_BRANCHES'])],Cannot refactor,-1,1,,,,robosuite
FairMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FairMOT/src/lib/models/networks/pose_hrnet.py,https://github.com/ifzhang/FairMOT/tree/master/src/lib/models/networks/pose_hrnet.py,PoseHighResolutionNet,forward$462,"def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    x = self.layer1(x)
    x_list = []
    for i in range(self.stage2_cfg['NUM_BRANCHES']):
        if self.transition1[i] is not None:
            x_list.append(self.transition1[i](x))
        else:
            x_list.append(x)
    y_list = self.stage2(x_list)
    x_list = []
    for i in range(self.stage3_cfg['NUM_BRANCHES']):
        if self.transition2[i] is not None:
            if i < self.stage2_cfg['NUM_BRANCHES']:
                x_list.append(self.transition2[i](y_list[i]))
            else:
                x_list.append(self.transition2[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    y_list = self.stage3(x_list)
    x_list = []
    for i in range(self.stage4_cfg['NUM_BRANCHES']):
        if self.transition3[i] is not None:
            if i < self.stage3_cfg['NUM_BRANCHES']:
                x_list.append(self.transition3[i](y_list[i]))
            else:
                x_list.append(self.transition3[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    x = self.stage4(x_list)
    (x0_h, x0_w) = (x[0].size(2), x[0].size(3))
    x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')
    x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')
    x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')
    x = torch.cat([x[0], x1, x2, x3], 1)
    z = {}
    for head in self.heads:
        z[head] = self.__getattr__(head)(x)
    return [z]","for i in range(self.stage4_cfg['NUM_BRANCHES']):
    if self.transition3[i] is not None:
        if i < self.stage3_cfg['NUM_BRANCHES']:
            x_list.append(self.transition3[i](y_list[i]))
        else:
            x_list.append(self.transition3[i](y_list[-1]))
    else:
        x_list.append(y_list[i])",x_list += [self.transition3[i](y_list[i]) if self.transition3[i] is not None and i < self.stage3_cfg['NUM_BRANCHES'] else self.transition3[i](y_list[-1]) if self.transition3[i] is not None else y_list[i] for i in range(self.stage4_cfg['NUM_BRANCHES'])],Cannot refactor,-1,1,,,,robosuite
CTGAN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CTGAN/ctgan/synthesizers/ctgan.py,https://github.com/sdv-dev/CTGAN/tree/master/ctgan/synthesizers/ctgan.py,CTGAN,_validate_discrete_columns$256,"def _validate_discrete_columns(self, train_data, discrete_columns):
    """"""Check whether ``discrete_columns`` exists in ``train_data``.

        Args:
            train_data (numpy.ndarray or pandas.DataFrame):
                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.
            discrete_columns (list-like):
                List of discrete columns to be used to generate the Conditional
                Vector. If ``train_data`` is a Numpy array, this list should
                contain the integer indices of the columns. Otherwise, if it is
                a ``pandas.DataFrame``, this list should contain the column names.
        """"""
    if isinstance(train_data, pd.DataFrame):
        invalid_columns = set(discrete_columns) - set(train_data.columns)
    elif isinstance(train_data, np.ndarray):
        invalid_columns = []
        for column in discrete_columns:
            if column < 0 or column >= train_data.shape[1]:
                invalid_columns.append(column)
    else:
        raise TypeError('``train_data`` should be either pd.DataFrame or np.array.')
    if invalid_columns:
        raise ValueError(f'Invalid columns found: {invalid_columns}')","for column in discrete_columns:
    if column < 0 or column >= train_data.shape[1]:
        invalid_columns.append(column)",invalid_columns = [column for column in discrete_columns if column < 0 or column >= train_data.shape[1]],invalid_columns = [column for column in discrete_columns if column < 0 or column >= train_data.shape[1]],1,,,,,robosuite
TransTrack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TransTrack/util/misc.py,https://github.com/PeizeSun/TransTrack/tree/master/util/misc.py,MetricLogger,__str__$204,"def __str__(self):
    loss_str = []
    for (name, meter) in self.meters.items():
        loss_str.append('{}: {}'.format(name, str(meter)))
    return self.delimiter.join(loss_str)","for (name, meter) in self.meters.items():
    loss_str.append('{}: {}'.format(name, str(meter)))","loss_str = ['{}: {}'.format(name, str(meter)) for (name, meter) in self.meters.items()]","loss_str = ['{}: {}'.format(name, str(meter)) for (name, meter) in self.meters.items()]",1,,,,,robosuite
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for pt in left:
    left_list.append(pt[0].tolist())",left_list = [pt[0].tolist() for pt in left],left_list = [pt[0].tolist() for pt in left],1,,,,,robosuite
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for pt in right:
    right_list.append(pt[0].tolist())",right_list = [pt[0].tolist() for pt in right],right_list = [pt[0].tolist() for pt in right],1,,,,,robosuite
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for pt in center_h:
    center_h_list.append(pt[0].tolist())",center_h_list = [pt[0].tolist() for pt in center_h],center_h_list = [pt[0].tolist() for pt in center_h],1,,,,,robosuite
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for i in range(1, 21):
    if i == 1:
        pt_max = y
        pt_min = y + inc * i
    else:
        pt_max = y + inc * (i - 1)
        pt_min = y + inc * i
    pts_max.append(pt_max)
    pts_min.append(pt_min)","pts_min = [y + inc * i for i in range(1, 21)]",Cannot refactor,-1,0,,2,1,robosuite
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for v in vals:
    value = obj[v == obj[:, 0, 1]]
    if len(value) > 0:
        largest = value[:, 0, 0].max()
        smallest = value[:, 0, 0].min()
        row_width = largest - smallest
        rows.append(row_width)
        lps.append(smallest)
        rps.append(largest)
    if len(value) == 0:
        row_width = 1
        rows.append(row_width)
        lps.append(1)
        rps.append(1)","rps = [value[:, 0, 0].max() if len(value) > 0 else 1 for v in vals for value in [obj[v == obj[:, 0, 1]]]]",Cannot refactor,-1,0,,2,1,robosuite
shinysdr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shinysdr/shinysdr/telemetry.py,https://github.com/kpreid/shinysdr/tree/master/shinysdr/telemetry.py,TelemetryStore,__flush_expired$203,"def __flush_expired(self):
    current_time = self.__time_source.seconds()
    deletes = []
    for (object_id, expiry) in six.iteritems(self.__expiry_times):
        if expiry <= current_time:
            deletes.append(object_id)
    for object_id in deletes:
        del self.__objects[object_id]
        del self.__expiry_times[object_id]
        if object_id in self.__interesting_objects:
            del self.__interesting_objects[object_id]
    self.__maybe_schedule_flush()","for (object_id, expiry) in six.iteritems(self.__expiry_times):
    if expiry <= current_time:
        deletes.append(object_id)","deletes = [object_id for (object_id, expiry) in six.iteritems(self.__expiry_times) if expiry <= current_time]","deletes = [object_id for (object_id, expiry) in six.iteritems(self.__expiry_times) if expiry <= current_time]",1,,,,,robosuite
satpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/satpy/satpy/readers/viirs_compact.py,https://github.com/pytroll/satpy/tree/master/satpy/readers/viirs_compact.py,VIIRSCompactFileHandler,expand_angle_and_nav$285,"def expand_angle_and_nav(self, arrays):
    """"""Expand angle and navigation datasets.""""""
    res = []
    for array in arrays:
        res.append(da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]))
    return res","for array in arrays:
    res.append(da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]))","res = [da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]) for array in arrays]","res = [da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]) for array in arrays]",1,,,,,robosuite
yolo3-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolo3-pytorch/utils/utils_map.py,https://github.com/bubbliiiing/yolo3-pytorch/tree/master/utils/utils_map.py,,voc_ap$95,"def voc_ap(rec, prec):
    """"""
    --- Official matlab code VOC2012---
    mrec=[0 ; rec ; 1];
    mpre=[0 ; prec ; 0];
    for i=numel(mpre)-1:-1:1
            mpre(i)=max(mpre(i),mpre(i+1));
    end
    i=find(mrec(2:end)~=mrec(1:end-1))+1;
    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));
    """"""
    rec.insert(0, 0.0)
    rec.append(1.0)
    mrec = rec[:]
    prec.insert(0, 0.0)
    prec.append(0.0)
    mpre = prec[:]
    '\n     This part makes the precision monotonically decreasing\n        (goes from the end to the beginning)\n        matlab: for i=numel(mpre)-1:-1:1\n                    mpre(i)=max(mpre(i),mpre(i+1));\n    '
    for i in range(len(mpre) - 2, -1, -1):
        mpre[i] = max(mpre[i], mpre[i + 1])
    '\n     This part creates a list of indexes where the recall changes\n        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n    '
    i_list = []
    for i in range(1, len(mrec)):
        if mrec[i] != mrec[i - 1]:
            i_list.append(i)
    '\n     The Average Precision (AP) is the area under the curve\n        (numerical integration)\n        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n    '
    ap = 0.0
    for i in i_list:
        ap += (mrec[i] - mrec[i - 1]) * mpre[i]
    return (ap, mrec, mpre)","for i in range(1, len(mrec)):
    if mrec[i] != mrec[i - 1]:
        i_list.append(i)","i_list = [i for i in range(1, len(mrec)) if mrec[i] != mrec[i - 1]]","i_list = [i for i in range(1, len(mrec)) if mrec[i] != mrec[i - 1]]",1,,,,,robosuite
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/448_Find_All_Numbers_Disappeared_in_an_Array.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/448_Find_All_Numbers_Disappeared_in_an_Array.py,Solution,findDisappearedNumbers$2,"def findDisappearedNumbers(self, nums):
    """"""
        :type nums: List[int]
        :rtype: List[int]
        """"""
    (currentIndex, length) = (0, len(nums))
    missingNumbers = []
    while currentIndex < length:
        finalIndex = nums[currentIndex] - 1
        if nums[currentIndex] != nums[finalIndex]:
            (nums[currentIndex], nums[finalIndex]) = (nums[finalIndex], nums[currentIndex])
        else:
            currentIndex += 1
    for index in range(length):
        if nums[index] != index + 1:
            missingNumbers.append(index + 1)
    return missingNumbers","for index in range(length):
    if nums[index] != index + 1:
        missingNumbers.append(index + 1)",missingNumbers = [index + 1 for index in range(length) if nums[index] != index + 1],missingNumbers = [index + 1 for index in range(length) if nums[index] != index + 1],1,,,,,robosuite
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py,https://github.com/apache/airflow/tree/master/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py,CassandraToGCSOperator,_write_local_schema_file$226,"def _write_local_schema_file(self, cursor):
    """"""
        Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.
        """"""
    schema = []
    tmp_schema_file_handle = NamedTemporaryFile(delete=True)
    for (name, type_) in zip(cursor.column_names, cursor.column_types):
        schema.append(self.generate_schema_dict(name, type_))
    json_serialized_schema = json.dumps(schema).encode('utf-8')
    tmp_schema_file_handle.write(json_serialized_schema)
    return {self.schema_filename: tmp_schema_file_handle}","for (name, type_) in zip(cursor.column_names, cursor.column_types):
    schema.append(self.generate_schema_dict(name, type_))","schema += [self.generate_schema_dict(name, type_) for (name, type_) in zip(cursor.column_names, cursor.column_types)]","schema = [self.generate_schema_dict(name, type_) for (name, type_) in zip(cursor.column_names, cursor.column_types)]",0,1,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/incubate/fleet/parameter_server/pslib/optimizer_factory.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/incubate/fleet/parameter_server/pslib/optimizer_factory.py,DistributedAdam,_find_multi_distributed_lookup_table$202,"def _find_multi_distributed_lookup_table(self, losses):
    """"""
        find multi-sparse-table
        """"""
    table_names = set()
    cnt = 0
    tmp_list = []
    ret_list = []
    for loss in losses:
        for op in loss.block.program.global_block().ops:
            if op.type in self.supported_embedding_types:
                if op.attr('is_distributed') is True:
                    table_name = op.input('W')[0]
                    if table_name not in table_names:
                        table_names.add(table_name)
                        tmp_list.append([table_name, cnt])
                        cnt += 1
    tmp_list.sort(key=lambda k: k[1])
    for x in tmp_list:
        ret_list.append(x[0])
    return ret_list","for x in tmp_list:
    ret_list.append(x[0])",ret_list = [x[0] for x in tmp_list],ret_list = [x[0] for x in tmp_list],1,,,,,robosuite
mmaction2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmaction2/tools/data/skeleton/ntu_pose_extraction.py,https://github.com/open-mmlab/mmaction2/tree/master/tools/data/skeleton/ntu_pose_extraction.py,,ntu_det_postproc$256,"def ntu_det_postproc(vid, det_results):
    det_results = [removedup(x) for x in det_results]
    label = int(vid.split('/')[-1].split('A')[1][:3])
    mpaction = list(range(50, 61)) + list(range(106, 121))
    n_person = 2 if label in mpaction else 1
    (is_easy, bboxes) = is_easy_example(det_results, n_person)
    if is_easy:
        print('\nEasy Example')
        return bboxes
    tracklets = bbox2tracklet(det_results)
    tracklets = drop_tracklet(tracklets)
    print(f'\nHard {n_person}-person Example, found {len(tracklets)} tracklet')
    if n_person == 1:
        if len(tracklets) == 1:
            tracklet = list(tracklets.values())[0]
            det_results = tracklet2bbox(tracklet, len(det_results))
            return np.stack(det_results)
        else:
            (bad, det_results) = tracklets2bbox(tracklets, len(det_results))
            return det_results
    if len(tracklets) <= 2:
        tracklets = list(tracklets.values())
        bboxes = []
        for tracklet in tracklets:
            bboxes.append(tracklet2bbox(tracklet, len(det_results))[:, None])
        bbox = np.concatenate(bboxes, axis=1)
        return bbox
    else:
        return bboxes2bbox(det_results, len(det_results))","for tracklet in tracklets:
    bboxes.append(tracklet2bbox(tracklet, len(det_results))[:, None])","bboxes = [tracklet2bbox(tracklet, len(det_results))[:, None] for tracklet in tracklets]","bboxes = [tracklet2bbox(tracklet, len(det_results))[:, None] for tracklet in tracklets]",1,,,,,robosuite
hivemind,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hivemind/examples/albert/tokenize_wikitext103.py,https://github.com/learning-at-home/hivemind/tree/master/examples/albert/tokenize_wikitext103.py,,create_instances_from_document$13,"def create_instances_from_document(tokenizer, document, max_seq_length):
    """"""
    Creates training instances from a single document.
    Reuses code from the original ALBERT implementation (Google AI, 2018)
    https://github.com/google-research/albert/blob/master/create_pretraining_data.py#L267
    """"""
    instances = []
    current_chunk = []
    current_length = 0
    segmented_sents = list(nltk.sent_tokenize(document))
    for (i, sent) in enumerate(segmented_sents):
        current_chunk.append(sent)
        current_length += len(tokenizer.tokenize(sent))
        if i == len(segmented_sents) - 1 or current_length >= max_seq_length:
            if len(current_chunk) > 1:
                a_end = random.randint(1, len(current_chunk) - 1)
                tokens_a = []
                for j in range(a_end):
                    tokens_a.append(current_chunk[j])
                tokens_b = []
                for j in range(a_end, len(current_chunk)):
                    tokens_b.append(current_chunk[j])
                if random.random() < 0.5:
                    is_random_next = True
                    (tokens_a, tokens_b) = (tokens_b, tokens_a)
                else:
                    is_random_next = False
                assert len(tokens_a) >= 1
                assert len(tokens_b) >= 1
                instance = tokenizer(' '.join(tokens_a), ' '.join(tokens_b), truncation='longest_first', max_length=max_seq_length, return_special_tokens_mask=True)
                assert len(instance['input_ids']) <= max_seq_length
                instance['sentence_order_label'] = 1 if is_random_next else 0
                instances.append(instance)
            current_chunk = []
            current_length = 0
    return instances","for j in range(a_end):
    tokens_a.append(current_chunk[j])",tokens_a = [current_chunk[j] for j in range(a_end)],tokens_a = [current_chunk[j] for j in range(a_end)],1,,,,,robosuite
hivemind,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hivemind/examples/albert/tokenize_wikitext103.py,https://github.com/learning-at-home/hivemind/tree/master/examples/albert/tokenize_wikitext103.py,,create_instances_from_document$13,"def create_instances_from_document(tokenizer, document, max_seq_length):
    """"""
    Creates training instances from a single document.
    Reuses code from the original ALBERT implementation (Google AI, 2018)
    https://github.com/google-research/albert/blob/master/create_pretraining_data.py#L267
    """"""
    instances = []
    current_chunk = []
    current_length = 0
    segmented_sents = list(nltk.sent_tokenize(document))
    for (i, sent) in enumerate(segmented_sents):
        current_chunk.append(sent)
        current_length += len(tokenizer.tokenize(sent))
        if i == len(segmented_sents) - 1 or current_length >= max_seq_length:
            if len(current_chunk) > 1:
                a_end = random.randint(1, len(current_chunk) - 1)
                tokens_a = []
                for j in range(a_end):
                    tokens_a.append(current_chunk[j])
                tokens_b = []
                for j in range(a_end, len(current_chunk)):
                    tokens_b.append(current_chunk[j])
                if random.random() < 0.5:
                    is_random_next = True
                    (tokens_a, tokens_b) = (tokens_b, tokens_a)
                else:
                    is_random_next = False
                assert len(tokens_a) >= 1
                assert len(tokens_b) >= 1
                instance = tokenizer(' '.join(tokens_a), ' '.join(tokens_b), truncation='longest_first', max_length=max_seq_length, return_special_tokens_mask=True)
                assert len(instance['input_ids']) <= max_seq_length
                instance['sentence_order_label'] = 1 if is_random_next else 0
                instances.append(instance)
            current_chunk = []
            current_length = 0
    return instances","for j in range(a_end, len(current_chunk)):
    tokens_b.append(current_chunk[j])","tokens_b = [current_chunk[j] for j in range(a_end, len(current_chunk))]","tokens_b = [current_chunk[j] for j in range(a_end, len(current_chunk))]",1,,,,,robosuite
wesng,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wesng/wes.py,https://github.com/bitsadmin/wesng/tree/master//wes.py,,get_hotfixes$654,"def get_hotfixes(text):
    hotfix_matches = re.findall('.*KB\\d+.*', text, re.MULTILINE | re.IGNORECASE)
    hotfixes = []
    for match in hotfix_matches:
        hotfixes.append(re.search('.*KB(\\d+).*', match, re.MULTILINE | re.IGNORECASE).group(1))
    return hotfixes","for match in hotfix_matches:
    hotfixes.append(re.search('.*KB(\\d+).*', match, re.MULTILINE | re.IGNORECASE).group(1))","hotfixes = [re.search('.*KB(\\d+).*', match, re.MULTILINE | re.IGNORECASE).group(1) for match in hotfix_matches]","hotfixes = [re.search('.*KB(\\d+).*', match, re.MULTILINE | re.IGNORECASE).group(1) for match in hotfix_matches]",1,,,,,robosuite
memegen,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/memegen/app/utils/html.py,https://github.com/jacebrowning/memegen/tree/master/app/utils/html.py,,_grid_debug$155,"def _grid_debug(urls: list[str], refresh: int, extra: str):
    elements = []
    for url in urls:
        elements.append(f'\n            <a href=""{url}"">\n                <img src=""{url}?time=0{extra}"">\n            </a>\n            ')
    elements.append(REFRESH_SCRIPT.replace('{interval}', str(refresh * 1000)))
    images = '\n'.join(elements).replace('\n' + ' ' * 12, '\n')
    head = '<title>Memegen.link | test</title>\n'
    body = images
    return HTML.format(head=head, body=body)","for url in urls:
    elements.append(f'\n            <a href=""{url}"">\n                <img src=""{url}?time=0{extra}"">\n            </a>\n            ')",elements = [f'\n            <a href="{url}">\n                <img src="{url}?time=0{extra}">\n            </a>\n            ' for url in urls],elements = [f'\n            <a href="{url}">\n                <img src="{url}?time=0{extra}">\n            </a>\n            ' for url in urls],1,,,,,robosuite
extruct,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/extruct/extruct/w3cmicrodata.py,https://github.com/scrapinghub/extruct/tree/master/extruct/w3cmicrodata.py,LxmlMicrodataExtractor,_extract_item$84,"def _extract_item(self, node, items_seen, base_url, itemids):
    itemid = self.get_docid(node, itemids)
    if self.nested:
        if itemid in items_seen:
            return
        items_seen.add(itemid)
    item = {}
    if not self.nested:
        item['iid'] = itemid
    types = node.get('itemtype', '').split()
    if types:
        if not self.strict and len(types) == 1:
            item['type'] = types[0]
        else:
            item['type'] = types
        nodeid = node.get('itemid')
        if nodeid:
            item['id'] = nodeid.strip()
    properties = collections.defaultdict(list)
    for (name, value) in self._extract_properties(node, items_seen=items_seen, base_url=base_url, itemids=itemids):
        properties[name].append(value)
    refs = node.get('itemref', '').split()
    if refs:
        for refid in refs:
            for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids):
                properties[name].append(value)
    props = []
    for (name, values) in properties.items():
        if not self.strict and len(values) == 1:
            props.append((name, values[0]))
        else:
            props.append((name, values))
    if props:
        item['properties'] = dict(props)
    else:
        item['value'] = self._extract_property_value(node, force=True, items_seen=items_seen, base_url=base_url, itemids=itemids)
    if self.add_text_content:
        textContent = self._extract_textContent(node)
        if textContent:
            item['textContent'] = textContent
    if self.add_html_node:
        item['htmlNode'] = node
    return item","for (name, values) in properties.items():
    if not self.strict and len(values) == 1:
        props.append((name, values[0]))
    else:
        props.append((name, values))","props = [(name, values[0]) if not self.strict and len(values) == 1 else (name, values) for (name, values) in properties.items()]","props = [(name, values[0]) if not self.strict and len(values) == 1 else (name, values) for (name, values) in properties.items()]",1,,,,,robosuite
extruct,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/extruct/extruct/w3cmicrodata.py,https://github.com/scrapinghub/extruct/tree/master/extruct/w3cmicrodata.py,LxmlMicrodataExtractor,_extract_item$84,"def _extract_item(self, node, items_seen, base_url, itemids):
    itemid = self.get_docid(node, itemids)
    if self.nested:
        if itemid in items_seen:
            return
        items_seen.add(itemid)
    item = {}
    if not self.nested:
        item['iid'] = itemid
    types = node.get('itemtype', '').split()
    if types:
        if not self.strict and len(types) == 1:
            item['type'] = types[0]
        else:
            item['type'] = types
        nodeid = node.get('itemid')
        if nodeid:
            item['id'] = nodeid.strip()
    properties = collections.defaultdict(list)
    for (name, value) in self._extract_properties(node, items_seen=items_seen, base_url=base_url, itemids=itemids):
        properties[name].append(value)
    refs = node.get('itemref', '').split()
    if refs:
        for refid in refs:
            for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids):
                properties[name].append(value)
    props = []
    for (name, values) in properties.items():
        if not self.strict and len(values) == 1:
            props.append((name, values[0]))
        else:
            props.append((name, values))
    if props:
        item['properties'] = dict(props)
    else:
        item['value'] = self._extract_property_value(node, force=True, items_seen=items_seen, base_url=base_url, itemids=itemids)
    if self.add_text_content:
        textContent = self._extract_textContent(node)
        if textContent:
            item['textContent'] = textContent
    if self.add_html_node:
        item['htmlNode'] = node
    return item","for refid in refs:
    for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids):
        properties[name].append(value)","properties[name] += [value for refid in refs for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids)]",Cannot refactor,-1,1,,,,robosuite
mmpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmpose/tests/test_backward_compatibility/test_dataset_info_compatibility/test_body3d_dataset_compatibility.py,https://github.com/open-mmlab/mmpose/tree/master/tests/test_backward_compatibility/test_dataset_info_compatibility/test_body3d_dataset_compatibility.py,,test_body3d_h36m_dataset_compatibility$11,"def test_body3d_h36m_dataset_compatibility():
    dataset = 'Body3DH36MDataset'
    dataset_class = DATASETS.get(dataset)
    data_cfg = dict(num_joints=17, seq_len=1, seq_frame_interval=1, joint_2d_src='pipeline', joint_2d_det_file=None, causal=False, need_camera_param=True, camera_param_file='tests/data/h36m/cameras.pkl')
    with pytest.warns(DeprecationWarning):
        _ = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=False)
    with pytest.warns(DeprecationWarning):
        custom_dataset = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=True)
    assert custom_dataset.test_mode is True
    _ = custom_dataset[0]
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs = []
        for result in custom_dataset:
            outputs.append({'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]})
        metrics = ['mpjpe', 'p-mpjpe', 'n-mpjpe']
        infos = custom_dataset.evaluate(outputs, tmpdir, metrics)
        np.testing.assert_almost_equal(infos['MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['P-MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['N-MPJPE'], 0.0)
    data_cfg = dict(num_joints=17, seq_len=27, seq_frame_interval=1, causal=True, temporal_padding=True, joint_2d_src='detection', joint_2d_det_file='tests/data/h36m/test_h36m_2d_detection.npy', need_camera_param=True, camera_param_file='tests/data/h36m/cameras.pkl')
    with pytest.warns(DeprecationWarning):
        _ = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=False)
    with pytest.warns(DeprecationWarning):
        custom_dataset = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=True)
    assert custom_dataset.test_mode is True
    _ = custom_dataset[0]
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs = []
        for result in custom_dataset:
            outputs.append({'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]})
        metrics = ['mpjpe', 'p-mpjpe', 'n-mpjpe']
        infos = custom_dataset.evaluate(outputs, tmpdir, metrics)
        np.testing.assert_almost_equal(infos['MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['P-MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['N-MPJPE'], 0.0)","for result in custom_dataset:
    outputs.append({'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]})","outputs = [{'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]} for result in custom_dataset]","outputs = [{'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]} for result in custom_dataset]",1,,,,,robosuite
mmpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmpose/tests/test_backward_compatibility/test_dataset_info_compatibility/test_body3d_dataset_compatibility.py,https://github.com/open-mmlab/mmpose/tree/master/tests/test_backward_compatibility/test_dataset_info_compatibility/test_body3d_dataset_compatibility.py,,test_body3d_h36m_dataset_compatibility$11,"def test_body3d_h36m_dataset_compatibility():
    dataset = 'Body3DH36MDataset'
    dataset_class = DATASETS.get(dataset)
    data_cfg = dict(num_joints=17, seq_len=1, seq_frame_interval=1, joint_2d_src='pipeline', joint_2d_det_file=None, causal=False, need_camera_param=True, camera_param_file='tests/data/h36m/cameras.pkl')
    with pytest.warns(DeprecationWarning):
        _ = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=False)
    with pytest.warns(DeprecationWarning):
        custom_dataset = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=True)
    assert custom_dataset.test_mode is True
    _ = custom_dataset[0]
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs = []
        for result in custom_dataset:
            outputs.append({'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]})
        metrics = ['mpjpe', 'p-mpjpe', 'n-mpjpe']
        infos = custom_dataset.evaluate(outputs, tmpdir, metrics)
        np.testing.assert_almost_equal(infos['MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['P-MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['N-MPJPE'], 0.0)
    data_cfg = dict(num_joints=17, seq_len=27, seq_frame_interval=1, causal=True, temporal_padding=True, joint_2d_src='detection', joint_2d_det_file='tests/data/h36m/test_h36m_2d_detection.npy', need_camera_param=True, camera_param_file='tests/data/h36m/cameras.pkl')
    with pytest.warns(DeprecationWarning):
        _ = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=False)
    with pytest.warns(DeprecationWarning):
        custom_dataset = dataset_class(ann_file='tests/data/h36m/test_h36m_body3d.npz', img_prefix='tests/data/h36m', data_cfg=data_cfg, pipeline=[], test_mode=True)
    assert custom_dataset.test_mode is True
    _ = custom_dataset[0]
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs = []
        for result in custom_dataset:
            outputs.append({'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]})
        metrics = ['mpjpe', 'p-mpjpe', 'n-mpjpe']
        infos = custom_dataset.evaluate(outputs, tmpdir, metrics)
        np.testing.assert_almost_equal(infos['MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['P-MPJPE'], 0.0)
        np.testing.assert_almost_equal(infos['N-MPJPE'], 0.0)","for result in custom_dataset:
    outputs.append({'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]})","outputs += [{'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]} for result in custom_dataset]","outputs = [{'preds': result['target'][None, ...], 'target_image_paths': [result['target_image_path']]} for result in custom_dataset]",0,1,,,,robosuite
data-driven-web-apps-with-flask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-driven-web-apps-with-flask/app/ch11_migrations/starter/pypi_org/bin/load_data.py,https://github.com/talkpython/data-driven-web-apps-with-flask/tree/master/app/ch11_migrations/starter/pypi_org/bin/load_data.py,,get_file_names$350,"def get_file_names(data_path: str) -> List[str]:
    files = []
    for f in os.listdir(data_path):
        if f.endswith('.json'):
            files.append(os.path.abspath(os.path.join(data_path, f)))
    files.sort()
    return files","for f in os.listdir(data_path):
    if f.endswith('.json'):
        files.append(os.path.abspath(os.path.join(data_path, f)))","files = [os.path.abspath(os.path.join(data_path, f)) for f in os.listdir(data_path) if f.endswith('.json')]","files = [os.path.abspath(os.path.join(data_path, f)) for f in os.listdir(data_path) if f.endswith('.json')]",1,,,,,robosuite
lc-all-solutions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lc-all-solutions/366.find-leaves-of-binary-tree/find-leaves-of-binary-tree.py,https://github.com/csujedihy/lc-all-solutions/tree/master/366.find-leaves-of-binary-tree/find-leaves-of-binary-tree.py,Solution,findLeaves$11,"def findLeaves(self, root):
    """"""
    :type root: TreeNode
    :rtype: List[List[int]]
    """"""

    def helper(p, res):
        if not p:
            return 0
        left = helper(p.left, res)
        right = helper(p.right, res)
        depth = max(left, right) + 1
        res[depth].append(p.val)
        return depth
    ans = []
    res = collections.defaultdict(list)
    helper(root, res)
    for i in range(1, len(res) + 1):
        ans.append(res[i])
    return ans","for i in range(1, len(res) + 1):
    ans.append(res[i])","ans = [res[i] for i in range(1, len(res) + 1)]","ans = [res[i] for i in range(1, len(res) + 1)]",1,,,,,robosuite
pytorch_Realtime_Multi-Person_Pose_Estimation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_Realtime_Multi-Person_Pose_Estimation/lib/network/rtpose_hourglass.py,https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/tree/master/lib/network/rtpose_hourglass.py,Hourglass,_make_residual$57,"def _make_residual(self, block, num_blocks, planes):
    layers = []
    for i in range(0, num_blocks):
        layers.append(block(planes * block.expansion, planes))
    return nn.Sequential(*layers)","for i in range(0, num_blocks):
    layers.append(block(planes * block.expansion, planes))","layers = [block(planes * block.expansion, planes) for i in range(0, num_blocks)]","layers = [block(planes * block.expansion, planes) for i in range(0, num_blocks)]",1,,,,,robosuite
mobly,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mobly/tests/lib/mock_second_controller.py,https://github.com/google/mobly/tree/master/tests/lib/mock_second_controller.py,,get_info$36,"def get_info(objs):
    infos = []
    for obj in objs:
        infos.append(obj.who_am_i())
    return infos","for obj in objs:
    infos.append(obj.who_am_i())",infos = [obj.who_am_i() for obj in objs],infos = [obj.who_am_i() for obj in objs],1,,,,,robosuite
pybossa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pybossa/test/test_jobs/test_engage_old_users.py,https://github.com/Scifabric/pybossa/tree/master/test/test_jobs/test_engage_old_users.py,TestEngageUsers,test_get_inactive_users_jobs_with_users$42,"def test_get_inactive_users_jobs_with_users(self):
    """"""Test JOB get with users returns empty list.""""""
    TaskRunFactory.create()
    jobs_generator = get_inactive_users_jobs()
    jobs = []
    for job in jobs_generator:
        jobs.append(job)
    msg = 'There should not be any job.'
    assert len(jobs) == 0, msg","for job in jobs_generator:
    jobs.append(job)",jobs += [job for job in jobs_generator],jobs = [job for job in jobs_generator],0,1,,,,robosuite
pybossa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pybossa/test/test_jobs/test_leaderboard_jobs.py,https://github.com/Scifabric/pybossa/tree/master/test/test_jobs/test_leaderboard_jobs.py,TestLeaderboard,test_leaderboard_foo_dash_key_current_user$166,"def test_leaderboard_foo_dash_key_current_user(self):
    """"""Test JOB leaderboard returns users for foo-dash key with current user.""""""
    users = []
    for score in range(1, 11):
        users.append(UserFactory.create(info={'foo-dash': score}))
    users.append(UserFactory.create(restrict=True, info={'foo-dash': 11}))
    leaderboard(info='foo-dash')
    top_users = get_leaderboard(user_id=users[0].id, info='foo-dash')
    assert len(top_users) == 11, len(top_users)
    score = 10
    for user in top_users[0:10]:
        (user['score'] == score, user)
        score = score - 1
    assert top_users[-1]['name'] == users[0].name
    assert top_users[-1]['score'] == users[0].info.get('foo-dash')
    results = db.session.execute('select * from ""users_rank_foo-dash""')
    for r in results:
        assert r.restrict is False, r","for score in range(1, 11):
    users.append(UserFactory.create(info={'foo-dash': score}))","users = [UserFactory.create(info={'foo-dash': score}) for score in range(1, 11)]","users = [UserFactory.create(info={'foo-dash': score}) for score in range(1, 11)]",1,,,,,robosuite
GyoiThon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GyoiThon/modules/Gyoi_Exploit.py,https://github.com/gyoisamurai/GyoiThon/tree/master/modules/Gyoi_Exploit.py,Exploit,get_exploit_tree$281,"def get_exploit_tree(self, all_exploit_list):
    self.utility.write_log(20, '[In] Get exploit tree [{}].'.format(self.file_name))
    self.utility.print_message(NOTE, 'Get exploit tree.')
    exploit_tree = {}
    if os.path.exists(os.path.join(self.data_path, 'exploit_tree.json')) is False:
        for (idx, exploit) in enumerate(all_exploit_list):
            exploit = exploit.replace('\n', '').replace('\r', '')
            temp_target_tree = {'targets': []}
            temp_tree = {}
            use_cmd = 'use exploit/' + exploit + '\n'
            _ = self.client.send_command(self.console_id, use_cmd, False)
            show_cmd = 'show targets\n'
            target_info = ''
            time_count = 0
            while True:
                ret = self.client.send_command(self.console_id, show_cmd, False)
                target_info = ret.get(b'data').decode('utf-8')
                if 'Exploit targets' in target_info:
                    break
                if time_count == 5:
                    self.utility.print_message(WARNING, 'Timeout: {}'.format(show_cmd))
                    self.utility.print_message(WARNING, 'No exist Targets.')
                    break
                time.sleep(1.0)
                time_count += 1
            target_list = self.cutting_strings('\\s*([0-9]{1,3}) .*[a-z|A-Z|0-9].*[\\r\\n]', target_info)
            for target in target_list:
                payload_list = self.client.get_target_compatible_payload_list(exploit, int(target))
                temp_tree[target] = payload_list
            options = self.client.get_module_options('exploit', exploit)
            key_list = options.keys()
            option = {}
            for key in key_list:
                sub_option = {}
                sub_key_list = options[key].keys()
                for sub_key in sub_key_list:
                    if isinstance(options[key][sub_key], list):
                        end_option = []
                        for end_key in options[key][sub_key]:
                            end_option.append(end_key.decode('utf-8'))
                        sub_option[sub_key.decode('utf-8')] = end_option
                    else:
                        end_option = {}
                        if isinstance(options[key][sub_key], bytes):
                            sub_option[sub_key.decode('utf-8')] = options[key][sub_key].decode('utf-8')
                        else:
                            sub_option[sub_key.decode('utf-8')] = options[key][sub_key]
                sub_option['user_specify'] = ''
                option[key.decode('utf-8')] = sub_option
            temp_target_tree['target_list'] = target_list
            temp_target_tree['targets'] = temp_tree
            temp_target_tree['options'] = option
            exploit_tree[exploit] = temp_target_tree
            msg = '{}/{} exploit:{}, targets:{}'.format(str(idx + 1), len(all_exploit_list), exploit, len(target_list))
            self.utility.print_message(OK, msg)
        with codecs.open(os.path.join(self.data_path, 'exploit_tree.json'), 'w', 'utf-8') as fout:
            json.dump(exploit_tree, fout, indent=4)
        self.utility.print_message(OK, 'Saved exploit tree.')
    else:
        exploit_tree = {}
        local_file = os.path.join(self.data_path, 'exploit_tree.json')
        self.utility.print_message(OK, 'Loading exploit tree from local file: {}'.format(local_file))
        with codecs.open(local_file, 'r', 'utf-8') as fin:
            exploit_tree = json.load(fin)
    self.utility.write_log(20, '[Out] Get exploit tree [{}].'.format(self.file_name))
    return exploit_tree","for end_key in options[key][sub_key]:
    end_option.append(end_key.decode('utf-8'))",end_option = [end_key.decode('utf-8') for end_key in options[key][sub_key]],end_option = [end_key.decode('utf-8') for end_key in options[key][sub_key]],1,,,,,robosuite
snscrape,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/snscrape/snscrape/modules/twitter.py,https://github.com/JustAnotherArchivist/snscrape/tree/master/snscrape/modules/twitter.py,TwitterAPIScraper,_tweet_to_tweet$347,"def _tweet_to_tweet(self, tweet, obj):
    kwargs = {}
    kwargs['id'] = tweet['id'] if 'id' in tweet else int(tweet['id_str'])
    kwargs['content'] = tweet['full_text']
    kwargs['renderedContent'] = self._render_text_with_urls(tweet['full_text'], tweet['entities'].get('urls'))
    kwargs['user'] = self._user_to_user(obj['globalObjects']['users'][tweet['user_id_str']])
    kwargs['date'] = email.utils.parsedate_to_datetime(tweet['created_at'])
    if tweet['entities'].get('urls'):
        kwargs['outlinks'] = [u['expanded_url'] for u in tweet['entities']['urls']]
        kwargs['tcooutlinks'] = [u['url'] for u in tweet['entities']['urls']]
    kwargs['url'] = f""https://twitter.com/{obj['globalObjects']['users'][tweet['user_id_str']]['screen_name']}/status/{kwargs['id']}""
    kwargs['replyCount'] = tweet['reply_count']
    kwargs['retweetCount'] = tweet['retweet_count']
    kwargs['likeCount'] = tweet['favorite_count']
    kwargs['quoteCount'] = tweet['quote_count']
    kwargs['conversationId'] = tweet['conversation_id'] if 'conversation_id' in tweet else int(tweet['conversation_id_str'])
    kwargs['lang'] = tweet['lang']
    kwargs['source'] = tweet['source']
    if (match := re.search('href=[\\\'""]?([^\\\'"" >]+)', tweet['source'])):
        kwargs['sourceUrl'] = match.group(1)
    if (match := re.search('>([^<]*)<', tweet['source'])):
        kwargs['sourceLabel'] = match.group(1)
    if 'extended_entities' in tweet and 'media' in tweet['extended_entities']:
        media = []
        for medium in tweet['extended_entities']['media']:
            if medium['type'] == 'photo':
                if '.' not in medium['media_url_https']:
                    logger.warning(f""Skipping malformed medium URL on tweet {kwargs['id']}: {medium['media_url_https']!r} contains no dot"")
                    continue
                (baseUrl, format) = medium['media_url_https'].rsplit('.', 1)
                if format not in ('jpg', 'png'):
                    logger.warning(f""Skipping photo with unknown format on tweet {kwargs['id']}: {format!r}"")
                    continue
                media.append(Photo(previewUrl=f'{baseUrl}?format={format}&name=small', fullUrl=f'{baseUrl}?format={format}&name=large'))
            elif medium['type'] == 'video' or medium['type'] == 'animated_gif':
                variants = []
                for variant in medium['video_info']['variants']:
                    variants.append(VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')))
                mKwargs = {'thumbnailUrl': medium['media_url_https'], 'variants': variants}
                if medium['type'] == 'video':
                    mKwargs['duration'] = medium['video_info']['duration_millis'] / 1000
                    if (ext := medium['ext']) and (mediaStats := ext['mediaStats']) and isinstance((r := mediaStats['r']), dict) and ('ok' in r) and isinstance(r['ok'], dict):
                        mKwargs['views'] = int(r['ok']['viewCount'])
                    cls = Video
                elif medium['type'] == 'animated_gif':
                    cls = Gif
                media.append(cls(**mKwargs))
        if media:
            kwargs['media'] = media
    if 'retweeted_status_id_str' in tweet:
        kwargs['retweetedTweet'] = self._tweet_to_tweet(obj['globalObjects']['tweets'][tweet['retweeted_status_id_str']], obj)
    if 'quoted_status_id_str' in tweet and tweet['quoted_status_id_str'] in obj['globalObjects']['tweets']:
        kwargs['quotedTweet'] = self._tweet_to_tweet(obj['globalObjects']['tweets'][tweet['quoted_status_id_str']], obj)
    if (inReplyToTweetId := tweet.get('in_reply_to_status_id_str')):
        kwargs['inReplyToTweetId'] = int(inReplyToTweetId)
        inReplyToUserId = int(tweet['in_reply_to_user_id_str'])
        if inReplyToUserId == kwargs['user'].id:
            kwargs['inReplyToUser'] = kwargs['user']
        elif tweet['entities'].get('user_mentions'):
            for u in tweet['entities']['user_mentions']:
                if u['id_str'] == tweet['in_reply_to_user_id_str']:
                    kwargs['inReplyToUser'] = User(username=u['screen_name'], id=u['id'] if 'id' in u else int(u['id_str']), displayname=u['name'])
        if 'inReplyToUser' not in kwargs:
            kwargs['inReplyToUser'] = User(username=tweet['in_reply_to_screen_name'], id=inReplyToUserId)
    if tweet['entities'].get('user_mentions'):
        kwargs['mentionedUsers'] = [User(username=u['screen_name'], id=u['id'] if 'id' in u else int(u['id_str']), displayname=u['name']) for u in tweet['entities']['user_mentions']]
    if tweet.get('coordinates'):
        if (coords := tweet['coordinates']['coordinates']) and len(coords) == 2:
            kwargs['coordinates'] = Coordinates(coords[0], coords[1])
    elif tweet.get('geo'):
        if (coords := tweet['geo']['coordinates']) and len(coords) == 2:
            kwargs['coordinates'] = Coordinates(coords[1], coords[0])
    if tweet.get('place'):
        kwargs['place'] = Place(tweet['place']['full_name'], tweet['place']['name'], tweet['place']['place_type'], tweet['place']['country'], tweet['place']['country_code'])
        if 'coordinates' not in kwargs and tweet['place']['bounding_box'] and (coords := tweet['place']['bounding_box']['coordinates']) and coords[0] and (len(coords[0][0]) == 2):
            kwargs['coordinates'] = Coordinates(coords[0][0][0], coords[0][0][1])
    if tweet['entities'].get('hashtags'):
        kwargs['hashtags'] = [o['text'] for o in tweet['entities']['hashtags']]
    if tweet['entities'].get('symbols'):
        kwargs['cashtags'] = [o['text'] for o in tweet['entities']['symbols']]
    return Tweet(**kwargs)","for medium in tweet['extended_entities']['media']:
    if medium['type'] == 'photo':
        if '.' not in medium['media_url_https']:
            logger.warning(f""Skipping malformed medium URL on tweet {kwargs['id']}: {medium['media_url_https']!r} contains no dot"")
            continue
        (baseUrl, format) = medium['media_url_https'].rsplit('.', 1)
        if format not in ('jpg', 'png'):
            logger.warning(f""Skipping photo with unknown format on tweet {kwargs['id']}: {format!r}"")
            continue
        media.append(Photo(previewUrl=f'{baseUrl}?format={format}&name=small', fullUrl=f'{baseUrl}?format={format}&name=large'))
    elif medium['type'] == 'video' or medium['type'] == 'animated_gif':
        variants = []
        for variant in medium['video_info']['variants']:
            variants.append(VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')))
        mKwargs = {'thumbnailUrl': medium['media_url_https'], 'variants': variants}
        if medium['type'] == 'video':
            mKwargs['duration'] = medium['video_info']['duration_millis'] / 1000
            if (ext := medium['ext']) and (mediaStats := ext['mediaStats']) and isinstance((r := mediaStats['r']), dict) and ('ok' in r) and isinstance(r['ok'], dict):
                mKwargs['views'] = int(r['ok']['viewCount'])
            cls = Video
        elif medium['type'] == 'animated_gif':
            cls = Gif
        media.append(cls(**mKwargs))","media += [Photo(previewUrl=f""{medium['media_url_https'].rsplit('.', 1)[0]}?format={medium['media_url_https'].rsplit('.', 1)[1]}&name=small"", fullUrl=f""{medium['media_url_https'].rsplit('.', 1)[0]}?format={medium['media_url_https'].rsplit('.', 1)[1]}&name=large"") if medium['type'] == 'photo' and '.' in medium['media_url_https'] and (medium['media_url_https'].rsplit('.', 1)[1] in ('jpg', 'png')) else Gif(thumbnailUrl=medium['media_url_https'], variants=[VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')) for variant in medium['video_info']['variants']]) if medium['type'] == 'animated_gif' else Video(thumbnailUrl=medium['media_url_https'], variants=[VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')) for variant in medium['video_info']['variants']], duration=medium['video_info']['duration_millis'] / 1000, views=int(medium['ext']['mediaStats']['r']['ok']['viewCount']) if 'ext' in medium and 'mediaStats' in medium['ext'] and isinstance(medium['ext']['mediaStats']['r'], dict) and ('ok' in medium['ext']['mediaStats']['r']) and isinstance(medium['ext']['mediaStats']['r']['ok'], dict) else None) if medium['type'] == 'video' else logger.warning(f""Skipping unknown medium type on tweet {kwargs['id']}: {medium['type']}"") for medium in tweet['extended_entities']['media']]",Cannot refactor,-1,0,,,,robosuite
snscrape,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/snscrape/snscrape/modules/twitter.py,https://github.com/JustAnotherArchivist/snscrape/tree/master/snscrape/modules/twitter.py,TwitterAPIScraper,_tweet_to_tweet$347,"def _tweet_to_tweet(self, tweet, obj):
    kwargs = {}
    kwargs['id'] = tweet['id'] if 'id' in tweet else int(tweet['id_str'])
    kwargs['content'] = tweet['full_text']
    kwargs['renderedContent'] = self._render_text_with_urls(tweet['full_text'], tweet['entities'].get('urls'))
    kwargs['user'] = self._user_to_user(obj['globalObjects']['users'][tweet['user_id_str']])
    kwargs['date'] = email.utils.parsedate_to_datetime(tweet['created_at'])
    if tweet['entities'].get('urls'):
        kwargs['outlinks'] = [u['expanded_url'] for u in tweet['entities']['urls']]
        kwargs['tcooutlinks'] = [u['url'] for u in tweet['entities']['urls']]
    kwargs['url'] = f""https://twitter.com/{obj['globalObjects']['users'][tweet['user_id_str']]['screen_name']}/status/{kwargs['id']}""
    kwargs['replyCount'] = tweet['reply_count']
    kwargs['retweetCount'] = tweet['retweet_count']
    kwargs['likeCount'] = tweet['favorite_count']
    kwargs['quoteCount'] = tweet['quote_count']
    kwargs['conversationId'] = tweet['conversation_id'] if 'conversation_id' in tweet else int(tweet['conversation_id_str'])
    kwargs['lang'] = tweet['lang']
    kwargs['source'] = tweet['source']
    if (match := re.search('href=[\\\'""]?([^\\\'"" >]+)', tweet['source'])):
        kwargs['sourceUrl'] = match.group(1)
    if (match := re.search('>([^<]*)<', tweet['source'])):
        kwargs['sourceLabel'] = match.group(1)
    if 'extended_entities' in tweet and 'media' in tweet['extended_entities']:
        media = []
        for medium in tweet['extended_entities']['media']:
            if medium['type'] == 'photo':
                if '.' not in medium['media_url_https']:
                    logger.warning(f""Skipping malformed medium URL on tweet {kwargs['id']}: {medium['media_url_https']!r} contains no dot"")
                    continue
                (baseUrl, format) = medium['media_url_https'].rsplit('.', 1)
                if format not in ('jpg', 'png'):
                    logger.warning(f""Skipping photo with unknown format on tweet {kwargs['id']}: {format!r}"")
                    continue
                media.append(Photo(previewUrl=f'{baseUrl}?format={format}&name=small', fullUrl=f'{baseUrl}?format={format}&name=large'))
            elif medium['type'] == 'video' or medium['type'] == 'animated_gif':
                variants = []
                for variant in medium['video_info']['variants']:
                    variants.append(VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')))
                mKwargs = {'thumbnailUrl': medium['media_url_https'], 'variants': variants}
                if medium['type'] == 'video':
                    mKwargs['duration'] = medium['video_info']['duration_millis'] / 1000
                    if (ext := medium['ext']) and (mediaStats := ext['mediaStats']) and isinstance((r := mediaStats['r']), dict) and ('ok' in r) and isinstance(r['ok'], dict):
                        mKwargs['views'] = int(r['ok']['viewCount'])
                    cls = Video
                elif medium['type'] == 'animated_gif':
                    cls = Gif
                media.append(cls(**mKwargs))
        if media:
            kwargs['media'] = media
    if 'retweeted_status_id_str' in tweet:
        kwargs['retweetedTweet'] = self._tweet_to_tweet(obj['globalObjects']['tweets'][tweet['retweeted_status_id_str']], obj)
    if 'quoted_status_id_str' in tweet and tweet['quoted_status_id_str'] in obj['globalObjects']['tweets']:
        kwargs['quotedTweet'] = self._tweet_to_tweet(obj['globalObjects']['tweets'][tweet['quoted_status_id_str']], obj)
    if (inReplyToTweetId := tweet.get('in_reply_to_status_id_str')):
        kwargs['inReplyToTweetId'] = int(inReplyToTweetId)
        inReplyToUserId = int(tweet['in_reply_to_user_id_str'])
        if inReplyToUserId == kwargs['user'].id:
            kwargs['inReplyToUser'] = kwargs['user']
        elif tweet['entities'].get('user_mentions'):
            for u in tweet['entities']['user_mentions']:
                if u['id_str'] == tweet['in_reply_to_user_id_str']:
                    kwargs['inReplyToUser'] = User(username=u['screen_name'], id=u['id'] if 'id' in u else int(u['id_str']), displayname=u['name'])
        if 'inReplyToUser' not in kwargs:
            kwargs['inReplyToUser'] = User(username=tweet['in_reply_to_screen_name'], id=inReplyToUserId)
    if tweet['entities'].get('user_mentions'):
        kwargs['mentionedUsers'] = [User(username=u['screen_name'], id=u['id'] if 'id' in u else int(u['id_str']), displayname=u['name']) for u in tweet['entities']['user_mentions']]
    if tweet.get('coordinates'):
        if (coords := tweet['coordinates']['coordinates']) and len(coords) == 2:
            kwargs['coordinates'] = Coordinates(coords[0], coords[1])
    elif tweet.get('geo'):
        if (coords := tweet['geo']['coordinates']) and len(coords) == 2:
            kwargs['coordinates'] = Coordinates(coords[1], coords[0])
    if tweet.get('place'):
        kwargs['place'] = Place(tweet['place']['full_name'], tweet['place']['name'], tweet['place']['place_type'], tweet['place']['country'], tweet['place']['country_code'])
        if 'coordinates' not in kwargs and tweet['place']['bounding_box'] and (coords := tweet['place']['bounding_box']['coordinates']) and coords[0] and (len(coords[0][0]) == 2):
            kwargs['coordinates'] = Coordinates(coords[0][0][0], coords[0][0][1])
    if tweet['entities'].get('hashtags'):
        kwargs['hashtags'] = [o['text'] for o in tweet['entities']['hashtags']]
    if tweet['entities'].get('symbols'):
        kwargs['cashtags'] = [o['text'] for o in tweet['entities']['symbols']]
    return Tweet(**kwargs)","for variant in medium['video_info']['variants']:
    variants.append(VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')))","variants += [VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')) for variant in medium['video_info']['variants']]","variants = [VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')) for variant in medium['video_info']['variants']]",0,1,,,,robosuite
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/datasets/common/utils/auto_lane_codec_utils.py,https://github.com/huawei-noah/vega/tree/master/vega/datasets/common/utils/auto_lane_codec_utils.py,,delete_repeat_y$301,"def delete_repeat_y(cur_line):
    """"""Avoid same y with multi x.

    :param cur_line: the raw line
    :type cur_line:list
    :return: the deduplicated line
    :rtype:list
    """"""
    list_x = []
    list_y = []
    for pt in cur_line:
        list_x.append(pt.x)
        list_y.append(pt.y)
    sorted_y = sorted(list_y)
    sorted_x = []
    for i in range(len(sorted_y)):
        sorted_x.append(list_x[list_y.index(sorted_y[i])])
    set_sorted_y = []
    set_sorted_x = []
    index = 0
    for i in sorted_y:
        if not i in set_sorted_y:
            set_sorted_y.append(i)
            set_sorted_x.append(sorted_x[index])
        index += 1
    new_lane = []
    if len(set_sorted_y) < 2:
        return new_lane
    for i in range(len(set_sorted_y)):
        new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})
    if new_lane[0]['y'] < new_lane[1]['y']:
        new_lane = new_lane[::-1]
    return new_lane","for pt in cur_line:
    list_x.append(pt.x)
    list_y.append(pt.y)",list_y = [pt.y for pt in cur_line],Cannot refactor,-1,0,,2,1,robosuite
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/datasets/common/utils/auto_lane_codec_utils.py,https://github.com/huawei-noah/vega/tree/master/vega/datasets/common/utils/auto_lane_codec_utils.py,,delete_repeat_y$301,"def delete_repeat_y(cur_line):
    """"""Avoid same y with multi x.

    :param cur_line: the raw line
    :type cur_line:list
    :return: the deduplicated line
    :rtype:list
    """"""
    list_x = []
    list_y = []
    for pt in cur_line:
        list_x.append(pt.x)
        list_y.append(pt.y)
    sorted_y = sorted(list_y)
    sorted_x = []
    for i in range(len(sorted_y)):
        sorted_x.append(list_x[list_y.index(sorted_y[i])])
    set_sorted_y = []
    set_sorted_x = []
    index = 0
    for i in sorted_y:
        if not i in set_sorted_y:
            set_sorted_y.append(i)
            set_sorted_x.append(sorted_x[index])
        index += 1
    new_lane = []
    if len(set_sorted_y) < 2:
        return new_lane
    for i in range(len(set_sorted_y)):
        new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})
    if new_lane[0]['y'] < new_lane[1]['y']:
        new_lane = new_lane[::-1]
    return new_lane","for i in range(len(sorted_y)):
    sorted_x.append(list_x[list_y.index(sorted_y[i])])",sorted_x = [list_x[list_y.index(sorted_y[i])] for i in range(len(sorted_y))],sorted_x = [list_x[list_y.index(sorted_y[i])] for i in range(len(sorted_y))],1,,,,,robosuite
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/datasets/common/utils/auto_lane_codec_utils.py,https://github.com/huawei-noah/vega/tree/master/vega/datasets/common/utils/auto_lane_codec_utils.py,,delete_repeat_y$301,"def delete_repeat_y(cur_line):
    """"""Avoid same y with multi x.

    :param cur_line: the raw line
    :type cur_line:list
    :return: the deduplicated line
    :rtype:list
    """"""
    list_x = []
    list_y = []
    for pt in cur_line:
        list_x.append(pt.x)
        list_y.append(pt.y)
    sorted_y = sorted(list_y)
    sorted_x = []
    for i in range(len(sorted_y)):
        sorted_x.append(list_x[list_y.index(sorted_y[i])])
    set_sorted_y = []
    set_sorted_x = []
    index = 0
    for i in sorted_y:
        if not i in set_sorted_y:
            set_sorted_y.append(i)
            set_sorted_x.append(sorted_x[index])
        index += 1
    new_lane = []
    if len(set_sorted_y) < 2:
        return new_lane
    for i in range(len(set_sorted_y)):
        new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})
    if new_lane[0]['y'] < new_lane[1]['y']:
        new_lane = new_lane[::-1]
    return new_lane","for i in range(len(set_sorted_y)):
    new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})","new_lane += [{'x': set_sorted_x[i], 'y': set_sorted_y[i]} for i in range(len(set_sorted_y))]",Cannot refactor,-1,1,,,,robosuite
neat-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neat-python/neat/parallel.py,https://github.com/CodeReclaimers/neat-python/tree/master/neat/parallel.py,ParallelEvaluator,evaluate$24,"def evaluate(self, genomes, config):
    jobs = []
    for (ignored_genome_id, genome) in genomes:
        jobs.append(self.pool.apply_async(self.eval_function, (genome, config)))
    for (job, (ignored_genome_id, genome)) in zip(jobs, genomes):
        genome.fitness = job.get(timeout=self.timeout)","for (ignored_genome_id, genome) in genomes:
    jobs.append(self.pool.apply_async(self.eval_function, (genome, config)))","jobs = [self.pool.apply_async(self.eval_function, (genome, config)) for (ignored_genome_id, genome) in genomes]","jobs = [self.pool.apply_async(self.eval_function, (genome, config)) for (ignored_genome_id, genome) in genomes]",1,,,,,robosuite
wavegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wavegan/train_specgan.py,https://github.com/chrisdonahue/wavegan/tree/master//train_specgan.py,,incept$466,"def incept(args):
    incept_dir = os.path.join(args.train_dir, 'incept')
    if not os.path.isdir(incept_dir):
        os.makedirs(incept_dir)
    gan_graph = tf.Graph()
    with gan_graph.as_default():
        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')
        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)
        score_saver = tf.train.Saver(max_to_keep=1)
    gan_z = gan_graph.get_tensor_by_name('z:0')
    gan_ngl = gan_graph.get_tensor_by_name('ngl:0')
    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]
    gan_step = gan_graph.get_tensor_by_name('global_step:0')
    z_fp = os.path.join(incept_dir, 'z.pkl')
    if os.path.exists(z_fp):
        with open(z_fp, 'rb') as f:
            _zs = pickle.load(f)
    else:
        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')
        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')
        with tf.Session(graph=gan_graph) as sess:
            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})
        with open(z_fp, 'wb') as f:
            pickle.dump(_zs, f)
    incept_graph = tf.Graph()
    with incept_graph.as_default():
        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)
    incept_x = incept_graph.get_tensor_by_name('x:0')
    incept_preds = incept_graph.get_tensor_by_name('scores:0')
    incept_sess = tf.Session(graph=incept_graph)
    incept_saver.restore(incept_sess, args.incept_ckpt_fp)
    summary_graph = tf.Graph()
    with summary_graph.as_default():
        incept_mean = tf.placeholder(tf.float32, [])
        incept_std = tf.placeholder(tf.float32, [])
        summaries = [tf.summary.scalar('incept_mean', incept_mean), tf.summary.scalar('incept_std', incept_std)]
        summaries = tf.summary.merge(summaries)
    summary_writer = tf.summary.FileWriter(incept_dir)
    ckpt_fp = None
    _best_score = 0.0
    while True:
        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)
        if latest_ckpt_fp != ckpt_fp:
            print('Incept: {}'.format(latest_ckpt_fp))
            sess = tf.Session(graph=gan_graph)
            gan_saver.restore(sess, latest_ckpt_fp)
            _step = sess.run(gan_step)
            _G_zs = []
            for i in xrange(0, args.incept_n, 100):
                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}))
            _G_zs = np.concatenate(_G_zs, axis=0)
            _preds = []
            for i in xrange(0, args.incept_n, 100):
                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}))
            _preds = np.concatenate(_preds, axis=0)
            _incept_scores = []
            split_size = args.incept_n // args.incept_k
            for i in xrange(args.incept_k):
                _split = _preds[i * split_size:(i + 1) * split_size]
                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))
                _kl = np.mean(np.sum(_kl, 1))
                _incept_scores.append(np.exp(_kl))
            (_incept_mean, _incept_std) = (np.mean(_incept_scores), np.std(_incept_scores))
            with tf.Session(graph=summary_graph) as summary_sess:
                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})
            summary_writer.add_summary(_summaries, _step)
            if _incept_mean > _best_score:
                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)
                _best_score = _incept_mean
            sess.close()
            print('Done')
            ckpt_fp = latest_ckpt_fp
        time.sleep(1)
    incept_sess.close()","for i in xrange(0, args.incept_n, 100):
    _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}))","_G_zs = [sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}) for i in xrange(0, args.incept_n, 100)]","_G_zs = [sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}) for i in xrange(0, args.incept_n, 100)]",1,,,,,robosuite
wavegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wavegan/train_specgan.py,https://github.com/chrisdonahue/wavegan/tree/master//train_specgan.py,,incept$466,"def incept(args):
    incept_dir = os.path.join(args.train_dir, 'incept')
    if not os.path.isdir(incept_dir):
        os.makedirs(incept_dir)
    gan_graph = tf.Graph()
    with gan_graph.as_default():
        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')
        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)
        score_saver = tf.train.Saver(max_to_keep=1)
    gan_z = gan_graph.get_tensor_by_name('z:0')
    gan_ngl = gan_graph.get_tensor_by_name('ngl:0')
    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]
    gan_step = gan_graph.get_tensor_by_name('global_step:0')
    z_fp = os.path.join(incept_dir, 'z.pkl')
    if os.path.exists(z_fp):
        with open(z_fp, 'rb') as f:
            _zs = pickle.load(f)
    else:
        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')
        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')
        with tf.Session(graph=gan_graph) as sess:
            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})
        with open(z_fp, 'wb') as f:
            pickle.dump(_zs, f)
    incept_graph = tf.Graph()
    with incept_graph.as_default():
        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)
    incept_x = incept_graph.get_tensor_by_name('x:0')
    incept_preds = incept_graph.get_tensor_by_name('scores:0')
    incept_sess = tf.Session(graph=incept_graph)
    incept_saver.restore(incept_sess, args.incept_ckpt_fp)
    summary_graph = tf.Graph()
    with summary_graph.as_default():
        incept_mean = tf.placeholder(tf.float32, [])
        incept_std = tf.placeholder(tf.float32, [])
        summaries = [tf.summary.scalar('incept_mean', incept_mean), tf.summary.scalar('incept_std', incept_std)]
        summaries = tf.summary.merge(summaries)
    summary_writer = tf.summary.FileWriter(incept_dir)
    ckpt_fp = None
    _best_score = 0.0
    while True:
        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)
        if latest_ckpt_fp != ckpt_fp:
            print('Incept: {}'.format(latest_ckpt_fp))
            sess = tf.Session(graph=gan_graph)
            gan_saver.restore(sess, latest_ckpt_fp)
            _step = sess.run(gan_step)
            _G_zs = []
            for i in xrange(0, args.incept_n, 100):
                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}))
            _G_zs = np.concatenate(_G_zs, axis=0)
            _preds = []
            for i in xrange(0, args.incept_n, 100):
                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}))
            _preds = np.concatenate(_preds, axis=0)
            _incept_scores = []
            split_size = args.incept_n // args.incept_k
            for i in xrange(args.incept_k):
                _split = _preds[i * split_size:(i + 1) * split_size]
                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))
                _kl = np.mean(np.sum(_kl, 1))
                _incept_scores.append(np.exp(_kl))
            (_incept_mean, _incept_std) = (np.mean(_incept_scores), np.std(_incept_scores))
            with tf.Session(graph=summary_graph) as summary_sess:
                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})
            summary_writer.add_summary(_summaries, _step)
            if _incept_mean > _best_score:
                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)
                _best_score = _incept_mean
            sess.close()
            print('Done')
            ckpt_fp = latest_ckpt_fp
        time.sleep(1)
    incept_sess.close()","for i in xrange(0, args.incept_n, 100):
    _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}))","_preds = [incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}) for i in range(0, args.incept_n, 100)]","_preds = [incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}) for i in xrange(0, args.incept_n, 100)]",0,1,,,,robosuite
wavegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wavegan/train_specgan.py,https://github.com/chrisdonahue/wavegan/tree/master//train_specgan.py,,incept$466,"def incept(args):
    incept_dir = os.path.join(args.train_dir, 'incept')
    if not os.path.isdir(incept_dir):
        os.makedirs(incept_dir)
    gan_graph = tf.Graph()
    with gan_graph.as_default():
        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')
        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)
        score_saver = tf.train.Saver(max_to_keep=1)
    gan_z = gan_graph.get_tensor_by_name('z:0')
    gan_ngl = gan_graph.get_tensor_by_name('ngl:0')
    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]
    gan_step = gan_graph.get_tensor_by_name('global_step:0')
    z_fp = os.path.join(incept_dir, 'z.pkl')
    if os.path.exists(z_fp):
        with open(z_fp, 'rb') as f:
            _zs = pickle.load(f)
    else:
        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')
        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')
        with tf.Session(graph=gan_graph) as sess:
            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})
        with open(z_fp, 'wb') as f:
            pickle.dump(_zs, f)
    incept_graph = tf.Graph()
    with incept_graph.as_default():
        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)
    incept_x = incept_graph.get_tensor_by_name('x:0')
    incept_preds = incept_graph.get_tensor_by_name('scores:0')
    incept_sess = tf.Session(graph=incept_graph)
    incept_saver.restore(incept_sess, args.incept_ckpt_fp)
    summary_graph = tf.Graph()
    with summary_graph.as_default():
        incept_mean = tf.placeholder(tf.float32, [])
        incept_std = tf.placeholder(tf.float32, [])
        summaries = [tf.summary.scalar('incept_mean', incept_mean), tf.summary.scalar('incept_std', incept_std)]
        summaries = tf.summary.merge(summaries)
    summary_writer = tf.summary.FileWriter(incept_dir)
    ckpt_fp = None
    _best_score = 0.0
    while True:
        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)
        if latest_ckpt_fp != ckpt_fp:
            print('Incept: {}'.format(latest_ckpt_fp))
            sess = tf.Session(graph=gan_graph)
            gan_saver.restore(sess, latest_ckpt_fp)
            _step = sess.run(gan_step)
            _G_zs = []
            for i in xrange(0, args.incept_n, 100):
                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}))
            _G_zs = np.concatenate(_G_zs, axis=0)
            _preds = []
            for i in xrange(0, args.incept_n, 100):
                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}))
            _preds = np.concatenate(_preds, axis=0)
            _incept_scores = []
            split_size = args.incept_n // args.incept_k
            for i in xrange(args.incept_k):
                _split = _preds[i * split_size:(i + 1) * split_size]
                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))
                _kl = np.mean(np.sum(_kl, 1))
                _incept_scores.append(np.exp(_kl))
            (_incept_mean, _incept_std) = (np.mean(_incept_scores), np.std(_incept_scores))
            with tf.Session(graph=summary_graph) as summary_sess:
                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})
            summary_writer.add_summary(_summaries, _step)
            if _incept_mean > _best_score:
                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)
                _best_score = _incept_mean
            sess.close()
            print('Done')
            ckpt_fp = latest_ckpt_fp
        time.sleep(1)
    incept_sess.close()","for i in xrange(args.incept_k):
    _split = _preds[i * split_size:(i + 1) * split_size]
    _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))
    _kl = np.mean(np.sum(_kl, 1))
    _incept_scores.append(np.exp(_kl))","_incept_scores = [np.exp(np.mean(np.sum(_split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0))), 1))) for i in xrange(args.incept_k) for _split in [_preds[i * split_size:(i + 1) * split_size]]]",Cannot refactor,-1,1,,,,robosuite
python-docs-samples,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-docs-samples/datastore/cloud-client/snippets.py,https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/datastore/cloud-client/snippets.py,,index_merge_queries$891,"def index_merge_queries(client):
    photo = datastore.Entity(client.key('Photo', 'sample_photo'))
    photo.update({'owner_id': 'user1234', 'size': 2, 'coloration': 2, 'tag': ['family', 'outside', 'camping']})
    client.put(photo)
    queries = []
    query_owner_id = client.query(kind='Photo', filters=[('owner_id', '=', 'user1234')])
    query_size = client.query(kind='Photo', filters=[('size', '=', 2)])
    query_coloration = client.query(kind='Photo', filters=[('coloration', '=', 2)])
    queries.append(query_owner_id)
    queries.append(query_size)
    queries.append(query_coloration)
    query_all_properties = client.query(kind='Photo', filters=[('owner_id', '=', 'user1234'), ('size', '=', 2), ('coloration', '=', 2), ('tag', '=', 'family')])
    queries.append(query_all_properties)
    query_tag = client.query(kind='Photo', filters=[('tag', '=', 'family'), ('tag', '=', 'outside'), ('tag', '=', 'camping')])
    query_owner_size_color_tags = client.query(kind='Photo', filters=[('owner_id', '=', 'user1234'), ('size', '=', 2), ('coloration', '=', 2), ('tag', '=', 'family'), ('tag', '=', 'outside'), ('tag', '=', 'camping')])
    queries.append(query_tag)
    queries.append(query_owner_size_color_tags)
    query_owner_size_tag = client.query(kind='Photo', filters=[('owner_id', '=', 'username'), ('size', '=', 2), ('tag', '=', 'family')])
    queries.append(query_owner_size_tag)
    query_size_coloration = client.query(kind='Photo', filters=[('size', '=', 2), ('coloration', '=', 1)])
    queries.append(query_size_coloration)
    results = []
    for query in queries:
        results.append(query.fetch())
    return results","for query in queries:
    results.append(query.fetch())",results = [query.fetch() for query in queries],results = [query.fetch() for query in queries],1,,,,,robosuite
SickChill,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SickChill/tests/test_numdict.py,https://github.com/SickChill/SickChill/tree/master/tests/test_numdict.py,NumDictTest,test_dict_access_and_mod$242,"def test_dict_access_and_mod(self):
    """"""
        Test num dict access and modification
        """"""
    dict_0 = {}
    dict_1 = {1: 'Elephant'}
    dict_2 = {1: 'Elephant', 2: 'Mouse'}
    num_dict_0 = NumDict()
    num_dict_1 = NumDict(dict_1)
    num_dict_2 = NumDict(dict_2)
    assert num_dict_2[1] == 'Elephant'
    with pytest.raises(KeyError):
        _ = num_dict_1['Mouse']
    with pytest.raises(KeyError):
        _ = num_dict_1.__getitem__('Mouse')
    with pytest.raises(KeyError):
        _ = num_dict_1[None]
    with pytest.raises(KeyError):
        _ = num_dict_1.__getitem__(None)
    num_dict_3 = NumDict(num_dict_2)
    assert num_dict_2 == num_dict_3
    num_dict_3[2] = 'Frog'
    assert num_dict_2 != num_dict_3
    num_dict_3['3'] = 'Armadillo'
    num_dict_3[None] = 'Cockroach'
    num_dict_3[12390809518259081208909880312] = 'Squid'
    num_dict_3['12390809518259081208909880312'] = 'Octopus'
    assert num_dict_3[12390809518259081208909880312] == 'Octopus'
    with pytest.raises(TypeError):
        num_dict_3.__setitem__('Gorilla', 1)
    with pytest.raises(TypeError):
        num_dict_3['Chimpanzee'] = 1
    with pytest.raises(TypeError):
        num_dict_3[4, 1] = 1
    with pytest.raises(TypeError):
        num_dict_3[[1, 3, 4]] = 1
    del num_dict_3[3]
    del num_dict_3[None]
    with pytest.raises(KeyError):
        del num_dict_3[3]
    with pytest.raises(KeyError):
        num_dict_3.__delitem__(3)
    with pytest.raises(KeyError):
        del num_dict_3['Mouse']
    num_dict_3.clear()
    assert num_dict_3 == {}
    num_dict_2a = dict_2.copy()
    assert num_dict_2 == num_dict_2a
    num_dict_2b = num_dict_2.copy()
    assert num_dict_2b == num_dict_2
    num_dict_2c = UserDict({1: 'Elephant', 2: 'Mouse'})
    num_dict_2d = num_dict_2c.copy()
    assert num_dict_2c == num_dict_2d

    class MyNumDict(NumDict):
        """"""
            subclass Numdict for testing
            """"""

        def display(self):
            """"""
                add a method to subclass to differentiate from superclass
                """"""
            print('MyNumDict:', self)
    my_num_dict = MyNumDict(num_dict_2)
    my_num_dict_a = my_num_dict.copy()
    assert my_num_dict_a == my_num_dict
    my_num_dict[1] = 'Frog'
    assert my_num_dict_a != my_num_dict
    assert sorted(num_dict_2) == sorted(dict_2)
    assert sorted(num_dict_2.items()) == sorted(dict_2.items())
    assert sorted(num_dict_2.values()) == sorted(dict_2.values())
    for i in num_dict_2:
        assert i in num_dict_2
        assert (i in num_dict_1) == (i in dict_1)
        assert (i in num_dict_0) == (i in dict_0)
    assert None not in num_dict_2
    assert (None in num_dict_2) is (None in dict_2)
    dict_2[None] = 'Cow'
    num_dict_2[None] = dict_2[None]
    assert None in num_dict_2
    assert (None in num_dict_2) is (None in dict_2)
    assert 'Penguin' not in num_dict_2
    test = NumDict()
    test.update(dict_2)
    assert test == num_dict_2
    for i in num_dict_2:
        assert num_dict_2.get(i) == num_dict_2[i]
        assert num_dict_1.get(i) == dict_1.get(i)
        assert num_dict_0.get(i) == dict_0.get(i)
    for i in ['purple', None, 12312301924091284, 23]:
        assert num_dict_2.get(i) == dict_2.get(i), i
    with pytest.raises(AssertionError):
        assert num_dict_2.get('1') == dict_2.get('1'), 1
    num_dict_2b = num_dict_2
    for i in range(20):
        num_dict_2[i] = str(i)
        num_dict_2b[str(i)] = str(i)
    assert num_dict_2 == num_dict_2b
    ikeys = []
    for k in num_dict_2:
        ikeys.append(k)
    assert set(ikeys) == set(num_dict_2)
    val = 1
    test = NumDict()
    assert test.setdefault(val, 42) == 42
    assert test.setdefault(val, '42') == 42
    assert test.setdefault(val, 42) != '42'
    assert test.setdefault(val, '42') != '42'
    assert val in test
    assert test.setdefault(val, 23) == 42
    assert test.setdefault(val, '23') == 42
    assert test.setdefault(val, 23) != '42'
    assert test.setdefault(val, '23') != '42'
    assert val in test
    val = 1
    test = NumDict({val: 42})
    assert test.pop(val) == 42
    pytest.raises(KeyError, test.pop, val)
    assert test.pop(val, 1) == 1
    test[val] = 42
    assert test.pop(val, 1) == 42
    val = 1
    test = NumDict({val: 42})
    assert test.popitem() == (val, 42)
    pytest.raises(KeyError, test.popitem)","for k in num_dict_2:
    ikeys.append(k)",ikeys = [k for k in num_dict_2],ikeys = [k for k in num_dict_2],1,,,,,robosuite
sdc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sdc/sdc/tests/indexes/test_int64_index.py,https://github.com/IntelPython/sdc/tree/master/sdc/tests/indexes/test_int64_index.py,TestInt64Index,test_impl$295,"def test_impl(index):
    res = []
    for (i, label) in enumerate(index):
        res.append((i, label))
    return res","for (i, label) in enumerate(index):
    res.append((i, label))","res = [(i, label) for (i, label) in enumerate(index)]","res = [(i, label) for (i, label) in enumerate(index)]",1,,,,,robosuite
joblib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joblib/benchmarks/bench_pickle.py,https://github.com/joblib/joblib/tree/master/benchmarks/bench_pickle.py,,generate_rand_list$109,"def generate_rand_list(size, with_arrays=False, with_string=False, array_shape=(10, 10)):
    """"""Generate list with random values from list of keys.""""""
    ret = []
    rnd = np.random.RandomState(0)
    for random in rnd.random_sample(size):
        if with_arrays:
            ret.append(rnd.random_sample(array_shape))
        elif with_string:
            ret.append(str(random))
        else:
            ret.append(int(random))
    return ret","for random in rnd.random_sample(size):
    if with_arrays:
        ret.append(rnd.random_sample(array_shape))
    elif with_string:
        ret.append(str(random))
    else:
        ret.append(int(random))",ret = [rnd.random_sample(array_shape) if with_arrays else str(random) if with_string else int(random) for random in rnd.random_sample(size)],ret = [rnd.random_sample(array_shape) if with_arrays else str(random) if with_string else int(random) for random in rnd.random_sample(size)],1,,,,,robosuite
dm_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm_control/dm_control/locomotion/arenas/mazes.py,https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/arenas/mazes.py,MazeWithTargets,grid_to_world_positions$357,"def grid_to_world_positions(self, grid_positions):
    out = []
    for (y, x) in grid_positions:
        out.append(np.array([(x - self._x_offset) * self._xy_scale, -(y - self._y_offset) * self._xy_scale, 0.0]))
    return out","for (y, x) in grid_positions:
    out.append(np.array([(x - self._x_offset) * self._xy_scale, -(y - self._y_offset) * self._xy_scale, 0.0]))","out = [np.array([(x - self._x_offset) * self._xy_scale, -(y - self._y_offset) * self._xy_scale, 0.0]) for (y, x) in grid_positions]","out = [np.array([(x - self._x_offset) * self._xy_scale, -(y - self._y_offset) * self._xy_scale, 0.0]) for (y, x) in grid_positions]",1,,,,,robosuite
LibreASR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LibreASR/libreasr/lib/learner.py,https://github.com/iceychris/LibreASR/tree/master/libreasr/lib/learner.py,ASRLearner,from_config$173,"def from_config(conf, db, m):
    cbs = [CudaCallback(), TerminateOnNaNCallback(), SaveModelCallback(), ReduceLROnPlateau(patience=1, min_lr=1e-05, factor=1.5)]
    optim = conf['training']['optimizer'].lower()
    if optim == 'ranger':
        opt_func = ranger
    elif optim == 'ranger_adabelief':
        opt_func = ranger_adabelief
    elif optim == 'adam':
        opt_func = Adam
    elif optim == 'lamb':
        opt_func = Lamb
    elif optim == 'apollo':
        from fastai2.optimizer import OptimWrapper

        def of(param_groups, **kwargs):
            lr_init = 0.0001
            lr = 0.001
            warmup = 10
            wd = 0.0004
            apollo = Apollo(param_groups, lr=lr, warmup=warmup)
            new_pgs = []
            for pg in param_groups:
                new_pgs.append({'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup})
            apollo.param_groups = new_pgs
            opt = OptimWrapper(apollo)
            return opt
        opt_func = of
    elif optim == 'adahessian':
        opt_func = AdaHessian
        cbs.append(HutchinsonTraceCallback())

        @patch
        def _backward(self: Learner):
            if self.opt._hutch_iter % HESSIAN_EVERY == 0:
                self.loss.backward(create_graph=True)
            else:
                self.loss.backward()
    else:
        raise Exception('No such optimizer')
    acnb = conf['accumulate_n_batches']
    if acnb > 1 and (not optim == 'adahessian'):
        cbs.append(GradAccumCallback(num_batches=acnb))
    extra_cbs = []
    if conf['mp']:
        extra_cbs.append(MixedPrecision(clip=conf['mp_clip']))
        _ = m.half()
    if conf['tensorboard']:
        _tb = partial(Tensorboard, wandb=conf['wandb'], test=True, tests_per_epoch=conf['tests_per_epoch'], mp=conf['mp'])()
        extra_cbs.append(_tb)
    learn = Learner(db, m, loss_func=get_loss_func('rnnt', conf['cuda']['device'], conf['model']['encoder']['reduction_factor'], debug=False, perf=False, div_by_len=False, entropy_loss=False), opt_func=opt_func, splitter=partial(transducer_splitter, adahessian=optim == 'adahessian'), cbs=cbs)
    learn.extra_cbs = extra_cbs
    if conf['mp']:
        learn.dls.device = torch.device('cuda:0')
    return learn","for pg in param_groups:
    new_pgs.append({'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup})","new_pgs = [{'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup} for pg in param_groups]","new_pgs = [{'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup} for pg in param_groups]",1,,,,,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for op in graph.get_operations():
    for x in op.inputs:
        op_to_all[op.name].append(x.name)
    for y in op.outputs:
        output_to_op[y.name].append(op.name)
        op_to_all[op.name].append(y.name)
    if str(op.type) == 'Assign':
        for y in op.outputs:
            for x in op.inputs:
                assign_out_to_in[y.name].append(x.name)",assign_out_to_in[y.name] = [x.name for op in graph.get_operations() if str(op.type) == 'Assign' for x in op.inputs for y in op.outputs if y.name == x.name],Cannot refactor,-1,0,,2,1,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for out_name in assign_out_to_in.keys():
    name_group = assign_out_to_in[out_name]
    for n1 in name_group:
        assign_groups[n1].append(out_name)
        for n2 in name_group:
            if n1 != n2:
                assign_groups[n1].append(n2)",assign_groups[n1] = [out_name for out_name in assign_out_to_in.keys() for n1 in assign_out_to_in[out_name] for n2 in assign_out_to_in[out_name] if n1 != n2] + [n2 for out_name in assign_out_to_in.keys() for n1 in assign_out_to_in[out_name] for n2 in assign_out_to_in[out_name] if n1 != n2],Cannot refactor,-1,0,,,,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for op in graph.get_operations():
    is_unreachable = False
    all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
    for name in all_names:
        if name not in seen_tensors:
            is_unreachable = True
    if is_unreachable:
        unreachable_ops.append(op)",unreachable_ops = [op for op in graph.get_operations() if any((name not in seen_tensors for name in [x.name for x in op.inputs] + [x.name for x in op.outputs]))],Cannot refactor,-1,1,,,,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for x in op.inputs:
    op_to_all[op.name].append(x.name)",op_to_all[op.name] = [x.name for x in op.inputs],Cannot refactor,-1,1,,,,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for y in op.outputs:
    output_to_op[y.name].append(op.name)
    op_to_all[op.name].append(y.name)",op_to_all[op.name] += [y.name for y in op.outputs],Cannot refactor,-1,0,,2,1,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for expanded_name in expanded_names:
    if expanded_name not in stack:
        stack.append(expanded_name)",stack += [expanded_name for expanded_name in expanded_names if expanded_name not in stack] + stack,Cannot refactor,-1,,,,,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for op_name in output_to_op[name]:
    if op_name in op_to_all:
        for input_name in op_to_all[op_name]:
            if input_name not in stack:
                stack.append(input_name)",stack += [input_name for op_name in output_to_op[name] for input_name in op_to_all[op_name] if op_name in op_to_all and input_name not in stack],Cannot refactor,-1,1,,,,robosuite
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for assign_name in assign_groups[name]:
    expanded_names.append(assign_name)",expanded_names = [assign_name for assign_name in assign_groups[name]],expanded_names = [assign_name for assign_name in assign_groups[name]],1,,,,,robosuite
my_first_tic_tac_toe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/my_first_tic_tac_toe/codegenerator/generate_ttt.py,https://github.com/asweigart/my_first_tic_tac_toe/tree/master/codegenerator/generate_ttt.py,,chooseRandomMoveFromList$61,"def chooseRandomMoveFromList(board, movesList):
    possibleMoves = []
    for i in movesList:
        if isSpaceFree(board, i):
            possibleMoves.append(i)
    if len(possibleMoves) != 0:
        return random.choice(possibleMoves)
    else:
        return None","for i in movesList:
    if isSpaceFree(board, i):
        possibleMoves.append(i)","possibleMoves = [i for i in movesList if isSpaceFree(board, i)]","possibleMoves = [i for i in movesList if isSpaceFree(board, i)]",1,,,,,robosuite
SogouMRCToolkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SogouMRCToolkit/sogou_mrc/libraries/modeling.py,https://github.com/sogou/SogouMRCToolkit/tree/master/sogou_mrc/libraries/modeling.py,,embedding_postprocessor$434,"def embedding_postprocessor(input_tensor, use_token_type=False, token_type_ids=None, token_type_vocab_size=16, token_type_embedding_name='token_type_embeddings', use_position_embeddings=True, position_embedding_name='position_embeddings', initializer_range=0.02, max_position_embeddings=512, dropout_prob=0.1, is_training=tf.constant(True, dtype=tf.bool)):
    """"""Performs various post-processing on a word embedding tensor.

  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length,
      embedding_size].
    use_token_type: bool. Whether to add embeddings for `token_type_ids`.
    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      Must be specified if `use_token_type` is True.
    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
    token_type_embedding_name: string. The name of the embedding table variable
      for token type ids.
    use_position_embeddings: bool. Whether to add position embeddings for the
      position of each token in the sequence.
    position_embedding_name: string. The name of the embedding table variable
      for positional embeddings.
    initializer_range: float. Range of the weight initialization.
    max_position_embeddings: int. Maximum sequence length that might ever be
      used with this model. This can be longer than the sequence length of
      input_tensor, but cannot be shorter.
    dropout_prob: float. Dropout probability applied to the final output tensor.

  Returns:
    float tensor with same shape as `input_tensor`.

  Raises:
    ValueError: One of the tensor shapes or input values is invalid.
  """"""
    input_shape = get_shape_list(input_tensor, expected_rank=3)
    batch_size = input_shape[0]
    seq_length = input_shape[1]
    width = input_shape[2]
    output = input_tensor
    if use_token_type:
        if token_type_ids is None:
            raise ValueError('`token_type_ids` must be specified if`use_token_type` is True.')
        token_type_table = tf.get_variable(name=token_type_embedding_name, shape=[token_type_vocab_size, width], initializer=create_initializer(initializer_range))
        flat_token_type_ids = tf.reshape(token_type_ids, [-1])
        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)
        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
        token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])
        output += token_type_embeddings
    if use_position_embeddings:
        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
        with tf.control_dependencies([assert_op]):
            full_position_embeddings = tf.get_variable(name=position_embedding_name, shape=[max_position_embeddings, width], initializer=create_initializer(initializer_range))
            position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])
            num_dims = len(output.shape.as_list())
            position_broadcast_shape = []
            for _ in range(num_dims - 2):
                position_broadcast_shape.append(1)
            position_broadcast_shape.extend([seq_length, width])
            position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)
            output += position_embeddings
    output = layer_norm_and_dropout(output, dropout_prob, is_training)
    return output","for _ in range(num_dims - 2):
    position_broadcast_shape.append(1)",position_broadcast_shape = [1 for _ in range(num_dims - 2)],position_broadcast_shape = [1 for _ in range(num_dims - 2)],1,,,,,robosuite
jira,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jira/jira/client.py,https://github.com/pycontribs/jira/tree/master/jira/client.py,JIRA,groups$1322,"def groups(self, query: Optional[str]=None, exclude: Optional[Any]=None, maxResults: int=9999) -> List[str]:
    """"""Return a list of groups matching the specified criteria.

        Args:
            query (Optional[str]): filter groups by name with this string
            exclude (Optional[Any]): filter out groups by name with this string
            maxResults (int): maximum results to return. (Default: 9999)
        Returns:
            List[str]

        """"""
    params: Dict[str, Any] = {}
    groups = []
    if query is not None:
        params['query'] = query
    if exclude is not None:
        params['exclude'] = exclude
    if maxResults is not None:
        params['maxResults'] = maxResults
    for group in self._get_json('groups/picker', params=params)['groups']:
        groups.append(group['name'])
    return sorted(groups)","for group in self._get_json('groups/picker', params=params)['groups']:
    groups.append(group['name'])","groups += [group['name'] for group in self._get_json('groups/picker', params=params)['groups']]","groups = [group['name'] for group in self._get_json('groups/picker', params=params)['groups']]",0,1,,,,robosuite
django-zappa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-zappa/django_zappa/management/commands/tail.py,https://github.com/Miserlou/django-zappa/tree/master/django_zappa/management/commands/tail.py,Command,handle$36,"def handle(self, *args, **options):
    """"""
        Execute the command.

        """"""
    self.require_settings(args, options)
    self.load_credentials()
    try:
        all_logs = self.zappa.fetch_logs(self.lambda_name)
        self.print_logs(all_logs)
        while True:
            all_logs_again = self.zappa.fetch_logs(self.lambda_name)
            new_logs = []
            for log in all_logs_again:
                if log not in all_logs:
                    new_logs.append(log)
            self.print_logs(new_logs)
            all_logs = all_logs + new_logs
    except KeyboardInterrupt:
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
    return","for log in all_logs_again:
    if log not in all_logs:
        new_logs.append(log)",new_logs = [log for log in all_logs_again if log not in all_logs],new_logs = [log for log in all_logs_again if log not in all_logs],1,,,,,robosuite
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_poly$127,"def may_augment_poly(self, aug, img_shape, polys):
    imgaug_polys = []
    for poly in polys:
        poly = poly[0]
        poly = poly.reshape(-1, 2)
        imgaug_polys.append(imgaug.Polygon(poly))
    imgaug_polys = aug.augment_polygons([imgaug.PolygonsOnImage(imgaug_polys, shape=img_shape)])[0].clip_out_of_image()
    new_polys = []
    for poly in imgaug_polys.polygons:
        new_poly = []
        for point in poly:
            new_poly.append(np.array(point, dtype=np.float32))
        new_poly = np.array(new_poly, dtype=np.float32).flatten()
        new_polys.append([new_poly])
    return new_polys","for poly in polys:
    poly = poly[0]
    poly = poly.reshape(-1, 2)
    imgaug_polys.append(imgaug.Polygon(poly))","imgaug_polys = [imgaug.Polygon(poly[0].reshape(-1, 2)) for poly in polys]",Cannot refactor,-1,1,,,,robosuite
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_poly$127,"def may_augment_poly(self, aug, img_shape, polys):
    imgaug_polys = []
    for poly in polys:
        poly = poly[0]
        poly = poly.reshape(-1, 2)
        imgaug_polys.append(imgaug.Polygon(poly))
    imgaug_polys = aug.augment_polygons([imgaug.PolygonsOnImage(imgaug_polys, shape=img_shape)])[0].clip_out_of_image()
    new_polys = []
    for poly in imgaug_polys.polygons:
        new_poly = []
        for point in poly:
            new_poly.append(np.array(point, dtype=np.float32))
        new_poly = np.array(new_poly, dtype=np.float32).flatten()
        new_polys.append([new_poly])
    return new_polys","for poly in imgaug_polys.polygons:
    new_poly = []
    for point in poly:
        new_poly.append(np.array(point, dtype=np.float32))
    new_poly = np.array(new_poly, dtype=np.float32).flatten()
    new_polys.append([new_poly])","new_polys = [[[np.array(point, dtype=np.float32) for point in poly].flatten()] for poly in imgaug_polys.polygons]",Cannot refactor,-1,1,,,,robosuite
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_poly$127,"def may_augment_poly(self, aug, img_shape, polys):
    imgaug_polys = []
    for poly in polys:
        poly = poly[0]
        poly = poly.reshape(-1, 2)
        imgaug_polys.append(imgaug.Polygon(poly))
    imgaug_polys = aug.augment_polygons([imgaug.PolygonsOnImage(imgaug_polys, shape=img_shape)])[0].clip_out_of_image()
    new_polys = []
    for poly in imgaug_polys.polygons:
        new_poly = []
        for point in poly:
            new_poly.append(np.array(point, dtype=np.float32))
        new_poly = np.array(new_poly, dtype=np.float32).flatten()
        new_polys.append([new_poly])
    return new_polys","for point in poly:
    new_poly.append(np.array(point, dtype=np.float32))","new_poly += [np.array(point, dtype=np.float32) for point in poly]","new_poly = [np.array(point, dtype=np.float32) for point in poly]",0,1,,,,robosuite
cats-blender-plugin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/tools/translate.py,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/tools/translate.py,TranslateBonesButton,execute$114,"def execute(self, context):
    to_translate = []
    for armature in Common.get_armature_objects():
        for bone in armature.data.bones:
            to_translate.append(bone.name)
    update_dictionary(to_translate, self=self)
    count = 0
    for armature in Common.get_armature_objects():
        for bone in armature.data.bones:
            (bone.name, translated) = translate(bone.name)
            if translated:
                count += 1
    self.report({'INFO'}, t('TranslateBonesButton.success', number=str(count)))
    return {'FINISHED'}","for armature in Common.get_armature_objects():
    for bone in armature.data.bones:
        to_translate.append(bone.name)",to_translate = [bone.name for armature in Common.get_armature_objects() for bone in armature.data.bones],to_translate = [bone.name for armature in Common.get_armature_objects() for bone in armature.data.bones],1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/visualization/plugins/interactive_environments.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/visualization/plugins/interactive_environments.py,InteractiveEnvironmentRequest,get_container_port_mapping$582,"def get_container_port_mapping(self, inspect_data):
    """"""
        :type inspect_data: dict
        :param inspect_data: output of docker inspect
        :returns: a list of triples containing (internal_port, external_ip,
                  external_port), of which the ports are probably the only
                  useful information.

        Someday code that calls this should be refactored whenever we get
        containers with multiple ports working.
        """"""
    mappings = []
    port_mappings = inspect_data[0]['NetworkSettings']['Ports']
    for port_name in port_mappings:
        for binding in port_mappings[port_name]:
            mappings.append((int(port_name.replace('/tcp', '').replace('/udp', '')), binding['HostIp'], int(binding['HostPort'])))
    return mappings","for port_name in port_mappings:
    for binding in port_mappings[port_name]:
        mappings.append((int(port_name.replace('/tcp', '').replace('/udp', '')), binding['HostIp'], int(binding['HostPort'])))","mappings += [(int(port_name.replace('/tcp', '').replace('/udp', '')), binding['HostIp'], int(binding['HostPort'])) for port_name in port_mappings for binding in port_mappings[port_name]]","mappings = [(int(port_name.replace('/tcp', '').replace('/udp', '')), binding['HostIp'], int(binding['HostPort'])) for port_name in port_mappings for binding in port_mappings[port_name]]",0,1,,,,robosuite
machine_learning_security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/machine_learning_security/DeepExploit/DeepExploit.py,https://github.com/13o-bbr-bbq/machine_learning_security/tree/master/DeepExploit/DeepExploit.py,Metasploit,set_pivoting$1593,"def set_pivoting(self, session_id, ip_list):
    temp_subnet = []
    for internal_ip in ip_list:
        temp_subnet.append(self.get_subnet(session_id, internal_ip))
    for subnet in list(map(list, set(map(tuple, temp_subnet)))):
        cmd = 'run autoroute -s ' + subnet[0] + ' ' + subnet[1] + '\n'
        _ = self.client.execute_meterpreter(session_id, cmd)
        time.sleep(3.0)
        _ = self.client.execute_meterpreter(session_id, 'run autoroute -p\n')","for internal_ip in ip_list:
    temp_subnet.append(self.get_subnet(session_id, internal_ip))","temp_subnet = [self.get_subnet(session_id, internal_ip) for internal_ip in ip_list]","temp_subnet = [self.get_subnet(session_id, internal_ip) for internal_ip in ip_list]",1,,,,,robosuite
subuser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/subuser/subuserlib/classes/installedImage.py,https://github.com/subuser-security/subuser/tree/master/subuserlib/classes/installedImage.py,InstalledImage,getImageLineage$122,"def getImageLineage(self):
    """"""
    Return the list(lineage) of InstalledImages which goes from a base image to this image including all of the image's ancestors in order of dependency.
    """"""
    lineage = []
    dockerImageLayers = self.getLineageLayers()
    for dockerImageLayer in dockerImageLayers:
        if dockerImageLayer in self.user.installedImages:
            lineage.append(self.user.installedImages[dockerImageLayer])
    return lineage","for dockerImageLayer in dockerImageLayers:
    if dockerImageLayer in self.user.installedImages:
        lineage.append(self.user.installedImages[dockerImageLayer])",lineage = [self.user.installedImages[dockerImageLayer] for dockerImageLayer in dockerImageLayers if dockerImageLayer in self.user.installedImages],lineage = [self.user.installedImages[dockerImageLayer] for dockerImageLayer in dockerImageLayers if dockerImageLayer in self.user.installedImages],1,,,,,robosuite
nlp-recipes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py,https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py,S2SAbsSumProcessor,s2s_dataset_from_iterable_sum_ds$245,"def s2s_dataset_from_iterable_sum_ds(self, sum_ds, train_mode, cached_features_file=None, local_rank=-1, top_n=-1):
    """"""
        Converts IterableSummarizationDataset to S2SAbsSumDataset.

        Args:
            sum_ds (IterableSummarizationDataset): Input dataset.
            train_mode (bool): Whether the input data is for training or testing.
            cached_features_file (str, optional): Path of the cached features file.
                If provided and the file already exists, it is loaded and used.
                If provided and the file doesn't exist, processed features are
                saved to this file.
                If not provided, processed features are saved to cache_dir.
                Defaults to None.
            local_rank (int, optional): Local rank of the device in distributed
                training. Defaults to -1, which means non-distributed training.
            top_n (int, optional): The number which specifies how many examples in the
                beginning of the input dataset that will be used to create the dataset.
                Defaults to -1, which means the whole dataset should be processsed.

        Returns:
            S2SAbsSumDataset
        """"""
    examples = []
    if train_mode:
        for (source, target) in zip(sum_ds, sum_ds.get_target()):
            examples.append({'src': source, 'tgt': target})
    else:
        for source in sum_ds:
            examples.append({'src': source})
    s2s_dataset = S2SAbsSumProcessor.create_s2s_dataset(examples=examples, train_mode=train_mode, tokenizer=self.tokenizer, output_dir=self.cache_dir, local_rank=local_rank, cached_features_file=cached_features_file, top_n=top_n)
    return s2s_dataset","for (source, target) in zip(sum_ds, sum_ds.get_target()):
    examples.append({'src': source, 'tgt': target})","examples = [{'src': source, 'tgt': target} for (source, target) in zip(sum_ds, sum_ds.get_target())]","examples = [{'src': source, 'tgt': target} for (source, target) in zip(sum_ds, sum_ds.get_target())]",1,,,,,robosuite
nlp-recipes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py,https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py,S2SAbsSumProcessor,s2s_dataset_from_iterable_sum_ds$245,"def s2s_dataset_from_iterable_sum_ds(self, sum_ds, train_mode, cached_features_file=None, local_rank=-1, top_n=-1):
    """"""
        Converts IterableSummarizationDataset to S2SAbsSumDataset.

        Args:
            sum_ds (IterableSummarizationDataset): Input dataset.
            train_mode (bool): Whether the input data is for training or testing.
            cached_features_file (str, optional): Path of the cached features file.
                If provided and the file already exists, it is loaded and used.
                If provided and the file doesn't exist, processed features are
                saved to this file.
                If not provided, processed features are saved to cache_dir.
                Defaults to None.
            local_rank (int, optional): Local rank of the device in distributed
                training. Defaults to -1, which means non-distributed training.
            top_n (int, optional): The number which specifies how many examples in the
                beginning of the input dataset that will be used to create the dataset.
                Defaults to -1, which means the whole dataset should be processsed.

        Returns:
            S2SAbsSumDataset
        """"""
    examples = []
    if train_mode:
        for (source, target) in zip(sum_ds, sum_ds.get_target()):
            examples.append({'src': source, 'tgt': target})
    else:
        for source in sum_ds:
            examples.append({'src': source})
    s2s_dataset = S2SAbsSumProcessor.create_s2s_dataset(examples=examples, train_mode=train_mode, tokenizer=self.tokenizer, output_dir=self.cache_dir, local_rank=local_rank, cached_features_file=cached_features_file, top_n=top_n)
    return s2s_dataset","for source in sum_ds:
    examples.append({'src': source})",examples += [{'src': source} for source in sum_ds],Cannot refactor,-1,1,,,,robosuite
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/network_operations.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/network_operations.py,,split_node$130,"def split_node(node: AbstractNode, left_edges: List[Edge], right_edges: List[Edge], max_singular_values: Optional[int]=None, max_truncation_err: Optional[float]=None, relative: Optional[bool]=False, left_name: Optional[Text]=None, right_name: Optional[Text]=None, edge_name: Optional[Text]=None) -> Tuple[AbstractNode, AbstractNode, Tensor]:
    """"""Split a `node` using Singular Value Decomposition.

  Let :math:`M` be the matrix created by flattening `left_edges` and 
  `right_edges` into 2 axes. 
  Let :math:`U S V^* = M` be the SVD of :math:`M`. 
  This will split the network into 2 nodes. 
  The left node's tensor will be :math:`U \\sqrt{S}` 
  and the right node's tensor will be
  :math:`\\sqrt{S} V^*` where :math:`V^*` is the adjoint of :math:`V`.

  The singular value decomposition is truncated if `max_singular_values` or
  `max_truncation_err` is not `None`.

  The truncation error is the 2-norm of the vector of truncated singular
  values. If only `max_truncation_err` is set, as many singular values will
  be truncated as possible while maintaining:
  `norm(truncated_singular_values) <= max_truncation_err`.
  If `relative` is set `True` then `max_truncation_err` is understood
  relative to the largest singular value.

  If only `max_singular_values` is set, the number of singular values kept
  will be `min(max_singular_values, number_of_singular_values)`, so that
  `max(0, number_of_singular_values - max_singular_values)` are truncated.

  If both `max_truncation_err` and `max_singular_values` are set,
  `max_singular_values` takes priority: The truncation error may be larger
  than `max_truncation_err` if required to satisfy `max_singular_values`.

  Args:
    node: The node you want to split.
    left_edges: The edges you want connected to the new left node.
    right_edges: The edges you want connected to the new right node.
    max_singular_values: The maximum number of singular values to keep.
    max_truncation_err: The maximum allowed truncation error.
    relative: Multiply `max_truncation_err` with the largest singular value.
    left_name: The name of the new left node. If `None`, a name will be 
      generated automatically.
    right_name: The name of the new right node. If `None`, a name will be
      generated automatically.
    edge_name: The name of the new `Edge` connecting the new left and
      right node. If `None`, a name will be generated automatically.
      The new axis will get the same name as the edge.

  Returns:
    A tuple containing:
      left_node:
        A new node created that connects to all of the `left_edges`.
        Its underlying tensor is :math:`U \\sqrt{S}`
      right_node:
        A new node created that connects to all of the `right_edges`.
        Its underlying tensor is :math:`\\sqrt{S} V^*`
      truncated_singular_values:
        The vector of truncated singular values.
  Raises:
    AttributeError: If `node` has no backend attribute
  """"""
    if not hasattr(node, 'backend'):
        raise AttributeError('Node {} of type {} has no `backend`'.format(node, type(node)))
    if node.axis_names and edge_name:
        left_axis_names = []
        right_axis_names = [edge_name]
        for edge in left_edges:
            left_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])
        for edge in right_edges:
            right_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])
        left_axis_names.append(edge_name)
    else:
        left_axis_names = None
        right_axis_names = None
    backend = node.backend
    transp_tensor = node.tensor_from_edge_order(left_edges + right_edges)
    (u, s, vh, trun_vals) = backend.svd(transp_tensor, len(left_edges), max_singular_values, max_truncation_err, relative=relative)
    sqrt_s = backend.sqrt(s)
    u_s = backend.broadcast_right_multiplication(u, sqrt_s)
    vh_s = backend.broadcast_left_multiplication(sqrt_s, vh)
    left_node = Node(u_s, name=left_name, axis_names=left_axis_names, backend=backend)
    left_axes_order = [edge.axis1 if edge.node1 is node else edge.axis2 for edge in left_edges]
    for (i, edge) in enumerate(left_edges):
        left_node.add_edge(edge, i)
        edge.update_axis(left_axes_order[i], node, i, left_node)
    right_node = Node(vh_s, name=right_name, axis_names=right_axis_names, backend=backend)
    right_axes_order = [edge.axis1 if edge.node1 is node else edge.axis2 for edge in right_edges]
    for (i, edge) in enumerate(right_edges):
        right_node.add_edge(edge, i + 1)
        edge.update_axis(right_axes_order[i], node, i + 1, right_node)
    connect(left_node.edges[-1], right_node.edges[0], name=edge_name)
    node.fresh_edges(node.axis_names)
    return (left_node, right_node, trun_vals)","for edge in left_edges:
    left_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])",left_axis_names = [node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2] for edge in left_edges],left_axis_names = [node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2] for edge in left_edges],1,,,,,robosuite
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/network_operations.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/network_operations.py,,split_node$130,"def split_node(node: AbstractNode, left_edges: List[Edge], right_edges: List[Edge], max_singular_values: Optional[int]=None, max_truncation_err: Optional[float]=None, relative: Optional[bool]=False, left_name: Optional[Text]=None, right_name: Optional[Text]=None, edge_name: Optional[Text]=None) -> Tuple[AbstractNode, AbstractNode, Tensor]:
    """"""Split a `node` using Singular Value Decomposition.

  Let :math:`M` be the matrix created by flattening `left_edges` and 
  `right_edges` into 2 axes. 
  Let :math:`U S V^* = M` be the SVD of :math:`M`. 
  This will split the network into 2 nodes. 
  The left node's tensor will be :math:`U \\sqrt{S}` 
  and the right node's tensor will be
  :math:`\\sqrt{S} V^*` where :math:`V^*` is the adjoint of :math:`V`.

  The singular value decomposition is truncated if `max_singular_values` or
  `max_truncation_err` is not `None`.

  The truncation error is the 2-norm of the vector of truncated singular
  values. If only `max_truncation_err` is set, as many singular values will
  be truncated as possible while maintaining:
  `norm(truncated_singular_values) <= max_truncation_err`.
  If `relative` is set `True` then `max_truncation_err` is understood
  relative to the largest singular value.

  If only `max_singular_values` is set, the number of singular values kept
  will be `min(max_singular_values, number_of_singular_values)`, so that
  `max(0, number_of_singular_values - max_singular_values)` are truncated.

  If both `max_truncation_err` and `max_singular_values` are set,
  `max_singular_values` takes priority: The truncation error may be larger
  than `max_truncation_err` if required to satisfy `max_singular_values`.

  Args:
    node: The node you want to split.
    left_edges: The edges you want connected to the new left node.
    right_edges: The edges you want connected to the new right node.
    max_singular_values: The maximum number of singular values to keep.
    max_truncation_err: The maximum allowed truncation error.
    relative: Multiply `max_truncation_err` with the largest singular value.
    left_name: The name of the new left node. If `None`, a name will be 
      generated automatically.
    right_name: The name of the new right node. If `None`, a name will be
      generated automatically.
    edge_name: The name of the new `Edge` connecting the new left and
      right node. If `None`, a name will be generated automatically.
      The new axis will get the same name as the edge.

  Returns:
    A tuple containing:
      left_node:
        A new node created that connects to all of the `left_edges`.
        Its underlying tensor is :math:`U \\sqrt{S}`
      right_node:
        A new node created that connects to all of the `right_edges`.
        Its underlying tensor is :math:`\\sqrt{S} V^*`
      truncated_singular_values:
        The vector of truncated singular values.
  Raises:
    AttributeError: If `node` has no backend attribute
  """"""
    if not hasattr(node, 'backend'):
        raise AttributeError('Node {} of type {} has no `backend`'.format(node, type(node)))
    if node.axis_names and edge_name:
        left_axis_names = []
        right_axis_names = [edge_name]
        for edge in left_edges:
            left_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])
        for edge in right_edges:
            right_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])
        left_axis_names.append(edge_name)
    else:
        left_axis_names = None
        right_axis_names = None
    backend = node.backend
    transp_tensor = node.tensor_from_edge_order(left_edges + right_edges)
    (u, s, vh, trun_vals) = backend.svd(transp_tensor, len(left_edges), max_singular_values, max_truncation_err, relative=relative)
    sqrt_s = backend.sqrt(s)
    u_s = backend.broadcast_right_multiplication(u, sqrt_s)
    vh_s = backend.broadcast_left_multiplication(sqrt_s, vh)
    left_node = Node(u_s, name=left_name, axis_names=left_axis_names, backend=backend)
    left_axes_order = [edge.axis1 if edge.node1 is node else edge.axis2 for edge in left_edges]
    for (i, edge) in enumerate(left_edges):
        left_node.add_edge(edge, i)
        edge.update_axis(left_axes_order[i], node, i, left_node)
    right_node = Node(vh_s, name=right_name, axis_names=right_axis_names, backend=backend)
    right_axes_order = [edge.axis1 if edge.node1 is node else edge.axis2 for edge in right_edges]
    for (i, edge) in enumerate(right_edges):
        right_node.add_edge(edge, i + 1)
        edge.update_axis(right_axes_order[i], node, i + 1, right_node)
    connect(left_node.edges[-1], right_node.edges[0], name=edge_name)
    node.fresh_edges(node.axis_names)
    return (left_node, right_node, trun_vals)","for edge in right_edges:
    right_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])",right_axis_names = [node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2] for edge in right_edges],Cannot refactor,-1,0,,2,1,robosuite
texar-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/examples/gpt-2/gpt2_train_main.py,https://github.com/asyml/texar-pytorch/tree/master/examples/gpt-2/gpt2_train_main.py,,_test_epoch$200,"def _test_epoch():
    """"""Generates samples on the test set.
        """"""
    iterator.switch_to_dataset('test')
    model.eval()
    _all_inputs = []
    _all_samples = []
    for batch in iterator:
        input_ids = batch['text_ids']
        length = batch['length']
        start_tokens = input_ids[:, 0]
        helper = _get_helper(start_tokens)
        (output, _) = model(context=input_ids, context_sequence_length=length, max_decoding_length=max_decoding_length, helper=helper)
        sample_id = output.sample_id
        _inputs = []
        for (i, l) in zip(input_ids, length):
            _inputs.append(i[:l].tolist())
        _all_inputs.extend(_inputs)
        _samples = []
        for (s, l) in zip(sample_id, length):
            _samples.append(s[l:].tolist())
        _all_samples.extend(_samples)
    eos_token_id = tokenizer.map_token_to_id('<|endoftext|>')
    _all_input_text = []
    for i in _all_inputs:
        if i[0] == eos_token_id:
            i = i[1:]
        i_text = tokenizer.map_id_to_text(i)
        _all_input_text.append(i_text)
    _all_input_text = tx.utils.strip_eos(_all_input_text, eos_token='<|endoftext|>')
    _all_samples_text = []
    for (i, s) in zip(_all_inputs, _all_samples):
        s_text = tokenizer.map_id_to_text(s)
        s_text = s_text.replace('\n', ' ')
        _all_samples_text.append(s_text)
    _all_samples_text = tx.utils.strip_eos(_all_samples_text, eos_token='<|endoftext|>')
    output_file = os.path.join(args.output_dir, 'test_samples.tsv')
    print('Write samples to {}'.format(output_file))
    tx.utils.write_paired_text(_all_input_text, _all_samples_text, output_file)","for i in _all_inputs:
    if i[0] == eos_token_id:
        i = i[1:]
    i_text = tokenizer.map_id_to_text(i)
    _all_input_text.append(i_text)",_all_input_text = [tokenizer.map_id_to_text(i[1:]) if i[0] == eos_token_id else tokenizer.map_id_to_text(i) for i in _all_inputs],Cannot refactor,-1,1,,,,robosuite
texar-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/examples/gpt-2/gpt2_train_main.py,https://github.com/asyml/texar-pytorch/tree/master/examples/gpt-2/gpt2_train_main.py,,_test_epoch$200,"def _test_epoch():
    """"""Generates samples on the test set.
        """"""
    iterator.switch_to_dataset('test')
    model.eval()
    _all_inputs = []
    _all_samples = []
    for batch in iterator:
        input_ids = batch['text_ids']
        length = batch['length']
        start_tokens = input_ids[:, 0]
        helper = _get_helper(start_tokens)
        (output, _) = model(context=input_ids, context_sequence_length=length, max_decoding_length=max_decoding_length, helper=helper)
        sample_id = output.sample_id
        _inputs = []
        for (i, l) in zip(input_ids, length):
            _inputs.append(i[:l].tolist())
        _all_inputs.extend(_inputs)
        _samples = []
        for (s, l) in zip(sample_id, length):
            _samples.append(s[l:].tolist())
        _all_samples.extend(_samples)
    eos_token_id = tokenizer.map_token_to_id('<|endoftext|>')
    _all_input_text = []
    for i in _all_inputs:
        if i[0] == eos_token_id:
            i = i[1:]
        i_text = tokenizer.map_id_to_text(i)
        _all_input_text.append(i_text)
    _all_input_text = tx.utils.strip_eos(_all_input_text, eos_token='<|endoftext|>')
    _all_samples_text = []
    for (i, s) in zip(_all_inputs, _all_samples):
        s_text = tokenizer.map_id_to_text(s)
        s_text = s_text.replace('\n', ' ')
        _all_samples_text.append(s_text)
    _all_samples_text = tx.utils.strip_eos(_all_samples_text, eos_token='<|endoftext|>')
    output_file = os.path.join(args.output_dir, 'test_samples.tsv')
    print('Write samples to {}'.format(output_file))
    tx.utils.write_paired_text(_all_input_text, _all_samples_text, output_file)","for (i, s) in zip(_all_inputs, _all_samples):
    s_text = tokenizer.map_id_to_text(s)
    s_text = s_text.replace('\n', ' ')
    _all_samples_text.append(s_text)","_all_samples_text = [tokenizer.map_id_to_text(s).replace('\n', ' ') for (i, s) in zip(_all_inputs, _all_samples)]",Cannot refactor,-1,1,,,,robosuite
texar-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/examples/gpt-2/gpt2_train_main.py,https://github.com/asyml/texar-pytorch/tree/master/examples/gpt-2/gpt2_train_main.py,,_test_epoch$200,"def _test_epoch():
    """"""Generates samples on the test set.
        """"""
    iterator.switch_to_dataset('test')
    model.eval()
    _all_inputs = []
    _all_samples = []
    for batch in iterator:
        input_ids = batch['text_ids']
        length = batch['length']
        start_tokens = input_ids[:, 0]
        helper = _get_helper(start_tokens)
        (output, _) = model(context=input_ids, context_sequence_length=length, max_decoding_length=max_decoding_length, helper=helper)
        sample_id = output.sample_id
        _inputs = []
        for (i, l) in zip(input_ids, length):
            _inputs.append(i[:l].tolist())
        _all_inputs.extend(_inputs)
        _samples = []
        for (s, l) in zip(sample_id, length):
            _samples.append(s[l:].tolist())
        _all_samples.extend(_samples)
    eos_token_id = tokenizer.map_token_to_id('<|endoftext|>')
    _all_input_text = []
    for i in _all_inputs:
        if i[0] == eos_token_id:
            i = i[1:]
        i_text = tokenizer.map_id_to_text(i)
        _all_input_text.append(i_text)
    _all_input_text = tx.utils.strip_eos(_all_input_text, eos_token='<|endoftext|>')
    _all_samples_text = []
    for (i, s) in zip(_all_inputs, _all_samples):
        s_text = tokenizer.map_id_to_text(s)
        s_text = s_text.replace('\n', ' ')
        _all_samples_text.append(s_text)
    _all_samples_text = tx.utils.strip_eos(_all_samples_text, eos_token='<|endoftext|>')
    output_file = os.path.join(args.output_dir, 'test_samples.tsv')
    print('Write samples to {}'.format(output_file))
    tx.utils.write_paired_text(_all_input_text, _all_samples_text, output_file)","for (i, l) in zip(input_ids, length):
    _inputs.append(i[:l].tolist())","_inputs += [i[:l].tolist() for (i, l) in zip(input_ids, length)]","_inputs = [i[:l].tolist() for (i, l) in zip(input_ids, length)]",0,1,,,,robosuite
texar-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/examples/gpt-2/gpt2_train_main.py,https://github.com/asyml/texar-pytorch/tree/master/examples/gpt-2/gpt2_train_main.py,,_test_epoch$200,"def _test_epoch():
    """"""Generates samples on the test set.
        """"""
    iterator.switch_to_dataset('test')
    model.eval()
    _all_inputs = []
    _all_samples = []
    for batch in iterator:
        input_ids = batch['text_ids']
        length = batch['length']
        start_tokens = input_ids[:, 0]
        helper = _get_helper(start_tokens)
        (output, _) = model(context=input_ids, context_sequence_length=length, max_decoding_length=max_decoding_length, helper=helper)
        sample_id = output.sample_id
        _inputs = []
        for (i, l) in zip(input_ids, length):
            _inputs.append(i[:l].tolist())
        _all_inputs.extend(_inputs)
        _samples = []
        for (s, l) in zip(sample_id, length):
            _samples.append(s[l:].tolist())
        _all_samples.extend(_samples)
    eos_token_id = tokenizer.map_token_to_id('<|endoftext|>')
    _all_input_text = []
    for i in _all_inputs:
        if i[0] == eos_token_id:
            i = i[1:]
        i_text = tokenizer.map_id_to_text(i)
        _all_input_text.append(i_text)
    _all_input_text = tx.utils.strip_eos(_all_input_text, eos_token='<|endoftext|>')
    _all_samples_text = []
    for (i, s) in zip(_all_inputs, _all_samples):
        s_text = tokenizer.map_id_to_text(s)
        s_text = s_text.replace('\n', ' ')
        _all_samples_text.append(s_text)
    _all_samples_text = tx.utils.strip_eos(_all_samples_text, eos_token='<|endoftext|>')
    output_file = os.path.join(args.output_dir, 'test_samples.tsv')
    print('Write samples to {}'.format(output_file))
    tx.utils.write_paired_text(_all_input_text, _all_samples_text, output_file)","for (s, l) in zip(sample_id, length):
    _samples.append(s[l:].tolist())","_samples += [s[l:].tolist() for (s, l) in zip(sample_id, length)]","_samples = [s[l:].tolist() for (s, l) in zip(sample_id, length)]",0,1,,,,robosuite
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/bzt/bza.py,https://github.com/Blazemeter/taurus/tree/master/bzt/bza.py,Workspace,locations$317,"def locations(self, include_private=False):
    if 'locations' not in self:
        self.fetch()
    res = []
    for loc in self['locations']:
        if not loc['id'].startswith('harbor-') or include_private:
            res.append(Location(self, loc))
    return BZAObjectsList(res)","for loc in self['locations']:
    if not loc['id'].startswith('harbor-') or include_private:
        res.append(Location(self, loc))","res = [Location(self, loc) for loc in self['locations'] if not loc['id'].startswith('harbor-') or include_private]","res = [Location(self, loc) for loc in self['locations'] if not loc['id'].startswith('harbor-') or include_private]",1,,,,,robosuite
nuscenes-devkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nuscenes-devkit/python-sdk/nuscenes/eval/detection/tests/test_evaluate.py,https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/nuscenes/eval/detection/tests/test_evaluate.py,TestMain,_mock_submission$33,"def _mock_submission(nusc: NuScenes, split: str) -> Dict[str, dict]:
    """"""
        Creates ""reasonable"" submission (results and metadata) by looping through the mini-val set, adding 1 GT
        prediction per sample. Predictions will be permuted randomly along all axes.
        """"""

    def random_class(category_name: str) -> str:
        class_names = sorted(DETECTION_NAMES)
        tmp = category_to_detection_name(category_name)
        if tmp is not None and np.random.rand() < 0.9:
            return tmp
        else:
            return class_names[np.random.randint(0, len(class_names) - 1)]

    def random_attr(name: str) -> str:
        """"""
            This is the most straight-forward way to generate a random attribute.
            Not currently used b/c we want the test fixture to be back-wards compatible.
            """"""
        rel_attributes = detection_name_to_rel_attributes(name)
        if len(rel_attributes) == 0:
            return ''
        else:
            return rel_attributes[np.random.randint(0, len(rel_attributes))]
    mock_meta = {'use_camera': False, 'use_lidar': True, 'use_radar': False, 'use_map': False, 'use_external': False}
    mock_results = {}
    splits = create_splits_scenes()
    val_samples = []
    for sample in nusc.sample:
        if nusc.get('scene', sample['scene_token'])['name'] in splits[split]:
            val_samples.append(sample)
    for sample in tqdm(val_samples, leave=False):
        sample_res = []
        for ann_token in sample['anns']:
            ann = nusc.get('sample_annotation', ann_token)
            detection_name = random_class(ann['category_name'])
            sample_res.append({'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': detection_name, 'detection_score': random.random(), 'attribute_name': random_attr(detection_name)})
        mock_results[sample['token']] = sample_res
    mock_submission = {'meta': mock_meta, 'results': mock_results}
    return mock_submission","for sample in nusc.sample:
    if nusc.get('scene', sample['scene_token'])['name'] in splits[split]:
        val_samples.append(sample)","val_samples = [sample for sample in nusc.sample if nusc.get('scene', sample['scene_token'])['name'] in splits[split]]","val_samples = [sample for sample in nusc.sample if nusc.get('scene', sample['scene_token'])['name'] in splits[split]]",1,,,,,robosuite
nuscenes-devkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nuscenes-devkit/python-sdk/nuscenes/eval/detection/tests/test_evaluate.py,https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/nuscenes/eval/detection/tests/test_evaluate.py,TestMain,_mock_submission$33,"def _mock_submission(nusc: NuScenes, split: str) -> Dict[str, dict]:
    """"""
        Creates ""reasonable"" submission (results and metadata) by looping through the mini-val set, adding 1 GT
        prediction per sample. Predictions will be permuted randomly along all axes.
        """"""

    def random_class(category_name: str) -> str:
        class_names = sorted(DETECTION_NAMES)
        tmp = category_to_detection_name(category_name)
        if tmp is not None and np.random.rand() < 0.9:
            return tmp
        else:
            return class_names[np.random.randint(0, len(class_names) - 1)]

    def random_attr(name: str) -> str:
        """"""
            This is the most straight-forward way to generate a random attribute.
            Not currently used b/c we want the test fixture to be back-wards compatible.
            """"""
        rel_attributes = detection_name_to_rel_attributes(name)
        if len(rel_attributes) == 0:
            return ''
        else:
            return rel_attributes[np.random.randint(0, len(rel_attributes))]
    mock_meta = {'use_camera': False, 'use_lidar': True, 'use_radar': False, 'use_map': False, 'use_external': False}
    mock_results = {}
    splits = create_splits_scenes()
    val_samples = []
    for sample in nusc.sample:
        if nusc.get('scene', sample['scene_token'])['name'] in splits[split]:
            val_samples.append(sample)
    for sample in tqdm(val_samples, leave=False):
        sample_res = []
        for ann_token in sample['anns']:
            ann = nusc.get('sample_annotation', ann_token)
            detection_name = random_class(ann['category_name'])
            sample_res.append({'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': detection_name, 'detection_score': random.random(), 'attribute_name': random_attr(detection_name)})
        mock_results[sample['token']] = sample_res
    mock_submission = {'meta': mock_meta, 'results': mock_results}
    return mock_submission","for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    detection_name = random_class(ann['category_name'])
    sample_res.append({'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': detection_name, 'detection_score': random.random(), 'attribute_name': random_attr(detection_name)})","sample_res = [{'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': random_class(ann['category_name']), 'detection_score': random.random(), 'attribute_name': random_attr(random_class(ann['category_name']))} for ann_token in sample['anns'] for ann in [nusc.get('sample_annotation', ann_token)]]",Cannot refactor,-1,1,,,,robosuite
SMAC3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMAC3/test/test_smbo/test_random_configuration_chooser.py,https://github.com/automl/SMAC3/tree/master/test/test_smbo/test_random_configuration_chooser.py,TestRandomConfigurationChooser,test_chooser_prob$75,"def test_chooser_prob(self):
    for i in range(10):
        c = ChooserProb(rng=np.random.RandomState(1), prob=0.1 * i)
        stats = []
        for j in range(100000):
            stats.append(c.check(j))
        print(np.sum(stats) / 100000, 0.1 * i)
        self.assertAlmostEqual(np.sum(stats) / 100000, 0.1 * i, places=2)","for j in range(100000):
    stats.append(c.check(j))",stats = [c.check(j) for j in range(100000)],stats = [c.check(j) for j in range(100000)],1,,,,,robosuite
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_duplicate_tracks$54,"def remove_duplicate_tracks(features, replace=False):
    if not replace:
        features = features.copy()
    file_names = features.file_names.unique()
    duplicates = []
    for file_name in file_names:
        file_features = features[features.file_names == file_name]
        number_notes = Counter(file_features.num_notes)
        notes = []
        for ele in number_notes:
            if number_notes[ele] > 1:
                notes.append(ele)
        h_pits = []
        for note in notes:
            number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
            for ele in number_h_pit:
                if number_h_pit[ele] > 1:
                    h_pits.append(ele)
        l_pits = []
        for h_pit in h_pits:
            number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
            for ele in number_l_pit:
                if number_l_pit[ele] > 1:
                    l_pits.append(ele)
        notes = list(set(notes))
        h_pits = list(set(h_pits))
        l_pits = list(set(l_pits))
        for note in notes:
            note_index = file_features[file_features.num_notes == note].index.values
            for h_pit in h_pits:
                h_pit_index = file_features[file_features.h_pit == h_pit].index.values
                for l_pit in l_pits:
                    l_pit_index = file_features[file_features.l_pit == l_pit].index.values
                    index_intersect = reduce(np.intersect1d, (note_index, h_pit_index, l_pit_index))
                    if len(index_intersect) > 1:
                        duplicates.append(index_intersect)
    melody_track_name = ['sing', 'vocals', 'vocal', 'melody', 'melody:']
    bass_track_name = ['bass', 'bass:']
    chord_track_name = ['chord', 'chords', 'harmony']
    for indices in duplicates:
        melody_track = False
        bass_track = False
        chord_track = False
        labels = features.loc[indices, 'trk_names']
        for label in labels:
            if label in melody_track_name:
                melody_track = True
            elif label in bass_track_name:
                bass_track = True
            elif label in chord_track_name:
                chord_track = True
            else:
                pass
        if melody_track:
            features.loc[indices, 'trk_names'] = 'melody'
        if bass_track:
            features.loc[indices, 'trk_names'] = 'bass'
        if chord_track:
            features.loc[indices, 'trk_names'] = 'chord'
        features.drop(indices[1:], inplace=True)
        print(indices[1:])
    return features","for ele in number_notes:
    if number_notes[ele] > 1:
        notes.append(ele)",notes += [ele for ele in number_notes if number_notes[ele] > 1],notes = [ele for ele in number_notes if number_notes[ele] > 1],0,1,,,,robosuite
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_duplicate_tracks$54,"def remove_duplicate_tracks(features, replace=False):
    if not replace:
        features = features.copy()
    file_names = features.file_names.unique()
    duplicates = []
    for file_name in file_names:
        file_features = features[features.file_names == file_name]
        number_notes = Counter(file_features.num_notes)
        notes = []
        for ele in number_notes:
            if number_notes[ele] > 1:
                notes.append(ele)
        h_pits = []
        for note in notes:
            number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
            for ele in number_h_pit:
                if number_h_pit[ele] > 1:
                    h_pits.append(ele)
        l_pits = []
        for h_pit in h_pits:
            number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
            for ele in number_l_pit:
                if number_l_pit[ele] > 1:
                    l_pits.append(ele)
        notes = list(set(notes))
        h_pits = list(set(h_pits))
        l_pits = list(set(l_pits))
        for note in notes:
            note_index = file_features[file_features.num_notes == note].index.values
            for h_pit in h_pits:
                h_pit_index = file_features[file_features.h_pit == h_pit].index.values
                for l_pit in l_pits:
                    l_pit_index = file_features[file_features.l_pit == l_pit].index.values
                    index_intersect = reduce(np.intersect1d, (note_index, h_pit_index, l_pit_index))
                    if len(index_intersect) > 1:
                        duplicates.append(index_intersect)
    melody_track_name = ['sing', 'vocals', 'vocal', 'melody', 'melody:']
    bass_track_name = ['bass', 'bass:']
    chord_track_name = ['chord', 'chords', 'harmony']
    for indices in duplicates:
        melody_track = False
        bass_track = False
        chord_track = False
        labels = features.loc[indices, 'trk_names']
        for label in labels:
            if label in melody_track_name:
                melody_track = True
            elif label in bass_track_name:
                bass_track = True
            elif label in chord_track_name:
                chord_track = True
            else:
                pass
        if melody_track:
            features.loc[indices, 'trk_names'] = 'melody'
        if bass_track:
            features.loc[indices, 'trk_names'] = 'bass'
        if chord_track:
            features.loc[indices, 'trk_names'] = 'chord'
        features.drop(indices[1:], inplace=True)
        print(indices[1:])
    return features","for note in notes:
    number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
    for ele in number_h_pit:
        if number_h_pit[ele] > 1:
            h_pits.append(ele)","h_pits = [ele for note in notes for (ele, count) in Counter(file_features[file_features.num_notes == note].h_pit).items() if count > 1]",Cannot refactor,-1,1,,,,robosuite
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_duplicate_tracks$54,"def remove_duplicate_tracks(features, replace=False):
    if not replace:
        features = features.copy()
    file_names = features.file_names.unique()
    duplicates = []
    for file_name in file_names:
        file_features = features[features.file_names == file_name]
        number_notes = Counter(file_features.num_notes)
        notes = []
        for ele in number_notes:
            if number_notes[ele] > 1:
                notes.append(ele)
        h_pits = []
        for note in notes:
            number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
            for ele in number_h_pit:
                if number_h_pit[ele] > 1:
                    h_pits.append(ele)
        l_pits = []
        for h_pit in h_pits:
            number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
            for ele in number_l_pit:
                if number_l_pit[ele] > 1:
                    l_pits.append(ele)
        notes = list(set(notes))
        h_pits = list(set(h_pits))
        l_pits = list(set(l_pits))
        for note in notes:
            note_index = file_features[file_features.num_notes == note].index.values
            for h_pit in h_pits:
                h_pit_index = file_features[file_features.h_pit == h_pit].index.values
                for l_pit in l_pits:
                    l_pit_index = file_features[file_features.l_pit == l_pit].index.values
                    index_intersect = reduce(np.intersect1d, (note_index, h_pit_index, l_pit_index))
                    if len(index_intersect) > 1:
                        duplicates.append(index_intersect)
    melody_track_name = ['sing', 'vocals', 'vocal', 'melody', 'melody:']
    bass_track_name = ['bass', 'bass:']
    chord_track_name = ['chord', 'chords', 'harmony']
    for indices in duplicates:
        melody_track = False
        bass_track = False
        chord_track = False
        labels = features.loc[indices, 'trk_names']
        for label in labels:
            if label in melody_track_name:
                melody_track = True
            elif label in bass_track_name:
                bass_track = True
            elif label in chord_track_name:
                chord_track = True
            else:
                pass
        if melody_track:
            features.loc[indices, 'trk_names'] = 'melody'
        if bass_track:
            features.loc[indices, 'trk_names'] = 'bass'
        if chord_track:
            features.loc[indices, 'trk_names'] = 'chord'
        features.drop(indices[1:], inplace=True)
        print(indices[1:])
    return features","for h_pit in h_pits:
    number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
    for ele in number_l_pit:
        if number_l_pit[ele] > 1:
            l_pits.append(ele)","l_pits = [ele for h_pit in h_pits for (ele, count) in Counter(file_features[file_features.h_pit == h_pit].l_pit).items() if count > 1]",Cannot refactor,-1,1,,,,robosuite
longformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/longformer/tvm/_ffi/function.py,https://github.com/allenai/longformer/tree/master/tvm/_ffi/function.py,,list_global_func_names$235,"def list_global_func_names():
    """"""Get list of global functions registered.

    Returns
    -------
    names : list
       List of global functions names.
    """"""
    plist = ctypes.POINTER(ctypes.c_char_p)()
    size = ctypes.c_uint()
    check_call(_LIB.TVMFuncListGlobalNames(ctypes.byref(size), ctypes.byref(plist)))
    fnames = []
    for i in range(size.value):
        fnames.append(py_str(plist[i]))
    return fnames","for i in range(size.value):
    fnames.append(py_str(plist[i]))",fnames = [py_str(plist[i]) for i in range(size.value)],fnames = [py_str(plist[i]) for i in range(size.value)],1,,,,,robosuite
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for minor in range(version_info[-1], -1, -1):
    versions.append(''.join(map(str, major + (minor,))))","versions = [''.join(map(str, major + (minor,))) for minor in range(version_info[-1], -1, -1)]","versions = [''.join(map(str, major + (minor,))) for minor in range(version_info[-1], -1, -1)]",1,,,,,robosuite
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for abi in abis:
    for arch in arches:
        tags.append(('%s%s' % (impl, versions[0]), abi, arch))","tags = [('%s%s' % (impl, versions[0]), abi, arch) for abi in abis for arch in arches]","tags = [('%s%s' % (impl, versions[0]), abi, arch) for abi in abis for arch in arches]",1,,,,,robosuite
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for arch in arches:
    tags.append(('py%s' % versions[0][0], 'none', arch))","tags += [('py%s' % versions[0][0], 'none', arch) for arch in arches]",Cannot refactor,-1,1,,,,robosuite
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for (i, version) in enumerate(versions):
    tags.append(('py%s' % (version,), 'none', 'any'))
    if i == 0:
        tags.append(('py%s' % version[0], 'none', 'any'))","tags += [('py%s' % (version,), 'none', 'any') if i != 0 else ('py%s' % version[0], 'none', 'any') for (i, version) in enumerate(versions)]",Cannot refactor,-1,0,,,,robosuite
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for version in versions[1:]:
    for arch in arches:
        tags.append(('%s%s' % (impl, version), 'abi3', arch))","tags += [('%s%s' % (impl, version), 'abi3', arch) for version in versions[1:] for arch in arches]",Cannot refactor,-1,1,,,,robosuite
DeepMosaics,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepMosaics/util/clean_cache.py,https://github.com/HypoX64/DeepMosaics/tree/master/util/clean_cache.py,,findalldir$4,"def findalldir(rootdir):
    dir_list = []
    for (root, dirs, files) in os.walk(rootdir):
        for dir in dirs:
            dir_list.append(os.path.join(root, dir))
    return dir_list","for (root, dirs, files) in os.walk(rootdir):
    for dir in dirs:
        dir_list.append(os.path.join(root, dir))","dir_list = [os.path.join(root, dir) for (root, dirs, files) in os.walk(rootdir) for dir in dirs]","dir_list = [os.path.join(root, dir) for (root, dirs, files) in os.walk(rootdir) for dir in dirs]",1,,,,,robosuite
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/ssm.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/ssm.py,UpdateOpsItem,process$269,"def process(self, resources):
    attrs = dict(self.data)
    attrs = filter_empty({'Description': attrs.get('description'), 'Title': attrs.get('title'), 'Priority': attrs.get('priority'), 'Status': attrs.get('status'), 'Notifications': [{'Arn': a} for a in attrs.get('topics', ())]})
    modified = []
    for r in resources:
        for (k, v) in attrs.items():
            if k not in r or r[k] != v:
                modified.append(r)
    self.log.debug('Updating %d of %d ops items', len(modified), len(resources))
    client = local_session(self.manager.session_factory).client('ssm')
    for m in modified:
        client.update_ops_item(OpsItemId=m['OpsItemId'], **attrs)","for r in resources:
    for (k, v) in attrs.items():
        if k not in r or r[k] != v:
            modified.append(r)","modified = [r for r in resources if any((k not in r or r[k] != v for (k, v) in attrs.items()))]","modified = [r for r in resources for (k, v) in attrs.items() if k not in r or r[k] != v]",0,1,,,,robosuite
CenterMask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CenterMask/maskrcnn_benchmark/modeling/backbone/fbnet_builder.py,https://github.com/youngwanLEE/CenterMask/tree/master/maskrcnn_benchmark/modeling/backbone/fbnet_builder.py,,expand_stages_cfg$583,"def expand_stages_cfg(stage_cfgs):
    """""" For a list of stages """"""
    assert isinstance(stage_cfgs, list)
    ret = []
    for x in stage_cfgs:
        ret.append(expand_stage_cfg(x))
    return ret","for x in stage_cfgs:
    ret.append(expand_stage_cfg(x))",ret = [expand_stage_cfg(x) for x in stage_cfgs],ret = [expand_stage_cfg(x) for x in stage_cfgs],1,,,,,robosuite
python-bitcoinlib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-bitcoinlib/bitcoin/core/serialize.py,https://github.com/petertodd/python-bitcoinlib/tree/master/bitcoin/core/serialize.py,intVectorSerializer,stream_deserialize$276,"def stream_deserialize(cls, f):
    l = VarIntSerializer.stream_deserialize(f)
    ints = []
    for i in range(l):
        ints.append(struct.unpack(b'<i', ser_read(f, 4))[0])
    return ints","for i in range(l):
    ints.append(struct.unpack(b'<i', ser_read(f, 4))[0])","ints = [struct.unpack(b'<i', ser_read(f, 4))[0] for i in range(l)]","ints = [struct.unpack(b'<i', ser_read(f, 4))[0] for i in range(l)]",1,,,,,robosuite
novelWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/novelWriter/tests/test_core/test_core_index.py,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True
    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile('Hello', nwItemClass.NOVEL, 'a508bb932959c')
    cHandle = theProject.newFile('Jane', nwItemClass.CHARACTER, 'afb3043c7b2b3')
    assert theIndex.getNovelData('', '') is None
    assert theIndex.getNovelData('a508bb932959c', '') is None
    assert theIndex.scanText(cHandle, '# Jane Smith\n@tag: Jane\n')
    assert theIndex.scanText(nHandle, '# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n')
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theProject.projTree[nHandle].setExported(False)
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)
    assert theKeys == []
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == []
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 62
    assert wC == 12
    assert pC == 2
    theRefs = theIndex.getReferences('Not a handle')
    assert theRefs['@pov'] == []
    assert theRefs['@char'] == []
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs['@pov'] == ['Jane']
    assert theRefs['@char'] == ['Jane']
    assert theIndex.getBackReferenceList(None) == {}
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: 'T000001'}
    assert theIndex.getTagSource('Jane') == (cHandle, 2, 'T000001')
    assert theIndex.getTagSource('John') == (None, 0, 'T000000')
    assert theIndex.scanText(nHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    assert theIndex.scanText(cHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    hHandle = theProject.newFile('Chapter', nwItemClass.NOVEL, 'a508bb932959c')
    sHandle = theProject.newFile('Scene One', nwItemClass.NOVEL, 'a508bb932959c')
    tHandle = theProject.newFile('Scene Two', nwItemClass.NOVEL, 'a508bb932959c')
    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT
    assert theIndex.scanText(hHandle, '## Chapter One\n\n')
    assert theIndex.scanText(sHandle, '### Scene One\n\n')
    assert theIndex.scanText(tHandle, '### Scene Two\n\n')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.append('0000000000000')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove('0000000000000')
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 6)]
    assert theIndex.getTableOfContents(3, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 2), ('%s:T000001' % sHandle, 3, 'Scene One', 2), ('%s:T000001' % tHandle, 3, 'Scene Two', 2)]
    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [('%s:T000001' % nHandle, 1, 'Hello World!', 12), ('%s:T000011' % nHandle, 1, 'Hello World!', 22)]
    bHandle = '0000000000000'
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [('%s:T000001' % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [('%s:T000001' % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [('%s:T000001' % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [('%s:T000001' % nHandle, 12), ('%s:T000011' % nHandle, 16)]
    assert theProject.closeProject()
    bHandle = '0000000000000'
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [('T000001', 'H2', 'Chapter One')]
    assert theIndex.getHandleHeaders(sHandle) == [('T000001', 'H3', 'Scene One')]
    assert theIndex.getHandleHeaders(tHandle) == [('T000001', 'H3', 'Scene Two')]
    assert theIndex.getHandleHeaders(nHandle) == [('T000001', 'H1', 'Hello World!'), ('T000011', 'H1', 'Hello World!')]","for (aKey, _, _, _) in theIndex.novelStructure():
    theKeys.append(aKey)","theKeys = [aKey for (aKey, _, _, _) in theIndex.novelStructure()]","theKeys = [aKey for (aKey, _, _, _) in theIndex.novelStructure()]",1,,,,,robosuite
novelWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/novelWriter/tests/test_core/test_core_index.py,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True
    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile('Hello', nwItemClass.NOVEL, 'a508bb932959c')
    cHandle = theProject.newFile('Jane', nwItemClass.CHARACTER, 'afb3043c7b2b3')
    assert theIndex.getNovelData('', '') is None
    assert theIndex.getNovelData('a508bb932959c', '') is None
    assert theIndex.scanText(cHandle, '# Jane Smith\n@tag: Jane\n')
    assert theIndex.scanText(nHandle, '# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n')
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theProject.projTree[nHandle].setExported(False)
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)
    assert theKeys == []
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == []
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 62
    assert wC == 12
    assert pC == 2
    theRefs = theIndex.getReferences('Not a handle')
    assert theRefs['@pov'] == []
    assert theRefs['@char'] == []
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs['@pov'] == ['Jane']
    assert theRefs['@char'] == ['Jane']
    assert theIndex.getBackReferenceList(None) == {}
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: 'T000001'}
    assert theIndex.getTagSource('Jane') == (cHandle, 2, 'T000001')
    assert theIndex.getTagSource('John') == (None, 0, 'T000000')
    assert theIndex.scanText(nHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    assert theIndex.scanText(cHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    hHandle = theProject.newFile('Chapter', nwItemClass.NOVEL, 'a508bb932959c')
    sHandle = theProject.newFile('Scene One', nwItemClass.NOVEL, 'a508bb932959c')
    tHandle = theProject.newFile('Scene Two', nwItemClass.NOVEL, 'a508bb932959c')
    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT
    assert theIndex.scanText(hHandle, '## Chapter One\n\n')
    assert theIndex.scanText(sHandle, '### Scene One\n\n')
    assert theIndex.scanText(tHandle, '### Scene Two\n\n')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.append('0000000000000')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove('0000000000000')
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 6)]
    assert theIndex.getTableOfContents(3, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 2), ('%s:T000001' % sHandle, 3, 'Scene One', 2), ('%s:T000001' % tHandle, 3, 'Scene Two', 2)]
    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [('%s:T000001' % nHandle, 1, 'Hello World!', 12), ('%s:T000011' % nHandle, 1, 'Hello World!', 22)]
    bHandle = '0000000000000'
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [('%s:T000001' % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [('%s:T000001' % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [('%s:T000001' % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [('%s:T000001' % nHandle, 12), ('%s:T000011' % nHandle, 16)]
    assert theProject.closeProject()
    bHandle = '0000000000000'
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [('T000001', 'H2', 'Chapter One')]
    assert theIndex.getHandleHeaders(sHandle) == [('T000001', 'H3', 'Scene One')]
    assert theIndex.getHandleHeaders(tHandle) == [('T000001', 'H3', 'Scene Two')]
    assert theIndex.getHandleHeaders(nHandle) == [('T000001', 'H1', 'Hello World!'), ('T000011', 'H1', 'Hello World!')]","for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False):
    theKeys.append(aKey)","theKeys += [aKey for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False)]","theKeys = [aKey for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False)]",0,1,,,,robosuite
novelWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/novelWriter/tests/test_core/test_core_index.py,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True
    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile('Hello', nwItemClass.NOVEL, 'a508bb932959c')
    cHandle = theProject.newFile('Jane', nwItemClass.CHARACTER, 'afb3043c7b2b3')
    assert theIndex.getNovelData('', '') is None
    assert theIndex.getNovelData('a508bb932959c', '') is None
    assert theIndex.scanText(cHandle, '# Jane Smith\n@tag: Jane\n')
    assert theIndex.scanText(nHandle, '# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n')
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theProject.projTree[nHandle].setExported(False)
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)
    assert theKeys == []
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == []
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 62
    assert wC == 12
    assert pC == 2
    theRefs = theIndex.getReferences('Not a handle')
    assert theRefs['@pov'] == []
    assert theRefs['@char'] == []
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs['@pov'] == ['Jane']
    assert theRefs['@char'] == ['Jane']
    assert theIndex.getBackReferenceList(None) == {}
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: 'T000001'}
    assert theIndex.getTagSource('Jane') == (cHandle, 2, 'T000001')
    assert theIndex.getTagSource('John') == (None, 0, 'T000000')
    assert theIndex.scanText(nHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    assert theIndex.scanText(cHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    hHandle = theProject.newFile('Chapter', nwItemClass.NOVEL, 'a508bb932959c')
    sHandle = theProject.newFile('Scene One', nwItemClass.NOVEL, 'a508bb932959c')
    tHandle = theProject.newFile('Scene Two', nwItemClass.NOVEL, 'a508bb932959c')
    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT
    assert theIndex.scanText(hHandle, '## Chapter One\n\n')
    assert theIndex.scanText(sHandle, '### Scene One\n\n')
    assert theIndex.scanText(tHandle, '### Scene Two\n\n')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.append('0000000000000')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove('0000000000000')
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 6)]
    assert theIndex.getTableOfContents(3, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 2), ('%s:T000001' % sHandle, 3, 'Scene One', 2), ('%s:T000001' % tHandle, 3, 'Scene Two', 2)]
    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [('%s:T000001' % nHandle, 1, 'Hello World!', 12), ('%s:T000011' % nHandle, 1, 'Hello World!', 22)]
    bHandle = '0000000000000'
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [('%s:T000001' % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [('%s:T000001' % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [('%s:T000001' % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [('%s:T000001' % nHandle, 12), ('%s:T000011' % nHandle, 16)]
    assert theProject.closeProject()
    bHandle = '0000000000000'
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [('T000001', 'H2', 'Chapter One')]
    assert theIndex.getHandleHeaders(sHandle) == [('T000001', 'H3', 'Scene One')]
    assert theIndex.getHandleHeaders(tHandle) == [('T000001', 'H3', 'Scene Two')]
    assert theIndex.getHandleHeaders(nHandle) == [('T000001', 'H1', 'Hello World!'), ('T000011', 'H1', 'Hello World!')]","for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True):
    theKeys.append(aKey)","theKeys += [aKey for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True)]","theKeys = [aKey for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True)]",0,1,,,,robosuite
novelWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/novelWriter/tests/test_core/test_core_index.py,https://github.com/vkbo/novelWriter/tree/master/tests/test_core/test_core_index.py,,testCoreIndex_ExtractData$465,"def testCoreIndex_ExtractData(nwMinimal, mockGUI):
    """"""Check the index data extraction functions.
    """"""
    theProject = NWProject(mockGUI)
    theProject.projTree.setSeed(42)
    assert theProject.openProject(nwMinimal) is True
    theIndex = NWIndex(theProject)
    nHandle = theProject.newFile('Hello', nwItemClass.NOVEL, 'a508bb932959c')
    cHandle = theProject.newFile('Jane', nwItemClass.CHARACTER, 'afb3043c7b2b3')
    assert theIndex.getNovelData('', '') is None
    assert theIndex.getNovelData('a508bb932959c', '') is None
    assert theIndex.scanText(cHandle, '# Jane Smith\n@tag: Jane\n')
    assert theIndex.scanText(nHandle, '# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n')
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theProject.projTree[nHandle].setExported(False)
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=False):
        theKeys.append(aKey)
    assert theKeys == ['%s:T000001' % nHandle]
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure(skipExcluded=True):
        theKeys.append(aKey)
    assert theKeys == []
    theKeys = []
    for (aKey, _, _, _) in theIndex.novelStructure():
        theKeys.append(aKey)
    assert theKeys == []
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 62
    assert wC == 12
    assert pC == 2
    theRefs = theIndex.getReferences('Not a handle')
    assert theRefs['@pov'] == []
    assert theRefs['@char'] == []
    theRefs = theIndex.getReferences(nHandle)
    assert theRefs['@pov'] == ['Jane']
    assert theRefs['@char'] == ['Jane']
    assert theIndex.getBackReferenceList(None) == {}
    theRefs = theIndex.getBackReferenceList(cHandle)
    assert theRefs == {nHandle: 'T000001'}
    assert theIndex.getTagSource('Jane') == (cHandle, 2, 'T000001')
    assert theIndex.getTagSource('John') == (None, 0, 'T000000')
    assert theIndex.scanText(nHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(nHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(nHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    assert theIndex.scanText(cHandle, ""# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really.\n\n# Hello World!\n@pov: Jane\n@char: Jane\n\n% this is a comment\n\nThis is a story about Jane Smith.\n\nWell, not really. She's still awesome though.\n"")
    (cC, wC, pC) = theIndex.getCounts(cHandle)
    assert cC == 152
    assert wC == 28
    assert pC == 4
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000001')
    assert cC == 62
    assert wC == 12
    assert pC == 2
    (cC, wC, pC) = theIndex.getCounts(cHandle, 'T000011')
    assert cC == 90
    assert wC == 16
    assert pC == 2
    hHandle = theProject.newFile('Chapter', nwItemClass.NOVEL, 'a508bb932959c')
    sHandle = theProject.newFile('Scene One', nwItemClass.NOVEL, 'a508bb932959c')
    tHandle = theProject.newFile('Scene Two', nwItemClass.NOVEL, 'a508bb932959c')
    theProject.projTree[hHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[sHandle].itemLayout == nwItemLayout.DOCUMENT
    theProject.projTree[tHandle].itemLayout == nwItemLayout.DOCUMENT
    assert theIndex.scanText(hHandle, '## Chapter One\n\n')
    assert theIndex.scanText(sHandle, '### Scene One\n\n')
    assert theIndex.scanText(tHandle, '### Scene Two\n\n')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    assert theIndex._listNovelHandles(True) == [hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.append('0000000000000')
    assert theIndex._listNovelHandles(False) == [nHandle, hHandle, sHandle, tHandle]
    theProject.projTree._treeOrder.remove('0000000000000')
    assert theIndex.getNovelWordCount(False) == 34
    assert theIndex.getNovelWordCount(True) == 6
    assert theIndex.getNovelTitleCounts(False) == [0, 2, 1, 2, 0]
    assert theIndex.getNovelTitleCounts(True) == [0, 0, 1, 2, 0]
    assert theIndex.getTableOfContents(0, True) == []
    assert theIndex.getTableOfContents(1, True) == []
    assert theIndex.getTableOfContents(2, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 6)]
    assert theIndex.getTableOfContents(3, True) == [('%s:T000001' % hHandle, 2, 'Chapter One', 2), ('%s:T000001' % sHandle, 3, 'Scene One', 2), ('%s:T000001' % tHandle, 3, 'Scene Two', 2)]
    assert theIndex.getTableOfContents(0, False) == []
    assert theIndex.getTableOfContents(1, False) == [('%s:T000001' % nHandle, 1, 'Hello World!', 12), ('%s:T000011' % nHandle, 1, 'Hello World!', 22)]
    bHandle = '0000000000000'
    assert theIndex.getHandleWordCounts(bHandle) == []
    assert theIndex.getHandleWordCounts(hHandle) == [('%s:T000001' % hHandle, 2)]
    assert theIndex.getHandleWordCounts(sHandle) == [('%s:T000001' % sHandle, 2)]
    assert theIndex.getHandleWordCounts(tHandle) == [('%s:T000001' % tHandle, 2)]
    assert theIndex.getHandleWordCounts(nHandle) == [('%s:T000001' % nHandle, 12), ('%s:T000011' % nHandle, 16)]
    assert theProject.closeProject()
    bHandle = '0000000000000'
    assert theIndex.getHandleHeaders(bHandle) == []
    assert theIndex.getHandleHeaders(hHandle) == [('T000001', 'H2', 'Chapter One')]
    assert theIndex.getHandleHeaders(sHandle) == [('T000001', 'H3', 'Scene One')]
    assert theIndex.getHandleHeaders(tHandle) == [('T000001', 'H3', 'Scene Two')]
    assert theIndex.getHandleHeaders(nHandle) == [('T000001', 'H1', 'Hello World!'), ('T000011', 'H1', 'Hello World!')]","for (aKey, _, _, _) in theIndex.novelStructure():
    theKeys.append(aKey)","theKeys += [aKey for (aKey, _, _, _) in theIndex.novelStructure()]","theKeys = [aKey for (aKey, _, _, _) in theIndex.novelStructure()]",0,1,,,,robosuite
review-heatmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/review-heatmap/src/review_heatmap/libaddon/_vendor_legacy/typing.py,https://github.com/glutanimate/review-heatmap/tree/master/src/review_heatmap/libaddon/_vendor_legacy/typing.py,_ProtocolMeta,_get_protocol_attrs$1659,"def _get_protocol_attrs(self):
    protocol_bases = []
    for c in self.__mro__:
        if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':
            protocol_bases.append(c)
    attrs = set()
    for base in protocol_bases:
        for attr in base.__dict__.keys():
            for c in self.__mro__:
                if c is not base and attr in c.__dict__ and (not getattr(c, '_is_protocol', False)):
                    break
            else:
                if not attr.startswith('_abc_') and attr != '__abstractmethods__' and (attr != '__annotations__') and (attr != '__weakref__') and (attr != '_is_protocol') and (attr != '_gorg') and (attr != '__dict__') and (attr != '__args__') and (attr != '__slots__') and (attr != '_get_protocol_attrs') and (attr != '__next_in_mro__') and (attr != '__parameters__') and (attr != '__origin__') and (attr != '__orig_bases__') and (attr != '__extra__') and (attr != '__tree_hash__') and (attr != '__module__'):
                    attrs.add(attr)
    return attrs","for c in self.__mro__:
    if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':
        protocol_bases.append(c)","protocol_bases = [c for c in self.__mro__ if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol']","protocol_bases = [c for c in self.__mro__ if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol']",1,,,,,robosuite
yt-dlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/acast.py,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/acast.py,ACastChannelIE,_real_extract$117,"def _real_extract(self, url):
    show_slug = self._match_id(url)
    show = self._call_api(show_slug, show_slug)
    show_info = self._extract_show_info(show)
    entries = []
    for episode in show.get('episodes') or []:
        entries.append(self._extract_episode(episode, show_info))
    return self.playlist_result(entries, show.get('id'), show.get('title'), show.get('description'))","for episode in show.get('episodes') or []:
    entries.append(self._extract_episode(episode, show_info))","entries = [self._extract_episode(episode, show_info) for episode in show.get('episodes', [])]","entries = [self._extract_episode(episode, show_info) for episode in show.get('episodes') or []]",0,0,1,1,,robosuite
tensorlayer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorlayer/tensorlayer/files/utils.py,https://github.com/tensorlayer/tensorlayer/tree/master/tensorlayer/files/utils.py,,load_file_list$2325,"def load_file_list(path=None, regx='\\.jpg', printable=True, keep_prefix=False):
    """"""Return a file list in a folder by given a path and regular expression.

    Parameters
    ----------
    path : str or None
        A folder path, if `None`, use the current directory.
    regx : str
        The regx of file name.
    printable : boolean
        Whether to print the files infomation.
    keep_prefix : boolean
        Whether to keep path in the file name.

    Examples
    ----------
    >>> file_list = tl.files.load_file_list(path=None, regx='w1pre_[0-9]+\\.(npz)')

    """"""
    if path is None:
        path = os.getcwd()
    file_list = os.listdir(path)
    return_list = []
    for (_, f) in enumerate(file_list):
        if re.search(regx, f):
            return_list.append(f)
    if keep_prefix:
        for (i, f) in enumerate(return_list):
            return_list[i] = os.path.join(path, f)
    if printable:
        logging.info('Match file list = %s' % return_list)
        logging.info('Number of files = %d' % len(return_list))
    return return_list","for (_, f) in enumerate(file_list):
    if re.search(regx, f):
        return_list.append(f)","return_list = [f for (_, f) in enumerate(file_list) if re.search(regx, f)]","return_list = [f for (_, f) in enumerate(file_list) if re.search(regx, f)]",1,,,,,robosuite
sdc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sdc/sdc/tests/test_tbb_hashmap.py,https://github.com/IntelPython/sdc/tree/master/sdc/tests/test_tbb_hashmap.py,TestHashmapGeneric,test_hashmap_generic_values$1000,"def test_hashmap_generic_values(self):

    @self.jit
    def test_impl(keys, values):
        a_dict = ConcurrentDict.fromkeys(keys, values[0])
        for (k, v) in zip(keys, values):
            a_dict[k] = v
        res = []
        for (k, v) in a_dict.items():
            res.append((k, v))
        return res
    n = 47
    np.random.seed(0)
    for (key_type, value_type) in self.key_value_combinations():
        keys = self.get_random_sequence(key_type, n)
        values = self.get_random_sequence(value_type, n)
        source_kv_pairs = list(zip(keys, values))
        with self.subTest(key_type=key_type, value_type=value_type, keys=keys, values=values):
            result = test_impl(keys, values)
            assert_dict_correct(self, dict(result), source_kv_pairs)","for (k, v) in a_dict.items():
    res.append((k, v))","res = [(k, v) for (k, v) in a_dict.items()]","res = [(k, v) for (k, v) in a_dict.items()]",1,,,,,robosuite
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/frontend/common.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/common.py,,infer_value$535,"def infer_value(input_val, params, mod=None):
    """"""A hack for getting the value of an expression by evaluating a
    portion of the relay graph. This is often needed for functions that
    whose output shape depends on the value of a tensor.
    """"""
    assert all((var.name_hint in params.keys() for var in analysis.free_vars(input_val))), 'All inputs to infer must be available in params.'
    assert tvm.runtime.enabled('llvm'), 'LLVM must be enabled to infer value.'
    try:
        from tvm.contrib import graph_executor
        func = _function.Function(analysis.free_vars(input_val), input_val)
        with tvm.transform.PassContext(opt_level=0):
            lib = tvm.relay.build(func, target='llvm', params=params)
        dev = tvm.cpu(0)
        m = graph_executor.GraphModule(lib['default'](dev))
        m.run()
        return m.get_output(0)
    except Exception:
        if isinstance(mod, IRModule):
            mod['main'] = _function.Function(analysis.free_vars(input_val), input_val)
        else:
            mod = IRModule.from_expr(input_val)
        inputs = []
        for param in mod['main'].params:
            inputs.append(params[param.name_hint])
        result = tvm.relay.create_executor('debug', mod=mod, device=tvm.cpu(), target='llvm').evaluate()(*inputs)
        return result","for param in mod['main'].params:
    inputs.append(params[param.name_hint])",inputs = [params[param.name_hint] for param in mod['main'].params],inputs = [params[param.name_hint] for param in mod['main'].params],1,,,,,robosuite
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/convert/convert.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/convert/convert.py,Converter,forward_function$213,"def forward_function(x):
    node_outputs = {}
    for (index, input) in enumerate(model_proto.graph.input):
        node_outputs[input.name] = x[index]
    for node in nodes:
        inputs = []
        for input in node.input:
            if input in node_outputs.keys():
                inputs.append(node_outputs[input])
        with tf.name_scope(node.name + '/forward'):
            res = tfe_nodes[node.name].forward(inputs)
        for (i, output) in enumerate(node.output):
            node_outputs[output] = res[i]
    res = []
    for output in model_proto.graph.output:
        res.append(node_outputs[output.name])
    return res","for output in model_proto.graph.output:
    res.append(node_outputs[output.name])",res = [node_outputs[output.name] for output in model_proto.graph.output],res = [node_outputs[output.name] for output in model_proto.graph.output],1,,,,,robosuite
sqlova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlova/sqlova/utils/utils_wikisql.py,https://github.com/naver/sqlova/tree/master/sqlova/utils/utils_wikisql.py,,get_wc1$300,"def get_wc1(conds):
    """"""
    [ [wc, wo, wv],
      [wc, wo, wv], ...
    ]
    """"""
    wc1 = []
    for cond in conds:
        wc1.append(cond[0])
    return wc1","for cond in conds:
    wc1.append(cond[0])",wc1 = [cond[0] for cond in conds],wc1 = [cond[0] for cond in conds],1,,,,,robosuite
open_model_zoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/annotation_converters/electricity_time_series_forecasting.py,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/annotation_converters/electricity_time_series_forecasting.py,ElectricityTimeSeriesForecastingConverter,convert$384,"def convert(self, check_content=False, progress_callback=None, progress_interval=100, **kwargs):
    (_, _, data) = self.formatter.split_data(aggregating_to_hourly_data(pd.read_csv(self.data_path_file, index_col=0, sep=';', decimal=',')))
    data = data.reset_index(drop=True)
    (data_index, col_mappings) = self.build_data_index(data)
    samples = []
    iterator = range(int(data_index.shape[0]))
    if not isinstance(tqdm, UnsupportedPackage):
        iterator = tqdm(iterator)
    for idx in iterator:
        samples.append(TimeSeriesForecastingAnnotation(f'inputs_{idx}', *self.get_sample(data, data_index, col_mappings, idx)))
    return ConverterReturn(samples, None, None)","for idx in iterator:
    samples.append(TimeSeriesForecastingAnnotation(f'inputs_{idx}', *self.get_sample(data, data_index, col_mappings, idx)))","samples = [TimeSeriesForecastingAnnotation(f'inputs_{idx}', *self.get_sample(data, data_index, col_mappings, idx)) for idx in iterator]","samples = [TimeSeriesForecastingAnnotation(f'inputs_{idx}', *self.get_sample(data, data_index, col_mappings, idx)) for idx in iterator]",1,,,,,robosuite
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,DwordToBits$273,"def DwordToBits(srcDword):
    """"""
	Converts a dword into an array of 32 bits
	""""""
    bit_array = []
    h_str = '%08x' % srcDword
    h_size = len(h_str) * 4
    bits = bin(int(h_str, 16))[2:].zfill(h_size)[::-1]
    for bit in bits:
        bit_array.append(int(bit))
    return bit_array","for bit in bits:
    bit_array.append(int(bit))",bit_array = [int(bit) for bit in bits],bit_array = [int(bit) for bit in bits],1,,,,,robosuite
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/core/doctype/user/user.py,https://github.com/frappe/frappe/tree/master/frappe/core/doctype/user/user.py,User,after_rename$453,"def after_rename(self, old_name, new_name, merge=False):
    tables = frappe.db.get_tables()
    for tab in tables:
        desc = frappe.db.get_table_columns_description(tab)
        has_fields = []
        for d in desc:
            if d.get('name') in ['owner', 'modified_by']:
                has_fields.append(d.get('name'))
        for field in has_fields:
            frappe.db.sql('UPDATE `%s`\n\t\t\t\t\tSET `%s` = %s\n\t\t\t\t\tWHERE `%s` = %s' % (tab, field, '%s', field, '%s'), (new_name, old_name))
    if frappe.db.exists('Notification Settings', old_name):
        frappe.rename_doc('Notification Settings', old_name, new_name, force=True, show_alert=False)
    frappe.db.set_value('User', new_name, 'email', new_name)","for d in desc:
    if d.get('name') in ['owner', 'modified_by']:
        has_fields.append(d.get('name'))","has_fields = [d.get('name') for d in desc if d.get('name') in ['owner', 'modified_by']]","has_fields = [d.get('name') for d in desc if d.get('name') in ['owner', 'modified_by']]",1,,,,,robosuite
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/io/votable/exceptions.py,https://github.com/astropy/astropy/tree/master/astropy/io/votable/exceptions.py,,_get_warning_and_exception_classes$1520,"def _get_warning_and_exception_classes(prefix):
    classes = []
    for (key, val) in globals().items():
        if re.match(prefix + '[0-9]{2}', key):
            classes.append((key, val))
    classes.sort()
    return classes","for (key, val) in globals().items():
    if re.match(prefix + '[0-9]{2}', key):
        classes.append((key, val))","classes = [(key, val) for (key, val) in globals().items() if re.match(prefix + '[0-9]{2}', key)]","classes = [(key, val) for (key, val) in globals().items() if re.match(prefix + '[0-9]{2}', key)]",1,,,,,robosuite
SSD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSD/ssd/modeling/backbone/efficient_net/utils.py,https://github.com/lufficc/SSD/tree/master/ssd/modeling/backbone/efficient_net/utils.py,BlockDecoder,decode$164,"def decode(string_list):
    """"""
        Decodes a list of string notations to specify blocks inside the network.

        :param string_list: a list of strings, each string is a notation of block
        :return: a list of BlockArgs namedtuples of block args
        """"""
    assert isinstance(string_list, list)
    blocks_args = []
    for block_string in string_list:
        blocks_args.append(BlockDecoder._decode_block_string(block_string))
    return blocks_args","for block_string in string_list:
    blocks_args.append(BlockDecoder._decode_block_string(block_string))",blocks_args = [BlockDecoder._decode_block_string(block_string) for block_string in string_list],blocks_args = [BlockDecoder._decode_block_string(block_string) for block_string in string_list],1,,,,,robosuite
LightAutoML,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightAutoML/lightautoml/report/report_deco.py,https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/report/report_deco.py,ReportDeco,generate_report$1109,"def generate_report(self):
    self._generate_results_section()
    sections_list = []
    for sec_name in self.sections_order:
        if sec_name in self._sections:
            sections_list.append(self._sections[sec_name])
    env = Environment(loader=FileSystemLoader(searchpath=self.template_path))
    report = env.get_template(self._base_template_path).render(title=self.title, sections=sections_list, pdf=self.pdf_file_name)
    with open(os.path.join(self.output_path, self.report_file_name), 'w', encoding='utf-8') as f:
        f.write(report)
    if self.pdf_file_name:
        try:
            from weasyprint import HTML
            HTML(string=report, base_url=self.output_path).write_pdf(os.path.join(self.output_path, self.pdf_file_name))
        except ModuleNotFoundError:
            print(""Can't generate PDF report: check manual for installing pdf extras."")","for sec_name in self.sections_order:
    if sec_name in self._sections:
        sections_list.append(self._sections[sec_name])",sections_list = [self._sections[sec_name] for sec_name in self.sections_order if sec_name in self._sections],sections_list = [self._sections[sec_name] for sec_name in self.sections_order if sec_name in self._sections],1,,,,,robosuite
osxphotos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osxphotos/osxphotos/photosdb/photosdb.py,https://github.com/RhetTbull/osxphotos/tree/master/osxphotos/photosdb/photosdb.py,PhotosDB,_get_album_uuids$2715,"def _get_album_uuids(self, shared=False, import_session=False):
    """"""Return list of album UUIDs found in photos database

            Filters out albums in the trash and any special album types

        Args:
            shared: boolean; if True, returns shared albums, else normal albums
            import_session: boolean, if True, returns import session albums, else normal or shared albums
            Note: flags (shared, import_session) are mutually exclusive

        Raises:
            ValueError: raised if mutually exclusive flags passed

        Returns: list of album UUIDs
        """"""
    if shared and import_session:
        raise ValueError('flags are mutually exclusive: pass zero or one of shared, import_session')
    if self._db_version <= _PHOTOS_4_VERSION:
        version4 = True
        if shared:
            logging.warning(f'Shared albums not implemented for Photos library version {self._db_version}')
            return []
        elif import_session:
            logging.warning(f'Import sessions not implemented for Photos library version {self._db_version}')
            return []
        else:
            album_kind = _PHOTOS_4_ALBUM_KIND
    else:
        version4 = False
        if shared:
            album_kind = _PHOTOS_5_SHARED_ALBUM_KIND
        elif import_session:
            album_kind = _PHOTOS_5_IMPORT_SESSION_ALBUM_KIND
        else:
            album_kind = _PHOTOS_5_ALBUM_KIND
    album_list = []
    for (album, detail) in self._dbalbum_details.items():
        if detail['kind'] == album_kind and (not detail['intrash']) and (shared and detail['cloudownerhashedpersonid'] is not None or (not shared and detail['cloudownerhashedpersonid'] is None)) and (not version4 or (version4 and detail['folderUuid'] != _PHOTOS_4_ROOT_FOLDER)):
            album_list.append(album)
    return album_list","for (album, detail) in self._dbalbum_details.items():
    if detail['kind'] == album_kind and (not detail['intrash']) and (shared and detail['cloudownerhashedpersonid'] is not None or (not shared and detail['cloudownerhashedpersonid'] is None)) and (not version4 or (version4 and detail['folderUuid'] != _PHOTOS_4_ROOT_FOLDER)):
        album_list.append(album)","album_list = [album for (album, detail) in self._dbalbum_details.items() if detail['kind'] == album_kind and (not detail['intrash']) and (shared and detail['cloudownerhashedpersonid'] is not None or (not shared and detail['cloudownerhashedpersonid'] is None)) and (not version4 or (version4 and detail['folderUuid'] != _PHOTOS_4_ROOT_FOLDER))]","album_list = [album for (album, detail) in self._dbalbum_details.items() if detail['kind'] == album_kind and (not detail['intrash']) and (shared and detail['cloudownerhashedpersonid'] is not None or (not shared and detail['cloudownerhashedpersonid'] is None)) and (not version4 or (version4 and detail['folderUuid'] != _PHOTOS_4_ROOT_FOLDER))]",1,,,,,robosuite
devicon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/devicon/.github/scripts/build_assets/filehandler.py,https://github.com/devicons/devicon/tree/master/.github/scripts/build_assets/filehandler.py,,find_new_icons_in_devicon_json$10,"def find_new_icons_in_devicon_json(devicon_json_path: str, icomoon_json_path: str):
    """"""
    Find the newly added icons by finding the difference between
    the devicon.json and the icomoon.json.
    :param devicon_json_path, the path to the devicon.json.
    :param icomoon_json_path: a path to the iconmoon.json.
    :return: a list of the new icons as JSON objects.
    """"""
    devicon_json = get_json_file_content(devicon_json_path)
    icomoon_json = get_json_file_content(icomoon_json_path)
    new_icons = []
    for icon in devicon_json:
        if is_not_in_icomoon_json(icon, icomoon_json):
            new_icons.append(icon)
    return new_icons","for icon in devicon_json:
    if is_not_in_icomoon_json(icon, icomoon_json):
        new_icons.append(icon)","new_icons = [icon for icon in devicon_json if is_not_in_icomoon_json(icon, icomoon_json)]","new_icons = [icon for icon in devicon_json if is_not_in_icomoon_json(icon, icomoon_json)]",1,,,,,robosuite
gsutil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gsutil/gslib/tests/test_cp.py,https://github.com/GoogleCloudPlatform/gsutil/tree/master/gslib/tests/test_cp.py,TestCp,test_gzip_transport_encoded_upload_and_download$2313,"def test_gzip_transport_encoded_upload_and_download(self):
    """"""Test gzip encoded files upload correctly.

    This checks that files are not tagged with a gzip content encoding and
    that the contents of the files are uncompressed in GCS. This test uses the
    -j flag to target specific extensions.
    """"""

    def _create_test_data():
        """"""Setup the bucket and local data to test with.

      Returns:
        Triplet containing the following values:
          bucket_uri: String URI of cloud storage bucket to upload mock data
                      to.
          tmpdir: String, path of a temporary directory to write mock data to.
          local_uris: Tuple of three strings; each is the file path to a file
                      containing mock data.
      """"""
        bucket_uri = self.CreateBucket()
        contents = b'x' * 10000
        tmpdir = self.CreateTempDir()
        local_uris = []
        for filename in ('test.html', 'test.js', 'test.txt'):
            local_uris.append(self.CreateTempFile(file_name=filename, tmpdir=tmpdir, contents=contents))
        return (bucket_uri, tmpdir, local_uris)

    def _upload_test_data(tmpdir, bucket_uri):
        """"""Upload local test data.

      Args:
        tmpdir: String, path of a temporary directory to write mock data to.
        bucket_uri: String URI of cloud storage bucket to upload mock data to.

      Returns:
        stderr: String output from running the gsutil command to upload mock
                  data.
      """"""
        if self._use_gcloud_storage:
            extension_list_string = 'js,html'
        else:
            extension_list_string = 'js, html'
        stderr = self.RunGsUtil(['-D', 'cp', '-j', extension_list_string, os.path.join(tmpdir, 'test*'), suri(bucket_uri)], return_stderr=True)
        self.AssertNObjectsInBucket(bucket_uri, 3)
        return stderr

    def _assert_sent_compressed(local_uris, stderr):
        """"""Ensure the correct files were marked for compression.

      Args:
        local_uris: Tuple of three strings; each is the file path to a file
                    containing mock data.
        stderr: String output from running the gsutil command to upload mock
                data.
      """"""
        (local_uri_html, local_uri_js, local_uri_txt) = local_uris
        assert_base_string = 'Using compressed transport encoding for file://{}.'
        self.assertIn(assert_base_string.format(local_uri_html), stderr)
        self.assertIn(assert_base_string.format(local_uri_js), stderr)
        self.assertNotIn(assert_base_string.format(local_uri_txt), stderr)

    def _assert_stored_uncompressed(bucket_uri, contents=b'x' * 10000):
        """"""Ensure the files are not compressed when they are stored in the bucket.

      Args:
        bucket_uri: String with URI for bucket containing uploaded test data.
        contents: Byte string that are stored in each file in the bucket.
      """"""
        local_uri_html = suri(bucket_uri, 'test.html')
        local_uri_js = suri(bucket_uri, 'test.js')
        local_uri_txt = suri(bucket_uri, 'test.txt')
        fpath4 = self.CreateTempFile()
        for uri in (local_uri_html, local_uri_js, local_uri_txt):
            stdout = self.RunGsUtil(['stat', uri], return_stdout=True)
            self.assertNotRegex(stdout, 'Content-Encoding:\\s+gzip')
            self.RunGsUtil(['cp', uri, suri(fpath4)])
            with open(fpath4, 'rb') as f:
                self.assertEqual(f.read(), contents)
    (bucket_uri, tmpdir, local_uris) = _create_test_data()
    stderr = _upload_test_data(tmpdir, bucket_uri)
    _assert_sent_compressed(local_uris, stderr)
    _assert_stored_uncompressed(bucket_uri)","for filename in ('test.html', 'test.js', 'test.txt'):
    local_uris.append(self.CreateTempFile(file_name=filename, tmpdir=tmpdir, contents=contents))","local_uris = [self.CreateTempFile(file_name=filename, tmpdir=tmpdir, contents=contents) for filename in ('test.html', 'test.js', 'test.txt')]","local_uris = [self.CreateTempFile(file_name=filename, tmpdir=tmpdir, contents=contents) for filename in ('test.html', 'test.js', 'test.txt')]",1,,,,,robosuite
NodeGraphQt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NodeGraphQt/NodeGraphQt/widgets/viewer.py,https://github.com/jchanvfx/NodeGraphQt/tree/master/NodeGraphQt/widgets/viewer.py,NodeViewer,mouseReleaseEvent$419,"def mouseReleaseEvent(self, event):
    if event.button() == QtCore.Qt.LeftButton:
        self.LMB_state = False
    elif event.button() == QtCore.Qt.RightButton:
        self.RMB_state = False
    elif event.button() == QtCore.Qt.MiddleButton:
        self.MMB_state = False
    if self._SLICER_PIPE.isVisible():
        self._on_pipes_sliced(self._SLICER_PIPE.path())
        p = QtCore.QPointF(0.0, 0.0)
        self._SLICER_PIPE.draw_path(p, p)
        self._SLICER_PIPE.setVisible(False)
    if self._rubber_band.isActive:
        self._rubber_band.isActive = False
        if self._rubber_band.isVisible():
            rect = self._rubber_band.rect()
            map_rect = self.mapToScene(rect).boundingRect()
            self._rubber_band.hide()
            rect = QtCore.QRect(self._origin_pos, event.pos()).normalized()
            rect_items = self.scene().items(self.mapToScene(rect).boundingRect())
            node_ids = []
            for item in rect_items:
                if isinstance(item, AbstractNodeItem):
                    node_ids.append(item.id)
            if node_ids:
                prev_ids = [n.id for n in self._prev_selection_nodes if not n.selected]
                self.node_selected.emit(node_ids[0])
                self.node_selection_changed.emit(node_ids, prev_ids)
            self.scene().update(map_rect)
            return
    moved_nodes = {n: xy_pos for (n, xy_pos) in self._node_positions.items() if n.xy_pos != xy_pos}
    if moved_nodes and (not self.COLLIDING_state):
        self.moved_nodes.emit(moved_nodes)
    self._node_positions = {}
    (nodes, pipes) = self.selected_items()
    if self.COLLIDING_state and nodes and pipes:
        self.insert_node.emit(pipes[0], nodes[0].id, moved_nodes)
    prev_ids = [n.id for n in self._prev_selection_nodes if not n.selected]
    node_ids = [n.id for n in nodes if n not in self._prev_selection_nodes]
    self.node_selection_changed.emit(node_ids, prev_ids)
    super(NodeViewer, self).mouseReleaseEvent(event)","for item in rect_items:
    if isinstance(item, AbstractNodeItem):
        node_ids.append(item.id)","node_ids = [item.id for item in rect_items if isinstance(item, AbstractNodeItem)]","node_ids = [item.id for item in rect_items if isinstance(item, AbstractNodeItem)]",1,,,,,robosuite
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/auth/validate.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/auth/validate.py,ValidationHandler,check_by_transactionid$243,"def check_by_transactionid(self, transid, passw, options=None):
    """"""
        check the passw against the open transaction

        :param transid: the transaction id
        :param passw: the pass parameter
        :param options: the additional optional parameters

        :return: tuple of boolean and detail dict
        """"""
    reply = {}
    serials = []
    challenges = Challenges.lookup_challenges(transid=transid)
    for challenge in challenges:
        serials.append(challenge.tokenserial)
    if not serials:
        reply['value'] = False
        reply['failure'] = 'No challenge for transaction %r found' % transid
        return (False, reply)
    ok = False
    reply['failcount'] = 0
    reply['value'] = False
    reply['token_type'] = ''
    token_type = options.get('token_type', None)
    for serial in serials:
        tokens = getTokens4UserOrSerial(serial=serial, token_type=token_type, read_for_update=True)
        if not tokens and token_type:
            continue
        if not tokens and (not token_type):
            raise Exception('tokenmismatch for token serial: %r' % serial)
        token = tokens[0]
        owner = get_token_owner(token)
        (ok, opt) = self.checkTokenList(tokens, passw, user=owner, options=options)
        if opt:
            reply.update(opt)
        reply['value'] = ok
        reply['token_type'] = token.getType()
        reply['failcount'] = token.getFailCount()
        reply['serial'] = token.getSerial()
        if ok:
            break
    return (ok, reply)","for challenge in challenges:
    serials.append(challenge.tokenserial)",serials = [challenge.tokenserial for challenge in challenges],serials = [challenge.tokenserial for challenge in challenges],1,,,,,robosuite
pigar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pigar/pigar/unpack.py,https://github.com/damnever/pigar/tree/master/pigar/unpack.py,Archive,_safe_extractall$67,"def _safe_extractall(self, to_path='.'):
    unsafe = []
    for name in self.names:
        if not self.is_safe(name):
            unsafe.append(name)
    if unsafe:
        raise ValueError('unsafe to unpack: {}'.format(unsafe))
    self._file.extractall(to_path)","for name in self.names:
    if not self.is_safe(name):
        unsafe.append(name)",unsafe = [name for name in self.names if not self.is_safe(name)],unsafe = [name for name in self.names if not self.is_safe(name)],1,,,,,robosuite
PowerDNS-Admin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/dashboard.py,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/dashboard.py,,domains_custom$46,"def domains_custom(boxId):
    if current_user.role.name in ['Administrator', 'Operator']:
        domains = Domain.query
    else:
        domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id))
    template = current_app.jinja_env.get_template('dashboard_domain.html')
    render = template.make_module(vars={'current_user': current_user, 'allow_user_view_history': Setting().get('allow_user_view_history')})
    columns = [Domain.name, Domain.dnssec, Domain.type, Domain.serial, Domain.master, Domain.account_id]
    order_by = []
    for i in range(len(columns)):
        column_index = request.args.get('order[{0}][column]'.format(i))
        sort_direction = request.args.get('order[{0}][dir]'.format(i))
        if column_index is None:
            break
        if sort_direction != 'asc' and sort_direction != 'desc':
            sort_direction = 'asc'
        column = columns[int(column_index)]
        order_by.append(getattr(column, sort_direction)())
    if order_by:
        domains = domains.order_by(*order_by)
    if boxId == 'reverse':
        for boxId in customBoxes.order:
            if boxId == 'reverse':
                continue
            domains = domains.filter(not_(Domain.name.ilike(customBoxes.boxes[boxId][1])))
    else:
        domains = domains.filter(Domain.name.ilike(customBoxes.boxes[boxId][1]))
    total_count = domains.count()
    search = request.args.get('search[value]')
    if search:
        start = '' if search.startswith('^') else '%'
        end = '' if search.endswith('$') else '%'
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = domains.outerjoin(Account).filter(Domain.name.ilike(start + search.strip('^$') + end) | Account.name.ilike(start + search.strip('^$') + end) | Account.description.ilike(start + search.strip('^$') + end))
        else:
            domains = domains.filter(Domain.name.ilike(start + search.strip('^$') + end))
    filtered_count = domains.count()
    start = int(request.args.get('start', 0))
    length = min(int(request.args.get('length', 0)), 100)
    if length != -1:
        domains = domains[start:start + length]
    data = []
    for domain in domains:
        data.append([render.name(domain), render.dnssec(domain), render.type(domain), render.serial(domain), render.master(domain), render.account(domain), render.actions(domain)])
    response_data = {'draw': int(request.args.get('draw', 0)), 'recordsTotal': total_count, 'recordsFiltered': filtered_count, 'data': data}
    return jsonify(response_data)","for i in range(len(columns)):
    column_index = request.args.get('order[{0}][column]'.format(i))
    sort_direction = request.args.get('order[{0}][dir]'.format(i))
    if column_index is None:
        break
    if sort_direction != 'asc' and sort_direction != 'desc':
        sort_direction = 'asc'
    column = columns[int(column_index)]
    order_by.append(getattr(column, sort_direction)())","order_by = [getattr(columns[int(request.args.get('order[{0}][column]'.format(i)))], 'asc' if request.args.get('order[{0}][dir]'.format(i)) != 'desc' and request.args.get('order[{0}][dir]'.format(i)) != 'asc' else request.args.get('order[{0}][dir]'.format(i)))() for i in range(len(columns)) if request.args.get('order[{0}][column]'.format(i)) is not None]",Cannot refactor,-1,1,,,,robosuite
PowerDNS-Admin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/dashboard.py,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/dashboard.py,,domains_custom$46,"def domains_custom(boxId):
    if current_user.role.name in ['Administrator', 'Operator']:
        domains = Domain.query
    else:
        domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id))
    template = current_app.jinja_env.get_template('dashboard_domain.html')
    render = template.make_module(vars={'current_user': current_user, 'allow_user_view_history': Setting().get('allow_user_view_history')})
    columns = [Domain.name, Domain.dnssec, Domain.type, Domain.serial, Domain.master, Domain.account_id]
    order_by = []
    for i in range(len(columns)):
        column_index = request.args.get('order[{0}][column]'.format(i))
        sort_direction = request.args.get('order[{0}][dir]'.format(i))
        if column_index is None:
            break
        if sort_direction != 'asc' and sort_direction != 'desc':
            sort_direction = 'asc'
        column = columns[int(column_index)]
        order_by.append(getattr(column, sort_direction)())
    if order_by:
        domains = domains.order_by(*order_by)
    if boxId == 'reverse':
        for boxId in customBoxes.order:
            if boxId == 'reverse':
                continue
            domains = domains.filter(not_(Domain.name.ilike(customBoxes.boxes[boxId][1])))
    else:
        domains = domains.filter(Domain.name.ilike(customBoxes.boxes[boxId][1]))
    total_count = domains.count()
    search = request.args.get('search[value]')
    if search:
        start = '' if search.startswith('^') else '%'
        end = '' if search.endswith('$') else '%'
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = domains.outerjoin(Account).filter(Domain.name.ilike(start + search.strip('^$') + end) | Account.name.ilike(start + search.strip('^$') + end) | Account.description.ilike(start + search.strip('^$') + end))
        else:
            domains = domains.filter(Domain.name.ilike(start + search.strip('^$') + end))
    filtered_count = domains.count()
    start = int(request.args.get('start', 0))
    length = min(int(request.args.get('length', 0)), 100)
    if length != -1:
        domains = domains[start:start + length]
    data = []
    for domain in domains:
        data.append([render.name(domain), render.dnssec(domain), render.type(domain), render.serial(domain), render.master(domain), render.account(domain), render.actions(domain)])
    response_data = {'draw': int(request.args.get('draw', 0)), 'recordsTotal': total_count, 'recordsFiltered': filtered_count, 'data': data}
    return jsonify(response_data)","for domain in domains:
    data.append([render.name(domain), render.dnssec(domain), render.type(domain), render.serial(domain), render.master(domain), render.account(domain), render.actions(domain)])","data = [[render.name(domain), render.dnssec(domain), render.type(domain), render.serial(domain), render.master(domain), render.account(domain), render.actions(domain)] for domain in domains]","data = [[render.name(domain), render.dnssec(domain), render.type(domain), render.serial(domain), render.master(domain), render.account(domain), render.actions(domain)] for domain in domains]",1,,,,,robosuite
anki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/qt/aqt/addons.py,https://github.com/ankitects/anki/tree/master/qt/aqt/addons.py,AddonManager,ankiweb_addons$319,"def ankiweb_addons(self) -> list[int]:
    ids = []
    for meta in self.all_addon_meta():
        if meta.ankiweb_id() is not None:
            ids.append(meta.ankiweb_id())
    return ids","for meta in self.all_addon_meta():
    if meta.ankiweb_id() is not None:
        ids.append(meta.ankiweb_id())",ids = [meta.ankiweb_id() for meta in self.all_addon_meta() if meta.ankiweb_id() is not None],ids = [meta.ankiweb_id() for meta in self.all_addon_meta() if meta.ankiweb_id() is not None],1,,,,,robosuite
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/bots.py,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/bots.py,,_convert_heartbeats_to_dicts$48,"def _convert_heartbeats_to_dicts(heartbeats):
    """"""Format heartbeats for template.""""""
    alive_cutoff = _get_alive_cutoff()
    result = []
    for heartbeat in heartbeats:
        result.append({'bot_name': heartbeat.bot_name, 'source_version': heartbeat.source_version, 'task_payload': heartbeat.task_payload, 'platform_id': heartbeat.platform_id, 'task_end_time': utils.utc_datetime_to_timestamp(heartbeat.task_end_time) if heartbeat.task_end_time else '', 'last_beat_time': utils.utc_datetime_to_timestamp(heartbeat.last_beat_time) if heartbeat.last_beat_time else '', 'alive': 'alive' if heartbeat.last_beat_time > alive_cutoff else 'dead'})
    return result","for heartbeat in heartbeats:
    result.append({'bot_name': heartbeat.bot_name, 'source_version': heartbeat.source_version, 'task_payload': heartbeat.task_payload, 'platform_id': heartbeat.platform_id, 'task_end_time': utils.utc_datetime_to_timestamp(heartbeat.task_end_time) if heartbeat.task_end_time else '', 'last_beat_time': utils.utc_datetime_to_timestamp(heartbeat.last_beat_time) if heartbeat.last_beat_time else '', 'alive': 'alive' if heartbeat.last_beat_time > alive_cutoff else 'dead'})","result = [{'bot_name': heartbeat.bot_name, 'source_version': heartbeat.source_version, 'task_payload': heartbeat.task_payload, 'platform_id': heartbeat.platform_id, 'task_end_time': utils.utc_datetime_to_timestamp(heartbeat.task_end_time) if heartbeat.task_end_time else '', 'last_beat_time': utils.utc_datetime_to_timestamp(heartbeat.last_beat_time) if heartbeat.last_beat_time else '', 'alive': 'alive' if heartbeat.last_beat_time > alive_cutoff else 'dead'} for heartbeat in heartbeats]","result = [{'bot_name': heartbeat.bot_name, 'source_version': heartbeat.source_version, 'task_payload': heartbeat.task_payload, 'platform_id': heartbeat.platform_id, 'task_end_time': utils.utc_datetime_to_timestamp(heartbeat.task_end_time) if heartbeat.task_end_time else '', 'last_beat_time': utils.utc_datetime_to_timestamp(heartbeat.last_beat_time) if heartbeat.last_beat_time else '', 'alive': 'alive' if heartbeat.last_beat_time > alive_cutoff else 'dead'} for heartbeat in heartbeats]",1,,,,,robosuite
codechecker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/codechecker/web/codechecker_web/shared/webserver_context.py,https://github.com/Ericsson/codechecker/tree/master/web/codechecker_web/shared/webserver_context.py,Context,path_env_extra$184,"def path_env_extra(self):
    extra_paths = self.pckg_layout.get('path_env_extra', [])
    paths = []
    for path in extra_paths:
        paths.append(os.path.join(self._data_files_dir_path, path))
    return paths","for path in extra_paths:
    paths.append(os.path.join(self._data_files_dir_path, path))","paths += [os.path.join(self._data_files_dir_path, path) for path in extra_paths]","paths = [os.path.join(self._data_files_dir_path, path) for path in extra_paths]",0,1,,,,robosuite
wagtail,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wagtail/wagtail/contrib/modeladmin/views.py,https://github.com/wagtail/wagtail/tree/master/wagtail/contrib/modeladmin/views.py,InspectView,get_fields_dict$951,"def get_fields_dict(self):
    """"""
        Return a list of `label`/`value` dictionaries to represent the
        fields named by the model_admin class's `get_inspect_view_fields` method
        """"""
    fields = []
    for field_name in self.model_admin.get_inspect_view_fields():
        fields.append(self.get_dict_for_field(field_name))
    return fields","for field_name in self.model_admin.get_inspect_view_fields():
    fields.append(self.get_dict_for_field(field_name))",fields += [self.get_dict_for_field(field_name) for field_name in self.model_admin.get_inspect_view_fields()],fields = [self.get_dict_for_field(field_name) for field_name in self.model_admin.get_inspect_view_fields()],0,1,,,,robosuite
BasicSR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BasicSR/basicsr/models/base_model.py,https://github.com/xinntao/BasicSR/tree/master/basicsr/models/base_model.py,BaseModel,update_learning_rate$181,"def update_learning_rate(self, current_iter, warmup_iter=-1):
    """"""Update learning rate.

        Args:
            current_iter (int): Current iteration.
            warmup_iter (int)閿 Warm-up iter numbers. -1 for no warm-up.
                Default閿 -1.
        """"""
    if current_iter > 1:
        for scheduler in self.schedulers:
            scheduler.step()
    if current_iter < warmup_iter:
        init_lr_g_l = self._get_init_lr()
        warm_up_lr_l = []
        for init_lr_g in init_lr_g_l:
            warm_up_lr_l.append([v / warmup_iter * current_iter for v in init_lr_g])
        self._set_lr(warm_up_lr_l)","for init_lr_g in init_lr_g_l:
    warm_up_lr_l.append([v / warmup_iter * current_iter for v in init_lr_g])",warm_up_lr_l = [[v / warmup_iter * current_iter for v in init_lr_g] for init_lr_g in init_lr_g_l],warm_up_lr_l = [[v / warmup_iter * current_iter for v in init_lr_g] for init_lr_g in init_lr_g_l],1,,,,,robosuite
PyTorch-StudioGAN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyTorch-StudioGAN/src/utils/sample.py,https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/tree/master/src/utils/sample.py,,make_target_cls_sampler$242,"def make_target_cls_sampler(dataset, target_class):
    try:
        targets = dataset.data.targets
    except:
        targets = dataset.labels
    label_indices = []
    for i in range(len(dataset)):
        if targets[i] == target_class:
            label_indices.append(i)
    num_samples = len(label_indices)
    sampler = torch.utils.data.sampler.SubsetRandomSampler(label_indices)
    return (num_samples, sampler)","for i in range(len(dataset)):
    if targets[i] == target_class:
        label_indices.append(i)",label_indices = [i for i in range(len(dataset)) if targets[i] == target_class],label_indices = [i for i in range(len(dataset)) if targets[i] == target_class],1,,,,,robosuite
sunpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/sunpy/net/fido_factory.py,https://github.com/sunpy/sunpy/tree/master/sunpy/net/fido_factory.py,UnifiedResponse,__getitem__$83,"def __getitem__(self, aslice):
    """"""
        Support slicing the UnifiedResponse as a 2D object.

        The first index is to the client and the second index is the records
        returned from those clients.
        """"""
    if isinstance(aslice, (int, slice)):
        ret = self._list[aslice]
    elif isinstance(aslice, str):
        ret = self._getitem_string(aslice)
    elif isinstance(aslice, tuple):
        if len(aslice) > 2:
            raise IndexError('UnifiedResponse objects can only be sliced with one or two indices.')
        if isinstance(aslice[0], str):
            intermediate = self._getitem_string(aslice[0])
        else:
            intermediate = self._list[aslice[0]]
        if isinstance(intermediate, list):
            ret = []
            for client_resp in intermediate:
                ret.append(client_resp[aslice[1]])
        else:
            ret = intermediate[aslice[1]]
    else:
        raise IndexError('UnifiedResponse objects must be sliced with integers or strings.')
    if isinstance(ret, (QueryResponseTable, QueryResponseColumn, QueryResponseRow)):
        return ret
    return UnifiedResponse(*ret)","for client_resp in intermediate:
    ret.append(client_resp[aslice[1]])",ret = [client_resp[aslice[1]] for client_resp in intermediate],ret = [client_resp[aslice[1]] for client_resp in intermediate],1,,,,,robosuite
pandera,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandera/pandera/strategies.py,https://github.com/pandera-dev/pandera/tree/master/pandera/strategies.py,,dataframe_strategy$933,"def dataframe_strategy(pandera_dtype: Optional[DataType]=None, strategy: Optional[SearchStrategy]=None, *, columns: Optional[Dict]=None, checks: Optional[Sequence]=None, unique: Optional[List[str]]=None, index: Optional[IndexComponent]=None, size: Optional[int]=None, n_regex_columns: int=1):
    """"""Strategy to generate a pandas DataFrame.

    :param pandera_dtype: :class:`pandera.dtypes.DataType` instance.
    :param strategy: if specified, this will raise a BaseStrategyOnlyError,
        since it cannot be chained to a prior strategy.
    :param columns: a dictionary where keys are column names and values
        are :class:`~pandera.schema_components.Column` objects.
    :param checks: sequence of :class:`~pandera.checks.Check` s to constrain
        the values of the data at the dataframe level.
    :param unique: a list of column names that should be jointly unique.
    :param index: Index or MultiIndex schema component.
    :param size: number of elements in the Series.
    :param n_regex_columns: number of regex columns to generate.
    :returns: ``hypothesis`` strategy.
    """"""
    if n_regex_columns < 1:
        raise ValueError(f'`n_regex_columns` must be a positive integer, found: {n_regex_columns}')
    if strategy:
        raise BaseStrategyOnlyError('The dataframe strategy is a base strategy. You cannot specify the strategy argument to chain it to a parent strategy.')
    columns = {} if columns is None else columns
    checks = [] if checks is None else checks

    def undefined_check_strategy(strategy, check, column=None):
        """"""Strategy for checks with undefined strategies.""""""

        def _element_wise_check_fn(element):
            return check._check_fn(element)

        def _column_check_fn(dataframe):
            return check(dataframe[column]).check_passed

        def _dataframe_check_fn(dataframe):
            return check(dataframe).check_passed
        if check.element_wise:
            check_fn = _element_wise_check_fn
            warning_type = 'Element-wise'
        elif column is None:
            check_fn = _dataframe_check_fn
            warning_type = 'Dataframe'
        else:
            check_fn = _column_check_fn
            warning_type = 'Column'
        warnings.warn(f""{warning_type} check doesn't have a defined strategy. Falling back to filtering drawn values based on the check definition. This can considerably slow down data-generation."")
        return strategy.filter(check_fn)

    def make_row_strategy(col, checks):
        strategy = None
        for check in checks:
            if hasattr(check, 'strategy'):
                strategy = check.strategy(col.dtype, strategy)
            else:
                strategy = undefined_check_strategy(strategy=pandas_dtype_strategy(col.dtype) if strategy is None else strategy, check=check)
        if strategy is None:
            strategy = pandas_dtype_strategy(col.dtype)
        return strategy

    @composite
    def _dataframe_strategy(draw):
        row_strategy_checks = []
        undefined_strat_df_checks = []
        for check in checks:
            if hasattr(check, 'strategy') or check.element_wise:
                row_strategy_checks.append(check)
            else:
                undefined_strat_df_checks.append(check)
        expanded_columns = {}
        for (col_name, column) in columns.items():
            if unique and col_name in unique:
                column = deepcopy(column)
                column.unique = True
            if not column.regex:
                expanded_columns[col_name] = column
            else:
                regex_columns = draw(st.lists(st.from_regex(column.name, fullmatch=True), min_size=n_regex_columns, max_size=n_regex_columns, unique=True))
                for regex_col in regex_columns:
                    expanded_columns[regex_col] = deepcopy(column).set_name(regex_col)
        undefined_strat_column_checks: Dict[str, list] = defaultdict(list)
        for (col_name, column) in expanded_columns.items():
            undefined_strat_column_checks[col_name].extend((check for check in column.checks if not hasattr(check, 'strategy') and (not check.element_wise)))
        col_dtypes = {col_name: str(col.dtype) if pandera_dtype is None else str(pandera_dtype) for (col_name, col) in expanded_columns.items()}
        nullable_columns = {col_name: col.nullable for (col_name, col) in expanded_columns.items()}
        row_strategy = None
        if row_strategy_checks:
            row_strategy = st.fixed_dictionaries({col_name: make_row_strategy(col, row_strategy_checks) for (col_name, col) in expanded_columns.items()})
        strategy = pdst.data_frames(columns=[column.strategy_component() for column in expanded_columns.values()], rows=row_strategy, index=pdst.range_indexes(min_size=0 if size is None else size, max_size=size))
        string_columns = []
        for (col_name, col_dtype) in col_dtypes.items():
            if col_dtype in {'object', 'str'} or col_dtype.startswith('string'):
                string_columns.append(col_name)
        if string_columns:
            strategy = strategy.map(lambda df: df.assign(**{col_name: df[col_name].map(str) for col_name in string_columns}))
        strategy = strategy.map(lambda df: df if df.empty else df.astype(col_dtypes))
        if size is not None and size > 0 and any(nullable_columns.values()):
            strategy = null_dataframe_masks(strategy, nullable_columns)
        if index is not None:
            strategy = set_pandas_index(strategy, index)
        for check in undefined_strat_df_checks:
            strategy = undefined_check_strategy(strategy, check)
        for (col_name, column_checks) in undefined_strat_column_checks.items():
            for check in column_checks:
                strategy = undefined_check_strategy(strategy, check, column=col_name)
        return draw(strategy)
    return _dataframe_strategy()","for check in checks:
    if hasattr(check, 'strategy') or check.element_wise:
        row_strategy_checks.append(check)
    else:
        undefined_strat_df_checks.append(check)","undefined_strat_df_checks = [check for check in checks if not hasattr(check, 'strategy') and (not check.element_wise)]",Cannot refactor,-1,0,,2,1,robosuite
pandera,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandera/pandera/strategies.py,https://github.com/pandera-dev/pandera/tree/master/pandera/strategies.py,,dataframe_strategy$933,"def dataframe_strategy(pandera_dtype: Optional[DataType]=None, strategy: Optional[SearchStrategy]=None, *, columns: Optional[Dict]=None, checks: Optional[Sequence]=None, unique: Optional[List[str]]=None, index: Optional[IndexComponent]=None, size: Optional[int]=None, n_regex_columns: int=1):
    """"""Strategy to generate a pandas DataFrame.

    :param pandera_dtype: :class:`pandera.dtypes.DataType` instance.
    :param strategy: if specified, this will raise a BaseStrategyOnlyError,
        since it cannot be chained to a prior strategy.
    :param columns: a dictionary where keys are column names and values
        are :class:`~pandera.schema_components.Column` objects.
    :param checks: sequence of :class:`~pandera.checks.Check` s to constrain
        the values of the data at the dataframe level.
    :param unique: a list of column names that should be jointly unique.
    :param index: Index or MultiIndex schema component.
    :param size: number of elements in the Series.
    :param n_regex_columns: number of regex columns to generate.
    :returns: ``hypothesis`` strategy.
    """"""
    if n_regex_columns < 1:
        raise ValueError(f'`n_regex_columns` must be a positive integer, found: {n_regex_columns}')
    if strategy:
        raise BaseStrategyOnlyError('The dataframe strategy is a base strategy. You cannot specify the strategy argument to chain it to a parent strategy.')
    columns = {} if columns is None else columns
    checks = [] if checks is None else checks

    def undefined_check_strategy(strategy, check, column=None):
        """"""Strategy for checks with undefined strategies.""""""

        def _element_wise_check_fn(element):
            return check._check_fn(element)

        def _column_check_fn(dataframe):
            return check(dataframe[column]).check_passed

        def _dataframe_check_fn(dataframe):
            return check(dataframe).check_passed
        if check.element_wise:
            check_fn = _element_wise_check_fn
            warning_type = 'Element-wise'
        elif column is None:
            check_fn = _dataframe_check_fn
            warning_type = 'Dataframe'
        else:
            check_fn = _column_check_fn
            warning_type = 'Column'
        warnings.warn(f""{warning_type} check doesn't have a defined strategy. Falling back to filtering drawn values based on the check definition. This can considerably slow down data-generation."")
        return strategy.filter(check_fn)

    def make_row_strategy(col, checks):
        strategy = None
        for check in checks:
            if hasattr(check, 'strategy'):
                strategy = check.strategy(col.dtype, strategy)
            else:
                strategy = undefined_check_strategy(strategy=pandas_dtype_strategy(col.dtype) if strategy is None else strategy, check=check)
        if strategy is None:
            strategy = pandas_dtype_strategy(col.dtype)
        return strategy

    @composite
    def _dataframe_strategy(draw):
        row_strategy_checks = []
        undefined_strat_df_checks = []
        for check in checks:
            if hasattr(check, 'strategy') or check.element_wise:
                row_strategy_checks.append(check)
            else:
                undefined_strat_df_checks.append(check)
        expanded_columns = {}
        for (col_name, column) in columns.items():
            if unique and col_name in unique:
                column = deepcopy(column)
                column.unique = True
            if not column.regex:
                expanded_columns[col_name] = column
            else:
                regex_columns = draw(st.lists(st.from_regex(column.name, fullmatch=True), min_size=n_regex_columns, max_size=n_regex_columns, unique=True))
                for regex_col in regex_columns:
                    expanded_columns[regex_col] = deepcopy(column).set_name(regex_col)
        undefined_strat_column_checks: Dict[str, list] = defaultdict(list)
        for (col_name, column) in expanded_columns.items():
            undefined_strat_column_checks[col_name].extend((check for check in column.checks if not hasattr(check, 'strategy') and (not check.element_wise)))
        col_dtypes = {col_name: str(col.dtype) if pandera_dtype is None else str(pandera_dtype) for (col_name, col) in expanded_columns.items()}
        nullable_columns = {col_name: col.nullable for (col_name, col) in expanded_columns.items()}
        row_strategy = None
        if row_strategy_checks:
            row_strategy = st.fixed_dictionaries({col_name: make_row_strategy(col, row_strategy_checks) for (col_name, col) in expanded_columns.items()})
        strategy = pdst.data_frames(columns=[column.strategy_component() for column in expanded_columns.values()], rows=row_strategy, index=pdst.range_indexes(min_size=0 if size is None else size, max_size=size))
        string_columns = []
        for (col_name, col_dtype) in col_dtypes.items():
            if col_dtype in {'object', 'str'} or col_dtype.startswith('string'):
                string_columns.append(col_name)
        if string_columns:
            strategy = strategy.map(lambda df: df.assign(**{col_name: df[col_name].map(str) for col_name in string_columns}))
        strategy = strategy.map(lambda df: df if df.empty else df.astype(col_dtypes))
        if size is not None and size > 0 and any(nullable_columns.values()):
            strategy = null_dataframe_masks(strategy, nullable_columns)
        if index is not None:
            strategy = set_pandas_index(strategy, index)
        for check in undefined_strat_df_checks:
            strategy = undefined_check_strategy(strategy, check)
        for (col_name, column_checks) in undefined_strat_column_checks.items():
            for check in column_checks:
                strategy = undefined_check_strategy(strategy, check, column=col_name)
        return draw(strategy)
    return _dataframe_strategy()","for (col_name, col_dtype) in col_dtypes.items():
    if col_dtype in {'object', 'str'} or col_dtype.startswith('string'):
        string_columns.append(col_name)","string_columns = [col_name for (col_name, col_dtype) in col_dtypes.items() if col_dtype in {'object', 'str'} or col_dtype.startswith('string')]","string_columns = [col_name for (col_name, col_dtype) in col_dtypes.items() if col_dtype in {'object', 'str'} or col_dtype.startswith('string')]",1,,,,,robosuite
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_webui/test_translation.py,https://github.com/freeipa/freeipa/tree/master/ipatests/test_webui/test_translation.py,ConfigPageBase,innerhtml_noscript$94,"def innerhtml_noscript(self, id, raw_page):
    """"""
        Extract html text from the given raw source of the page under the
        'noscript' html tag with the given id
        """"""
    html_tree = html.fromstring(raw_page)
    noscript_tree = html_tree.xpath(""//div[@id='{}']/noscript/*"".format(id))
    noscript_html_text = ''.join([html.tostring(elem, encoding='unicode') for elem in noscript_tree])
    noscript_html = []
    for html_row in noscript_html_text.split('\n'):
        noscript_html.append(sub('^[ ]+(?=(<|[ ]*$))', '', html_row))
    return noscript_html","for html_row in noscript_html_text.split('\n'):
    noscript_html.append(sub('^[ ]+(?=(<|[ ]*$))', '', html_row))","noscript_html += [sub('^[ ]+(?=(<|[ ]*$))', '', html_row) for html_row in noscript_html_text.split('\n')]","noscript_html = [sub('^[ ]+(?=(<|[ ]*$))', '', html_row) for html_row in noscript_html_text.split('\n')]",0,1,,,,robosuite
espresso,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/tests/test_dictionary.py,https://github.com/freewym/espresso/tree/master/tests/test_dictionary.py,TestDictionary,test_finalize$18,"def test_finalize(self):
    txt = ['A B C D', 'B C D', 'C D', 'D']
    ref_ids1 = list(map(torch.IntTensor, [[4, 5, 6, 7, 2], [5, 6, 7, 2], [6, 7, 2], [7, 2]]))
    ref_ids2 = list(map(torch.IntTensor, [[7, 6, 5, 4, 2], [6, 5, 4, 2], [5, 4, 2], [4, 2]]))
    d = Dictionary()
    for line in txt:
        d.encode_line(line, add_if_not_exist=True)

    def get_ids(dictionary):
        ids = []
        for line in txt:
            ids.append(dictionary.encode_line(line, add_if_not_exist=False))
        return ids

    def assertMatch(ids, ref_ids):
        for (toks, ref_toks) in zip(ids, ref_ids):
            self.assertEqual(toks.size(), ref_toks.size())
            self.assertEqual(0, (toks != ref_toks).sum().item())
    ids = get_ids(d)
    assertMatch(ids, ref_ids1)
    d.finalize()
    finalized_ids = get_ids(d)
    assertMatch(finalized_ids, ref_ids2)
    with tempfile.NamedTemporaryFile(mode='w') as tmp_dict:
        d.save(tmp_dict.name)
        d = Dictionary.load(tmp_dict.name)
        reload_ids = get_ids(d)
        assertMatch(reload_ids, ref_ids2)
        assertMatch(finalized_ids, reload_ids)","for line in txt:
    ids.append(dictionary.encode_line(line, add_if_not_exist=False))","ids = [dictionary.encode_line(line, add_if_not_exist=False) for line in txt]","ids = [dictionary.encode_line(line, add_if_not_exist=False) for line in txt]",1,,,,,robosuite
urh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/urh/src/urh/awre/Preprocessor.py,https://github.com/jopohl/urh/tree/master/src/urh/awre/Preprocessor.py,Preprocessor,determine_sync_candidates$121,"def determine_sync_candidates(self, raw_preamble_positions: np.ndarray, difference_matrix: np.ndarray, n_gram_length=4) -> list:
    possible_sync_words = awre_util.find_possible_sync_words(difference_matrix, raw_preamble_positions, self.bitvectors, n_gram_length)
    self.__debug('Possible sync words', possible_sync_words)
    if len(possible_sync_words) == 0:
        return []
    possible_sync_words = self.merge_possible_sync_words(possible_sync_words, n_gram_length)
    self.__debug('Merged sync words', possible_sync_words)
    scores = self.__score_sync_lengths(possible_sync_words)
    sorted_scores = sorted(scores, reverse=True, key=scores.get)
    estimated_sync_length = sorted_scores[0]
    if estimated_sync_length % 8 != 0:
        for other in filter(lambda x: 0 < estimated_sync_length - x < 7, sorted_scores):
            if other % 8 == 0:
                estimated_sync_length = other
                break
    sync_words = {word: frequency for (word, frequency) in possible_sync_words.items() if len(word) == estimated_sync_length}
    self.__debug('Sync words', sync_words)
    additional_syncs = self.__find_additional_sync_words(estimated_sync_length, sync_words, possible_sync_words)
    if additional_syncs:
        self.__debug('Found additional sync words', additional_syncs)
        sync_words.update(additional_syncs)
    result = []
    for sync_word in sorted(sync_words, key=sync_words.get, reverse=True):
        result.append(''.join((str(c) for c in sync_word)))
    return result","for sync_word in sorted(sync_words, key=sync_words.get, reverse=True):
    result.append(''.join((str(c) for c in sync_word)))","result = [''.join((str(c) for c in sync_word)) for sync_word in sorted(sync_words, key=sync_words.get, reverse=True)]","result = [''.join((str(c) for c in sync_word)) for sync_word in sorted(sync_words, key=sync_words.get, reverse=True)]",1,,,,,robosuite
cfn-lint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cfn-lint/src/cfnlint/rules/parameters/Used.py,https://github.com/aws-cloudformation/cfn-lint/tree/master/src/cfnlint/rules/parameters/Used.py,Used,match$35,"def match(self, cfn):
    matches = []
    reftrees = cfn.transform_pre.get('Ref')
    subtrees = cfn.transform_pre.get('Fn::Sub')
    refs = []
    for reftree in reftrees:
        refs.append(reftree[-1])
    subs = []
    for subtree in subtrees:
        if isinstance(subtree[-1], list):
            subs.extend(cfn.get_sub_parameters(subtree[-1][0]))
        elif isinstance(subtree[-1], str):
            subs.extend(cfn.get_sub_parameters(subtree[-1]))
    for (paramname, _) in cfn.get_parameters().items():
        if paramname not in refs:
            if paramname not in subs:
                message = 'Parameter {0} not used.'
                matches.append(RuleMatch(['Parameters', paramname], message.format(paramname)))
    return matches","for reftree in reftrees:
    refs.append(reftree[-1])",refs = [reftree[-1] for reftree in reftrees],refs = [reftree[-1] for reftree in reftrees],1,,,,,robosuite
cfn-lint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cfn-lint/src/cfnlint/rules/parameters/Used.py,https://github.com/aws-cloudformation/cfn-lint/tree/master/src/cfnlint/rules/parameters/Used.py,Used,match$35,"def match(self, cfn):
    matches = []
    reftrees = cfn.transform_pre.get('Ref')
    subtrees = cfn.transform_pre.get('Fn::Sub')
    refs = []
    for reftree in reftrees:
        refs.append(reftree[-1])
    subs = []
    for subtree in subtrees:
        if isinstance(subtree[-1], list):
            subs.extend(cfn.get_sub_parameters(subtree[-1][0]))
        elif isinstance(subtree[-1], str):
            subs.extend(cfn.get_sub_parameters(subtree[-1]))
    for (paramname, _) in cfn.get_parameters().items():
        if paramname not in refs:
            if paramname not in subs:
                message = 'Parameter {0} not used.'
                matches.append(RuleMatch(['Parameters', paramname], message.format(paramname)))
    return matches","for (paramname, _) in cfn.get_parameters().items():
    if paramname not in refs:
        if paramname not in subs:
            message = 'Parameter {0} not used.'
            matches.append(RuleMatch(['Parameters', paramname], message.format(paramname)))","matches = [RuleMatch(['Parameters', paramname], 'Parameter {0} not used.'.format(paramname)) for (paramname, _) in cfn.get_parameters().items() if paramname not in refs and paramname not in subs]",Cannot refactor,-1,1,,,,robosuite
Machine-Learning-in-Action,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Machine-Learning-in-Action/Ch03-DecisionTree/3.6.2-2.py,https://github.com/TeFuirnever/Machine-Learning-in-Action/tree/master/Ch03-DecisionTree/3.6.2-2.py,,if_main_my$12,"if __name__ == '__main__':
    with open('lenses.txt', 'r') as fr:
        lenses = [inst.strip().split('\t') for inst in fr.readlines()]
    lenses_target = []
    for each in lenses:
        lenses_target.append(each[-1])
    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']
    lenses_list = []
    lenses_dict = {}
    for each_label in lensesLabels:
        for each in lenses:
            lenses_list.append(each[lensesLabels.index(each_label)])
        lenses_dict[each_label] = lenses_list
        lenses_list = []
    lenses_pd = pd.DataFrame(lenses_dict)
    print(lenses_pd)
    le = LabelEncoder()
    for col in lenses_pd.columns:
        lenses_pd[col] = le.fit_transform(lenses_pd[col])
    print(lenses_pd)","for each in lenses:
    lenses_target.append(each[-1])",lenses_target = [each[-1] for each in lenses],lenses_target = [each[-1] for each in lenses],1,,,,,robosuite
Machine-Learning-in-Action,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Machine-Learning-in-Action/Ch03-DecisionTree/3.6.2-2.py,https://github.com/TeFuirnever/Machine-Learning-in-Action/tree/master/Ch03-DecisionTree/3.6.2-2.py,,if_main_my$12,"if __name__ == '__main__':
    with open('lenses.txt', 'r') as fr:
        lenses = [inst.strip().split('\t') for inst in fr.readlines()]
    lenses_target = []
    for each in lenses:
        lenses_target.append(each[-1])
    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']
    lenses_list = []
    lenses_dict = {}
    for each_label in lensesLabels:
        for each in lenses:
            lenses_list.append(each[lensesLabels.index(each_label)])
        lenses_dict[each_label] = lenses_list
        lenses_list = []
    lenses_pd = pd.DataFrame(lenses_dict)
    print(lenses_pd)
    le = LabelEncoder()
    for col in lenses_pd.columns:
        lenses_pd[col] = le.fit_transform(lenses_pd[col])
    print(lenses_pd)","for each in lenses:
    lenses_list.append(each[lensesLabels.index(each_label)])",lenses_list = [each[lensesLabels.index(each_label)] for each in lenses],lenses_list = [each[lensesLabels.index(each_label)] for each in lenses],1,,,,,robosuite
vint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vint/vint/_bundles/vimlparser.py,https://github.com/Vimjas/vint/tree/master/vint/_bundles/vimlparser.py,,viml_readfile$179,"def viml_readfile(path):
    lines = []
    f = open(path)
    for line in f.readlines():
        lines.append(line.rstrip('\r\n'))
    f.close()
    return lines","for line in f.readlines():
    lines.append(line.rstrip('\r\n'))",lines += [line.rstrip('\r\n') for line in f.readlines()],lines = [line.rstrip('\r\n') for line in f.readlines()],0,1,,,,robosuite
Deep-Learning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Learning/Tutorial/lesson-3/bp.py,https://github.com/Jack-Cherish/Deep-Learning/tree/master/Tutorial/lesson-3/bp.py,Layer,__init__$108,"def __init__(self, layer_index, node_count):
    """"""
        閸掓繂瀣瀵叉稉鐏
        layer_index: 鐏炲倻绱閸
        node_count: 鐏炲倹澧嶉崠鍛鎯堥惃鍕濡閻愰涢嚋閺
        """"""
    self.layer_index = layer_index
    self.nodes = []
    for i in range(node_count):
        self.nodes.append(Node(layer_index, i))
    self.nodes.append(ConstNode(layer_index, node_count))","for i in range(node_count):
    self.nodes.append(Node(layer_index, i))","self.nodes = [Node(layer_index, i) for i in range(node_count)]","self.nodes = [Node(layer_index, i) for i in range(node_count)]",1,,,,,robosuite
hivemind,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hivemind/tests/test_averaging.py,https://github.com/learning-at-home/hivemind/tree/master/tests/test_averaging.py,,_test_allreduce_once$58,"def _test_allreduce_once(n_clients, n_aux):
    n_peers = 4
    modes = [AveragingMode.CLIENT] * n_clients + [AveragingMode.AUX] * n_aux + [AveragingMode.NODE] * (n_peers - n_clients - n_aux)
    random.shuffle(modes)
    tensors1 = [torch.randn(123), torch.zeros(3)]
    tensors2 = [torch.rand(123), torch.ones(3)]
    tensors3 = [-torch.rand(123), torch.arange(3).to(torch.float32)]
    tensors4 = [torch.randn(123) ** 3, torch.arange(3).to(torch.float32) / 2]
    peer_tensors = [tensors1, tensors2, tensors3, tensors4]
    reference = [sum((tensors[i] for (tensors, mode) in zip(peer_tensors, modes) if mode != AveragingMode.AUX)) / max(1, n_peers - n_aux) for i in range(len(tensors1))]
    dht_instances = launch_dht_instances(len(peer_tensors))
    averagers = [hivemind.averaging.DecentralizedAverager(tensors, dht=dht, target_group_size=4, averaging_expiration=15, prefix='mygroup', client_mode=mode == AveragingMode.CLIENT, auxiliary=mode == AveragingMode.AUX, start=True) for (tensors, dht, mode) in zip(peer_tensors, dht_instances, modes)]
    futures = []
    for averager in averagers:
        futures.append(averager.step(wait=False))
    for future in futures:
        result = future.result()
        for averager in averagers:
            assert averager.peer_id in result
    for averager in averagers:
        if averager.mode != AveragingMode.AUX:
            with averager.get_tensors() as averaged_tensors:
                for (ref, our) in zip(reference, averaged_tensors):
                    assert torch.allclose(ref, our, atol=1e-06)
    for process in averagers + dht_instances:
        process.shutdown()","for averager in averagers:
    futures.append(averager.step(wait=False))",futures = [averager.step(wait=False) for averager in averagers],futures = [averager.step(wait=False) for averager in averagers],1,,,,,robosuite
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/lib/ansible_test/_util/controller/sanity/validate-modules/validate_modules/main.py,https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_util/controller/sanity/validate-modules/validate_modules/main.py,GitCache,_get_module_files$2395,"def _get_module_files():
    module_files = []
    for (dir_path, dir_names, file_names) in os.walk('lib/ansible/modules/'):
        for file_name in file_names:
            module_files.append(os.path.join(dir_path, file_name))
    return module_files","for (dir_path, dir_names, file_names) in os.walk('lib/ansible/modules/'):
    for file_name in file_names:
        module_files.append(os.path.join(dir_path, file_name))","module_files = [os.path.join(dir_path, file_name) for (dir_path, dir_names, file_names) in os.walk('lib/ansible/modules/') for file_name in file_names]","module_files = [os.path.join(dir_path, file_name) for (dir_path, dir_names, file_names) in os.walk('lib/ansible/modules/') for file_name in file_names]",1,,,,,robosuite
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,procToBp$18241,"def procToBp(args):
    """"""
			Generate WinDBG syntax to create a logging breakpoint on a given location
			""""""
    addy = 0
    addyerror = False
    executenow = False
    locsyntax = ''
    regsyntax = ''
    poisyntax = ''
    dmpsyntax = ''
    instructionparts = []
    global silent
    oldsilent = silent
    regs = dbg.getRegs()
    silent = True
    if 'a' in args:
        if type(args['a']).__name__.lower() != 'bool':
            (addy, addyok) = getAddyArg(args['a'])
            if not addyok:
                addyerror = True
    else:
        addy = regs['EIP']
    if 'e' in args:
        executenow = True
    if addyerror:
        dbg.log(' *** Please provide a valid address with argument -a ***', highlight=1)
        return
    bpdest = '0x%08x' % addy
    instruction = ''
    ptrx = MnPointer(addy)
    modname = ptrx.belongsTo()
    if not modname == '':
        mod = MnModule(modname)
        m = mod.moduleBase
        rva = addy - m
        bpdest = '%s+0x%02x' % (modname, rva)
        thisopcode = dbg.disasm(addy)
        instruction = getDisasmInstruction(thisopcode)
    locsyntax = 'bp %s' % bpdest
    instructionparts = multiSplit(instruction, [' ', ','])
    usedregs = []
    for reg in regs:
        for ipart in instructionparts:
            if reg.upper() in ipart.upper():
                usedregs.append(reg)
    if len(usedregs) > 0:
        regsyntax = '.printf \\""'
        argsyntax = ''
        for ipart in instructionparts:
            for reg in regs:
                if reg.upper() in ipart.upper():
                    if '[' in ipart:
                        regsyntax += ipart.replace('[', '').replace(']', '')
                        regsyntax += ': 0x%08x, '
                        argsyntax += '%s,' % ipart.replace('[', '').replace(']', '')
                        regsyntax += ipart
                        regsyntax += ': 0x%08x, '
                        argsyntax += '%s,' % ipart.replace('[', 'poi(').replace(']', ')')
                        iparttxt = ipart.replace('[', '').replace(']', '')
                        dmpsyntax += '.echo;.echo %s:;dds %s L 0x24/4;' % (iparttxt, iparttxt)
                    else:
                        regsyntax += ipart
                        regsyntax += ': 0x%08x, '
                        argsyntax += '%s,' % ipart
        argsyntax = argsyntax.strip(',')
        regsyntax = regsyntax.strip(', ')
        regsyntax += '\\"",%s;' % argsyntax
    if 'CALL' in instruction.upper():
        dmpsyntax += '.echo;.printf \\""Stack (esp: 0x%08x):\\"",esp;.echo;dds esp L 0x4;'
    if instruction.upper().startswith('RET'):
        dmpsyntax += '.echo;.printf \\""EAX: 0x%08x, Ret To: 0x%08x, Arg1: 0x%08x, Arg2: 0x%08x, Arg3: 0x%08x, Arg4: 0x%08x\\"",eax,poi(esp),poi(esp+4),poi(esp+8),poi(esp+c),poi(esp+10);'
    bpsyntax = locsyntax + ' "".echo ---------------;u eip L 1;' + regsyntax + dmpsyntax + '.echo;g' + '""'
    filename = 'logbps.txt'
    logfile = MnLog(filename)
    thislog = logfile.reset(False, False)
    with open(thislog, 'a') as fh:
        fh.write(bpsyntax + '\n')
    silent = oldsilent
    dbg.log('%s' % bpsyntax)
    dbg.log('Updated %s' % thislog)
    if executenow:
        dbg.nativeCommand(bpsyntax)
        dbg.log('> Breakpoint set at 0x%08x' % addy)
    return","for reg in regs:
    for ipart in instructionparts:
        if reg.upper() in ipart.upper():
            usedregs.append(reg)",usedregs = [reg for reg in regs for ipart in instructionparts if reg.upper() in ipart.upper()],usedregs = [reg for reg in regs for ipart in instructionparts if reg.upper() in ipart.upper()],1,,,,,robosuite
aws-deployment-framework,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-deployment-framework/src/lambda_codebase/initial_commit/bootstrap_repository/adf-build/shared/cdk/generate_pipeline_inputs.py,https://github.com/awslabs/aws-deployment-framework/tree/master/src/lambda_codebase/initial_commit/bootstrap_repository/adf-build/shared/cdk/generate_pipeline_inputs.py,,worker_thread$90,"def worker_thread(p, organizations, auto_create_repositories, deployment_map, parameter_store):
    LOGGER.debug('Worker Thread started for %s', p.get('name'))
    pipeline = Pipeline(p)
    if auto_create_repositories == 'enabled':
        code_account_id = p.get('default_providers', {}).get('source', {}).get('properties', {}).get('account_id', {})
        has_custom_repo = p.get('default_providers', {}).get('source', {}).get('properties', {}).get('repository', {})
        if auto_create_repositories and code_account_id and str(code_account_id).isdigit() and (not has_custom_repo):
            repo = Repo(code_account_id, p.get('name'), p.get('description'))
            repo.create_update()
    regions = []
    for target in p.get('targets', []):
        target_structure = TargetStructure(target)
        for step in target_structure.target:
            regions = step.get('regions', p.get('regions', DEPLOYMENT_ACCOUNT_REGION))
            paths_tags = []
            for path in step.get('path', []):
                paths_tags.append(path)
            if step.get('tags') is not None:
                paths_tags.append(step.get('tags', {}))
            for path_or_tag in paths_tags:
                pipeline.stage_regions.append(regions)
                pipeline_target = Target(path_or_tag, target_structure, organizations, step, regions)
                pipeline_target.fetch_accounts_for_target()
            pipeline.template_dictionary['targets'].append(target.target_structure.generate_waves())
    if DEPLOYMENT_ACCOUNT_REGION not in regions:
        pipeline.stage_regions.append(DEPLOYMENT_ACCOUNT_REGION)
    pipeline.generate_input()
    ssm_params = fetch_required_ssm_params(pipeline.input['regions'] or [DEPLOYMENT_ACCOUNT_REGION])
    deployment_map.update_deployment_parameters(pipeline)
    store_regional_parameter_config(pipeline, parameter_store)
    with open('cdk_inputs/{0}.json'.format(pipeline.input['name']), 'w') as outfile:
        data = {}
        data['input'] = pipeline.input
        data['input']['default_scm_branch'] = ssm_params.get('default_scm_branch')
        data['ssm_params'] = ssm_params
        json.dump(data, outfile)","for path in step.get('path', []):
    paths_tags.append(path)","paths_tags = [path for path in step.get('path', [])]","paths_tags = [path for path in step.get('path', [])]",1,,,,,robosuite
python-libnmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-libnmap/libnmap/objects/os.py,https://github.com/savon-noir/python-libnmap/tree/master/libnmap/objects/os.py,NmapOSMatch,get_cpe$134,"def get_cpe(self):
    """"""
        This method return a list of cpe stings and not CPE objects as
        the NmapOSClass.cpelist property. This method is a helper to
        simplify data management.

        For more advanced handling of CPE data, use NmapOSClass.cpelist
        and use the methods from CPE class
        """"""
    _cpelist = []
    for osc in self.osclasses:
        for cpe in osc.cpelist:
            _cpelist.append(cpe.cpestring)
    return _cpelist","for osc in self.osclasses:
    for cpe in osc.cpelist:
        _cpelist.append(cpe.cpestring)",_cpelist = [cpe.cpestring for osc in self.osclasses for cpe in osc.cpelist],_cpelist = [cpe.cpestring for osc in self.osclasses for cpe in osc.cpelist],1,,,,,robosuite
AlgorithmsAndDataStructure,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsAndDataStructure/Python/Algorithms/OperatingSystem/bankers_algorithm.py,https://github.com/codePerfectPlus/AlgorithmsAndDataStructure/tree/master/Python/Algorithms/OperatingSystem/bankers_algorithm.py,,isSafe$22,"def isSafe(P, R, processes, available_array, max_R, alloted):
    need = []
    for i in range(P):
        temp = []
        for j in range(R):
            temp.append(0)
        need.append(temp)
    calculateNeed(P, R, need, max_R, alloted)
    finish = [0] * P
    safeSeq = [0] * P
    work = [0] * R
    for i in range(R):
        work[i] = available_array[i]
    count = 0
    while count < P:
        found = False
        for p in range(P):
            if finish[p] == 0:
                for j in range(R):
                    if need[p][j] > work[j]:
                        break
                if j == R - 1:
                    for k in range(R):
                        work[k] += alloted[p][k]
                    safeSeq[count] = p
                    count += 1
                    finish[p] = 1
                    found = True
        if found is False:
            print('System not in safe state')
            return False
    print('System is in safe state')
    print('Safe Sequence is: ')
    print(*safeSeq)
    return True","for i in range(P):
    temp = []
    for j in range(R):
        temp.append(0)
    need.append(temp)",need = [[0 for j in range(R)] for i in range(P)],Cannot refactor,-1,1,,,,robosuite
AlgorithmsAndDataStructure,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsAndDataStructure/Python/Algorithms/OperatingSystem/bankers_algorithm.py,https://github.com/codePerfectPlus/AlgorithmsAndDataStructure/tree/master/Python/Algorithms/OperatingSystem/bankers_algorithm.py,,isSafe$22,"def isSafe(P, R, processes, available_array, max_R, alloted):
    need = []
    for i in range(P):
        temp = []
        for j in range(R):
            temp.append(0)
        need.append(temp)
    calculateNeed(P, R, need, max_R, alloted)
    finish = [0] * P
    safeSeq = [0] * P
    work = [0] * R
    for i in range(R):
        work[i] = available_array[i]
    count = 0
    while count < P:
        found = False
        for p in range(P):
            if finish[p] == 0:
                for j in range(R):
                    if need[p][j] > work[j]:
                        break
                if j == R - 1:
                    for k in range(R):
                        work[k] += alloted[p][k]
                    safeSeq[count] = p
                    count += 1
                    finish[p] = 1
                    found = True
        if found is False:
            print('System not in safe state')
            return False
    print('System is in safe state')
    print('Safe Sequence is: ')
    print(*safeSeq)
    return True","for j in range(R):
    temp.append(0)",temp = [0 for j in range(R)],temp = [0 for j in range(R)],1,,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/contrib/gsm_um.py,https://github.com/phaethon/kamene/tree/master/kamene/contrib/gsm_um.py,GroupChannelDescription,post_build$9878,"def post_build(self, p, pay):
    aList = []
    a = []
    i = 0
    for i in range(0, len(self.fields_desc)):
        aList.append(self.fields_desc[i].name)
    for i in aList:
        a.append(getattr(self, i))
    res = adapt(4, 13, a, self.fields_desc, 1)
    if self.lengthGCD is None:
        p = struct.pack('>B', res[1]) + p[1:]
    if res[0] is not 0:
        p = p[:-res[0]]
    return p + pay","for i in range(0, len(self.fields_desc)):
    aList.append(self.fields_desc[i].name)","aList = [self.fields_desc[i].name for i in range(0, len(self.fields_desc))]","aList = [self.fields_desc[i].name for i in range(0, len(self.fields_desc))]",1,,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/contrib/gsm_um.py,https://github.com/phaethon/kamene/tree/master/kamene/contrib/gsm_um.py,GroupChannelDescription,post_build$9878,"def post_build(self, p, pay):
    aList = []
    a = []
    i = 0
    for i in range(0, len(self.fields_desc)):
        aList.append(self.fields_desc[i].name)
    for i in aList:
        a.append(getattr(self, i))
    res = adapt(4, 13, a, self.fields_desc, 1)
    if self.lengthGCD is None:
        p = struct.pack('>B', res[1]) + p[1:]
    if res[0] is not 0:
        p = p[:-res[0]]
    return p + pay","for i in aList:
    a.append(getattr(self, i))","a += [getattr(self, i) for i in aList]","a = [getattr(self, i) for i in aList]",0,1,,,,robosuite
NLP-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NLP-Tutorials/pytorch/utils.py,https://github.com/MorvanZhou/NLP-Tutorials/tree/master/pytorch/utils.py,,process_w2v_data$75,"def process_w2v_data(corpus, skip_window=2, method='skip_gram'):
    all_words = [sentence.split(' ') for sentence in corpus]
    all_words = np.array(list(itertools.chain(*all_words)))
    (vocab, v_count) = np.unique(all_words, return_counts=True)
    vocab = vocab[np.argsort(v_count)[::-1]]
    print('All vocabularies are sorted by frequency in decresing oreder')
    v2i = {v: i for (i, v) in enumerate(vocab)}
    i2v = {i: v for (v, i) in v2i.items()}
    pairs = []
    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]
    for c in corpus:
        words = c.split(' ')
        w_idx = [v2i[w] for w in words]
        if method == 'skip_gram':
            for i in range(len(w_idx)):
                for j in js:
                    if i + j < 0 or i + j >= len(w_idx):
                        continue
                    pairs.append((w_idx[i], w_idx[i + j]))
        elif method.lower() == 'cbow':
            for i in range(skip_window, len(w_idx) - skip_window):
                context = []
                for j in js:
                    context.append(w_idx[i + j])
                pairs.append(context + [w_idx[i]])
        else:
            raise ValueError
    pairs = np.array(pairs)
    print('5 expample pairs:\n', pairs[:5])
    if method.lower() == 'skip_gram':
        (x, y) = (pairs[:, 0], pairs[:, 1])
    elif method.lower() == 'cbow':
        (x, y) = (pairs[:, :-1], pairs[:, -1])
    else:
        raise ValueError
    return Dataset(x, y, v2i, i2v)","for i in range(len(w_idx)):
    for j in js:
        if i + j < 0 or i + j >= len(w_idx):
            continue
        pairs.append((w_idx[i], w_idx[i + j]))","pairs = [(w_idx[i], w_idx[i + j]) for i in range(len(w_idx)) for j in js if i + j >= 0 and i + j < len(w_idx)]",Cannot refactor,-1,1,,,,robosuite
NLP-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NLP-Tutorials/pytorch/utils.py,https://github.com/MorvanZhou/NLP-Tutorials/tree/master/pytorch/utils.py,,process_w2v_data$75,"def process_w2v_data(corpus, skip_window=2, method='skip_gram'):
    all_words = [sentence.split(' ') for sentence in corpus]
    all_words = np.array(list(itertools.chain(*all_words)))
    (vocab, v_count) = np.unique(all_words, return_counts=True)
    vocab = vocab[np.argsort(v_count)[::-1]]
    print('All vocabularies are sorted by frequency in decresing oreder')
    v2i = {v: i for (i, v) in enumerate(vocab)}
    i2v = {i: v for (v, i) in v2i.items()}
    pairs = []
    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]
    for c in corpus:
        words = c.split(' ')
        w_idx = [v2i[w] for w in words]
        if method == 'skip_gram':
            for i in range(len(w_idx)):
                for j in js:
                    if i + j < 0 or i + j >= len(w_idx):
                        continue
                    pairs.append((w_idx[i], w_idx[i + j]))
        elif method.lower() == 'cbow':
            for i in range(skip_window, len(w_idx) - skip_window):
                context = []
                for j in js:
                    context.append(w_idx[i + j])
                pairs.append(context + [w_idx[i]])
        else:
            raise ValueError
    pairs = np.array(pairs)
    print('5 expample pairs:\n', pairs[:5])
    if method.lower() == 'skip_gram':
        (x, y) = (pairs[:, 0], pairs[:, 1])
    elif method.lower() == 'cbow':
        (x, y) = (pairs[:, :-1], pairs[:, -1])
    else:
        raise ValueError
    return Dataset(x, y, v2i, i2v)","for i in range(skip_window, len(w_idx) - skip_window):
    context = []
    for j in js:
        context.append(w_idx[i + j])
    pairs.append(context + [w_idx[i]])","pairs += [[w_idx[i + j] for j in js] + [w_idx[i]] for i in range(skip_window, len(w_idx) - skip_window)]",Cannot refactor,-1,1,,,,robosuite
NLP-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NLP-Tutorials/pytorch/utils.py,https://github.com/MorvanZhou/NLP-Tutorials/tree/master/pytorch/utils.py,,process_w2v_data$75,"def process_w2v_data(corpus, skip_window=2, method='skip_gram'):
    all_words = [sentence.split(' ') for sentence in corpus]
    all_words = np.array(list(itertools.chain(*all_words)))
    (vocab, v_count) = np.unique(all_words, return_counts=True)
    vocab = vocab[np.argsort(v_count)[::-1]]
    print('All vocabularies are sorted by frequency in decresing oreder')
    v2i = {v: i for (i, v) in enumerate(vocab)}
    i2v = {i: v for (v, i) in v2i.items()}
    pairs = []
    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]
    for c in corpus:
        words = c.split(' ')
        w_idx = [v2i[w] for w in words]
        if method == 'skip_gram':
            for i in range(len(w_idx)):
                for j in js:
                    if i + j < 0 or i + j >= len(w_idx):
                        continue
                    pairs.append((w_idx[i], w_idx[i + j]))
        elif method.lower() == 'cbow':
            for i in range(skip_window, len(w_idx) - skip_window):
                context = []
                for j in js:
                    context.append(w_idx[i + j])
                pairs.append(context + [w_idx[i]])
        else:
            raise ValueError
    pairs = np.array(pairs)
    print('5 expample pairs:\n', pairs[:5])
    if method.lower() == 'skip_gram':
        (x, y) = (pairs[:, 0], pairs[:, 1])
    elif method.lower() == 'cbow':
        (x, y) = (pairs[:, :-1], pairs[:, -1])
    else:
        raise ValueError
    return Dataset(x, y, v2i, i2v)","for j in js:
    context.append(w_idx[i + j])",context = [w_idx[i + j] for j in js],context = [w_idx[i + j] for j in js],1,,,,,robosuite
skweak,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/skweak/skweak/aggregation.py,https://github.com/NorskRegnesentral/skweak/tree/master/skweak/aggregation.py,TextAggregatorMixin,_get_spans$304,"def _get_spans(self, agg_df: pandas.DataFrame, threshold=0.5) -> List[Tuple[int, int, str]]:
    """"""Takes as input a 2D dataframe of shape (nb_entries, nb_labels)
        assocating each token/span to the probability of an output label, and 
        returns a list of tuples (start, end, label).        
        """"""
    results = []
    for ((start, end), preds) in agg_df.to_dict(orient='index').items():
        for (label, prob) in preds.items():
            if prob > threshold:
                results.append((start, end, label))
    return results","for ((start, end), preds) in agg_df.to_dict(orient='index').items():
    for (label, prob) in preds.items():
        if prob > threshold:
            results.append((start, end, label))","results = [(start, end, label) for ((start, end), preds) in agg_df.to_dict(orient='index').items() for (label, prob) in preds.items() if prob > threshold]","results = [(start, end, label) for ((start, end), preds) in agg_df.to_dict(orient='index').items() for (label, prob) in preds.items() if prob > threshold]",1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/auto_parallel/reshard.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/auto_parallel/reshard.py,Resharder,compute_complete_shape$1196,"def compute_complete_shape(slice_shape, process_shape, dims_mapping):
    """"""compute the complete shape of the slice tensor  with its process mesh and dims mapping""""""
    complete_shape = []
    for (idx, item) in enumerate(slice_shape):
        if dims_mapping[idx] == -1:
            complete_shape.append(item)
        else:
            complete_shape.append(item * process_shape[dims_mapping[idx]])
    return complete_shape","for (idx, item) in enumerate(slice_shape):
    if dims_mapping[idx] == -1:
        complete_shape.append(item)
    else:
        complete_shape.append(item * process_shape[dims_mapping[idx]])","complete_shape = [item if dims_mapping[idx] == -1 else item * process_shape[dims_mapping[idx]] for (idx, item) in enumerate(slice_shape)]","complete_shape = [item if dims_mapping[idx] == -1 else item * process_shape[dims_mapping[idx]] for (idx, item) in enumerate(slice_shape)]",1,,,,,robosuite
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/tests/test_bugs.py,https://github.com/mysql/mysql-connector-python/tree/master/tests/test_bugs.py,BugOra21492815,test_set$4253,"def test_set(self):
    cur = self.cnx.cursor()
    cur.callproc(self.proc1)
    self.assertEqual((bytearray(b'1234'),), next(cur.stored_results()).fetchone())
    cur.callproc(self.proc2)
    exp = [[(bytearray(b'9876'),)], [(bytearray(b'abcd'),)]]
    results = []
    for result in cur.stored_results():
        results.append(result.fetchall())
    self.assertEqual(exp, results)
    cur.close()","for result in cur.stored_results():
    results.append(result.fetchall())",results += [result.fetchall() for result in cur.stored_results()],results = [result.fetchall() for result in cur.stored_results()],0,1,,,,robosuite
mmdetection-mini,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection-mini/mmdet/cv_core/runner/hooks/logger/text.py,https://github.com/hhaAndroid/mmdetection-mini/tree/master/mmdet/cv_core/runner/hooks/logger/text.py,TextLoggerHook,_log_info$60,"def _log_info(self, log_dict, runner):
    if runner.meta is not None and 'exp_name' in runner.meta:
        if self.every_n_iters(runner, self.interval_exp_name) or (self.by_epoch and self.end_of_epoch(runner)):
            exp_info = f""Exp name: {runner.meta['exp_name']}""
            runner.logger.info(exp_info)
    if log_dict['mode'] == 'train':
        if isinstance(log_dict['lr'], dict):
            lr_str = []
            for (k, val) in log_dict['lr'].items():
                lr_str.append(f'lr_{k}: {val:.3e}')
            lr_str = ' '.join(lr_str)
        else:
            lr_str = f""lr: {log_dict['lr']:.3e}""
        if self.by_epoch:
            log_str = f""Epoch [{log_dict['epoch']}][{log_dict['iter']}/{len(runner.data_loader)}]\t""
        else:
            log_str = f""Iter [{log_dict['iter']}/{runner.max_iters}]\t""
        log_str += f'{lr_str}, '
        if 'time' in log_dict.keys():
            self.time_sec_tot += log_dict['time'] * self.interval
            time_sec_avg = self.time_sec_tot / (runner.iter - self.start_iter + 1)
            eta_sec = time_sec_avg * (runner.max_iters - runner.iter - 1)
            eta_str = str(datetime.timedelta(seconds=int(eta_sec)))
            log_str += f'eta: {eta_str}, '
            log_str += f""time: {log_dict['time']:.3f}, data_time: {log_dict['data_time']:.3f}, ""
            if torch.cuda.is_available():
                log_str += f""memory: {log_dict['memory']}, ""
    elif self.by_epoch:
        log_str = f""Epoch({log_dict['mode']}) [{log_dict['epoch']}][{log_dict['iter']}]\t""
    else:
        log_str = f""Iter({log_dict['mode']}) [{log_dict['iter']}]\t""
    log_items = []
    for (name, val) in log_dict.items():
        if name in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']:
            continue
        if isinstance(val, float):
            val = f'{val:.4f}'
        log_items.append(f'{name}: {val}')
    log_str += ', '.join(log_items)
    runner.logger.info(log_str)","for (name, val) in log_dict.items():
    if name in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']:
        continue
    if isinstance(val, float):
        val = f'{val:.4f}'
    log_items.append(f'{name}: {val}')","log_items = [f'{name}: {val:.4f}' if isinstance(val, float) else f'{name}: {val}' for (name, val) in log_dict.items() if name not in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']]",Cannot refactor,-1,1,,,,robosuite
mmdetection-mini,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection-mini/mmdet/cv_core/runner/hooks/logger/text.py,https://github.com/hhaAndroid/mmdetection-mini/tree/master/mmdet/cv_core/runner/hooks/logger/text.py,TextLoggerHook,_log_info$60,"def _log_info(self, log_dict, runner):
    if runner.meta is not None and 'exp_name' in runner.meta:
        if self.every_n_iters(runner, self.interval_exp_name) or (self.by_epoch and self.end_of_epoch(runner)):
            exp_info = f""Exp name: {runner.meta['exp_name']}""
            runner.logger.info(exp_info)
    if log_dict['mode'] == 'train':
        if isinstance(log_dict['lr'], dict):
            lr_str = []
            for (k, val) in log_dict['lr'].items():
                lr_str.append(f'lr_{k}: {val:.3e}')
            lr_str = ' '.join(lr_str)
        else:
            lr_str = f""lr: {log_dict['lr']:.3e}""
        if self.by_epoch:
            log_str = f""Epoch [{log_dict['epoch']}][{log_dict['iter']}/{len(runner.data_loader)}]\t""
        else:
            log_str = f""Iter [{log_dict['iter']}/{runner.max_iters}]\t""
        log_str += f'{lr_str}, '
        if 'time' in log_dict.keys():
            self.time_sec_tot += log_dict['time'] * self.interval
            time_sec_avg = self.time_sec_tot / (runner.iter - self.start_iter + 1)
            eta_sec = time_sec_avg * (runner.max_iters - runner.iter - 1)
            eta_str = str(datetime.timedelta(seconds=int(eta_sec)))
            log_str += f'eta: {eta_str}, '
            log_str += f""time: {log_dict['time']:.3f}, data_time: {log_dict['data_time']:.3f}, ""
            if torch.cuda.is_available():
                log_str += f""memory: {log_dict['memory']}, ""
    elif self.by_epoch:
        log_str = f""Epoch({log_dict['mode']}) [{log_dict['epoch']}][{log_dict['iter']}]\t""
    else:
        log_str = f""Iter({log_dict['mode']}) [{log_dict['iter']}]\t""
    log_items = []
    for (name, val) in log_dict.items():
        if name in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']:
            continue
        if isinstance(val, float):
            val = f'{val:.4f}'
        log_items.append(f'{name}: {val}')
    log_str += ', '.join(log_items)
    runner.logger.info(log_str)","for (k, val) in log_dict['lr'].items():
    lr_str.append(f'lr_{k}: {val:.3e}')","lr_str = [f'lr_{k}: {val:.3e}' for (k, val) in log_dict['lr'].items()]","lr_str = [f'lr_{k}: {val:.3e}' for (k, val) in log_dict['lr'].items()]",1,,,,,robosuite
napalm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/nxos/nxos.py,https://github.com/napalm-automation/napalm/tree/master/napalm/nxos/nxos.py,NXOSDriverBase,_get_merge_diff$164,"def _get_merge_diff(self):
    """"""
        The merge diff is not necessarily what needs to be loaded
        for example under NTP, even though the 'ntp commit' command might be
        alread configured, it is mandatory to be sent
        otherwise it won't take the new configuration - see:
        https://github.com/napalm-automation/napalm-nxos/issues/59
        therefore this method will return the real diff (but not necessarily what is
        being sent by the merge_load_config()
        """"""
    diff = []
    running_config = self.get_config(retrieve='running')['running']
    running_lines = running_config.splitlines()
    for line in self.merge_candidate.splitlines():
        if line not in running_lines and line:
            if line[0].strip() != '!':
                diff.append(line)
    return '\n'.join(diff)","for line in self.merge_candidate.splitlines():
    if line not in running_lines and line:
        if line[0].strip() != '!':
            diff.append(line)",diff = [line for line in self.merge_candidate.splitlines() if line not in running_lines and line and (line[0].strip() != '!')],diff = [line for line in self.merge_candidate.splitlines() if line not in running_lines and line if line[0].strip() != '!'],0,1,,,,robosuite
imgaug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/imgaug/augmenters/meta.py,https://github.com/aleju/imgaug/tree/master/imgaug/augmenters/meta.py,Augmenter,draw_grid$2113,"def draw_grid(self, images, rows, cols):
    """"""Augment images and draw the results as a single grid-like image.

        This method applies this augmenter to the provided images and returns
        a grid image of the results. Each cell in the grid contains a single
        augmented version of an input image.

        If multiple input images are provided, the row count is multiplied by
        the number of images and each image gets its own row.
        E.g. for ``images = [A, B]``, ``rows=2``, ``cols=3``::

            A A A
            B B B
            A A A
            B B B

        for ``images = [A]``, ``rows=2``, ``cols=3``::

            A A A
            A A A

        Parameters
        -------
        images : (N,H,W,3) ndarray or (H,W,3) ndarray or (H,W) ndarray or list of (H,W,3) ndarray or list of (H,W) ndarray
            List of images to augment and draw in the grid.
            If a list, then each element is expected to have shape ``(H, W)``
            or ``(H, W, 3)``. If a single array, then it is expected to have
            shape ``(N, H, W, 3)`` or ``(H, W, 3)`` or ``(H, W)``.

        rows : int
            Number of rows in the grid.
            If ``N`` input images are given, this value will automatically be
            multiplied by ``N`` to create rows for each image.

        cols : int
            Number of columns in the grid.

        Returns
        -------
        (Hg, Wg, 3) ndarray
            The generated grid image with augmented versions of the input
            images. Here, ``Hg`` and ``Wg`` reference the output size of the
            grid, and *not* the sizes of the input images.

        """"""
    if ia.is_np_array(images):
        if len(images.shape) == 4:
            images = [images[i] for i in range(images.shape[0])]
        elif len(images.shape) == 3:
            images = [images]
        elif len(images.shape) == 2:
            images = [images[:, :, np.newaxis]]
        else:
            raise Exception('Unexpected images shape, expected 2-, 3- or 4-dimensional array, got shape %s.' % (images.shape,))
    else:
        assert isinstance(images, list), ""Expected 'images' to be an ndarray or list of ndarrays. Got %s."" % (type(images),)
        for (i, image) in enumerate(images):
            if len(image.shape) == 3:
                continue
            if len(image.shape) == 2:
                images[i] = image[:, :, np.newaxis]
            else:
                raise Exception('Unexpected image shape at index %d, expected 2- or 3-dimensional array, got shape %s.' % (i, image.shape))
    det = self if self.deterministic else self.to_deterministic()
    augs = []
    for image in images:
        augs.append(det.augment_images([image] * (rows * cols)))
    augs_flat = list(itertools.chain(*augs))
    cell_height = max([image.shape[0] for image in augs_flat])
    cell_width = max([image.shape[1] for image in augs_flat])
    width = cell_width * cols
    height = cell_height * (rows * len(images))
    grid = np.zeros((height, width, 3), dtype=augs[0][0].dtype)
    for row_idx in range(rows):
        for (img_idx, image) in enumerate(images):
            for col_idx in range(cols):
                image_aug = augs[img_idx][row_idx * cols + col_idx]
                cell_y1 = cell_height * (row_idx * len(images) + img_idx)
                cell_y2 = cell_y1 + image_aug.shape[0]
                cell_x1 = cell_width * col_idx
                cell_x2 = cell_x1 + image_aug.shape[1]
                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image_aug
    return grid","for image in images:
    augs.append(det.augment_images([image] * (rows * cols)))",augs = [det.augment_images([image] * (rows * cols)) for image in images],augs = [det.augment_images([image] * (rows * cols)) for image in images],1,,,,,robosuite
coa_tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/functions.py,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/functions.py,,hide_base_sprite$258,"def hide_base_sprite(obj):
    context = bpy.context
    selected_object = bpy.data.objects[context.active_object.name]
    if 'coa_sprite' in obj and obj.type == 'MESH':
        orig_mode = obj.mode
        context.scene.objects.active = obj
        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.ops.object.mode_set(mode='EDIT')
        me = obj.data
        bm = bmesh.from_edit_mesh(me)
        bm.verts.ensure_lookup_table()
        vertex_idxs = []
        if 'coa_base_sprite' in obj.vertex_groups:
            v_group_idx = obj.vertex_groups['coa_base_sprite'].index
            for (i, vert) in enumerate(obj.data.vertices):
                for g in vert.groups:
                    if g.group == v_group_idx:
                        vertex_idxs.append(i)
        for idx in vertex_idxs:
            vert = bm.verts[idx]
            vert.hide = True
            vert.select = False
            for edge in vert.link_edges:
                edge.hide = True
                edge.select = False
            for face in vert.link_faces:
                face.hide = obj.data.coa_hide_base_sprite
                face.select = False
        if 'coa_base_sprite' in obj.modifiers:
            mod = obj.modifiers['coa_base_sprite']
            mod.show_viewport = obj.data.coa_hide_base_sprite
            mod.show_render = obj.data.coa_hide_base_sprite
        bmesh.update_edit_mesh(me)
        bpy.ops.object.mode_set(mode=orig_mode)
    context.scene.objects.active = selected_object","for (i, vert) in enumerate(obj.data.vertices):
    for g in vert.groups:
        if g.group == v_group_idx:
            vertex_idxs.append(i)","vertex_idxs = [i for (i, vert) in enumerate(obj.data.vertices) for g in vert.groups if g.group == v_group_idx]","vertex_idxs = [i for (i, vert) in enumerate(obj.data.vertices) for g in vert.groups if g.group == v_group_idx]",1,,,,,robosuite
youtube-dl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/iqiyi.py,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/iqiyi.py,IqiyiSDK,preprocess$50,"def preprocess(self, chunksize):
    self.target = md5_text(self.target)
    chunks = []
    for i in range(32 // chunksize):
        chunks.append(self.target[chunksize * i:chunksize * (i + 1)])
    if 32 % chunksize:
        chunks.append(self.target[32 - 32 % chunksize:])
    return (chunks, list(map(int, self.ip.split('.'))))","for i in range(32 // chunksize):
    chunks.append(self.target[chunksize * i:chunksize * (i + 1)])",chunks += [self.target[chunksize * i:chunksize * (i + 1)] for i in range(32 // chunksize)],chunks = [self.target[chunksize * i:chunksize * (i + 1)] for i in range(32 // chunksize)],0,1,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_fusion_repeated_fc_relu_op.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_fusion_repeated_fc_relu_op.py,TestFusionRepeatedFCReluOp,setUp$23,"def setUp(self):
    self.bs = 3
    self.ic = 9
    self.oc = [2, 4, 3]
    assert len(self.oc) > 1, 'Should larger than 1'
    self.set_conf()
    self.op_type = 'fusion_repeated_fc_relu'
    sz = len(self.oc)
    ics = [self.ic] + self.oc[0:sz - 1]
    assert len(ics) == len(self.oc)
    weights = []
    biases = []
    outs = []
    i = 0
    matrix = MatrixGenerate(self.bs, ics[i], self.oc[i], 1, 1)
    inp = np.reshape(matrix.input, [self.bs, ics[i]])
    weights.append(('W_{0}'.format(i), np.reshape(matrix.weights, [ics[i], self.oc[i]])))
    biases.append(('B_{0}'.format(i), matrix.bias))
    outs.append(np.reshape(np.maximum(fc_refer(matrix, True), 0), [self.bs, self.oc[i]]))
    for i in range(sz - 1):
        matrix = MatrixGenerate(self.bs, ics[i + 1], self.oc[i + 1], 1, 1)
        matrix.input = np.reshape(outs[i], [self.bs, ics[i + 1], 1, 1])
        out = fc_refer(matrix, True)
        weights.append(('W_{0}'.format(i + 1), np.reshape(matrix.weights, [ics[i + 1], self.oc[i + 1]])))
        biases.append(('B_{0}'.format(i + 1), matrix.bias))
        outs.append(np.reshape(np.maximum(out, 0), [self.bs, self.oc[i + 1]]))
    relu_outs = []
    for i in range(sz - 1):
        relu_outs.append(('ReluOut_{0}'.format(i), outs[i]))
    self.inputs = {'X': inp, 'W': weights, 'Bias': biases}
    self.outputs = {'Out': outs[-1], 'ReluOut': relu_outs}","for i in range(sz - 1):
    relu_outs.append(('ReluOut_{0}'.format(i), outs[i]))","relu_outs = [('ReluOut_{0}'.format(i), outs[i]) for i in range(sz - 1)]","relu_outs = [('ReluOut_{0}'.format(i), outs[i]) for i in range(sz - 1)]",1,,,,,robosuite
pytorch_face_landmark,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_face_landmark/vision/ssd/config/fd_config.py,https://github.com/cunjian/pytorch_face_landmark/tree/master/vision/ssd/config/fd_config.py,,define_img_size$18,"def define_img_size(size):
    global image_size, feature_map_w_h_list, priors
    img_size_dict = {128: [128, 96], 160: [160, 120], 320: [320, 240], 480: [480, 360], 640: [640, 480], 1280: [1280, 960]}
    image_size = img_size_dict[size]
    feature_map_w_h_list_dict = {128: [[16, 8, 4, 2], [12, 6, 3, 2]], 160: [[20, 10, 5, 3], [15, 8, 4, 2]], 320: [[40, 20, 10, 5], [30, 15, 8, 4]], 480: [[60, 30, 15, 8], [45, 23, 12, 6]], 640: [[80, 40, 20, 10], [60, 30, 15, 8]], 1280: [[160, 80, 40, 20], [120, 60, 30, 15]]}
    feature_map_w_h_list = feature_map_w_h_list_dict[size]
    for i in range(0, len(image_size)):
        item_list = []
        for k in range(0, len(feature_map_w_h_list[i])):
            item_list.append(image_size[i] / feature_map_w_h_list[i][k])
        shrinkage_list.append(item_list)
    priors = generate_priors(feature_map_w_h_list, shrinkage_list, image_size, min_boxes)","for k in range(0, len(feature_map_w_h_list[i])):
    item_list.append(image_size[i] / feature_map_w_h_list[i][k])","item_list = [image_size[i] / feature_map_w_h_list[i][k] for k in range(0, len(feature_map_w_h_list[i]))]","item_list = [image_size[i] / feature_map_w_h_list[i][k] for k in range(0, len(feature_map_w_h_list[i]))]",1,,,,,robosuite
graph-rcnn.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph-rcnn.pytorch/lib/data/samplers/grouped_batch_sampler.py,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/data/samplers/grouped_batch_sampler.py,GroupedBatchSampler,_prepare_batches$40,"def _prepare_batches(self):
    dataset_size = len(self.group_ids)
    sampled_ids = torch.as_tensor(list(self.sampler))
    order = torch.full((dataset_size,), -1, dtype=torch.int64)
    order[sampled_ids] = torch.arange(len(sampled_ids))
    mask = order >= 0
    clusters = [(self.group_ids == i) & mask for i in self.groups]
    relative_order = [order[cluster] for cluster in clusters]
    permutation_ids = [s[s.sort()[1]] for s in relative_order]
    permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]
    splits = [c.split(self.batch_size) for c in permuted_clusters]
    merged = tuple(itertools.chain.from_iterable(splits))
    first_element_of_batch = [t[0].item() for t in merged]
    inv_sampled_ids_map = {v: k for (k, v) in enumerate(sampled_ids.tolist())}
    first_index_of_batch = torch.as_tensor([inv_sampled_ids_map[s] for s in first_element_of_batch])
    permutation_order = first_index_of_batch.sort(0)[1].tolist()
    batches = [merged[i].tolist() for i in permutation_order]
    if self.drop_uneven:
        kept = []
        for batch in batches:
            if len(batch) == self.batch_size:
                kept.append(batch)
        batches = kept
    return batches","for batch in batches:
    if len(batch) == self.batch_size:
        kept.append(batch)",kept = [batch for batch in batches if len(batch) == self.batch_size],kept = [batch for batch in batches if len(batch) == self.batch_size],1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,,create_bf16_test_class$318,"def create_bf16_test_class(parent):

    @OpTestTool.skip_if_not_cpu_bf16()
    class TestMatMulV2Bf16OneDNNOp(parent):

        def set_inputs(self, x, y):
            self.inputs = {'X': convert_float_to_uint16(x), 'Y': convert_float_to_uint16(y)}
            self.x_fp32 = x
            self.y_fp32 = y

        def set_dtype_attr(self):
            self.attrs['mkldnn_data_type'] = 'bfloat16'

        def test_check_output(self):
            self.check_output_with_place(core.CPUPlace())

        def test_check_grad(self):
            self.calculate_grads()
            self.check_grad_with_place(core.CPUPlace(), ['X', 'Y'], 'Out', user_defined_grads=[self.dx, self.dy], user_defined_grad_outputs=[convert_float_to_uint16(self.dout)])

        def matmul_grad(self, x, transpose_x, y, transpose_y):
            x = np.transpose(x, self.shape_transpose_axes[x.ndim]) if transpose_x else x
            y = np.transpose(y, self.shape_transpose_axes[y.ndim]) if transpose_y else y
            return np.matmul(x, y)

        def calculate_grads(self):
            self.shape_transpose_axes = {2: [1, 0], 3: [0, 2, 1], 4: [0, 1, 3, 2], 5: [0, 1, 2, 4, 3], 6: [0, 1, 2, 3, 5, 4]}
            if self.x_fp32.ndim == 1:
                self.x_fp32 = np.expand_dims(self.x_fp32, axis=0)
            if self.y_fp32.ndim == 1:
                self.y_fp32 = np.expand_dims(self.y_fp32, axis=1)
            x_transpose_axes = self.shape_transpose_axes[self.x_fp32.ndim]
            y_transpose_axes = self.shape_transpose_axes[self.y_fp32.ndim]
            x = np.transpose(self.x_fp32, x_transpose_axes) if self.attrs['trans_x'] is True else self.x_fp32
            y = np.transpose(self.y_fp32, y_transpose_axes) if self.attrs['trans_y'] is True else self.y_fp32
            dout = np.matmul(x, y)
            x_shape = x.shape
            y_shape = y.shape
            if x.ndim <= 2 or y.ndim <= 2:
                is_broadcast = False
            elif x.ndim != y.ndim:
                is_broadcast = True
            else:
                is_broadcast = x.shape[0:-2] != y.shape[0:-2]
            if self.attrs['trans_x'] is True and self.attrs['trans_y'] is True:
                self.dx = self.matmul_grad(self.y_fp32, True, dout, True)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, True)
            elif self.attrs['trans_x'] is True and self.attrs['trans_y'] is False:
                self.dx = self.matmul_grad(self.y_fp32, False, dout, True)
                self.dy = self.matmul_grad(self.x_fp32, False, dout, False)
            elif self.attrs['trans_x'] is False and self.attrs['trans_y'] is True:
                self.dx = self.matmul_grad(dout, False, self.y_fp32, False)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, False)
            else:
                self.dx = self.matmul_grad(dout, False, self.y_fp32, True)
                self.dy = self.matmul_grad(self.x_fp32, True, dout, False)
            if is_broadcast:
                x_reduce_axis = []
                y_reduce_axis = []
                for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])):
                    if first != second:
                        x_reduce_axis.append(index)
                for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])):
                    if first != second:
                        y_reduce_axis.append(index)
                if x_reduce_axis:
                    self.dx = self.dx.sum(axis=tuple(x_reduce_axis), keepdims=True)
                if y_reduce_axis:
                    self.dy = self.dy.sum(axis=tuple(y_reduce_axis), keepdims=True)
            if len(x_shape) == 2 and x_shape[0] == 1:
                dout = dout.sum(axis=-2)
            if len(y_shape) == 2 and y_shape[1] == 1:
                dout = dout.sum(axis=-1)
            self.dout = dout
    cls_name = '{0}_{1}'.format(parent.__name__, 'BF16')
    TestMatMulV2Bf16OneDNNOp.__name__ = cls_name
    globals()[cls_name] = TestMatMulV2Bf16OneDNNOp","for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])):
    if first != second:
        x_reduce_axis.append(index)","x_reduce_axis = [index for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])) if first != second]","x_reduce_axis = [index for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])) if first != second]",1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,,create_bf16_test_class$318,"def create_bf16_test_class(parent):

    @OpTestTool.skip_if_not_cpu_bf16()
    class TestMatMulV2Bf16OneDNNOp(parent):

        def set_inputs(self, x, y):
            self.inputs = {'X': convert_float_to_uint16(x), 'Y': convert_float_to_uint16(y)}
            self.x_fp32 = x
            self.y_fp32 = y

        def set_dtype_attr(self):
            self.attrs['mkldnn_data_type'] = 'bfloat16'

        def test_check_output(self):
            self.check_output_with_place(core.CPUPlace())

        def test_check_grad(self):
            self.calculate_grads()
            self.check_grad_with_place(core.CPUPlace(), ['X', 'Y'], 'Out', user_defined_grads=[self.dx, self.dy], user_defined_grad_outputs=[convert_float_to_uint16(self.dout)])

        def matmul_grad(self, x, transpose_x, y, transpose_y):
            x = np.transpose(x, self.shape_transpose_axes[x.ndim]) if transpose_x else x
            y = np.transpose(y, self.shape_transpose_axes[y.ndim]) if transpose_y else y
            return np.matmul(x, y)

        def calculate_grads(self):
            self.shape_transpose_axes = {2: [1, 0], 3: [0, 2, 1], 4: [0, 1, 3, 2], 5: [0, 1, 2, 4, 3], 6: [0, 1, 2, 3, 5, 4]}
            if self.x_fp32.ndim == 1:
                self.x_fp32 = np.expand_dims(self.x_fp32, axis=0)
            if self.y_fp32.ndim == 1:
                self.y_fp32 = np.expand_dims(self.y_fp32, axis=1)
            x_transpose_axes = self.shape_transpose_axes[self.x_fp32.ndim]
            y_transpose_axes = self.shape_transpose_axes[self.y_fp32.ndim]
            x = np.transpose(self.x_fp32, x_transpose_axes) if self.attrs['trans_x'] is True else self.x_fp32
            y = np.transpose(self.y_fp32, y_transpose_axes) if self.attrs['trans_y'] is True else self.y_fp32
            dout = np.matmul(x, y)
            x_shape = x.shape
            y_shape = y.shape
            if x.ndim <= 2 or y.ndim <= 2:
                is_broadcast = False
            elif x.ndim != y.ndim:
                is_broadcast = True
            else:
                is_broadcast = x.shape[0:-2] != y.shape[0:-2]
            if self.attrs['trans_x'] is True and self.attrs['trans_y'] is True:
                self.dx = self.matmul_grad(self.y_fp32, True, dout, True)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, True)
            elif self.attrs['trans_x'] is True and self.attrs['trans_y'] is False:
                self.dx = self.matmul_grad(self.y_fp32, False, dout, True)
                self.dy = self.matmul_grad(self.x_fp32, False, dout, False)
            elif self.attrs['trans_x'] is False and self.attrs['trans_y'] is True:
                self.dx = self.matmul_grad(dout, False, self.y_fp32, False)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, False)
            else:
                self.dx = self.matmul_grad(dout, False, self.y_fp32, True)
                self.dy = self.matmul_grad(self.x_fp32, True, dout, False)
            if is_broadcast:
                x_reduce_axis = []
                y_reduce_axis = []
                for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])):
                    if first != second:
                        x_reduce_axis.append(index)
                for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])):
                    if first != second:
                        y_reduce_axis.append(index)
                if x_reduce_axis:
                    self.dx = self.dx.sum(axis=tuple(x_reduce_axis), keepdims=True)
                if y_reduce_axis:
                    self.dy = self.dy.sum(axis=tuple(y_reduce_axis), keepdims=True)
            if len(x_shape) == 2 and x_shape[0] == 1:
                dout = dout.sum(axis=-2)
            if len(y_shape) == 2 and y_shape[1] == 1:
                dout = dout.sum(axis=-1)
            self.dout = dout
    cls_name = '{0}_{1}'.format(parent.__name__, 'BF16')
    TestMatMulV2Bf16OneDNNOp.__name__ = cls_name
    globals()[cls_name] = TestMatMulV2Bf16OneDNNOp","for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])):
    if first != second:
        y_reduce_axis.append(index)","y_reduce_axis = [index for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])) if first != second]","y_reduce_axis = [index for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])) if first != second]",1,,,,,robosuite
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for child in ss(root, '.collinsToggle .ol li'):
    p = ss(child, 'p')
    if len(p) == 0:
        continue
    p = p[0]
    desc = ''
    cx = ''
    for node in p.children:
        if isinstance(node, str):
            desc += node
        if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
            cx = node.text
    desc = multi_space_to_single(desc.strip())
    examples = []
    for el in ss(child, '.exampleLists'):
        examp = []
        for p in ss(el, '.examples p'):
            examp.append(p.text.strip())
        examples.append(examp)
    word_struct['sentence'].append([desc, cx, examples])","word_struct['sentence'] += [[multi_space_to_single(node.strip()) for node in p.children if isinstance(node, str)] + [node.text for node in ss(p, 'span')] + [[p.text.strip() for p in ss(el, '.examples p')] for el in ss(child, '.exampleLists')] for child in ss(root, '.collinsToggle .ol li') if len(ss(child, 'p')) > 0]",Cannot refactor,-1,0,,,,robosuite
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for el in ss(child, '.exampleLists'):
    examp = []
    for p in ss(el, '.examples p'):
        examp.append(p.text.strip())
    examples.append(examp)","examples += [[p.text.strip() for p in ss(el, '.examples p')] for el in ss(child, '.exampleLists')]",Cannot refactor,-1,1,,,,robosuite
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for v in root.select('#bilingual ul li'):
    p = ss(v, 'p')
    ll = []
    for p in ss(v, 'p'):
        if len(p) == 0:
            continue
        if 'class' not in p.attrs:
            ll.append(p.text.strip())
    if len(ll) != 0:
        word_struct['sentence'].append(ll)","word_struct['sentence'] += [[p.text.strip() for p in ss(v, 'p') if len(p) != 0 and 'class' not in p.attrs] for v in root.select('#bilingual ul li')]",Cannot refactor,-1,1,,,,robosuite
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for li in ul.children:
    if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
        continue
    basic_desc.append(li.text.strip())","basic_desc = [li.text.strip() for li in ul.children if isinstance(li, bs4.Tag) and li.name.lower() == 'li']",Cannot refactor,-1,1,,,,robosuite
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for p in ss(v, 'p'):
    if len(p) == 0:
        continue
    if 'class' not in p.attrs:
        ll.append(p.text.strip())","ll = [p.text.strip() for p in ss(v, 'p') if len(p) != 0 and 'class' not in p.attrs]",Cannot refactor,-1,1,,,,robosuite
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/absorbing.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/absorbing.py,Interaction,sparse$376,"def sparse(self) -> sp.csc_matrix:
    """"""
        Construct a sparse interaction matrix

        Returns
        -------
        csc_matrix
            Dummy interaction constructed from the cartesian product of
            the categories and each of the continuous variables.

        Notes
        -----
        The number of columns in `dummy_interact` is

        .. math::

            ncont \\times \\prod_{i=1}^{ncat} |c_i|

        where :math:`|c_i|` is the number distinct categories in column i.
        """"""
    if self.cat.shape[1] and self.cont.shape[1]:
        out = []
        for col in self.cont:
            out.append(category_continuous_interaction(self.cat, self.cont[col], precondition=False))
        return sp.hstack(out, format='csc')
    elif self.cat.shape[1]:
        return category_interaction(category_product(self.cat), precondition=False)
    elif self.cont.shape[1]:
        return sp.csc_matrix(self._cont_data.ndarray)
    else:
        return sp.csc_matrix(empty((self._cat_data.shape[0], 0)))","for col in self.cont:
    out.append(category_continuous_interaction(self.cat, self.cont[col], precondition=False))","out = [category_continuous_interaction(self.cat, self.cont[col], precondition=False) for col in self.cont]","out = [category_continuous_interaction(self.cat, self.cont[col], precondition=False) for col in self.cont]",1,,,,,robosuite
Flat-Lattice-Transformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Flat-Lattice-Transformer/V0/add_lattice.py,https://github.com/LeeSureman/Flat-Lattice-Transformer/tree/master/V0/add_lattice.py,,normalize_char$228,"def normalize_char(inp):
    result = []
    for c in inp:
        if c.isdigit():
            result.append('0')
        else:
            result.append(c)
    return result","for c in inp:
    if c.isdigit():
        result.append('0')
    else:
        result.append(c)",result = ['0' if c.isdigit() else c for c in inp],result = ['0' if c.isdigit() else c for c in inp],1,,,,,robosuite
KBQA-BERT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KBQA-BERT/bert/modeling_test.py,https://github.com/WenRichard/KBQA-BERT/tree/master/bert/modeling_test.py,BertModelTest,ids_tensor$147,"def ids_tensor(cls, shape, vocab_size, rng=None, name=None):
    """"""Creates a random int32 tensor of the shape within the vocab size.""""""
    if rng is None:
        rng = random.Random()
    total_dims = 1
    for dim in shape:
        total_dims *= dim
    values = []
    for _ in range(total_dims):
        values.append(rng.randint(0, vocab_size - 1))
    return tf.constant(value=values, dtype=tf.int32, shape=shape, name=name)","for _ in range(total_dims):
    values.append(rng.randint(0, vocab_size - 1))","values = [rng.randint(0, vocab_size - 1) for _ in range(total_dims)]","values = [rng.randint(0, vocab_size - 1) for _ in range(total_dims)]",1,,,,,robosuite
python-weixin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-weixin/weixin/bind.py,https://github.com/gusibi/python-weixin/tree/master/weixin/bind.py,WeixinAPIMethod,_do_api_request$122,"def _do_api_request(self, url, method='GET', body=None, json_body=None, headers=None):
    headers = headers or {}
    if self.signature and self.api.app_secret is not None:
        secret = self.api.app_secret
        signature = hmac.new(secret, sha256).hexdigest()
        headers['X-Weixin-Forwarded-For'] = '|'.join([signature])
    response = OAuth2Request(self.api).make_request(url, method=method, body=body, json_body=json_body, headers=headers)
    status_code = response.status_code
    try:
        content_obj = simplejson.loads(response.content)
    except ValueError:
        raise WeixinClientError('Unable to parse response, not valid JSON.', status_code=status_code)
    api_responses = []
    if status_code == 200:
        if not self.objectify_response:
            return (content_obj, None)
        if self.response_type == 'list':
            for entry in content_obj['data']:
                if self.return_json:
                    api_responses.append(entry)
        elif self.response_type == 'entry':
            data = content_obj
            if self.return_json:
                api_responses = data
        elif self.response_type == 'empty':
            pass
        return (api_responses, self._build_pagination_info(content_obj))
    else:
        raise WeixinAPIError(status_code, content_obj['errcode'], content_obj['errmsg'])","for entry in content_obj['data']:
    if self.return_json:
        api_responses.append(entry)",api_responses = [entry for entry in content_obj['data'] if self.return_json],api_responses = [entry for entry in content_obj['data'] if self.return_json],1,,,,,robosuite
OnlineSchemaChange,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlineSchemaChange/core/lib/sqlparse/models.py,https://github.com/facebookincubator/OnlineSchemaChange/tree/master/core/lib/sqlparse/models.py,TableIndex,__str__$135,"def __str__(self):
    idx_str = []
    idx_str.append('NAME: {}'.format(self.name))
    idx_str.append('IS UNIQUE: {}'.format(self.is_unique))
    idx_str.append('TYPE: {}'.format(self.key_type))
    col_list_str = []
    for col_str in self.column_list:
        col_list_str.append(str(col_str))
    idx_str.append('KEY LIST: {}'.format(','.join(col_list_str)))
    if self.using:
        idx_str.append('USING: {}'.format(self.using))
    idx_str.append('KEY_BLOCK_SIZE: {}'.format(self.key_block_size))
    idx_str.append('COMMENT: {}'.format(self.comment))
    return '/ '.join(idx_str)","for col_str in self.column_list:
    col_list_str.append(str(col_str))",col_list_str = [str(col_str) for col_str in self.column_list],col_list_str = [str(col_str) for col_str in self.column_list],1,,,,,robosuite
mmaction2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmaction2/tools/analysis/analyze_logs.py,https://github.com/open-mmlab/mmaction2/tree/master/tools/analysis/analyze_logs.py,,plot_curve$34,"def plot_curve(log_dicts, args):
    if args.backend is not None:
        plt.switch_backend(args.backend)
    sns.set_style(args.style)
    legend = args.legend
    if legend is None:
        legend = []
        for json_log in args.json_logs:
            for metric in args.keys:
                legend.append(f'{json_log}_{metric}')
    assert len(legend) == len(args.json_logs) * len(args.keys)
    metrics = args.keys
    num_metrics = len(metrics)
    for (i, log_dict) in enumerate(log_dicts):
        epochs = list(log_dict.keys())
        for (j, metric) in enumerate(metrics):
            print(f'plot curve of {args.json_logs[i]}, metric is {metric}')
            if metric not in log_dict[epochs[0]]:
                raise KeyError(f'{args.json_logs[i]} does not contain metric {metric}')
            xs = []
            ys = []
            for epoch in epochs:
                iters = log_dict[epoch]['iter']
                if log_dict[epoch]['mode'][-1] == 'val':
                    iters = iters[:-1]
                num_iters_per_epoch = iters[-1]
                xs.append(np.array(iters) + (epoch - 1) * num_iters_per_epoch)
                ys.append(np.array(log_dict[epoch][metric][:len(iters)]))
            xs = np.concatenate(xs)
            ys = np.concatenate(ys)
            plt.xlabel('iter')
            plt.plot(xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)
            plt.legend()
        if args.title is not None:
            plt.title(args.title)
    if args.out is None:
        plt.show()
    else:
        print(f'save curve to: {args.out}')
        plt.savefig(args.out)
        plt.cla()","for json_log in args.json_logs:
    for metric in args.keys:
        legend.append(f'{json_log}_{metric}')",legend += [f'{json_log}_{metric}' for json_log in args.json_logs for metric in args.keys],legend = [f'{json_log}_{metric}' for json_log in args.json_logs for metric in args.keys],0,1,,,,robosuite
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/src/transformers/models/dpr/tokenization_dpr.py,https://github.com/huggingface/transformers/tree/master/src/transformers/models/dpr/tokenization_dpr.py,CustomDPRReaderTokenizerMixin,_get_best_spans$357,"def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:
    """"""
        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending
        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.
        """"""
    scores = []
    for (start_index, start_score) in enumerate(start_logits):
        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):
            scores.append(((start_index, start_index + answer_length), start_score + end_score))
    scores = sorted(scores, key=lambda x: x[1], reverse=True)
    chosen_span_intervals = []
    for ((start_index, end_index), score) in scores:
        if start_index > end_index:
            raise ValueError(f'Wrong span indices: [{start_index}:{end_index}]')
        length = end_index - start_index + 1
        if length > max_answer_length:
            raise ValueError(f'Span is too long: {length} > {max_answer_length}')
        if any([start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals]):
            continue
        chosen_span_intervals.append((start_index, end_index))
        if len(chosen_span_intervals) == top_spans:
            break
    return chosen_span_intervals","for (start_index, start_score) in enumerate(start_logits):
    for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):
        scores.append(((start_index, start_index + answer_length), start_score + end_score))","scores = [((start_index, start_index + answer_length), start_score + end_score) for (start_index, start_score) in enumerate(start_logits) for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length])]","scores = [((start_index, start_index + answer_length), start_score + end_score) for (start_index, start_score) in enumerate(start_logits) for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length])]",1,,,,,robosuite
detectron2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detectron2/detectron2/utils/visualizer.py,https://github.com/facebookresearch/detectron2/tree/master/detectron2/utils/visualizer.py,Visualizer,_convert_masks$1214,"def _convert_masks(self, masks_or_polygons):
    """"""
        Convert different format of masks or polygons to a tuple of masks and polygons.

        Returns:
            list[GenericMask]:
        """"""
    m = masks_or_polygons
    if isinstance(m, PolygonMasks):
        m = m.polygons
    if isinstance(m, BitMasks):
        m = m.tensor.numpy()
    if isinstance(m, torch.Tensor):
        m = m.numpy()
    ret = []
    for x in m:
        if isinstance(x, GenericMask):
            ret.append(x)
        else:
            ret.append(GenericMask(x, self.output.height, self.output.width))
    return ret","for x in m:
    if isinstance(x, GenericMask):
        ret.append(x)
    else:
        ret.append(GenericMask(x, self.output.height, self.output.width))","ret = [x if isinstance(x, GenericMask) else GenericMask(x, self.output.height, self.output.width) for x in m]","ret = [x if isinstance(x, GenericMask) else GenericMask(x, self.output.height, self.output.width) for x in m]",1,,,,,robosuite
ODIN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ODIN/lib/reporter.py,https://github.com/chrismaddalena/ODIN/tree/master/lib/reporter.py,Reporter,create_lookalike_table$917,"def create_lookalike_table(self, client, domain):
    """"""Record lookalike domains and the threat feed results for each domain.

        Parameters:
        client      The name of the target organization
        domain      The domain name to use for the typo-squatting checks
        """"""
    lookalike_results = self.lookalike_toolkit.run_domain_twister(domain)
    if lookalike_results:
        for result in lookalike_results:
            domain = result['domain']
            rank = result['rank']
            a_records = result['a_records']
            mx_records = result['mx_records']
            malicious = result['malicious']
            self.c.execute('INSERT INTO lookalike VALUES (?,?,?,?,?,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL)', (domain, str(rank), a_records, mx_records, malicious))
            self.conn.commit()
            tree = self.lookalike_toolkit.run_urlvoid_lookup(domain)
            count = ''
            engines = ''
            if tree is not None:
                try:
                    for child in tree:
                        malicious_check = child.tag
                        if malicious_check == 'detections':
                            detections = tree[1]
                            engines = detections[0]
                            count = ET.tostring(detections[1], method='text').rstrip().decode('ascii')
                            temp = []
                            for engine in engines:
                                temp.append(ET.tostring(engine, method='text').rstrip().decode('ascii'))
                            engines = ', '.join(temp)
                    rep_data = tree[0]
                    if len(rep_data) == 0:
                        pass
                    else:
                        target = ET.tostring(rep_data[0], method='text').rstrip().decode('ascii')
                        domain_age = ET.tostring(rep_data[3], method='text').rstrip().decode('ascii')
                        google_rank = ET.tostring(rep_data[4], method='text').rstrip().decode('ascii')
                        alexa_rank = ET.tostring(rep_data[5], method='text').rstrip().decode('ascii')
                        if rep_data[11]:
                            ip_data = rep_data[11]
                            ip_add = ET.tostring(ip_data[0], method='text').rstrip().decode('ascii')
                            hostnames = ET.tostring(ip_data[1], method='text').rstrip().decode('ascii')
                            asn = ET.tostring(ip_data[2], method='text').rstrip().decode('ascii')
                            asn_name = ET.tostring(ip_data[3], method='text').rstrip().decode('ascii')
                        else:
                            ip_add = None
                            hostnames = None
                            asn = None
                            asn_name = None
                        self.c.execute(""UPDATE lookalike\n                                            SET 'urlvoid_ip'=?,\n                                                'hostname'=?,\n                                                'domain_age'=?,\n                                                'google_rank'=?,\n                                                'alexa_rank'=?,\n                                                'asn'=?,\n                                                'asn_name'=?,\n                                                'urlvoid_hit'=?,\n                                                'urlvoid_engines'=?\n                                            WHERE domain = ?"", (ip_add, hostnames, domain_age, google_rank, alexa_rank, asn, asn_name, count, engines, target))
                        self.conn.commit()
                except:
                    pass","for engine in engines:
    temp.append(ET.tostring(engine, method='text').rstrip().decode('ascii'))","temp = [ET.tostring(engine, method='text').rstrip().decode('ascii') for engine in engines]","temp = [ET.tostring(engine, method='text').rstrip().decode('ascii') for engine in engines]",1,,,,,robosuite
fuck-coding-interviews,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuck-coding-interviews/problems/tests/test_combinations.py,https://github.com/vinta/fuck-coding-interviews/tree/master/problems/tests/test_combinations.py,TestCase,test2$26,"def test2(self):
    expected = [[1]]
    results = []
    for result in self.solution.combine(n=1, k=1):
        results.append(list(result))
    self.assertCountEqual(results, expected)","for result in self.solution.combine(n=1, k=1):
    results.append(list(result))","results = [list(result) for result in self.solution.combine(n=1, k=1)]","results = [list(result) for result in self.solution.combine(n=1, k=1)]",1,,,,,robosuite
rally,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/esrally/telemetry.py,https://github.com/elastic/rally/tree/master/esrally/telemetry.py,IndexStats,primary_shard_stats$2027,"def primary_shard_stats(self, stats, path):
    shard_stats = []
    try:
        for shards in stats['indices'].values():
            for shard in shards['shards'].values():
                for shard_metrics in shard:
                    if shard_metrics['routing']['primary']:
                        shard_stats.append(self.extract_value(shard_metrics, path, default_value=0))
    except KeyError:
        self.logger.warning('Could not determine primary shard stats at path [%s].', ','.join(path))
    return shard_stats","for shards in stats['indices'].values():
    for shard in shards['shards'].values():
        for shard_metrics in shard:
            if shard_metrics['routing']['primary']:
                shard_stats.append(self.extract_value(shard_metrics, path, default_value=0))","shard_stats = [self.extract_value(shard_metrics, path, default_value=0) for shards in stats['indices'].values() for shard in shards['shards'].values() for shard_metrics in shard if shard_metrics['routing']['primary']]","shard_stats = [self.extract_value(shard_metrics, path, default_value=0) for shards in stats['indices'].values() for shard in shards['shards'].values() for shard_metrics in shard if shard_metrics['routing']['primary']]",1,,,,,robosuite
AutoCrawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoCrawler/main.py,https://github.com/YoongiKim/AutoCrawler/tree/master//main.py,AutoCrawler,all_files$93,"def all_files(path):
    paths = []
    for (root, dirs, files) in os.walk(path):
        for file in files:
            if os.path.isfile(path + '/' + file):
                paths.append(path + '/' + file)
    return paths","for (root, dirs, files) in os.walk(path):
    for file in files:
        if os.path.isfile(path + '/' + file):
            paths.append(path + '/' + file)","paths = [os.path.join(root, file) for (root, dirs, files) in os.walk(path) for file in files if os.path.isfile(os.path.join(root, file))]","paths = [path + '/' + file for (root, dirs, files) in os.walk(path) for file in files if os.path.isfile(path + '/' + file)]",0,1,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/ext/tornado/httputil.py,https://github.com/saltstack/salt/tree/master/salt/ext/tornado/httputil.py,HTTPHeaders,__str__$244,"def __str__(self):
    lines = []
    for (name, value) in self.get_all():
        lines.append('%s: %s\n' % (name, value))
    return ''.join(lines)","for (name, value) in self.get_all():
    lines.append('%s: %s\n' % (name, value))","lines = ['%s: %s\n' % (name, value) for (name, value) in self.get_all()]","lines = ['%s: %s\n' % (name, value) for (name, value) in self.get_all()]",1,,,,,robosuite
mmdetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection/mmdet/models/dense_heads/autoassign_head.py,https://github.com/open-mmlab/mmdetection/tree/master/mmdet/models/dense_heads/autoassign_head.py,AutoAssignHead,loss$307,"def loss(self, cls_scores, bbox_preds, objectnesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None):
    """"""Compute loss of the head.

        Args:
            cls_scores (list[Tensor]): Box scores for each scale level,
                each is a 4D-tensor, the channel number is
                num_points * num_classes.
            bbox_preds (list[Tensor]): Box energies / deltas for each scale
                level, each is a 4D-tensor, the channel number is
                num_points * 4.
            objectnesses (list[Tensor]): objectness for each scale level, each
                is a 4D-tensor, the channel number is num_points * 1.
            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with
                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
            gt_labels (list[Tensor]): class indices corresponding to each box
            img_metas (list[dict]): Meta information of each image, e.g.,
                image size, scaling factor, etc.
            gt_bboxes_ignore (None | list[Tensor]): specify which bounding
                boxes can be ignored when computing the loss.

        Returns:
            dict[str, Tensor]: A dictionary of loss components.
        """"""
    assert len(cls_scores) == len(bbox_preds) == len(objectnesses)
    all_num_gt = sum([len(item) for item in gt_bboxes])
    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]
    all_level_points = self.prior_generator.grid_priors(featmap_sizes, dtype=bbox_preds[0].dtype, device=bbox_preds[0].device)
    (inside_gt_bbox_mask_list, bbox_targets_list) = self.get_targets(all_level_points, gt_bboxes)
    center_prior_weight_list = []
    temp_inside_gt_bbox_mask_list = []
    for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list):
        (center_prior_weight, inside_gt_bbox_mask) = self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)
        center_prior_weight_list.append(center_prior_weight)
        temp_inside_gt_bbox_mask_list.append(inside_gt_bbox_mask)
    inside_gt_bbox_mask_list = temp_inside_gt_bbox_mask_list
    mlvl_points = torch.cat(all_level_points, dim=0)
    bbox_preds = levels_to_images(bbox_preds)
    cls_scores = levels_to_images(cls_scores)
    objectnesses = levels_to_images(objectnesses)
    reg_loss_list = []
    ious_list = []
    num_points = len(mlvl_points)
    for (bbox_pred, encoded_targets, inside_gt_bbox_mask) in zip(bbox_preds, bbox_targets_list, inside_gt_bbox_mask_list):
        temp_num_gt = encoded_targets.size(1)
        expand_mlvl_points = mlvl_points[:, None, :].expand(num_points, temp_num_gt, 2).reshape(-1, 2)
        encoded_targets = encoded_targets.reshape(-1, 4)
        expand_bbox_pred = bbox_pred[:, None, :].expand(num_points, temp_num_gt, 4).reshape(-1, 4)
        decoded_bbox_preds = self.bbox_coder.decode(expand_mlvl_points, expand_bbox_pred)
        decoded_target_preds = self.bbox_coder.decode(expand_mlvl_points, encoded_targets)
        with torch.no_grad():
            ious = bbox_overlaps(decoded_bbox_preds, decoded_target_preds, is_aligned=True)
            ious = ious.reshape(num_points, temp_num_gt)
            if temp_num_gt:
                ious = ious.max(dim=-1, keepdim=True).values.repeat(1, temp_num_gt)
            else:
                ious = ious.new_zeros(num_points, temp_num_gt)
            ious[~inside_gt_bbox_mask] = 0
            ious_list.append(ious)
        loss_bbox = self.loss_bbox(decoded_bbox_preds, decoded_target_preds, weight=None, reduction_override='none')
        reg_loss_list.append(loss_bbox.reshape(num_points, temp_num_gt))
    cls_scores = [item.sigmoid() for item in cls_scores]
    objectnesses = [item.sigmoid() for item in objectnesses]
    (pos_loss_list,) = multi_apply(self.get_pos_loss_single, cls_scores, objectnesses, reg_loss_list, gt_labels, center_prior_weight_list)
    pos_avg_factor = reduce_mean(bbox_pred.new_tensor(all_num_gt)).clamp_(min=1)
    pos_loss = sum(pos_loss_list) / pos_avg_factor
    (neg_loss_list,) = multi_apply(self.get_neg_loss_single, cls_scores, objectnesses, gt_labels, ious_list, inside_gt_bbox_mask_list)
    neg_avg_factor = sum((item.data.sum() for item in center_prior_weight_list))
    neg_avg_factor = reduce_mean(neg_avg_factor).clamp_(min=1)
    neg_loss = sum(neg_loss_list) / neg_avg_factor
    center_loss = []
    for i in range(len(img_metas)):
        if inside_gt_bbox_mask_list[i].any():
            center_loss.append(len(gt_bboxes[i]) / center_prior_weight_list[i].sum().clamp_(min=EPS))
        else:
            center_loss.append(center_prior_weight_list[i].sum() * 0)
    center_loss = torch.stack(center_loss).mean() * self.center_loss_weight
    if all_num_gt == 0:
        pos_loss = bbox_preds[0].sum() * 0
        dummy_center_prior_loss = self.center_prior.mean.sum() * 0 + self.center_prior.sigma.sum() * 0
        center_loss = objectnesses[0].sum() * 0 + dummy_center_prior_loss
    loss = dict(loss_pos=pos_loss, loss_neg=neg_loss, loss_center=center_loss)
    return loss","for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list):
    (center_prior_weight, inside_gt_bbox_mask) = self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)
    center_prior_weight_list.append(center_prior_weight)
    temp_inside_gt_bbox_mask_list.append(inside_gt_bbox_mask)","temp_inside_gt_bbox_mask_list = [self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)[1] for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list)]",Cannot refactor,-1,0,,2,1,robosuite
mmdetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection/mmdet/models/dense_heads/autoassign_head.py,https://github.com/open-mmlab/mmdetection/tree/master/mmdet/models/dense_heads/autoassign_head.py,AutoAssignHead,loss$307,"def loss(self, cls_scores, bbox_preds, objectnesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None):
    """"""Compute loss of the head.

        Args:
            cls_scores (list[Tensor]): Box scores for each scale level,
                each is a 4D-tensor, the channel number is
                num_points * num_classes.
            bbox_preds (list[Tensor]): Box energies / deltas for each scale
                level, each is a 4D-tensor, the channel number is
                num_points * 4.
            objectnesses (list[Tensor]): objectness for each scale level, each
                is a 4D-tensor, the channel number is num_points * 1.
            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with
                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
            gt_labels (list[Tensor]): class indices corresponding to each box
            img_metas (list[dict]): Meta information of each image, e.g.,
                image size, scaling factor, etc.
            gt_bboxes_ignore (None | list[Tensor]): specify which bounding
                boxes can be ignored when computing the loss.

        Returns:
            dict[str, Tensor]: A dictionary of loss components.
        """"""
    assert len(cls_scores) == len(bbox_preds) == len(objectnesses)
    all_num_gt = sum([len(item) for item in gt_bboxes])
    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]
    all_level_points = self.prior_generator.grid_priors(featmap_sizes, dtype=bbox_preds[0].dtype, device=bbox_preds[0].device)
    (inside_gt_bbox_mask_list, bbox_targets_list) = self.get_targets(all_level_points, gt_bboxes)
    center_prior_weight_list = []
    temp_inside_gt_bbox_mask_list = []
    for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list):
        (center_prior_weight, inside_gt_bbox_mask) = self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)
        center_prior_weight_list.append(center_prior_weight)
        temp_inside_gt_bbox_mask_list.append(inside_gt_bbox_mask)
    inside_gt_bbox_mask_list = temp_inside_gt_bbox_mask_list
    mlvl_points = torch.cat(all_level_points, dim=0)
    bbox_preds = levels_to_images(bbox_preds)
    cls_scores = levels_to_images(cls_scores)
    objectnesses = levels_to_images(objectnesses)
    reg_loss_list = []
    ious_list = []
    num_points = len(mlvl_points)
    for (bbox_pred, encoded_targets, inside_gt_bbox_mask) in zip(bbox_preds, bbox_targets_list, inside_gt_bbox_mask_list):
        temp_num_gt = encoded_targets.size(1)
        expand_mlvl_points = mlvl_points[:, None, :].expand(num_points, temp_num_gt, 2).reshape(-1, 2)
        encoded_targets = encoded_targets.reshape(-1, 4)
        expand_bbox_pred = bbox_pred[:, None, :].expand(num_points, temp_num_gt, 4).reshape(-1, 4)
        decoded_bbox_preds = self.bbox_coder.decode(expand_mlvl_points, expand_bbox_pred)
        decoded_target_preds = self.bbox_coder.decode(expand_mlvl_points, encoded_targets)
        with torch.no_grad():
            ious = bbox_overlaps(decoded_bbox_preds, decoded_target_preds, is_aligned=True)
            ious = ious.reshape(num_points, temp_num_gt)
            if temp_num_gt:
                ious = ious.max(dim=-1, keepdim=True).values.repeat(1, temp_num_gt)
            else:
                ious = ious.new_zeros(num_points, temp_num_gt)
            ious[~inside_gt_bbox_mask] = 0
            ious_list.append(ious)
        loss_bbox = self.loss_bbox(decoded_bbox_preds, decoded_target_preds, weight=None, reduction_override='none')
        reg_loss_list.append(loss_bbox.reshape(num_points, temp_num_gt))
    cls_scores = [item.sigmoid() for item in cls_scores]
    objectnesses = [item.sigmoid() for item in objectnesses]
    (pos_loss_list,) = multi_apply(self.get_pos_loss_single, cls_scores, objectnesses, reg_loss_list, gt_labels, center_prior_weight_list)
    pos_avg_factor = reduce_mean(bbox_pred.new_tensor(all_num_gt)).clamp_(min=1)
    pos_loss = sum(pos_loss_list) / pos_avg_factor
    (neg_loss_list,) = multi_apply(self.get_neg_loss_single, cls_scores, objectnesses, gt_labels, ious_list, inside_gt_bbox_mask_list)
    neg_avg_factor = sum((item.data.sum() for item in center_prior_weight_list))
    neg_avg_factor = reduce_mean(neg_avg_factor).clamp_(min=1)
    neg_loss = sum(neg_loss_list) / neg_avg_factor
    center_loss = []
    for i in range(len(img_metas)):
        if inside_gt_bbox_mask_list[i].any():
            center_loss.append(len(gt_bboxes[i]) / center_prior_weight_list[i].sum().clamp_(min=EPS))
        else:
            center_loss.append(center_prior_weight_list[i].sum() * 0)
    center_loss = torch.stack(center_loss).mean() * self.center_loss_weight
    if all_num_gt == 0:
        pos_loss = bbox_preds[0].sum() * 0
        dummy_center_prior_loss = self.center_prior.mean.sum() * 0 + self.center_prior.sigma.sum() * 0
        center_loss = objectnesses[0].sum() * 0 + dummy_center_prior_loss
    loss = dict(loss_pos=pos_loss, loss_neg=neg_loss, loss_center=center_loss)
    return loss","for i in range(len(img_metas)):
    if inside_gt_bbox_mask_list[i].any():
        center_loss.append(len(gt_bboxes[i]) / center_prior_weight_list[i].sum().clamp_(min=EPS))
    else:
        center_loss.append(center_prior_weight_list[i].sum() * 0)",center_loss = [len(gt_bboxes[i]) / center_prior_weight_list[i].sum().clamp_(min=EPS) if inside_gt_bbox_mask_list[i].any() else center_prior_weight_list[i].sum() * 0 for i in range(len(img_metas))],center_loss = [len(gt_bboxes[i]) / center_prior_weight_list[i].sum().clamp_(min=EPS) if inside_gt_bbox_mask_list[i].any() else center_prior_weight_list[i].sum() * 0 for i in range(len(img_metas))],1,,,,,robosuite
cartography,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cartography/cartography/intel/aws/dynamodb.py,https://github.com/lyft/cartography/tree/master/cartography/intel/aws/dynamodb.py,,get_dynamodb_tables$17,"def get_dynamodb_tables(boto3_session: boto3.session.Session, region: str) -> List[Dict]:
    client = boto3_session.client('dynamodb', region_name=region)
    paginator = client.get_paginator('list_tables')
    dynamodb_tables = []
    for page in paginator.paginate():
        for table_name in page['TableNames']:
            dynamodb_tables.append(client.describe_table(TableName=table_name))
    return dynamodb_tables","for page in paginator.paginate():
    for table_name in page['TableNames']:
        dynamodb_tables.append(client.describe_table(TableName=table_name))",dynamodb_tables = [client.describe_table(TableName=table_name) for page in paginator.paginate() for table_name in page['TableNames']],dynamodb_tables = [client.describe_table(TableName=table_name) for page in paginator.paginate() for table_name in page['TableNames']],1,,,,,robosuite
mmpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmpose/mmpose/models/losses/multi_loss_factory.py,https://github.com/open-mmlab/mmpose/tree/master/mmpose/models/losses/multi_loss_factory.py,AELoss,singleTagLoss$81,"def singleTagLoss(self, pred_tag, joints):
    """"""Associative embedding loss for one image.

        Note:
            - heatmaps weight: W
            - heatmaps height: H
            - max_num_people: M
            - num_keypoints: K

        Args:
            pred_tag (torch.Tensor[KxHxW,1]): tag of output for one image.
            joints (torch.Tensor[M,K,2]): joints information for one image.
        """"""
    tags = []
    pull = 0
    for joints_per_person in joints:
        tmp = []
        for joint in joints_per_person:
            if joint[1] > 0:
                tmp.append(pred_tag[joint[0]])
        if len(tmp) == 0:
            continue
        tmp = torch.stack(tmp)
        tags.append(torch.mean(tmp, dim=0))
        pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp)) ** 2)
    num_tags = len(tags)
    if num_tags == 0:
        return (_make_input(torch.zeros(1).float(), device=pred_tag.device), _make_input(torch.zeros(1).float(), device=pred_tag.device))
    elif num_tags == 1:
        return (_make_input(torch.zeros(1).float(), device=pred_tag.device), pull)
    tags = torch.stack(tags)
    size = (num_tags, num_tags)
    A = tags.expand(*size)
    B = A.permute(1, 0)
    diff = A - B
    if self.loss_type == 'exp':
        diff = torch.pow(diff, 2)
        push = torch.exp(-diff)
        push = torch.sum(push) - num_tags
    elif self.loss_type == 'max':
        diff = 1 - torch.abs(diff)
        push = torch.clamp(diff, min=0).sum() - num_tags
    else:
        raise ValueError('Unknown ae loss type')
    push_loss = push / ((num_tags - 1) * num_tags) * 0.5
    pull_loss = pull / num_tags
    return (push_loss, pull_loss)","for joint in joints_per_person:
    if joint[1] > 0:
        tmp.append(pred_tag[joint[0]])",tmp = [pred_tag[joint[0]] for joint in joints_per_person if joint[1] > 0],tmp = [pred_tag[joint[0]] for joint in joints_per_person if joint[1] > 0],1,,,,,robosuite
corona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/corona/lib.py,https://github.com/geohot/corona/tree/master//lib.py,,translate$41,"def translate(x, protein=False):
    x = x.lower()
    aa = []
    for i in range(0, len(x) - 2, 3):
        aa.append(dec[x[i:i + 3]])
    aa = ''.join(aa)
    if protein:
        if aa[0] != 'M' or aa[-1] != '*':
            print('BAD PROTEIN')
            print(aa)
            return None
        aa = aa[:-1]
    return aa","for i in range(0, len(x) - 2, 3):
    aa.append(dec[x[i:i + 3]])","aa = [dec[x[i:i + 3]] for i in range(0, len(x) - 2, 3)]","aa = [dec[x[i:i + 3]] for i in range(0, len(x) - 2, 3)]",1,,,,,robosuite
dask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask/dask/dataframe/io/parquet/utils.py,https://github.com/dask/dask/tree/master/dask/dataframe/io/parquet/utils.py,,_analyze_paths$410,"def _analyze_paths(file_list, fs, root=False):
    """"""Consolidate list of file-paths into parquet relative paths

    Note: This function was mostly copied from dask/fastparquet to
    use in both `FastParquetEngine` and `ArrowEngine`.""""""

    def _join_path(*path):

        def _scrub(i, p):
            p = p.replace(fs.sep, '/')
            if p == '':
                return '.'
            if p[-1] == '/':
                p = p[:-1]
            if i > 0 and p[0] == '/':
                p = p[1:]
            return p
        abs_prefix = ''
        if path and path[0]:
            if path[0][0] == '/':
                abs_prefix = '/'
                path = list(path)
                path[0] = path[0][1:]
            elif fs.sep == '\\' and path[0][1:].startswith(':/'):
                abs_prefix = path[0][0:3]
                path = list(path)
                path[0] = path[0][3:]
        _scrubbed = []
        for (i, p) in enumerate(path):
            _scrubbed.extend(_scrub(i, p).split('/'))
        simpler = []
        for s in _scrubbed:
            if s == '.':
                pass
            elif s == '..':
                if simpler:
                    if simpler[-1] == '..':
                        simpler.append(s)
                    else:
                        simpler.pop()
                elif abs_prefix:
                    raise Exception('can not get parent of root')
                else:
                    simpler.append(s)
            else:
                simpler.append(s)
        if not simpler:
            if abs_prefix:
                joined = abs_prefix
            else:
                joined = '.'
        else:
            joined = abs_prefix + '/'.join(simpler)
        return joined
    path_parts_list = [_join_path(fn).split('/') for fn in file_list]
    if root is False:
        basepath = path_parts_list[0][:-1]
        for path_parts in path_parts_list:
            j = len(path_parts) - 1
            for (k, (base_part, path_part)) in enumerate(zip(basepath, path_parts)):
                if base_part != path_part:
                    j = k
                    break
            basepath = basepath[:j]
        l = len(basepath)
    else:
        basepath = _join_path(root).split('/')
        l = len(basepath)
        assert all((p[:l] == basepath for p in path_parts_list)), 'All paths must begin with the given root'
    out_list = []
    for path_parts in path_parts_list:
        out_list.append('/'.join(path_parts[l:]))
    return ('/'.join(basepath), out_list)","for path_parts in path_parts_list:
    out_list.append('/'.join(path_parts[l:]))",out_list = ['/'.join(path_parts[l:]) for path_parts in path_parts_list],out_list = ['/'.join(path_parts[l:]) for path_parts in path_parts_list],1,,,,,robosuite
demon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/demon/python/depthmotionnet/v2/losses.py,https://github.com/lmb-freiburg/demon/tree/master/python/depthmotionnet/v2/losses.py,,scale_invariant_gradient_loss$83,"def scale_invariant_gradient_loss(inp, gt, epsilon):
    """"""Computes the scale invariant gradient loss

    inp: Tensor
        Tensor with the scale invariant gradient images computed on the prediction

    gt: Tensor
        Tensor with the scale invariant gradient images computed on the ground truth

    epsilon: float
      epsilon value for avoiding division by zero
    """"""
    num_channels_inp = inp.get_shape().as_list()[1]
    num_channels_gt = gt.get_shape().as_list()[1]
    assert num_channels_inp % 2 == 0
    assert num_channels_inp == num_channels_gt
    tmp = []
    for i in range(num_channels_inp // 2):
        tmp.append(pointwise_l2_loss(inp[:, i * 2:i * 2 + 2, :, :], gt[:, i * 2:i * 2 + 2, :, :], epsilon))
    return tf.add_n(tmp)","for i in range(num_channels_inp // 2):
    tmp.append(pointwise_l2_loss(inp[:, i * 2:i * 2 + 2, :, :], gt[:, i * 2:i * 2 + 2, :, :], epsilon))","tmp = [pointwise_l2_loss(inp[:, i * 2:i * 2 + 2, :, :], gt[:, i * 2:i * 2 + 2, :, :], epsilon) for i in range(num_channels_inp // 2)]","tmp = [pointwise_l2_loss(inp[:, i * 2:i * 2 + 2, :, :], gt[:, i * 2:i * 2 + 2, :, :], epsilon) for i in range(num_channels_inp // 2)]",1,,,,,robosuite
trezor-firmware,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trezor-firmware/core/src/trezor/crypto/cashaddr.py,https://github.com/trezor/trezor-firmware/tree/master/core/src/trezor/crypto/cashaddr.py,,_calculate_checksum$53,"def _calculate_checksum(prefix: str, payload: list[int]) -> list[int]:
    poly = cashaddr_polymod(prefix_expand(prefix) + payload + [0, 0, 0, 0, 0, 0, 0, 0])
    out = []
    for i in range(8):
        out.append(poly >> 5 * (7 - i) & 31)
    return out","for i in range(8):
    out.append(poly >> 5 * (7 - i) & 31)",out = [poly >> 5 * (7 - i) & 31 for i in range(8)],out = [poly >> 5 * (7 - i) & 31 for i in range(8)],1,,,,,robosuite
keract,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keract/keract/keract.py,https://github.com/philipperemy/keract/tree/master/keract/keract.py,,_convert_1d_to_2d$19,"def _convert_1d_to_2d(num_units: int):
    divisors = []
    for i in range(1, num_units + 1):
        q = num_units / i
        if int(q) == q:
            divisors.append(i)
    divisors = list(reversed(divisors))
    pairs = []
    for d in divisors:
        for e in divisors[1:]:
            if d * e == num_units:
                pairs.append((d, e))
    if len(pairs) == 0:
        return (num_units, 1)
    close_to_square_id = int(np.argmin(np.sum(np.array(pairs), axis=1)))
    return pairs[close_to_square_id]","for i in range(1, num_units + 1):
    q = num_units / i
    if int(q) == q:
        divisors.append(i)","divisors = [i for i in range(1, num_units + 1) if num_units % i == 0]",Cannot refactor,-1,1,,,,robosuite
keract,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keract/keract/keract.py,https://github.com/philipperemy/keract/tree/master/keract/keract.py,,_convert_1d_to_2d$19,"def _convert_1d_to_2d(num_units: int):
    divisors = []
    for i in range(1, num_units + 1):
        q = num_units / i
        if int(q) == q:
            divisors.append(i)
    divisors = list(reversed(divisors))
    pairs = []
    for d in divisors:
        for e in divisors[1:]:
            if d * e == num_units:
                pairs.append((d, e))
    if len(pairs) == 0:
        return (num_units, 1)
    close_to_square_id = int(np.argmin(np.sum(np.array(pairs), axis=1)))
    return pairs[close_to_square_id]","for d in divisors:
    for e in divisors[1:]:
        if d * e == num_units:
            pairs.append((d, e))","pairs = [(d, e) for d in divisors for e in divisors[1:] if d * e == num_units]","pairs = [(d, e) for d in divisors for e in divisors[1:] if d * e == num_units]",1,,,,,robosuite
torchgan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchgan/torchgan/layers/denseblock.py,https://github.com/torchgan/torchgan/tree/master/torchgan/layers/denseblock.py,DenseBlock2d,__init__$328,"def __init__(self, depth, in_channels, growth_rate, block, kernel, stride=1, padding=0, batchnorm=True, nonlinearity=None):
    super(DenseBlock2d, self).__init__()
    nl = nn.LeakyReLU(0.2) if nonlinearity is None else nonlinearity
    model = []
    for i in range(depth):
        model.append(block(in_channels + i * growth_rate, growth_rate, kernel, stride, padding, batchnorm=batchnorm, nonlinearity=nl))
    self.model = nn.Sequential(*model)","for i in range(depth):
    model.append(block(in_channels + i * growth_rate, growth_rate, kernel, stride, padding, batchnorm=batchnorm, nonlinearity=nl))","model = [block(in_channels + i * growth_rate, growth_rate, kernel, stride, padding, batchnorm=batchnorm, nonlinearity=nl) for i in range(depth)]","model = [block(in_channels + i * growth_rate, growth_rate, kernel, stride, padding, batchnorm=batchnorm, nonlinearity=nl) for i in range(depth)]",1,,,,,robosuite
finetune,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/finetune/finetune/target_models/multiple_choice.py,https://github.com/IndicoDataSolutions/finetune/tree/master/finetune/target_models/multiple_choice.py,MultipleChoice,predict_proba$191,"def predict_proba(self, questions, answers, context=None, **kwargs):
    """"""
        Produces a probability distribution over classes for each example in X.


        :param question: List or array of text, shape [batch]
        :param answers: List or array of text, shape [batch, n_answers]
        :returns: list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.
        """"""
    answers = list_transpose(answers)
    zipped_data = self.input_pipeline.zip_list_to_dict(X=list(zip(questions, answers)), context=context)
    raw_probas = self._inference(zipped_data, predict_keys=[PredictMode.PROBAS], **kwargs)
    formatted_predictions = []
    for (probas, *answers_per_sample) in zip(raw_probas, *answers):
        formatted_predictions.append(dict(zip(answers_per_sample, probas)))
    return formatted_predictions","for (probas, *answers_per_sample) in zip(raw_probas, *answers):
    formatted_predictions.append(dict(zip(answers_per_sample, probas)))","formatted_predictions = [dict(zip(answers_per_sample, probas)) for (probas, *answers_per_sample) in zip(raw_probas, *answers)]","formatted_predictions = [dict(zip(answers_per_sample, probas)) for (probas, *answers_per_sample) in zip(raw_probas, *answers)]",1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/containers/docker_model.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/containers/docker_model.py,DockerContainer,ports$132,"def ports(self):
    rval = []
    try:
        port_mappings = self.inspect['NetworkSettings']['Ports']
    except KeyError:
        log.warning(""Failed to get ports for container %s from `docker inspect` output at ['NetworkSettings']['Ports']: %s: %s"", self.id, exc_info=True)
        return None
    for port_name in port_mappings:
        for binding in port_mappings[port_name]:
            rval.append(ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])))
    return rval","for port_name in port_mappings:
    for binding in port_mappings[port_name]:
        rval.append(ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])))","rval = [ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])) for port_name in port_mappings for binding in port_mappings[port_name]]","rval = [ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])) for port_name in port_mappings for binding in port_mappings[port_name]]",1,,,,,robosuite
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/baselines/marl_benchmark/metrics/basic_metrics.py,https://github.com/huawei-noah/SMARTS/tree/master/baselines/marl_benchmark/metrics/basic_metrics.py,BehaviorMetric,_compute_agility_index$55,"def _compute_agility_index(self, agent_speed_seq):
    average_speed = []
    for agent_speed in agent_speed_seq:
        mean_speed_list = []
        for speed_list in agent_speed.values():
            mean_speed_list.append(np.mean(speed_list))
        average_speed.append(np.mean(mean_speed_list))
    self.agility = np.mean(average_speed)","for agent_speed in agent_speed_seq:
    mean_speed_list = []
    for speed_list in agent_speed.values():
        mean_speed_list.append(np.mean(speed_list))
    average_speed.append(np.mean(mean_speed_list))",average_speed = [np.mean([np.mean(speed_list) for speed_list in agent_speed.values()]) for agent_speed in agent_speed_seq],Cannot refactor,-1,1,,,,robosuite
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/baselines/marl_benchmark/metrics/basic_metrics.py,https://github.com/huawei-noah/SMARTS/tree/master/baselines/marl_benchmark/metrics/basic_metrics.py,BehaviorMetric,_compute_agility_index$55,"def _compute_agility_index(self, agent_speed_seq):
    average_speed = []
    for agent_speed in agent_speed_seq:
        mean_speed_list = []
        for speed_list in agent_speed.values():
            mean_speed_list.append(np.mean(speed_list))
        average_speed.append(np.mean(mean_speed_list))
    self.agility = np.mean(average_speed)","for speed_list in agent_speed.values():
    mean_speed_list.append(np.mean(speed_list))",mean_speed_list = [np.mean(speed_list) for speed_list in agent_speed.values()],mean_speed_list = [np.mean(speed_list) for speed_list in agent_speed.values()],1,,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/master.py,https://github.com/saltstack/salt/tree/master/salt/master.py,AESFuncs,__verify_minion_publish$1268,"def __verify_minion_publish(self, clear_load):
    """"""
        Verify that the passed information authorized a minion to execute

        :param dict clear_load: A publication load from a minion

        :rtype: bool
        :return: A boolean indicating if the minion is allowed to publish the command in the load
        """"""
    if 'peer' not in self.opts:
        return False
    if not isinstance(self.opts['peer'], dict):
        return False
    if any((key not in clear_load for key in ('fun', 'arg', 'tgt', 'ret', 'tok', 'id'))):
        return False
    if clear_load['fun'].startswith('publish.'):
        return False
    if not self.__verify_minion(clear_load['id'], clear_load['tok']):
        log.warning('Minion id %s is not who it says it is and is attempting to issue a peer command', clear_load['id'])
        return False
    clear_load.pop('tok')
    perms = []
    for match in self.opts['peer']:
        if re.match(match, clear_load['id']):
            if isinstance(self.opts['peer'][match], list):
                perms.extend(self.opts['peer'][match])
    if ',' in clear_load['fun']:
        clear_load['fun'] = clear_load['fun'].split(',')
        arg_ = []
        for arg in clear_load['arg']:
            arg_.append(arg.split())
        clear_load['arg'] = arg_
    return self.ckminions.auth_check(perms, clear_load['fun'], clear_load['arg'], clear_load['tgt'], clear_load.get('tgt_type', 'glob'), publish_validate=True)","for arg in clear_load['arg']:
    arg_.append(arg.split())",arg_ = [arg.split() for arg in clear_load['arg']],arg_ = [arg.split() for arg in clear_load['arg']],1,,,,,robosuite
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size // args.number_of_molecules):
    for k in range(args.number_of_molecules):
        fingerprint_columns.append(f'fp_{j}_mol_{k}')",fingerprint_columns = [f'fp_{j}_mol_{k}' for j in range(total_fp_size // args.number_of_molecules) for k in range(args.number_of_molecules)],fingerprint_columns = [f'fp_{j}_mol_{k}' for j in range(total_fp_size // args.number_of_molecules) for k in range(args.number_of_molecules)],1,,,,,robosuite
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size // args.number_of_molecules):
    for i in range(len(args.checkpoint_paths)):
        for k in range(args.number_of_molecules):
            fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')",fingerprint_columns += [f'fp_{j}_mol_{k}_model_{i}' for j in range(total_fp_size // args.number_of_molecules) for i in range(len(args.checkpoint_paths)) for k in range(args.number_of_molecules)],Cannot refactor,-1,1,,,,robosuite
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size):
    fingerprint_columns.append(f'fp_{j}')",fingerprint_columns += [f'fp_{j}' for j in range(total_fp_size)],Cannot refactor,-1,1,,,,robosuite
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size):
    for i in range(len(args.checkpoint_paths)):
        fingerprint_columns.append(f'fp_{j}_model_{i}')",fingerprint_columns += [f'fp_{j}_model_{i}' for j in range(total_fp_size) for i in range(len(args.checkpoint_paths))],Cannot refactor,-1,1,,,,robosuite
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/torchie/utils/misc.py,https://github.com/poodarchu/Det3D/tree/master/det3d/torchie/utils/misc.py,,wrapped_func$161,"def wrapped_func(*args, **kwargs):
    requirements = [prerequisites] if isinstance(prerequisites, str) else prerequisites
    missing = []
    for item in requirements:
        if not checker(item):
            missing.append(item)
    if missing:
        print(msg_tmpl.format(', '.join(missing), func.__name__))
        raise RuntimeError('Prerequisites not meet.')
    else:
        return func(*args, **kwargs)","for item in requirements:
    if not checker(item):
        missing.append(item)",missing = [item for item in requirements if not checker(item)],missing = [item for item in requirements if not checker(item)],1,,,,,robosuite
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/api/serializers/models/processingissue.py,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/processingissue.py,ProcessingIssueSerializer,get_attrs$7,"def get_attrs(self, item_list, user):
    counts = {i.id: getattr(i, 'num_events', None) for i in item_list}
    missing_counts = []
    for (pk, events) in counts.items():
        if events is None:
            missing_counts.append(pk)
    if missing_counts:
        counts.update(dict(ProcessingIssue.objects.with_num_events().filter(pk__in=list(missing_counts)).values_list('id', 'num_events')))
    result = {}
    for item in item_list:
        result[item] = {'num_events': counts.get(item.id) or 0}
    return result","for (pk, events) in counts.items():
    if events is None:
        missing_counts.append(pk)","missing_counts = [pk for (pk, events) in counts.items() if events is None]","missing_counts = [pk for (pk, events) in counts.items() if events is None]",1,,,,,robosuite
Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Vuln_Cluster_Celery.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Vuln_Cluster_Celery.py,,get_functions_on_cluster$132,"def get_functions_on_cluster(all_functions, centroid_number, labels):
    ret_list = []
    for x in range(len(all_functions)):
        if labels[x] == centroid_number:
            ret_list.append(all_functions[x])
    return ret_list","for x in range(len(all_functions)):
    if labels[x] == centroid_number:
        ret_list.append(all_functions[x])",ret_list = [all_functions[x] for x in range(len(all_functions)) if labels[x] == centroid_number],ret_list = [all_functions[x] for x in range(len(all_functions)) if labels[x] == centroid_number],1,,,,,robosuite
bubbles,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bubbles/bubbles/backends/sql/ops.py,https://github.com/Stiivi/bubbles/tree/master/bubbles/backends/sql/ops.py,,_$435,"def _(ctx, master, details, joins):
    """"""Creates left inner master-detail join (star schema) where `master` is an
    iterator if the ""bigger"" table `details` are details. `joins` is a list of
    tuples `(master, detail)` where the master is index of master key and
    detail is index of detail key to be matched.

    If `inner` is `True` then inner join is performed. That means that only
    rows from master that have corresponding details are returned.

    .. warning::

        all detail iterators are consumed and result is held in memory. Do not
        use for large datasets.
    """"""
    if not details:
        raise ArgumentError('No details provided, nothing to join')
    if not joins:
        raise ArgumentError('No joins specified')
    if len(details) != len(joins):
        raise ArgumentError('For every detail there should be a join (%d:%d).' % (len(details), len(joins)))
    if not all((master.can_compose(detail) for detail in details)):
        raise RetryOperation(['rows', 'rows[]'], reason='Can not compose')
    out_fields = master.fields.clone()
    master_stmt = master.sql_statement().alias('master')
    selection = list(master_stmt.columns)
    joined = master_stmt
    i = 0
    for (detail, join) in zip(details, joins):
        alias = 'detail%s' % i
        det_stmt = detail.sql_statement().alias(alias)
        master_key = join['master']
        detail_key = join['detail']
        onclause = master_stmt.c[master_key] == det_stmt.c[detail_key]
        for (field, col) in zip(detail.fields, det_stmt.columns):
            if str(field) != str(detail_key):
                selection.append(col)
                out_fields.append(field.clone())
        joined = sql.expression.join(joined, det_stmt, onclause=onclause)
    aliased = []
    for (col, field) in zip(selection, out_fields):
        aliased.append(col.label(field.name))
    select = sql.expression.select(aliased, from_obj=joined, use_labels=True)
    return master.clone_statement(statement=select, fields=out_fields)","for (col, field) in zip(selection, out_fields):
    aliased.append(col.label(field.name))","aliased = [col.label(field.name) for (col, field) in zip(selection, out_fields)]","aliased = [col.label(field.name) for (col, field) in zip(selection, out_fields)]",1,,,,,robosuite
bubbles,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bubbles/bubbles/backends/sql/ops.py,https://github.com/Stiivi/bubbles/tree/master/bubbles/backends/sql/ops.py,,_$435,"def _(ctx, master, details, joins):
    """"""Creates left inner master-detail join (star schema) where `master` is an
    iterator if the ""bigger"" table `details` are details. `joins` is a list of
    tuples `(master, detail)` where the master is index of master key and
    detail is index of detail key to be matched.

    If `inner` is `True` then inner join is performed. That means that only
    rows from master that have corresponding details are returned.

    .. warning::

        all detail iterators are consumed and result is held in memory. Do not
        use for large datasets.
    """"""
    if not details:
        raise ArgumentError('No details provided, nothing to join')
    if not joins:
        raise ArgumentError('No joins specified')
    if len(details) != len(joins):
        raise ArgumentError('For every detail there should be a join (%d:%d).' % (len(details), len(joins)))
    if not all((master.can_compose(detail) for detail in details)):
        raise RetryOperation(['rows', 'rows[]'], reason='Can not compose')
    out_fields = master.fields.clone()
    master_stmt = master.sql_statement().alias('master')
    selection = list(master_stmt.columns)
    joined = master_stmt
    i = 0
    for (detail, join) in zip(details, joins):
        alias = 'detail%s' % i
        det_stmt = detail.sql_statement().alias(alias)
        master_key = join['master']
        detail_key = join['detail']
        onclause = master_stmt.c[master_key] == det_stmt.c[detail_key]
        for (field, col) in zip(detail.fields, det_stmt.columns):
            if str(field) != str(detail_key):
                selection.append(col)
                out_fields.append(field.clone())
        joined = sql.expression.join(joined, det_stmt, onclause=onclause)
    aliased = []
    for (col, field) in zip(selection, out_fields):
        aliased.append(col.label(field.name))
    select = sql.expression.select(aliased, from_obj=joined, use_labels=True)
    return master.clone_statement(statement=select, fields=out_fields)","for (field, col) in zip(detail.fields, det_stmt.columns):
    if str(field) != str(detail_key):
        selection.append(col)
        out_fields.append(field.clone())","out_fields = [field.clone() for (field, col) in zip(detail.fields, det_stmt.columns) if str(field) != str(detail_key)]",Cannot refactor,-1,0,,2,1,robosuite
Gradient-Free-Optimizers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Gradient-Free-Optimizers/gradient_free_optimizers/converter.py,https://github.com/SimonBlanke/Gradient-Free-Optimizers/tree/master/gradient_free_optimizers/converter.py,Converter,para2value$69,"def para2value(self, para: Optional[dict]) -> Optional[list]:
    value = []
    for para_name in self.para_names:
        value.append(para[para_name])
    return value","for para_name in self.para_names:
    value.append(para[para_name])",value = [para[para_name] for para_name in self.para_names],value = [para[para_name] for para_name in self.para_names],1,,,,,robosuite
oio-sds,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oio-sds/oio/api/ec.py,https://github.com/open-io/oio-sds/tree/master/oio/api/ec.py,ECStream,_decode_segments$289,"def _decode_segments(self, fragment_iterators):
    """"""
        Reads from fragments and yield full segments
        """"""
    queues = []
    for _j in range(len(fragment_iterators)):
        queues.append(LightQueue(1))

    def put_in_queue(fragment_iterator, queue):
        """"""
            Coroutine to read the fragments from the iterator
            """"""
        try:
            for fragment in fragment_iterator:
                queue.put(fragment)
        except GreenletExit:
            pass
        except ChunkReadTimeout as err:
            self.logger.error('%s (reqid=%s)', err, self.reqid)
        except Exception:
            self.logger.exception('Exception on reading (reqid=%s)', self.reqid)
        finally:
            queue.resize(2)
            queue.put(None)
            fragment_iterator.close()
    with ContextPool(len(fragment_iterators)) as pool:
        for (fragment_iterator, queue) in zip(fragment_iterators, queues):
            pool.spawn(put_in_queue, fragment_iterator, queue)
        while True:
            data = []
            for queue in queues:
                fragment = queue.get()
                data.append(fragment)
            if not all(data):
                break
            if self.perfdata is not None:
                ec_start = monotonic_time()
            try:
                segment = self.storage_method.driver.decode(data)
            except exceptions.ECError:
                self.logger.exception('ERROR decoding fragments (reqid=%s)', self.reqid)
                raise
            finally:
                if self.perfdata is not None:
                    ec_end = monotonic_time()
                    duration = ec_end - ec_start
                    rawx_pdata = self.perfdata.setdefault('rawx', dict())
                    rawx_pdata['ec.segments'] = rawx_pdata.get('ec.segments', 0) + 1
                    rawx_pdata['ec.total'] = rawx_pdata.get('ec.total', 0.0) + duration
                    if 'ec.firstsegment' not in rawx_pdata:
                        rawx_pdata['ec.firstsegment'] = duration
            yield segment","for _j in range(len(fragment_iterators)):
    queues.append(LightQueue(1))",queues = [LightQueue(1) for _j in range(len(fragment_iterators))],queues = [LightQueue(1) for _j in range(len(fragment_iterators))],1,,,,,robosuite
oio-sds,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oio-sds/oio/api/ec.py,https://github.com/open-io/oio-sds/tree/master/oio/api/ec.py,ECStream,_decode_segments$289,"def _decode_segments(self, fragment_iterators):
    """"""
        Reads from fragments and yield full segments
        """"""
    queues = []
    for _j in range(len(fragment_iterators)):
        queues.append(LightQueue(1))

    def put_in_queue(fragment_iterator, queue):
        """"""
            Coroutine to read the fragments from the iterator
            """"""
        try:
            for fragment in fragment_iterator:
                queue.put(fragment)
        except GreenletExit:
            pass
        except ChunkReadTimeout as err:
            self.logger.error('%s (reqid=%s)', err, self.reqid)
        except Exception:
            self.logger.exception('Exception on reading (reqid=%s)', self.reqid)
        finally:
            queue.resize(2)
            queue.put(None)
            fragment_iterator.close()
    with ContextPool(len(fragment_iterators)) as pool:
        for (fragment_iterator, queue) in zip(fragment_iterators, queues):
            pool.spawn(put_in_queue, fragment_iterator, queue)
        while True:
            data = []
            for queue in queues:
                fragment = queue.get()
                data.append(fragment)
            if not all(data):
                break
            if self.perfdata is not None:
                ec_start = monotonic_time()
            try:
                segment = self.storage_method.driver.decode(data)
            except exceptions.ECError:
                self.logger.exception('ERROR decoding fragments (reqid=%s)', self.reqid)
                raise
            finally:
                if self.perfdata is not None:
                    ec_end = monotonic_time()
                    duration = ec_end - ec_start
                    rawx_pdata = self.perfdata.setdefault('rawx', dict())
                    rawx_pdata['ec.segments'] = rawx_pdata.get('ec.segments', 0) + 1
                    rawx_pdata['ec.total'] = rawx_pdata.get('ec.total', 0.0) + duration
                    if 'ec.firstsegment' not in rawx_pdata:
                        rawx_pdata['ec.firstsegment'] = duration
            yield segment","for queue in queues:
    fragment = queue.get()
    data.append(fragment)",data = [queue.get() for queue in queues],Cannot refactor,-1,1,,,,robosuite
hfnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hfnet/hfnet/datasets/utils/pipeline.py,https://github.com/ethz-asl/hfnet/tree/master/hfnet/datasets/utils/pipeline.py,,photometric_augmentation$23,"def photometric_augmentation(data, **config):
    with tf.name_scope('photometric_augmentation'):
        primitives = parse_primitives(config['primitives'], photaug.augmentations)
        prim_configs = [config['params'].get(p, {}) for p in primitives]
        indices = tf.range(len(primitives))
        if config['random_order']:
            indices = tf.random_shuffle(indices)

        def step(i, image):
            fn_pairs = []
            for (j, (p, c)) in enumerate(zip(primitives, prim_configs)):
                fn_pairs.append((tf.equal(indices[i], j), lambda p=p, c=c: getattr(photaug, p)(image, **c)))
            image = tf.case(fn_pairs)
            return (i + 1, image)
        (_, image) = tf.while_loop(lambda i, image: tf.less(i, len(primitives)), step, [0, data['image']], parallel_iterations=1)
    return {**data, 'image': image}","for (j, (p, c)) in enumerate(zip(primitives, prim_configs)):
    fn_pairs.append((tf.equal(indices[i], j), lambda p=p, c=c: getattr(photaug, p)(image, **c)))","fn_pairs = [(tf.equal(indices[i], j), lambda p=p, c=c: getattr(photaug, p)(image, **c)) for (j, (p, c)) in enumerate(zip(primitives, prim_configs))]","fn_pairs = [(tf.equal(indices[i], j), lambda p=p, c=c: getattr(photaug, p)(image, **c)) for (j, (p, c)) in enumerate(zip(primitives, prim_configs))]",1,,,,,robosuite
qutebrowser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutebrowser/qutebrowser/misc/crashdialog.py,https://github.com/qutebrowser/qutebrowser/tree/master/qutebrowser/misc/crashdialog.py,,_get_environment_vars$83,"def _get_environment_vars():
    """"""Gather environment variables for the crash info.""""""
    masks = ('DESKTOP_SESSION', 'DE', 'QT_*', 'PYTHON*', 'LC_*', 'LANG', 'XDG_*', 'QUTE_*', 'PATH', 'XMODIFIERS', 'XIM_*', 'QTWEBENGINE_*')
    info = []
    for (key, value) in os.environ.items():
        for m in masks:
            if fnmatch.fnmatch(key, m):
                info.append('{} = {}'.format(key, value))
    return '\n'.join(sorted(info))","for (key, value) in os.environ.items():
    for m in masks:
        if fnmatch.fnmatch(key, m):
            info.append('{} = {}'.format(key, value))","info = ['{} = {}'.format(key, value) for (key, value) in os.environ.items() for m in masks if fnmatch.fnmatch(key, m)]","info = ['{} = {}'.format(key, value) for (key, value) in os.environ.items() for m in masks if fnmatch.fnmatch(key, m)]",1,,,,,robosuite
keras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras/keras/feature_column/sequence_feature_column.py,https://github.com/keras-team/keras/tree/master/keras/feature_column/sequence_feature_column.py,,_assert_all_equal_and_return$170,"def _assert_all_equal_and_return(tensors, name=None):
    """"""Asserts that all tensors are equal and returns the first one.""""""
    with backend.name_scope(name or 'assert_all_equal'):
        if len(tensors) == 1:
            return tensors[0]
        assert_equal_ops = []
        for t in tensors[1:]:
            assert_equal_ops.append(tf.compat.v1.assert_equal(tensors[0], t))
        with tf.control_dependencies(assert_equal_ops):
            return tf.identity(tensors[0])","for t in tensors[1:]:
    assert_equal_ops.append(tf.compat.v1.assert_equal(tensors[0], t))","assert_equal_ops = [tf.compat.v1.assert_equal(tensors[0], t) for t in tensors[1:]]","assert_equal_ops = [tf.compat.v1.assert_equal(tensors[0], t) for t in tensors[1:]]",1,,,,,robosuite
primerpython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/tools/graph_bobject.py,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/graph_bobject.py,GraphBobject,add_bounded_region$849,"def add_bounded_region(self, index=0, color=3, mat_modifier=None, z_shift=0):
    coords = deepcopy(self.functions_coords[index])
    if index == 0:
        lower_func = []
        for coord in coords:
            lower_func.append([coord[0], 0, 0])
    else:
        for coord in coords:
            if coord[1] == 0:
                coord[1] = 0.01
        lower_func = deepcopy(self.functions_coords[index - 1])
        for i in range(index - 1):
            for j in range(len(lower_func)):
                lower_func[j][1] += self.functions_coords[i][j][1]
        for i in range(len(coords)):
            coords[i][1] += lower_func[i][1]
    lower_func.reverse()
    coords += lower_func
    data = bpy.data.curves.new(name='function_curve_data', type='CURVE')
    data.dimensions = '2D'
    if isinstance(self.functions[index], list):
        data.splines.new('BEZIER')
        points = data.splines[0].bezier_points
        points.add(len(coords) - 1)
        for i in range(len(points)):
            point = points[i]
            point.handle_left_type = 'VECTOR'
            point.handle_right_type = 'VECTOR'
            (x, y, z) = coords[i]
            if i == 0 and mat_modifier != 'fade':
                z = -2 * CURVE_Z_OFFSET
            z += z_shift
            point.co = (x, y, z)
            if i > 0:
                point.handle_left = coords[i - 1]
            else:
                point.handle_left = coords[i]
            if i < len(points) - 1:
                point.handle_right = coords[i + 1]
            else:
                point.handle_right = coords[i]
    else:
        raise Warning('Not implemented')
        data.splines.new('NURBS')
        start_coord = [coords[0][0] - (coords[1][0] - coords[0][0]), coords[0][1] - (coords[1][1] - coords[0][1]), coords[0][2] - (coords[1][2] - coords[0][2])]
        coords.insert(0, start_coord)
        end_coord = start_coord = [coords[-1][0] + (coords[-2][0] - coords[-1][0]), coords[-1][1] + (coords[-2][1] - coords[-1][1]), coords[-1][2] + (coords[-2][2] - coords[-1][2])]
        coords.append(end_coord)
        points = data.splines[0].points
        points.add(len(coords) - 1)
        for i in range(len(points)):
            (x, y, z) = coords[i]
            if i == 0 and mat_modifier != 'fade':
                z = -2 * CURVE_Z_OFFSET
            z += z_shift
            points[i].co = (x, y, z, 1)
    cur = bpy.data.objects.new(name='bounded_region', object_data=data)
    if mat_modifier == None:
        mat_string = 'color' + str(color)
    elif mat_modifier == 'fade':
        mat_string = 'trans_color' + str(color)
    apply_material(cur, mat_string)
    cur_bobj = bobject.Bobject(objects=[cur], name='function_curve_container')
    self.add_subbobject(cur_bobj)
    self.regions_curves.append(cur_bobj)
    data.splines[0].use_cyclic_u = True
    if index < 0:
        index += len(self.regions_curves)
    if len(self.regions_curves) != index + 1:
        raise Warning('Function count and index are out of sync.')","for coord in coords:
    lower_func.append([coord[0], 0, 0])","lower_func = [[coord[0], 0, 0] for coord in coords]","lower_func = [[coord[0], 0, 0] for coord in coords]",1,,,,,robosuite
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/financial_statements.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/financial_statements.py,,get_filtered_list_for_consolidated_report$570,"def get_filtered_list_for_consolidated_report(filters, period_list):
    filtered_summary_list = []
    for period in period_list:
        if period == filters.get('company'):
            filtered_summary_list.append(period)
    return filtered_summary_list","for period in period_list:
    if period == filters.get('company'):
        filtered_summary_list.append(period)",filtered_summary_list = [period for period in period_list if period == filters.get('company')],filtered_summary_list = [period for period in period_list if period == filters.get('company')],1,,,,,robosuite
synapse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/synapse/tests/rest/admin/test_device.py,https://github.com/matrix-org/synapse/tree/master/tests/rest/admin/test_device.py,DeleteDevicesRestTestCase,test_delete_devices$511,"def test_delete_devices(self) -> None:
    """"""
        Tests that a remove of devices is successfully
        """"""
    number_devices = 5
    for _ in range(number_devices):
        self.login('user', 'pass')
    res = self.get_success(self.handler.get_devices_by_user(self.other_user))
    self.assertEqual(number_devices, len(res))
    device_ids = []
    for d in res:
        device_ids.append(str(d['device_id']))
    channel = self.make_request('POST', self.url, access_token=self.admin_user_tok, content={'devices': device_ids})
    self.assertEqual(200, channel.code, msg=channel.json_body)
    res = self.get_success(self.handler.get_devices_by_user(self.other_user))
    self.assertEqual(0, len(res))","for d in res:
    device_ids.append(str(d['device_id']))",device_ids = [str(d['device_id']) for d in res],device_ids = [str(d['device_id']) for d in res],1,,,,,robosuite
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers_next/md/people.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers_next/md/people.py,PersonDetail,parse_address_block$7,"def parse_address_block(self, block):
    state = 'address'
    values = {'address': [], 'phone': [], 'fax': []}
    for line in block.splitlines():
        line = line.strip()
        if not line:
            continue
        if line.startswith('Phone'):
            state = 'phone'
        elif line.startswith('Fax'):
            state = 'fax'
        values[state].append(line)
    phones = []
    for line in values['phone']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            phones.append(match)
    faxes = []
    for line in values['fax']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            faxes.append(match)
    return {'address': '; '.join(values['address']), 'phones': phones, 'faxes': faxes}","for line in block.splitlines():
    line = line.strip()
    if not line:
        continue
    if line.startswith('Phone'):
        state = 'phone'
    elif line.startswith('Fax'):
        state = 'fax'
    values[state].append(line)",values[state] = [line for line in block.splitlines() if line.strip() and (state := ('phone' if line.startswith('Phone') else 'fax' if line.startswith('Fax') else True))],Cannot refactor,-1,0,,,,robosuite
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers_next/md/people.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers_next/md/people.py,PersonDetail,parse_address_block$7,"def parse_address_block(self, block):
    state = 'address'
    values = {'address': [], 'phone': [], 'fax': []}
    for line in block.splitlines():
        line = line.strip()
        if not line:
            continue
        if line.startswith('Phone'):
            state = 'phone'
        elif line.startswith('Fax'):
            state = 'fax'
        values[state].append(line)
    phones = []
    for line in values['phone']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            phones.append(match)
    faxes = []
    for line in values['fax']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            faxes.append(match)
    return {'address': '; '.join(values['address']), 'phones': phones, 'faxes': faxes}","for line in values['phone']:
    for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
        phones.append(match)","phones = [match for line in values['phone'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]","phones = [match for line in values['phone'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]",1,,,,,robosuite
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers_next/md/people.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers_next/md/people.py,PersonDetail,parse_address_block$7,"def parse_address_block(self, block):
    state = 'address'
    values = {'address': [], 'phone': [], 'fax': []}
    for line in block.splitlines():
        line = line.strip()
        if not line:
            continue
        if line.startswith('Phone'):
            state = 'phone'
        elif line.startswith('Fax'):
            state = 'fax'
        values[state].append(line)
    phones = []
    for line in values['phone']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            phones.append(match)
    faxes = []
    for line in values['fax']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            faxes.append(match)
    return {'address': '; '.join(values['address']), 'phones': phones, 'faxes': faxes}","for line in values['fax']:
    for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
        faxes.append(match)","faxes = [match for line in values['fax'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]","faxes = [match for line in values['fax'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]",1,,,,,robosuite
imgclsmob,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/common.py,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/common.py,ParametricConcurrent,__call__$1778,"def __call__(self, x, **kwargs):
    out = []
    for name in self.layer_names:
        out.append(self[name](x, **kwargs))
    out = F.concat(tuple(out), axis=self.axis)
    return out","for name in self.layer_names:
    out.append(self[name](x, **kwargs))","out = [self[name](x, **kwargs) for name in self.layer_names]","out = [self[name](x, **kwargs) for name in self.layer_names]",1,,,,,robosuite
KubiScan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KubiScan/engine/utils.py,https://github.com/cyberark/KubiScan/tree/master/engine/utils.py,,list_boostrap_tokens_decoded$548,"def list_boostrap_tokens_decoded():
    tokens = []
    secrets = api_client.CoreV1Api.list_namespaced_secret(namespace='kube-system', field_selector='type=bootstrap.kubernetes.io/token')
    import base64
    for secret in secrets.items:
        tokens.append('.'.join((base64.b64decode(secret.data['token-id']).decode('utf-8'), base64.b64decode(secret.data['token-secret']).decode('utf-8'))))
    return tokens","for secret in secrets.items:
    tokens.append('.'.join((base64.b64decode(secret.data['token-id']).decode('utf-8'), base64.b64decode(secret.data['token-secret']).decode('utf-8'))))","tokens = ['.'.join((base64.b64decode(secret.data['token-id']).decode('utf-8'), base64.b64decode(secret.data['token-secret']).decode('utf-8'))) for secret in secrets.items]","tokens = ['.'.join((base64.b64decode(secret.data['token-id']).decode('utf-8'), base64.b64decode(secret.data['token-secret']).decode('utf-8'))) for secret in secrets.items]",1,,,,,robosuite
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for pair in b:
    k = pair[0]
    for m in self.shutit_modules:
        if m.module_id == k:
            count += 1
            compatible = True
            if not cfg[m.module_id]['shutit.core.module.build']:
                cfg[m.module_id]['shutit.core.module.build'] = True
                compatible = self.determine_compatibility(m.module_id) == 0
                cfg[m.module_id]['shutit.core.module.build'] = False
            if long_output:
                table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
            else:
                table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])","table_list += [[str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] if long_output else [m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] for pair in b for m in self.shutit_modules if m.module_id == pair[0] and (count := (count + 1))]",Cannot refactor,-1,0,,,,robosuite
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for n in range(0, len(item)):
    colwidths.append(10)","colwidths = [10 for n in range(0, len(item))]",Cannot refactor,-1,1,,,,robosuite
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for m in self.shutit_modules:
    l.append(m.module_id)",l += [m.module_id for m in self.shutit_modules],l = [m.module_id for m in self.shutit_modules],0,1,,,,robosuite
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for k in l:
    for m in self.shutit_modules:
        if m.module_id == k:
            count = 1
            compatible = True
            if not cfg[m.module_id]['shutit.core.module.build']:
                cfg[m.module_id]['shutit.core.module.build'] = True
                compatible = self.determine_compatibility(m.module_id) == 0
            if long_output:
                table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
            else:
                table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])","table_list += [[str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] if not cfg[m.module_id]['shutit.core.module.build'] else [m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] for (count, k) in enumerate(l) for m in self.shutit_modules if m.module_id == k]",Cannot refactor,-1,0,,,,robosuite
azure-devops-cli-extension,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-devops-cli-extension/azure-devops/azext_devops/dev/repos/pull_request.py,https://github.com/Azure/azure-devops-cli-extension/tree/master/azure-devops/azext_devops/dev/repos/pull_request.py,,add_pull_request_work_items$381,"def add_pull_request_work_items(id, work_items, organization=None, detect=None):
    """"""Link one or more work items to a pull request.
    :param id: ID of the pull request.
    :type id: int
    :param work_items: IDs of the work items to link. Space separated.
    :type work_items: list of int
    :rtype: list of :class:`AssociatedWorkItem <v5_0.git.models.AssociatedWorkItem>`
    """"""
    organization = resolve_instance(detect=detect, organization=organization)
    client = get_git_client(organization)
    existing_pr = client.get_pull_request_by_id(id)
    if work_items is not None and work_items:
        work_items = list(set(work_items))
        wit_client = get_work_item_tracking_client(organization)
        pr_url = 'vstfs:///Git/PullRequestId/{project}%2F{repo}%2F{id}'.format(project=existing_pr.repository.project.id, repo=existing_pr.repository.id, id=id)
        for work_item_id in work_items:
            patch_document = []
            patch_operation = JsonPatchOperation()
            patch_operation.op = 0
            patch_operation.path = '/relations/-'
            patch_operation.value = WorkItemRelation()
            patch_operation.value.attributes = {'name': 'Pull Request'}
            patch_operation.value.rel = 'ArtifactLink'
            patch_operation.value.url = pr_url
            patch_document.append(patch_operation)
            try:
                wit_client.update_work_item(document=patch_document, id=work_item_id)
            except AzureDevOpsClientRequestError as ex:
                logger.debug(ex, exc_info=True)
                message = ex.args[0]
                if message != 'Relation already exists.':
                    raise CLIError(ex)
        refs = client.get_pull_request_work_item_refs(project=existing_pr.repository.project.id, repository_id=existing_pr.repository.id, pull_request_id=id)
    ids = []
    for ref in refs:
        ids.append(ref.id)
    return wit_client.get_work_items(ids=ids)","for ref in refs:
    ids.append(ref.id)",ids = [ref.id for ref in refs],ids = [ref.id for ref in refs],1,,,,,robosuite
CRNN_Tensorflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CRNN_Tensorflow/data_provider/tf_io_pipline_fast_tools.py,https://github.com/MaybeShewill-CV/CRNN_Tensorflow/tree/master/data_provider/tf_io_pipline_fast_tools.py,_FeatureIO,sparse_tensor_to_str_for_tf_serving$241,"def sparse_tensor_to_str_for_tf_serving(self, decode_indices, decode_values, decode_dense_shape):
    """"""

        :param decode_indices:
        :param decode_values:
        :param decode_dense_shape:
        :return:
        """"""
    indices = decode_indices
    values = decode_values
    values = np.array([self._ord_map[str(tmp) + '_index'] for tmp in values])
    dense_shape = decode_dense_shape
    number_lists = np.ones(dense_shape, dtype=values.dtype)
    str_lists = []
    res = []
    for (i, index) in enumerate(indices):
        number_lists[index[0], index[1]] = values[i]
    for number_list in number_lists:
        str_lists.append([self.int_to_char(val) for val in number_list])
    for str_list in str_lists:
        res.append(''.join((c for c in str_list if c != '\x00')))
    return res","for number_list in number_lists:
    str_lists.append([self.int_to_char(val) for val in number_list])",str_lists = [[self.int_to_char(val) for val in number_list] for number_list in number_lists],str_lists = [[self.int_to_char(val) for val in number_list] for number_list in number_lists],1,,,,,robosuite
CRNN_Tensorflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CRNN_Tensorflow/data_provider/tf_io_pipline_fast_tools.py,https://github.com/MaybeShewill-CV/CRNN_Tensorflow/tree/master/data_provider/tf_io_pipline_fast_tools.py,_FeatureIO,sparse_tensor_to_str_for_tf_serving$241,"def sparse_tensor_to_str_for_tf_serving(self, decode_indices, decode_values, decode_dense_shape):
    """"""

        :param decode_indices:
        :param decode_values:
        :param decode_dense_shape:
        :return:
        """"""
    indices = decode_indices
    values = decode_values
    values = np.array([self._ord_map[str(tmp) + '_index'] for tmp in values])
    dense_shape = decode_dense_shape
    number_lists = np.ones(dense_shape, dtype=values.dtype)
    str_lists = []
    res = []
    for (i, index) in enumerate(indices):
        number_lists[index[0], index[1]] = values[i]
    for number_list in number_lists:
        str_lists.append([self.int_to_char(val) for val in number_list])
    for str_list in str_lists:
        res.append(''.join((c for c in str_list if c != '\x00')))
    return res","for str_list in str_lists:
    res.append(''.join((c for c in str_list if c != '\x00')))",res = [''.join((c for c in str_list if c != '\x00')) for str_list in str_lists],res = [''.join((c for c in str_list if c != '\x00')) for str_list in str_lists],1,,,,,robosuite
imgurpython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgurpython/imgurpython/helpers/format.py,https://github.com/Imgur/imgurpython/tree/master/imgurpython/helpers/format.py,,build_gallery_images_and_albums$31,"def build_gallery_images_and_albums(response):
    if isinstance(response, list):
        result = []
        for item in response:
            if item['is_album']:
                result.append(GalleryAlbum(item))
            else:
                result.append(GalleryImage(item))
    elif response['is_album']:
        result = GalleryAlbum(response)
    else:
        result = GalleryImage(response)
    return result","for item in response:
    if item['is_album']:
        result.append(GalleryAlbum(item))
    else:
        result.append(GalleryImage(item))",result = [GalleryAlbum(item) if item['is_album'] else GalleryImage(item) for item in response],result = [GalleryAlbum(item) if item['is_album'] else GalleryImage(item) for item in response],1,,,,,robosuite
rssant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rssant/actorlib/queue.py,https://github.com/anyant/rssant/tree/master/actorlib/queue.py,ActorQueue,outbox_info$229,"def outbox_info(self):
    dst_info = []
    dst_node_info = []
    for (dst, box) in self.dst_outbox.items():
        if box:
            dst_info.append(dst)
    for (dst_node, dst_box) in self.dst_node_outbox.items():
        for (dst, box) in dst_box.items():
            if box:
                dst_node_info.append((dst_node, dst))
    return (dst_info, dst_node_info)","for (dst, box) in self.dst_outbox.items():
    if box:
        dst_info.append(dst)","dst_info = [dst for (dst, box) in self.dst_outbox.items() if box]","dst_info = [dst for (dst, box) in self.dst_outbox.items() if box]",1,,,,,robosuite
rssant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rssant/actorlib/queue.py,https://github.com/anyant/rssant/tree/master/actorlib/queue.py,ActorQueue,outbox_info$229,"def outbox_info(self):
    dst_info = []
    dst_node_info = []
    for (dst, box) in self.dst_outbox.items():
        if box:
            dst_info.append(dst)
    for (dst_node, dst_box) in self.dst_node_outbox.items():
        for (dst, box) in dst_box.items():
            if box:
                dst_node_info.append((dst_node, dst))
    return (dst_info, dst_node_info)","for (dst_node, dst_box) in self.dst_node_outbox.items():
    for (dst, box) in dst_box.items():
        if box:
            dst_node_info.append((dst_node, dst))","dst_node_info = [(dst_node, dst) for (dst_node, dst_box) in self.dst_node_outbox.items() for (dst, box) in dst_box.items() if box]","dst_node_info = [(dst_node, dst) for (dst_node, dst_box) in self.dst_node_outbox.items() for (dst, box) in dst_box.items() if box]",1,,,,,robosuite
whatsapp-framework,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whatsapp-framework/libs/yowsup/yowsup/structs/protocoltreenode.py,https://github.com/danielcardeenas/whatsapp-framework/tree/master/libs/yowsup/yowsup/structs/protocoltreenode.py,ProtocolTreeNode,getAllChildren$159,"def getAllChildren(self, tag=None):
    ret = []
    if tag is None:
        return self.children
    for c in self.children:
        if tag == c.tag:
            ret.append(c)
    return ret","for c in self.children:
    if tag == c.tag:
        ret.append(c)",ret += [c for c in self.children if tag == c.tag],ret = [c for c in self.children if tag == c.tag],0,1,,,,robosuite
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/www/book_appointment/index.py,https://github.com/frappe/erpnext/tree/master/erpnext/www/book_appointment/index.py,,_get_records$149,"def _get_records(start_time, end_time, settings):
    records = []
    for record in settings.availability_of_slots:
        if record.day_of_week == WEEKDAYS[start_time.weekday()] or record.day_of_week == WEEKDAYS[end_time.weekday()]:
            records.append(record)
    return records","for record in settings.availability_of_slots:
    if record.day_of_week == WEEKDAYS[start_time.weekday()] or record.day_of_week == WEEKDAYS[end_time.weekday()]:
        records.append(record)",records = [record for record in settings.availability_of_slots if record.day_of_week == WEEKDAYS[start_time.weekday()] or record.day_of_week == WEEKDAYS[end_time.weekday()]],records = [record for record in settings.availability_of_slots if record.day_of_week == WEEKDAYS[start_time.weekday()] or record.day_of_week == WEEKDAYS[end_time.weekday()]],1,,,,,robosuite
meta-dataset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meta-dataset/meta_dataset/learners/experimental/optimization_learners.py,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/learners/experimental/optimization_learners.py,ExperimentalOptimizationLearner,detailed_forward_pass$328,"def detailed_forward_pass(self, data):
    """"""Returns all information from a forward pass of the `OptimizationLearner`.

    Args:
      data: A `meta_dataset.providers.Episode` containing the data for the
        episode.

    Returns:
      A `collections.NamedTuple` that contains the results of the forward pass.
    """"""
    init_loop_variables = self.task_parameters
    init_loop_variable_refs = [v.experimental_ref() for v in init_loop_variables]
    episodic_init_ops = self.episodic_init_ops(labels=data.support_labels, embeddings=self.embedding_fn(data.support_images, training=True), task_parameters=init_loop_variables)

    def _forward_pass(iteration_idx_, variables_mapping_, images_, onehot_labels_):
        """"""Helper function to compute the outputs of a forward pass.""""""
        with self.embedding_fn.reparameterize(variables_mapping_):
            embeddings_ = self.embedding_fn(images_, training=True)
        with self.head_fn.reparameterize(variables_mapping_):
            predictions_ = self.head_fn(embeddings_)[:, :data.way]
        accuracy_ = tf.reduce_mean(input_tensor=self.compute_accuracy(onehot_labels=onehot_labels_, predictions=predictions_))
        inner_objective_ = self.inner_objective(onehot_labels=onehot_labels_, predictions=predictions_, iteration_idx=iteration_idx_)
        outer_objective_ = self.outer_objective(onehot_labels=onehot_labels_, predictions=predictions_)
        return ForwardPass(embeddings=embeddings_, predictions=predictions_, inner_objective_value=inner_objective_, outer_objective_value=outer_objective_, accuracy=accuracy_)

    def _objective_fn(loop_variables_, iteration_idx_):
        """"""Evaluate the support set objective given `loop_variables_`.""""""
        loop_variables_mapping_ = dict(zip(init_loop_variable_refs, loop_variables_))
        adaptation_support_results = _forward_pass(iteration_idx_=iteration_idx_, variables_mapping_=loop_variables_mapping_, images_=data.support_images, onehot_labels_=data.onehot_support_labels)
        return adaptation_support_results.inner_objective_value

    def _e_step(loop_variables_):
        """"""Evaluate expectations given `loop_variables_`.""""""
        loop_variables_dict_ = dict(zip(init_loop_variable_refs, loop_variables_))
        with self.embedding_fn.reparameterize(loop_variables_dict_):
            train_embeddings_ = self.embedding_fn(data.train_images, training=True)
        class_embeddings_ = learner_base.class_specific_data(data.onehot_train_labels, train_embeddings_, self.logit_dim)

        def _compute_responsibilities(examples_, class_idx):
            train_predictions_ = tf.squeeze(self.head_fn(embeddings=examples_, components=True, class_idx=[class_idx]), axis=1)
            return tf.nn.softmax(train_predictions_, axis=-1)
        with self.head_fn.reparameterize(loop_variables_dict_):
            class_responsibilities_ = [_compute_responsibilities(embeddings_, class_idx=i) for (i, embeddings_) in enumerate(class_embeddings_)]
        return (class_embeddings_, class_responsibilities_)

    def _m_step(preupdate_vars, all_embeddings_, all_responsibilities_):
        """"""Compute parameter estimates given `loop_variables_`.""""""
        (means, log_scales, logits) = zip(*map(reparameterizable_distributions.fit_gaussian_mixture, all_embeddings_, all_responsibilities_, itertools.repeat(self.head_fn.damping)))

        def flatten(x):
            return list(itertools.chain.from_iterable(x))
        means = flatten(means)
        log_scales = flatten(log_scales)
        logits = flatten(logits)
        if not self.head_fn.estimate_loc:
            means = [None for _ in means]
        if not self.head_fn.estimate_scale:
            log_scales = [None for _ in log_scales]
        if not self.head_fn.estimate_logits:
            logits = [None for _ in logits]
        updated_vars = means + log_scales + logits
        no_none_updated_vars = []
        for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars):
            if updated_var is None:
                no_none_updated_vars.append(preupdate_var)
            else:
                no_none_updated_vars.append(updated_var)
        return no_none_updated_vars
    with tf.control_dependencies(episodic_init_ops):
        num_em_steps = getattr(self, 'num_em_steps', 0)
        if num_em_steps > 0:
            loop_variables = em_loop(num_updates=self.num_em_steps, e_step=_e_step, m_step=_m_step, variables=loop_variables)
        num_optimizer_steps = self.num_update_steps + (self.additional_evaluation_update_steps if not self.is_training else 0)
        if num_optimizer_steps > 0:
            final_loop_variables = optimizer_loop(num_updates=num_optimizer_steps, objective_fn=_objective_fn, update_fn=self.update_fn, variables=init_loop_variables, first_order=self.first_order, clip_grad_norm=self.clip_grad_norm)
        if num_optimizer_steps + num_em_steps == 0:
            loop_variables = [tf.identity(v) for v in init_loop_variables]
    init_loop_variables_mapping = dict(zip(init_loop_variable_refs, init_loop_variables))
    final_loop_variables_mapping = dict(zip(init_loop_variable_refs, final_loop_variables))
    with tf.compat.v1.name_scope('pre-adaptation'):
        with tf.compat.v1.name_scope('support'):
            pre_adaptation_support_results = _forward_pass(iteration_idx_=0, variables_mapping_=init_loop_variables_mapping, images_=data.support_images, onehot_labels_=data.onehot_support_labels)
        with tf.compat.v1.name_scope('query'):
            pre_adaptation_query_results = _forward_pass(iteration_idx_=0, variables_mapping_=init_loop_variables_mapping, images_=data.query_images, onehot_labels_=data.onehot_query_labels)
    with tf.compat.v1.name_scope('post-adaptation'):
        with tf.compat.v1.name_scope('support'):
            post_adaptation_support_results = _forward_pass(iteration_idx_=num_optimizer_steps, variables_mapping_=final_loop_variables_mapping, images_=data.support_images, onehot_labels_=data.onehot_support_labels)
        with tf.compat.v1.name_scope('query'):
            post_adaptation_query_results = _forward_pass(iteration_idx_=num_optimizer_steps, variables_mapping_=final_loop_variables_mapping, images_=data.query_images, onehot_labels_=data.onehot_query_labels)

    def _support_module_objective_fn(module_variables_, module_variable_refs_):
        """"""Evaluate the query set objective given `module_variables_`.""""""
        variables_mapping_ = final_loop_variables_mapping.copy()
        for (module_variable_ref, module_variable) in zip(module_variable_refs_, module_variables_):
            variables_mapping_[module_variable_ref] = module_variable
        adaptation_query_results = _forward_pass(iteration_idx_=num_optimizer_steps, variables_mapping_=variables_mapping_, images_=data.support_images, onehot_labels_=data.onehot_support_labels)
        return adaptation_query_results.inner_objective_value

    def _query_module_objective_fn(module_variables_, module_variable_refs_):
        """"""Evaluate the query set objective given `module_variables_`.""""""
        variables_mapping_ = final_loop_variables_mapping.copy()
        for (module_variable_ref, module_variable) in zip(module_variable_refs_, module_variables_):
            variables_mapping_[module_variable_ref] = module_variable
        adaptation_query_results = _forward_pass(iteration_idx_=num_optimizer_steps, variables_mapping_=variables_mapping_, images_=data.query_images, onehot_labels_=data.onehot_query_labels)
        return adaptation_query_results.inner_objective_value
    return Adaptation(pre_adaptation_support_results=pre_adaptation_support_results, post_adaptation_support_results=post_adaptation_support_results, pre_adaptation_query_results=pre_adaptation_query_results, post_adaptation_query_results=post_adaptation_query_results, objective_fn=_objective_fn, support_module_objective_fn=_support_module_objective_fn, query_module_objective_fn=_query_module_objective_fn, forward_pass_fn=_forward_pass, init_loop_variables_mapping=init_loop_variables_mapping, final_loop_variables_mapping=final_loop_variables_mapping)","for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars):
    if updated_var is None:
        no_none_updated_vars.append(preupdate_var)
    else:
        no_none_updated_vars.append(updated_var)","no_none_updated_vars = [preupdate_var if updated_var is None else updated_var for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars)]","no_none_updated_vars = [preupdate_var if updated_var is None else updated_var for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars)]",1,,,,,robosuite
reid-strong-baseline,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/reid-strong-baseline/utils/re_ranking.py,https://github.com/michuanhaohao/reid-strong-baseline/tree/master/utils/re_ranking.py,,re_ranking$29,"def re_ranking(probFea, galFea, k1, k2, lambda_value, local_distmat=None, only_local=False):
    query_num = probFea.size(0)
    all_num = query_num + galFea.size(0)
    if only_local:
        original_dist = local_distmat
    else:
        feat = torch.cat([probFea, galFea])
        print('using GPU to compute original distance')
        distmat = torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num) + torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()
        distmat.addmm_(1, -2, feat, feat.t())
        original_dist = distmat.cpu().numpy()
        del feat
        if not local_distmat is None:
            original_dist = original_dist + local_distmat
    gallery_num = original_dist.shape[0]
    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))
    V = np.zeros_like(original_dist).astype(np.float16)
    initial_rank = np.argsort(original_dist).astype(np.int32)
    print('starting re_ranking')
    for i in range(all_num):
        forward_k_neigh_index = initial_rank[i, :k1 + 1]
        backward_k_neigh_index = initial_rank[forward_k_neigh_index, :k1 + 1]
        fi = np.where(backward_k_neigh_index == i)[0]
        k_reciprocal_index = forward_k_neigh_index[fi]
        k_reciprocal_expansion_index = k_reciprocal_index
        for j in range(len(k_reciprocal_index)):
            candidate = k_reciprocal_index[j]
            candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]
            candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index, :int(np.around(k1 / 2)) + 1]
            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]
            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]
            if len(np.intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)) > 2 / 3 * len(candidate_k_reciprocal_index):
                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index, candidate_k_reciprocal_index)
        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)
        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])
        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)
    original_dist = original_dist[:query_num,]
    if k2 != 1:
        V_qe = np.zeros_like(V, dtype=np.float16)
        for i in range(all_num):
            V_qe[i, :] = np.mean(V[initial_rank[i, :k2], :], axis=0)
        V = V_qe
        del V_qe
    del initial_rank
    invIndex = []
    for i in range(gallery_num):
        invIndex.append(np.where(V[:, i] != 0)[0])
    jaccard_dist = np.zeros_like(original_dist, dtype=np.float16)
    for i in range(query_num):
        temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float16)
        indNonZero = np.where(V[i, :] != 0)[0]
        indImages = [invIndex[ind] for ind in indNonZero]
        for j in range(len(indNonZero)):
            temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]], V[indImages[j], indNonZero[j]])
        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)
    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value
    del original_dist
    del V
    del jaccard_dist
    final_dist = final_dist[:query_num, query_num:]
    return final_dist","for i in range(gallery_num):
    invIndex.append(np.where(V[:, i] != 0)[0])","invIndex = [np.where(V[:, i] != 0)[0] for i in range(gallery_num)]","invIndex = [np.where(V[:, i] != 0)[0] for i in range(gallery_num)]",1,,,,,robosuite
Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161,"def get_all_funcs_async(file_list):
    async_group = []
    done_list = []
    for file in file_list:
        async_group.append(async_get_funcs.apply_async(args=[file], time_limit=3600))
    bar = tqdm.tqdm(total=len(async_group), desc='[~] Recovering function prototypes')
    while not all([x.successful() or x.failed() for x in async_group]):
        done_count = len([x.successful() or x.failed() for x in async_group if x.successful() or x.failed()])
        bar.update(done_count - bar.n)
        time.sleep(1)
    bar.close()
    return [x.get(propagate=False) for x in async_group if not x.failed()]","for file in file_list:
    async_group.append(async_get_funcs.apply_async(args=[file], time_limit=3600))","async_group = [async_get_funcs.apply_async(args=[file], time_limit=3600) for file in file_list]","async_group = [async_get_funcs.apply_async(args=[file], time_limit=3600) for file in file_list]",1,,,,,robosuite
OpenSeq2Seq,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSeq2Seq/open_seq2seq/utils/hooks.py,https://github.com/NVIDIA/OpenSeq2Seq/tree/master/open_seq2seq/utils/hooks.py,BroadcastGlobalVariablesHook,broadcast_global_variables$39,"def broadcast_global_variables(root_rank):
    from horovod.tensorflow.mpi_ops import broadcast
    ops = []
    for var in tf.global_variables():
        if var.dtype.base_dtype == tf.float16:
            ops.append(tf.assign(var, tf.cast(broadcast(tf.cast(var, tf.float32), root_rank), tf.float16)))
        else:
            ops.append(tf.assign(var, broadcast(var, root_rank)))
    return tf.group(*ops)","for var in tf.global_variables():
    if var.dtype.base_dtype == tf.float16:
        ops.append(tf.assign(var, tf.cast(broadcast(tf.cast(var, tf.float32), root_rank), tf.float16)))
    else:
        ops.append(tf.assign(var, broadcast(var, root_rank)))","ops = [tf.assign(var, tf.cast(broadcast(tf.cast(var, tf.float32), root_rank), tf.float16)) if var.dtype.base_dtype == tf.float16 else tf.assign(var, broadcast(var, root_rank)) for var in tf.global_variables()]","ops = [tf.assign(var, tf.cast(broadcast(tf.cast(var, tf.float32), root_rank), tf.float16)) if var.dtype.base_dtype == tf.float16 else tf.assign(var, broadcast(var, root_rank)) for var in tf.global_variables()]",1,,,,,robosuite
crossbar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crossbar/crossbar/worker/router.py,https://github.com/crossbario/crossbar/tree/master/crossbar/worker/router.py,RouterController,get_router_components$595,"def get_router_components(self, details=None):
    """"""
        Get app components currently running in this router worker.

        :param details: Call details.
        :type details: :class:`autobahn.wamp.types.CallDetails`

        :returns: List of app components currently running.
        :rtype: list[dict]
        """"""
    self.log.debug('{name}.get_router_components', name=self.__class__.__name__)
    res = []
    for component in sorted(self.components.values(), key=lambda c: c.created):
        res.append({'id': component.id, 'created': utcstr(component.created), 'config': component.config})
    return res","for component in sorted(self.components.values(), key=lambda c: c.created):
    res.append({'id': component.id, 'created': utcstr(component.created), 'config': component.config})","res = [{'id': component.id, 'created': utcstr(component.created), 'config': component.config} for component in sorted(self.components.values(), key=lambda c: c.created)]","res = [{'id': component.id, 'created': utcstr(component.created), 'config': component.config} for component in sorted(self.components.values(), key=lambda c: c.created)]",1,,,,,robosuite
powerfulseal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/powerfulseal/powerfulseal/cli/pscmd.py,https://github.com/powerfulseal/powerfulseal/tree/master/powerfulseal/cli/pscmd.py,PSCmd,completedefault$96,"def completedefault(self, text, line, begidx, endidx):
    suggestions = []
    text_lower = text.lower()
    try:
        for az in self.inventory.get_azs():
            if text_lower in az.lower():
                suggestions.append(str(az))
        for group in self.inventory.get_groups():
            if text_lower in group.lower():
                suggestions.append(str(group))
        for node in self.inventory.find_nodes():
            for attr in ['ip', 'id', 'state', 'name', 'no']:
                val = str(getattr(node, attr))
                if text_lower in val.lower():
                    suggestions.append(val)
        for extra in ('all',):
            if extra.lower().startswith(text.lower()):
                suggestions.append(extra)
    except Exception as e:
        print(e)
    return suggestions","for az in self.inventory.get_azs():
    if text_lower in az.lower():
        suggestions.append(str(az))",suggestions = [str(az) for az in self.inventory.get_azs() if text_lower in az.lower()],suggestions = [str(az) for az in self.inventory.get_azs() if text_lower in az.lower()],1,,,,,robosuite
powerfulseal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/powerfulseal/powerfulseal/cli/pscmd.py,https://github.com/powerfulseal/powerfulseal/tree/master/powerfulseal/cli/pscmd.py,PSCmd,completedefault$96,"def completedefault(self, text, line, begidx, endidx):
    suggestions = []
    text_lower = text.lower()
    try:
        for az in self.inventory.get_azs():
            if text_lower in az.lower():
                suggestions.append(str(az))
        for group in self.inventory.get_groups():
            if text_lower in group.lower():
                suggestions.append(str(group))
        for node in self.inventory.find_nodes():
            for attr in ['ip', 'id', 'state', 'name', 'no']:
                val = str(getattr(node, attr))
                if text_lower in val.lower():
                    suggestions.append(val)
        for extra in ('all',):
            if extra.lower().startswith(text.lower()):
                suggestions.append(extra)
    except Exception as e:
        print(e)
    return suggestions","for group in self.inventory.get_groups():
    if text_lower in group.lower():
        suggestions.append(str(group))",suggestions += [str(group) for group in self.inventory.get_groups() if text_lower in group.lower()],Cannot refactor,-1,1,,,,robosuite
powerfulseal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/powerfulseal/powerfulseal/cli/pscmd.py,https://github.com/powerfulseal/powerfulseal/tree/master/powerfulseal/cli/pscmd.py,PSCmd,completedefault$96,"def completedefault(self, text, line, begidx, endidx):
    suggestions = []
    text_lower = text.lower()
    try:
        for az in self.inventory.get_azs():
            if text_lower in az.lower():
                suggestions.append(str(az))
        for group in self.inventory.get_groups():
            if text_lower in group.lower():
                suggestions.append(str(group))
        for node in self.inventory.find_nodes():
            for attr in ['ip', 'id', 'state', 'name', 'no']:
                val = str(getattr(node, attr))
                if text_lower in val.lower():
                    suggestions.append(val)
        for extra in ('all',):
            if extra.lower().startswith(text.lower()):
                suggestions.append(extra)
    except Exception as e:
        print(e)
    return suggestions","for node in self.inventory.find_nodes():
    for attr in ['ip', 'id', 'state', 'name', 'no']:
        val = str(getattr(node, attr))
        if text_lower in val.lower():
            suggestions.append(val)","suggestions += [str(getattr(node, attr)) for node in self.inventory.find_nodes() for attr in ['ip', 'id', 'state', 'name', 'no'] if text_lower in str(getattr(node, attr)).lower()]",Cannot refactor,-1,1,,,,robosuite
Ghostwriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Ghostwriter/ghostwriter/reporting/tests/test_models.py,https://github.com/GhostManager/Ghostwriter/tree/master/ghostwriter/reporting/tests/test_models.py,ReportFindingLinkModelTests,test_position_change_on_delete$465,"def test_position_change_on_delete(self):
    report = ReportFactory()
    num_of_findings = 5
    findings = []
    for finding_id in range(num_of_findings):
        findings.append(ReportFindingLinkFactory(report=report, severity=self.critical_severity))
    for finding_id in range(num_of_findings):
        findings.append(ReportFindingLinkFactory(report=report, severity=self.high_severity))
    with transaction.atomic():
        findings[3].delete()
        findings[5].delete()
        findings[8].delete()
    cleaned_findings = []
    for f in findings:
        try:
            f.refresh_from_db()
            cleaned_findings.append(f)
        except self.ReportFindingLink.DoesNotExist:
            pass
    self.assertEqual(cleaned_findings[0].position, 1)
    self.assertEqual(cleaned_findings[1].position, 2)
    self.assertEqual(cleaned_findings[2].position, 3)
    self.assertEqual(cleaned_findings[3].position, 4)
    self.assertEqual(cleaned_findings[4].position, 1)
    self.assertEqual(cleaned_findings[5].position, 2)
    self.assertEqual(cleaned_findings[6].position, 3)","for finding_id in range(num_of_findings):
    findings.append(ReportFindingLinkFactory(report=report, severity=self.critical_severity))","findings += [ReportFindingLinkFactory(report=report, severity=self.critical_severity) for finding_id in range(num_of_findings)]","findings = [ReportFindingLinkFactory(report=report, severity=self.critical_severity) for finding_id in range(num_of_findings)]",0,1,,,,robosuite
Ghostwriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Ghostwriter/ghostwriter/reporting/tests/test_models.py,https://github.com/GhostManager/Ghostwriter/tree/master/ghostwriter/reporting/tests/test_models.py,ReportFindingLinkModelTests,test_position_change_on_delete$465,"def test_position_change_on_delete(self):
    report = ReportFactory()
    num_of_findings = 5
    findings = []
    for finding_id in range(num_of_findings):
        findings.append(ReportFindingLinkFactory(report=report, severity=self.critical_severity))
    for finding_id in range(num_of_findings):
        findings.append(ReportFindingLinkFactory(report=report, severity=self.high_severity))
    with transaction.atomic():
        findings[3].delete()
        findings[5].delete()
        findings[8].delete()
    cleaned_findings = []
    for f in findings:
        try:
            f.refresh_from_db()
            cleaned_findings.append(f)
        except self.ReportFindingLink.DoesNotExist:
            pass
    self.assertEqual(cleaned_findings[0].position, 1)
    self.assertEqual(cleaned_findings[1].position, 2)
    self.assertEqual(cleaned_findings[2].position, 3)
    self.assertEqual(cleaned_findings[3].position, 4)
    self.assertEqual(cleaned_findings[4].position, 1)
    self.assertEqual(cleaned_findings[5].position, 2)
    self.assertEqual(cleaned_findings[6].position, 3)","for finding_id in range(num_of_findings):
    findings.append(ReportFindingLinkFactory(report=report, severity=self.high_severity))","findings += [ReportFindingLinkFactory(report=report, severity=self.high_severity) for finding_id in range(num_of_findings)]",Cannot refactor,-1,1,,,,robosuite
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/backup/custom_common.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/backup/custom_common.py,,check_rp_move_readiness$256,"def check_rp_move_readiness(paged_recovery_points, target_tier, is_ready_for_move):
    if target_tier and is_ready_for_move is not None:
        filter_rps = []
        for rp in paged_recovery_points:
            if rp.properties.recovery_point_move_readiness_info is not None and rp.properties.recovery_point_move_readiness_info['ArchivedRP'].is_ready_for_move == is_ready_for_move:
                filter_rps.append(rp)
        return filter_rps
    if target_tier or is_ready_for_move is not None:
        raise RequiredArgumentMissingError('--is-ready-for-move or --target-tier is missing. Please provide\n        the required arguments.')
    return paged_recovery_points","for rp in paged_recovery_points:
    if rp.properties.recovery_point_move_readiness_info is not None and rp.properties.recovery_point_move_readiness_info['ArchivedRP'].is_ready_for_move == is_ready_for_move:
        filter_rps.append(rp)",filter_rps = [rp for rp in paged_recovery_points if rp.properties.recovery_point_move_readiness_info is not None and rp.properties.recovery_point_move_readiness_info['ArchivedRP'].is_ready_for_move == is_ready_for_move],filter_rps = [rp for rp in paged_recovery_points if rp.properties.recovery_point_move_readiness_info is not None and rp.properties.recovery_point_move_readiness_info['ArchivedRP'].is_ready_for_move == is_ready_for_move],1,,,,,robosuite
HigherHRNet-Human-Pose-Estimation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HigherHRNet-Human-Pose-Estimation/lib/fp16_utils/fp16_optimizer.py,https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation/tree/master/lib/fp16_utils/fp16_optimizer.py,FP16_Optimizer,_check_overflow$225,"def _check_overflow(self):
    params = []
    for group in self.fp16_groups:
        for param in group:
            params.append(param)
    for group in self.fp32_from_fp32_groups:
        for param in group:
            params.append(param)
    self.overflow = self.loss_scaler.has_overflow(params)","for group in self.fp16_groups:
    for param in group:
        params.append(param)",params = [param for group in self.fp16_groups for param in group],params = [param for group in self.fp16_groups for param in group],1,,,,,robosuite
HigherHRNet-Human-Pose-Estimation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HigherHRNet-Human-Pose-Estimation/lib/fp16_utils/fp16_optimizer.py,https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation/tree/master/lib/fp16_utils/fp16_optimizer.py,FP16_Optimizer,_check_overflow$225,"def _check_overflow(self):
    params = []
    for group in self.fp16_groups:
        for param in group:
            params.append(param)
    for group in self.fp32_from_fp32_groups:
        for param in group:
            params.append(param)
    self.overflow = self.loss_scaler.has_overflow(params)","for group in self.fp32_from_fp32_groups:
    for param in group:
        params.append(param)",params += [param for group in self.fp32_from_fp32_groups for param in group],Cannot refactor,-1,1,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/saltutil.py,https://github.com/saltstack/salt/tree/master/salt/modules/saltutil.py,,kill_all_jobs$1480,"def kill_all_jobs():
    """"""
    Sends a kill signal (SIGKILL 9) to all currently running jobs

    CLI Example:

    .. code-block:: bash

        salt '*' saltutil.kill_all_jobs
    """"""
    ret = []
    for data in running():
        ret.append(signal_job(data['jid'], salt_SIGKILL))
    return ret","for data in running():
    ret.append(signal_job(data['jid'], salt_SIGKILL))","ret = [signal_job(data['jid'], salt_SIGKILL) for data in running()]","ret = [signal_job(data['jid'], salt_SIGKILL) for data in running()]",1,,,,,robosuite
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/transpiler/distribute_transpiler.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/transpiler/distribute_transpiler.py,DistributeTranspiler,_get_distributed_optimizer_var$1712,"def _get_distributed_optimizer_var(endpoint):
    opt_op_on_pserver = []
    for (_, op) in enumerate(self.optimize_ops):
        if self._is_optimizer_op(op) and self._is_opt_op_on_pserver(endpoint, op):
            opt_op_on_pserver.append(op)
    for opt_op in opt_op_on_pserver:
        dist_var = None
        for key in opt_op.input_names:
            if key == 'Param':
                param_name = opt_op.input(key)[0]
                dist_var = self.vars_overview.get_distributed_var_by_origin_and_ep(param_name, endpoint)
                break
        for key in opt_op.input_names:
            if key in ['Param', 'Grad', 'LearningRate', 'Beta1Tensor', 'Beta2Tensor']:
                continue
            origin_var = self.origin_program.global_block().vars[opt_op.input(key)[0]]
            new_shape = self._get_optimizer_input_shape(opt_op.type, key, origin_var.shape, dist_var.slice.shape)
            if new_shape == dist_var.slice.shape:
                splited_var = VarStruct(name=origin_var.name, shape=new_shape, dtype=origin_var.dtype, type=origin_var.type, lod_level=origin_var.lod_level, persistable=origin_var.persistable)
                self.vars_overview.add_distributed_var(origin_var=origin_var, slice_var=splited_var, is_slice=dist_var.is_slice, block_id=dist_var.block_id, offset=dist_var.offset, vtype='Optimizer', endpoint=endpoint)
            else:
                self.vars_overview.add_distributed_var(origin_var=origin_var, slice_var=origin_var, is_slice=False, block_id=0, offset=0, vtype='Optimizer', endpoint=endpoint)","for (_, op) in enumerate(self.optimize_ops):
    if self._is_optimizer_op(op) and self._is_opt_op_on_pserver(endpoint, op):
        opt_op_on_pserver.append(op)","opt_op_on_pserver = [op for (_, op) in enumerate(self.optimize_ops) if self._is_optimizer_op(op) and self._is_opt_op_on_pserver(endpoint, op)]","opt_op_on_pserver = [op for (_, op) in enumerate(self.optimize_ops) if self._is_optimizer_op(op) and self._is_opt_op_on_pserver(endpoint, op)]",1,,,,,robosuite
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/domains/c.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/domains/c.py,Symbol,get_lookup_key$1651,"def get_lookup_key(self) -> 'LookupKey':
    symbols = []
    s = self
    while s.parent:
        symbols.append(s)
        s = s.parent
    symbols.reverse()
    key = []
    for s in symbols:
        if s.declaration is not None:
            key.append((s.ident, s.declaration.get_newest_id()))
        else:
            key.append((s.ident, None))
    return LookupKey(key)","for s in symbols:
    if s.declaration is not None:
        key.append((s.ident, s.declaration.get_newest_id()))
    else:
        key.append((s.ident, None))","key = [(s.ident, s.declaration.get_newest_id()) if s.declaration is not None else (s.ident, None) for s in symbols]","key = [(s.ident, s.declaration.get_newest_id()) if s.declaration is not None else (s.ident, None) for s in symbols]",1,,,,,robosuite
yt-dlc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/dispeak.py,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/dispeak.py,DigitallySpeakingIE,_parse_flv$75,"def _parse_flv(self, metadata):
    formats = []
    akamai_url = xpath_text(metadata, './akamaiHost', fatal=True)
    audios = metadata.findall('./audios/audio')
    for audio in audios:
        formats.append({'url': 'rtmp://%s/ondemand?ovpfv=1.1' % akamai_url, 'play_path': remove_end(audio.get('url'), '.flv'), 'ext': 'flv', 'vcodec': 'none', 'format_id': audio.get('code')})
    slide_video_path = xpath_text(metadata, './slideVideo', fatal=True)
    formats.append({'url': 'rtmp://%s/ondemand?ovpfv=1.1' % akamai_url, 'play_path': remove_end(slide_video_path, '.flv'), 'ext': 'flv', 'format_note': 'slide deck video', 'quality': -2, 'preference': -2, 'format_id': 'slides', 'acodec': 'none'})
    speaker_video_path = xpath_text(metadata, './speakerVideo', fatal=True)
    formats.append({'url': 'rtmp://%s/ondemand?ovpfv=1.1' % akamai_url, 'play_path': remove_end(speaker_video_path, '.flv'), 'ext': 'flv', 'format_note': 'speaker video', 'quality': -1, 'preference': -1, 'format_id': 'speaker'})
    return formats","for audio in audios:
    formats.append({'url': 'rtmp://%s/ondemand?ovpfv=1.1' % akamai_url, 'play_path': remove_end(audio.get('url'), '.flv'), 'ext': 'flv', 'vcodec': 'none', 'format_id': audio.get('code')})","formats = [{'url': 'rtmp://%s/ondemand?ovpfv=1.1' % akamai_url, 'play_path': remove_end(audio.get('url'), '.flv'), 'ext': 'flv', 'vcodec': 'none', 'format_id': audio.get('code')} for audio in audios]","formats = [{'url': 'rtmp://%s/ondemand?ovpfv=1.1' % akamai_url, 'play_path': remove_end(audio.get('url'), '.flv'), 'ext': 'flv', 'vcodec': 'none', 'format_id': audio.get('code')} for audio in audios]",1,,,,,robosuite
code2flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/code2flow/code2flow/engine.py,https://github.com/scottrogowski/code2flow/tree/master/code2flow/engine.py,,_filter_edges_for_subset$159,"def _filter_edges_for_subset(new_nodes, edges):
    """"""
    Given the subset of nodes, filter for edges within this subset
    :param new_nodes set[Node]:
    :param edges list[Edge]:
    :rtype: list[Edge]
    """"""
    new_edges = []
    for edge in edges:
        if edge.node0 in new_nodes and edge.node1 in new_nodes:
            new_edges.append(edge)
    return new_edges","for edge in edges:
    if edge.node0 in new_nodes and edge.node1 in new_nodes:
        new_edges.append(edge)",new_edges = [edge for edge in edges if edge.node0 in new_nodes and edge.node1 in new_nodes],new_edges = [edge for edge in edges if edge.node0 in new_nodes and edge.node1 in new_nodes],1,,,,,robosuite
amazing-qr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/amazing-qr/amzqr/amzqr.py,https://github.com/x-hw/amazing-qr/tree/master/amzqr/amzqr.py,,combine$51,"def combine(ver, qr_name, bg_name, colorized, contrast, brightness, save_dir, save_name=None):
    from amzqr.mylibs.constant import alig_location
    from PIL import ImageEnhance, ImageFilter
    qr = Image.open(qr_name)
    qr = qr.convert('RGBA') if colorized else qr
    bg0 = Image.open(bg_name).convert('RGBA')
    bg0 = ImageEnhance.Contrast(bg0).enhance(contrast)
    bg0 = ImageEnhance.Brightness(bg0).enhance(brightness)
    if bg0.size[0] < bg0.size[1]:
        bg0 = bg0.resize((qr.size[0] - 24, (qr.size[0] - 24) * int(bg0.size[1] / bg0.size[0])))
    else:
        bg0 = bg0.resize(((qr.size[1] - 24) * int(bg0.size[0] / bg0.size[1]), qr.size[1] - 24))
    bg = bg0 if colorized else bg0.convert('1')
    aligs = []
    if ver > 1:
        aloc = alig_location[ver - 2]
        for a in range(len(aloc)):
            for b in range(len(aloc)):
                if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)):
                    for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)):
                        for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3)):
                            aligs.append((i, j))
    for i in range(qr.size[0] - 24):
        for j in range(qr.size[1] - 24):
            if not (i in (18, 19, 20) or j in (18, 19, 20) or (i < 24 and j < 24) or (i < 24 and j > qr.size[1] - 49) or (i > qr.size[0] - 49 and j < 24) or ((i, j) in aligs) or (i % 3 == 1 and j % 3 == 1) or (bg0.getpixel((i, j))[3] == 0)):
                qr.putpixel((i + 12, j + 12), bg.getpixel((i, j)))
    qr_name = os.path.join(save_dir, os.path.splitext(os.path.basename(bg_name))[0] + '_qrcode.png') if not save_name else os.path.join(save_dir, save_name)
    qr.resize((qr.size[0] * 3, qr.size[1] * 3)).save(qr_name)
    return qr_name","for a in range(len(aloc)):
    for b in range(len(aloc)):
        if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)):
            for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)):
                for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3)):
                    aligs.append((i, j))","aligs = [(i, j) for a in range(len(aloc)) for b in range(len(aloc)) if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)) for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)) for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3))]","aligs = [(i, j) for a in range(len(aloc)) for b in range(len(aloc)) if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)) for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)) for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3))]",1,,,,,robosuite
JsFormat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JsFormat/libs/diff_match_patch/python2/diff_match_patch.py,https://github.com/jdavisclark/JsFormat/tree/master/libs/diff_match_patch/python2/diff_match_patch.py,diff_match_patch,diff_text1$1082,"def diff_text1(self, diffs):
    """"""Compute and return the source text (all equalities and deletions).

    Args:
      diffs: Array of diff tuples.

    Returns:
      Source text.
    """"""
    text = []
    for (op, data) in diffs:
        if op != self.DIFF_INSERT:
            text.append(data)
    return ''.join(text)","for (op, data) in diffs:
    if op != self.DIFF_INSERT:
        text.append(data)","text = [data for (op, data) in diffs if op != self.DIFF_INSERT]","text = [data for (op, data) in diffs if op != self.DIFF_INSERT]",1,,,,,robosuite
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/ec2/responses/instances.py,https://github.com/spulec/moto/tree/master/moto/ec2/responses/instances.py,InstanceResponse,_security_grp_instance_attribute_handler$281,"def _security_grp_instance_attribute_handler(self):
    new_security_grp_list = []
    for (key, value) in self.querystring.items():
        if 'GroupId.' in key:
            new_security_grp_list.append(self.querystring.get(key)[0])
    instance_id = self._get_param('InstanceId')
    if self.is_not_dryrun('ModifyInstanceSecurityGroups'):
        self.ec2_backend.modify_instance_security_groups(instance_id, new_security_grp_list)
        return EC2_MODIFY_INSTANCE_ATTRIBUTE","for (key, value) in self.querystring.items():
    if 'GroupId.' in key:
        new_security_grp_list.append(self.querystring.get(key)[0])","new_security_grp_list = [self.querystring.get(key)[0] for (key, value) in self.querystring.items() if 'GroupId.' in key]","new_security_grp_list = [self.querystring.get(key)[0] for (key, value) in self.querystring.items() if 'GroupId.' in key]",1,,,,,robosuite
dm-haiku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm-haiku/haiku/_src/base_test.py,https://github.com/deepmind/dm-haiku/tree/master/haiku/_src/base_test.py,BaseTest,test_maybe_rng_vs_not$178,"def test_maybe_rng_vs_not(self):
    """"""If we have an rng, then next_rng_key() == maybe_next_rng_key().""""""
    rngs = []
    maybes = []

    @test_utils.transform_and_run
    def three():
        for _ in range(3):
            rngs.append(base.next_rng_key())

    @test_utils.transform_and_run
    def maybe_three():
        for _ in range(3):
            maybes.append(base.maybe_next_rng_key())
    three()
    maybe_three()
    self.assertLen(rngs, 6)
    self.assertTrue(jnp.all(jnp.array(rngs) == jnp.array(maybes)))","for _ in range(3):
    rngs.append(base.next_rng_key())",rngs = [base.next_rng_key() for _ in range(3)],rngs = [base.next_rng_key() for _ in range(3)],1,,,,,robosuite
dm-haiku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm-haiku/haiku/_src/base_test.py,https://github.com/deepmind/dm-haiku/tree/master/haiku/_src/base_test.py,BaseTest,test_maybe_rng_vs_not$178,"def test_maybe_rng_vs_not(self):
    """"""If we have an rng, then next_rng_key() == maybe_next_rng_key().""""""
    rngs = []
    maybes = []

    @test_utils.transform_and_run
    def three():
        for _ in range(3):
            rngs.append(base.next_rng_key())

    @test_utils.transform_and_run
    def maybe_three():
        for _ in range(3):
            maybes.append(base.maybe_next_rng_key())
    three()
    maybe_three()
    self.assertLen(rngs, 6)
    self.assertTrue(jnp.all(jnp.array(rngs) == jnp.array(maybes)))","for _ in range(3):
    maybes.append(base.maybe_next_rng_key())",maybes = [base.maybe_next_rng_key() for _ in range(3)],maybes = [base.maybe_next_rng_key() for _ in range(3)],1,,,,,robosuite
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/contract/register.py,https://github.com/zvtvz/zvt/tree/master/zvt/contract/register.py,,register_schema$42,"def register_schema(providers: List[str], db_name: str, schema_base: DeclarativeMeta, entity_type: str=None):
    """"""
    function for register schema,please declare them before register

    :param providers: the supported providers for the schema
    :type providers:
    :param db_name: database name for the schema
    :type db_name:
    :param schema_base:
    :type schema_base:
    :param entity_type: the schema related entity_type
    :type entity_type:
    :return:
    :rtype:
    """"""
    schemas = []
    for item in schema_base.registry.mappers:
        cls = item.class_
        if type(cls) == DeclarativeMeta:
            for provider in providers:
                if issubclass(cls, Mixin):
                    cls.register_provider(provider)
            if zvt_context.dbname_map_schemas.get(db_name):
                schemas = zvt_context.dbname_map_schemas[db_name]
            zvt_context.schemas.append(cls)
            if entity_type:
                add_to_map_list(the_map=zvt_context.entity_map_schemas, key=entity_type, value=cls)
            schemas.append(cls)
    zvt_context.dbname_map_schemas[db_name] = schemas
    for provider in providers:
        if provider not in zvt_context.providers:
            zvt_context.providers.append(provider)
        if not zvt_context.provider_map_dbnames.get(provider):
            zvt_context.provider_map_dbnames[provider] = []
        zvt_context.provider_map_dbnames[provider].append(db_name)
        zvt_context.dbname_map_base[db_name] = schema_base
        engine = get_db_engine(provider, db_name=db_name)
        schema_base.metadata.create_all(engine)
        session_fac = get_db_session_factory(provider, db_name=db_name)
        session_fac.configure(bind=engine)
    for provider in providers:
        engine = get_db_engine(provider, db_name=db_name)
        for (table_name, table) in iter(schema_base.metadata.tables.items()):
            index_list = []
            with engine.connect() as con:
                rs = con.execute(""PRAGMA INDEX_LIST('{}')"".format(table_name))
                for row in rs:
                    index_list.append(row[1])
            logger.debug('engine:{},table:{},index:{}'.format(engine, table_name, index_list))
            for col in ['timestamp', 'entity_id', 'code', 'report_period', 'created_timestamp', 'updated_timestamp']:
                if col in table.c:
                    column = eval('table.c.{}'.format(col))
                    index_name = '{}_{}_index'.format(table_name, col)
                    if index_name not in index_list:
                        index = sqlalchemy.schema.Index(index_name, column)
                        index.create(engine)
            for cols in [('timestamp', 'entity_id'), ('timestamp', 'code')]:
                if cols[0] in table.c and col[1] in table.c:
                    column0 = eval('table.c.{}'.format(col[0]))
                    column1 = eval('table.c.{}'.format(col[1]))
                    index_name = '{}_{}_{}_index'.format(table_name, col[0], col[1])
                    if index_name not in index_list:
                        index = sqlalchemy.schema.Index(index_name, column0, column1)
                        index.create(engine)","for row in rs:
    index_list.append(row[1])",index_list = [row[1] for row in rs],index_list = [row[1] for row in rs],1,,,,,robosuite
nlp-architect,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-architect/solutions/trend_analysis/scoring_utils.py,https://github.com/IntelLabs/nlp-architect/tree/master/solutions/trend_analysis/scoring_utils.py,TextSpanScoring,interpolate_scores$160,"def interpolate_scores(phrase_lists, weights=None):
    if weights is None:
        weights = [1.0 / len(phrase_lists)]
    else:
        assert len(weights) == len(phrase_lists)
    list_sizes = [len(lst) for lst in phrase_lists]
    for lsize in list_sizes:
        assert len(phrase_lists[0]) == lsize, 'list sizes not equal'
    phrase_list_dicts = []
    for lst in phrase_lists:
        phrase_list_dicts.append({tuple(k): v for (k, v) in lst})
    phrases = phrase_list_dicts[0].keys()
    interp_scores = {}
    for p in phrases:
        interp_scores[p] = 0.0
        for (ref, w) in zip(phrase_list_dicts, weights):
            interp_scores[p] += ref[p] * w
    return sorted(interp_scores.items(), key=lambda x: x[1], reverse=True)","for lst in phrase_lists:
    phrase_list_dicts.append({tuple(k): v for (k, v) in lst})","phrase_list_dicts = [{tuple(k): v for (k, v) in lst} for lst in phrase_lists]","phrase_list_dicts = [{tuple(k): v for (k, v) in lst} for lst in phrase_lists]",1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/parser.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/parser.py,BaseStepProxy,inputs_to_dicts$981,"def inputs_to_dicts(self):
    inputs_as_dicts = []
    for input_proxy in self.input_proxies:
        inputs_as_dicts.append(input_proxy.to_dict())
    return inputs_as_dicts","for input_proxy in self.input_proxies:
    inputs_as_dicts.append(input_proxy.to_dict())",inputs_as_dicts = [input_proxy.to_dict() for input_proxy in self.input_proxies],inputs_as_dicts = [input_proxy.to_dict() for input_proxy in self.input_proxies],1,,,,,robosuite
qlib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qlib/qlib/workflow/online/strategy.py,https://github.com/microsoft/qlib/tree/master/qlib/workflow/online/strategy.py,RollingStrategy,_list_latest$194,"def _list_latest(self, rec_list: List[Recorder]):
    """"""
        List latest recorder form rec_list

        Args:
            rec_list (List[Recorder]): a list of Recorder

        Returns:
            List[Recorder], pd.Timestamp: the latest recorders and their test end time
        """"""
    if len(rec_list) == 0:
        return (rec_list, None)
    max_test = max((rec.load_object('task')['dataset']['kwargs']['segments']['test'] for rec in rec_list))
    latest_rec = []
    for rec in rec_list:
        if rec.load_object('task')['dataset']['kwargs']['segments']['test'] == max_test:
            latest_rec.append(rec)
    return (latest_rec, max_test)","for rec in rec_list:
    if rec.load_object('task')['dataset']['kwargs']['segments']['test'] == max_test:
        latest_rec.append(rec)",latest_rec = [rec for rec in rec_list if rec.load_object('task')['dataset']['kwargs']['segments']['test'] == max_test],latest_rec = [rec for rec in rec_list if rec.load_object('task')['dataset']['kwargs']['segments']['test'] == max_test],1,,,,,robosuite
CHINESE-OCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CHINESE-OCR/ctpn/lib/rpn_msr/generate_anchors.py,https://github.com/xiaofengShi/CHINESE-OCR/tree/master/ctpn/lib/rpn_msr/generate_anchors.py,,generate_anchors$25,"def generate_anchors(base_size=16, ratios=[0.5, 1, 2], scales=2 ** np.arange(3, 6)):
    heights = [11, 16, 23, 33, 48, 68, 97, 139, 198, 283]
    widths = [16]
    sizes = []
    for h in heights:
        for w in widths:
            sizes.append((h, w))
    return generate_basic_anchors(sizes)","for h in heights:
    for w in widths:
        sizes.append((h, w))","sizes = [(h, w) for h in heights for w in widths]","sizes = [(h, w) for h in heights for w in widths]",1,,,,,robosuite
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/tests/protection/v3/test_domains.py,https://github.com/openstack/keystone/tree/master/keystone/tests/protection/v3/test_domains.py,_SystemUserDomainTests,test_user_can_list_domains$33,"def test_user_can_list_domains(self):
    domain = PROVIDERS.resource_api.create_domain(uuid.uuid4().hex, unit.new_domain_ref())
    with self.test_client() as c:
        r = c.get('/v3/domains', headers=self.headers)
        domain_ids = []
        for domain in r.json['domains']:
            domain_ids.append(domain['id'])
        self.assertIn(domain['id'], domain_ids)","for domain in r.json['domains']:
    domain_ids.append(domain['id'])",domain_ids = [domain['id'] for domain in r.json['domains']],domain_ids = [domain['id'] for domain in r.json['domains']],1,,,,,robosuite
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/lib/ansible/module_utils/common/process.py,https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/common/process.py,,get_bin_path$12,"def get_bin_path(arg, opt_dirs=None, required=None):
    """"""
    Find system executable in PATH. Raises ValueError if executable is not found.
    Optional arguments:
       - required:  [Deprecated] Prior to 2.10, if executable is not found and required is true it raises an Exception.
                    In 2.10 and later, an Exception is always raised. This parameter will be removed in 2.14.
       - opt_dirs:  optional list of directories to search in addition to PATH
    If found return full path, otherwise raise ValueError.
    """"""
    opt_dirs = [] if opt_dirs is None else opt_dirs
    sbin_paths = ['/sbin', '/usr/sbin', '/usr/local/sbin']
    paths = []
    for d in opt_dirs:
        if d is not None and os.path.exists(d):
            paths.append(d)
    paths += os.environ.get('PATH', '').split(os.pathsep)
    bin_path = None
    for p in sbin_paths:
        if p not in paths and os.path.exists(p):
            paths.append(p)
    for d in paths:
        if not d:
            continue
        path = os.path.join(d, arg)
        if os.path.exists(path) and (not os.path.isdir(path)) and is_executable(path):
            bin_path = path
            break
    if bin_path is None:
        raise ValueError('Failed to find required executable ""%s"" in paths: %s' % (arg, os.pathsep.join(paths)))
    return bin_path","for d in opt_dirs:
    if d is not None and os.path.exists(d):
        paths.append(d)",paths = [d for d in opt_dirs if d is not None and os.path.exists(d)],paths = [d for d in opt_dirs if d is not None and os.path.exists(d)],1,,,,,robosuite
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/lib/ansible/module_utils/common/process.py,https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/common/process.py,,get_bin_path$12,"def get_bin_path(arg, opt_dirs=None, required=None):
    """"""
    Find system executable in PATH. Raises ValueError if executable is not found.
    Optional arguments:
       - required:  [Deprecated] Prior to 2.10, if executable is not found and required is true it raises an Exception.
                    In 2.10 and later, an Exception is always raised. This parameter will be removed in 2.14.
       - opt_dirs:  optional list of directories to search in addition to PATH
    If found return full path, otherwise raise ValueError.
    """"""
    opt_dirs = [] if opt_dirs is None else opt_dirs
    sbin_paths = ['/sbin', '/usr/sbin', '/usr/local/sbin']
    paths = []
    for d in opt_dirs:
        if d is not None and os.path.exists(d):
            paths.append(d)
    paths += os.environ.get('PATH', '').split(os.pathsep)
    bin_path = None
    for p in sbin_paths:
        if p not in paths and os.path.exists(p):
            paths.append(p)
    for d in paths:
        if not d:
            continue
        path = os.path.join(d, arg)
        if os.path.exists(path) and (not os.path.isdir(path)) and is_executable(path):
            bin_path = path
            break
    if bin_path is None:
        raise ValueError('Failed to find required executable ""%s"" in paths: %s' % (arg, os.pathsep.join(paths)))
    return bin_path","for p in sbin_paths:
    if p not in paths and os.path.exists(p):
        paths.append(p)",paths += [p for p in sbin_paths if p not in paths and os.path.exists(p)],Cannot refactor,-1,1,,,,robosuite
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/common/djangoapps/terrain/stubs/http.py,https://github.com/edx/edx-platform/tree/master/common/djangoapps/terrain/stubs/http.py,,wrapper$39,"def wrapper(self, *args, **kwargs):
    if method == 'GET':
        params = self.get_params
    elif method == 'POST':
        params = self.post_dict
    else:
        raise ValueError(f""Unsupported method '{method}'"")
    missing = []
    for key in required_keys:
        if params.get(key) is None:
            missing.append(key)
    if len(missing) > 0:
        msg = 'Missing required key(s) {keys}'.format(keys=','.join(missing))
        self.send_response(400, content=msg, headers={'Content-type': 'text/plain'})
    else:
        return func(self, *args, **kwargs)","for key in required_keys:
    if params.get(key) is None:
        missing.append(key)",missing = [key for key in required_keys if params.get(key) is None],missing = [key for key in required_keys if params.get(key) is None],1,,,,,robosuite
rl_algorithms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rl_algorithms/rl_algorithms/common/saliency_map.py,https://github.com/medipixel/rl_algorithms/tree/master/rl_algorithms/common/saliency_map.py,,compute_saliency_maps$36,"def compute_saliency_maps(X, y, model, device):
    """"""Compute a class saliency map using the model for images X and labels y.""""""
    if isinstance(X, list):
        input_list = []
        for x in X:
            input_list.append(x.requires_grad_())
        saliency = None
        X = input_list
        (scores, _) = model(X[0], X[1], X[2], X[3])
        scores = scores.gather(1, y.unsqueeze(0)).squeeze(0)
        scores.backward(torch.FloatTensor([1.0]).to(device))
        (saliency, _) = torch.max(X[0].grad.data.abs(), dim=1)
    else:
        X.requires_grad_()
        saliency = None
        scores = model(X)
        scores = scores.gather(1, y.unsqueeze(0)).squeeze(0)
        scores.backward(torch.FloatTensor([1.0]).to(device))
        (saliency, _) = torch.max(X.grad.data.abs(), dim=1)
    return saliency","for x in X:
    input_list.append(x.requires_grad_())",input_list = [x.requires_grad_() for x in X],input_list = [x.requires_grad_() for x in X],1,,,,,robosuite
dedupe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dedupe/dedupe/variables/base.py,https://github.com/dedupeio/dedupe/tree/master/dedupe/variables/base.py,,indexPredicates$101,"def indexPredicates(predicates, thresholds, field):
    index_predicates = []
    for predicate in predicates:
        for threshold in thresholds:
            index_predicates.append(predicate(threshold, field))
    return index_predicates","for predicate in predicates:
    for threshold in thresholds:
        index_predicates.append(predicate(threshold, field))","index_predicates = [predicate(threshold, field) for predicate in predicates for threshold in thresholds]","index_predicates = [predicate(threshold, field) for predicate in predicates for threshold in thresholds]",1,,,,,robosuite
DeepLabCut,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,SORTBox,match_detections_to_trackers$708,"def match_detections_to_trackers(detections, trackers, iou_threshold):
    """"""
        Assigns detections to tracked object (both represented as bounding boxes)

        Returns 3 lists of matches, unmatched_detections and unmatched_trackers
        """"""
    if not len(trackers):
        return (np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int))
    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)
    for (d, det) in enumerate(detections):
        for (t, trk) in enumerate(trackers):
            iou_matrix[d, t] = calc_iou(det, trk)
    (row_indices, col_indices) = linear_sum_assignment(-iou_matrix)
    unmatched_detections = []
    for (d, det) in enumerate(detections):
        if d not in row_indices:
            unmatched_detections.append(d)
    unmatched_trackers = []
    for (t, trk) in enumerate(trackers):
        if t not in col_indices:
            unmatched_trackers.append(t)
    matches = []
    for (row, col) in zip(row_indices, col_indices):
        if iou_matrix[row, col] < iou_threshold:
            unmatched_detections.append(row)
            unmatched_trackers.append(col)
        else:
            matches.append([row, col])
    if not len(matches):
        matches = np.empty((0, 2), dtype=int)
    else:
        matches = np.stack(matches)
    return (matches, np.array(unmatched_detections), np.array(unmatched_trackers))","for (d, det) in enumerate(detections):
    if d not in row_indices:
        unmatched_detections.append(d)","unmatched_detections = [d for (d, det) in enumerate(detections) if d not in row_indices]","unmatched_detections = [d for (d, det) in enumerate(detections) if d not in row_indices]",1,,,,,robosuite
DeepLabCut,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,SORTBox,match_detections_to_trackers$708,"def match_detections_to_trackers(detections, trackers, iou_threshold):
    """"""
        Assigns detections to tracked object (both represented as bounding boxes)

        Returns 3 lists of matches, unmatched_detections and unmatched_trackers
        """"""
    if not len(trackers):
        return (np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int))
    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)
    for (d, det) in enumerate(detections):
        for (t, trk) in enumerate(trackers):
            iou_matrix[d, t] = calc_iou(det, trk)
    (row_indices, col_indices) = linear_sum_assignment(-iou_matrix)
    unmatched_detections = []
    for (d, det) in enumerate(detections):
        if d not in row_indices:
            unmatched_detections.append(d)
    unmatched_trackers = []
    for (t, trk) in enumerate(trackers):
        if t not in col_indices:
            unmatched_trackers.append(t)
    matches = []
    for (row, col) in zip(row_indices, col_indices):
        if iou_matrix[row, col] < iou_threshold:
            unmatched_detections.append(row)
            unmatched_trackers.append(col)
        else:
            matches.append([row, col])
    if not len(matches):
        matches = np.empty((0, 2), dtype=int)
    else:
        matches = np.stack(matches)
    return (matches, np.array(unmatched_detections), np.array(unmatched_trackers))","for (t, trk) in enumerate(trackers):
    if t not in col_indices:
        unmatched_trackers.append(t)","unmatched_trackers = [t for (t, trk) in enumerate(trackers) if t not in col_indices]","unmatched_trackers = [t for (t, trk) in enumerate(trackers) if t not in col_indices]",1,,,,,robosuite
DeepLabCut,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,SORTBox,match_detections_to_trackers$708,"def match_detections_to_trackers(detections, trackers, iou_threshold):
    """"""
        Assigns detections to tracked object (both represented as bounding boxes)

        Returns 3 lists of matches, unmatched_detections and unmatched_trackers
        """"""
    if not len(trackers):
        return (np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int))
    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)
    for (d, det) in enumerate(detections):
        for (t, trk) in enumerate(trackers):
            iou_matrix[d, t] = calc_iou(det, trk)
    (row_indices, col_indices) = linear_sum_assignment(-iou_matrix)
    unmatched_detections = []
    for (d, det) in enumerate(detections):
        if d not in row_indices:
            unmatched_detections.append(d)
    unmatched_trackers = []
    for (t, trk) in enumerate(trackers):
        if t not in col_indices:
            unmatched_trackers.append(t)
    matches = []
    for (row, col) in zip(row_indices, col_indices):
        if iou_matrix[row, col] < iou_threshold:
            unmatched_detections.append(row)
            unmatched_trackers.append(col)
        else:
            matches.append([row, col])
    if not len(matches):
        matches = np.empty((0, 2), dtype=int)
    else:
        matches = np.stack(matches)
    return (matches, np.array(unmatched_detections), np.array(unmatched_trackers))","for (row, col) in zip(row_indices, col_indices):
    if iou_matrix[row, col] < iou_threshold:
        unmatched_detections.append(row)
        unmatched_trackers.append(col)
    else:
        matches.append([row, col])","unmatched_trackers = [col for (row, col) in zip(row_indices, col_indices) if iou_matrix[row, col] < iou_threshold]",Cannot refactor,-1,0,,2,1,robosuite
plaso,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plaso/plaso/filters/parser_filter.py,https://github.com/log2timeline/plaso/tree/master/plaso/filters/parser_filter.py,ParserFilterExpressionHelper,_GetParserAndPluginsList$22,"def _GetParserAndPluginsList(self, parsers_and_plugins):
    """"""Flattens the parsers and plugins dictionary into a list.

    Args:
      parsers_and_plugins (dict[str, set[str]]): parsers and plugins.

    Returns:
      list[str]: alphabetically sorted list of the parsers and plugins.
    """"""
    parser_filters = []
    for (parser_name, plugins) in sorted(parsers_and_plugins.items()):
        for plugin_name in sorted(plugins):
            if plugin_name == '*':
                parser_filters.append(parser_name)
            else:
                parser_filters.append('/'.join([parser_name, plugin_name]))
    return parser_filters","for (parser_name, plugins) in sorted(parsers_and_plugins.items()):
    for plugin_name in sorted(plugins):
        if plugin_name == '*':
            parser_filters.append(parser_name)
        else:
            parser_filters.append('/'.join([parser_name, plugin_name]))","parser_filters = [parser_name if plugin_name == '*' else '/'.join([parser_name, plugin_name]) for (parser_name, plugins) in sorted(parsers_and_plugins.items()) for plugin_name in sorted(plugins)]","parser_filters = [parser_name if plugin_name == '*' else '/'.join([parser_name, plugin_name]) for (parser_name, plugins) in sorted(parsers_and_plugins.items()) for plugin_name in sorted(plugins)]",1,,,,,robosuite
fedlearner,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fedlearner/fedlearner/data_join/cmd/rsa_psi_preprocessor_cli.py,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/cmd/rsa_psi_preprocessor_cli.py,,if_main_my$29,"if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Rsa Psi Preprocessor!')
    parser.add_argument('--preprocessor_name', type=str, default='test', help='the name of rsa psi preprocessor')
    parser.add_argument('-r', '--psi_role', type=str, required=True, help='the role of rsa psi(Leader/Follower)')
    parser.add_argument('--rsa_key_path', type=str, help='the file path for the rsa key')
    parser.add_argument('--rsa_key_pem', type=str, help='the rsa key stroe by pem format')
    parser.add_argument('--input_file_paths', type=str, nargs='+', help='the file path input rsa psi preprocessor')
    parser.add_argument('--input_dir', type=str, help='the raw data file appointed by dir')
    parser.add_argument('--input_file_subscribe_dir', type=str, default='', help='if the use appoint the args, then will ignore input file paths and input dir')
    parser.add_argument('--output_file_dir', type=str, required=True, help='the directory to store the result of processor')
    parser.add_argument('--raw_data_publish_dir', type=str, required=True, help='the mysql base dir to publish new raw data')
    parser.add_argument('--leader_rsa_psi_signer_addr', type=str, help='the ras psi follower should set give the addr of rsa psi signer of leader')
    parser.add_argument('--process_batch_size', type=int, default=128, help='the batch size for preprocessor')
    parser.add_argument('--max_flying_sign_batch', type=int, default=1024, help='the max flying sign batch')
    parser.add_argument('--max_flying_sign_rpc', type=int, default=128, help='the max flying sign rpc request')
    parser.add_argument('--sign_rpc_timeout_ms', type=int, default=64000, help='the rpc time ms for rpc sign')
    parser.add_argument('--stub_fanout', type=int, default=4, help='the max stub for follower of rpc of processor')
    parser.add_argument('--slow_sign_threshold', type=int, default=16, help='the threshold to record as slow sign')
    parser.add_argument('--sort_run_merger_read_ahead_buffer', type=int, default=512 << 10, help='the read ahead buffer for the reader of sort run reader')
    parser.add_argument('--sort_run_merger_read_batch_size', type=int, default=64, help='the read batch size for the sort run reader')
    parser.add_argument('--partition_id', type=int, required=True, help='the partition id will be processed')
    parser.add_argument('--kvstore_type', type=str, default='etcd', help='the type of kvstore')
    parser.add_argument('--raw_data_iter', type=str, default='TF_RECORD', choices=['TF_RECORD', 'CSV_DICT'], help='the type for raw data file')
    parser.add_argument('--compressed_type', type=str, default='', choices=['', 'ZLIB', 'GZIP'], help='the compressed type for raw data')
    parser.add_argument('--read_ahead_size', type=int, default=8 << 20, help='the read ahead size for raw data')
    parser.add_argument('--read_batch_size', type=int, default=512, help='the read batch size for tf record iter')
    parser.add_argument('--output_builder', type=str, default='TF_RECORD', choices=['TF_RECORD', 'CSV_DICT'], help='the builder for ouput file')
    parser.add_argument('--builder_compressed_type', type=str, default='', choices=['', 'ZLIB', 'GZIP'], help='the compressed type for TF_RECORD builder')
    parser.add_argument('--preprocessor_offload_processor_number', type=int, default=-1, help='the offload processor for preprocessor')
    args = parser.parse_args()
    set_logger()
    if args.raw_data_iter == 'TF_RECORD' or args.output_builder == 'TF_RECORD':
        import tensorflow
        tensorflow.compat.v1.enable_eager_execution()
    all_fpaths = []
    if len(args.input_file_subscribe_dir) == 0:
        if args.input_file_paths is not None:
            for fp in args.input_file_paths:
                all_fpaths.append(fp)
        if args.input_dir is not None:
            all_fpaths += [os.path.join(args.input_dir, f) for f in gfile.ListDirectory(args.input_dir)]
        if len(all_fpaths) == 0:
            raise RuntimeError('no input files for preprocessor')
    rsa_key_pem = args.rsa_key_pem
    if rsa_key_pem is None or len(rsa_key_pem) == 0:
        assert args.rsa_key_path is not None
        with gfile.GFile(args.rsa_key_path, 'rb') as f:
            rsa_key_pem = f.read()
    offload_processor_number = args.preprocessor_offload_processor_number
    if offload_processor_number < 0:
        offload_processor_number = int(os.environ.get('CPU_LIMIT', '2')) - 1
    if offload_processor_number < 1:
        offload_processor_number = 1
    preprocessor_options = dj_pb.RsaPsiPreProcessorOptions(preprocessor_name=args.preprocessor_name, rsa_key_pem=rsa_key_pem, input_file_paths=list(set(all_fpaths)), input_file_subscribe_dir=args.input_file_subscribe_dir, output_file_dir=args.output_file_dir, raw_data_publish_dir=args.raw_data_publish_dir, partition_id=args.partition_id, leader_rsa_psi_signer_addr=args.leader_rsa_psi_signer_addr, offload_processor_number=offload_processor_number, max_flying_sign_batch=args.max_flying_sign_batch, max_flying_sign_rpc=args.max_flying_sign_rpc, sign_rpc_timeout_ms=args.sign_rpc_timeout_ms, stub_fanout=args.stub_fanout, slow_sign_threshold=args.slow_sign_threshold, sort_run_merger_read_ahead_buffer=args.sort_run_merger_read_ahead_buffer, sort_run_merger_read_batch_size=args.sort_run_merger_read_batch_size, batch_processor_options=dj_pb.BatchProcessorOptions(batch_size=args.process_batch_size, max_flying_item=-1), input_raw_data=dj_pb.RawDataOptions(raw_data_iter=args.raw_data_iter, compressed_type=args.compressed_type, read_ahead_size=args.read_ahead_size, read_batch_size=args.read_batch_size), writer_options=dj_pb.WriterOptions(output_writer=args.output_builder, compressed_type=args.builder_compressed_type))
    if args.psi_role.upper() == 'LEADER':
        preprocessor_options.role = common_pb.FLRole.Leader
    else:
        assert args.psi_role.upper() == 'FOLLOWER'
        preprocessor_options.role = common_pb.FLRole.Follower
    preprocessor = RsaPsiPreProcessor(preprocessor_options, args.kvstore_type)
    preprocessor.start_process()
    logging.info('PreProcessor launched for %s of RSA PSI', args.psi_role)
    preprocessor.wait_for_finished()
    logging.info('PreProcessor finished for %s of RSA PSI', args.psi_role)","for fp in args.input_file_paths:
    all_fpaths.append(fp)",all_fpaths = [fp for fp in args.input_file_paths],all_fpaths = [fp for fp in args.input_file_paths],1,,,,,robosuite
fredapi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fredapi/fredapi/fred.py,https://github.com/mortada/fredapi/tree/master/fredapi/fred.py,Fred,get_series_vintage_dates$258,"def get_series_vintage_dates(self, series_id):
    """"""
        Get a list of vintage dates for a series. Vintage dates are the dates in history when a
        series' data values were revised or new data values were released.

        Parameters
        ----------
        series_id : str
            Fred series id such as 'CPIAUCSL'

        Returns
        -------
        dates : list
            list of vintage dates
        """"""
    url = '%s/series/vintagedates?series_id=%s' % (self.root_url, series_id)
    root = self.__fetch_data(url)
    if root is None:
        raise ValueError('No vintage date exists for series id: ' + series_id)
    dates = []
    for child in root:
        dates.append(self._parse(child.text))
    return dates","for child in root:
    dates.append(self._parse(child.text))",dates = [self._parse(child.text) for child in root],dates = [self._parse(child.text) for child in root],1,,,,,robosuite
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py,https://github.com/apache/airflow/tree/master/airflow/providers/microsoft/azure/hooks/cosmos.py,AzureCosmosDBHook,insert_documents$260,"def insert_documents(self, documents, database_name: Optional[str]=None, collection_name: Optional[str]=None) -> list:
    """"""Insert a list of new documents into an existing collection in the CosmosDB database.""""""
    if documents is None:
        raise AirflowBadRequest('You cannot insert empty documents')
    created_documents = []
    for single_document in documents:
        created_documents.append(self.get_conn().CreateItem(get_collection_link(self.__get_database_name(database_name), self.__get_collection_name(collection_name)), single_document))
    return created_documents","for single_document in documents:
    created_documents.append(self.get_conn().CreateItem(get_collection_link(self.__get_database_name(database_name), self.__get_collection_name(collection_name)), single_document))","created_documents = [self.get_conn().CreateItem(get_collection_link(self.__get_database_name(database_name), self.__get_collection_name(collection_name)), single_document) for single_document in documents]","created_documents = [self.get_conn().CreateItem(get_collection_link(self.__get_database_name(database_name), self.__get_collection_name(collection_name)), single_document) for single_document in documents]",1,,,,,robosuite
python-binance,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-binance/tests/test_historical_klines.py,https://github.com/sammchardy/python-binance/tree/master/tests/test_historical_klines.py,,test_historical_kline_generator$184,"def test_historical_kline_generator():
    """"""Test kline historical generator""""""
    first_available_res = [[1500004800000, '0.00005000', '0.00005300', '0.00001000', '0.00004790', '663152.00000000', 1500004859999, '30.55108144', 43, '559224.00000000', '25.65468144', '83431971.04346950']]
    first_res = []
    row = [1519892340000, '0.00099400', '0.00099810', '0.00099400', '0.00099810', '4806.04000000', 1519892399999, '4.78553253', 154, '1785.14000000', '1.77837524', '0']
    for i in range(0, 300):
        first_res.append(row)
    with requests_mock.mock() as m:
        m.get('https://api.binance.com/api/v3/klines?interval=1m&limit=1&startTime=0&symbol=BNBBTC', json=first_available_res)
        m.get('https://api.binance.com/api/v3/klines?interval=1m&limit=1000&startTime=1519862400000&endTime=1519880400000&symbol=BNBBTC', json=first_res)
        klines = client.get_historical_klines_generator(symbol='BNBBTC', interval=Client.KLINE_INTERVAL_1MINUTE, start_str=1519862400000, end_str=1519880400000)
        for i in range(300):
            assert len(next(klines)) > 0
        with pytest.raises(StopIteration):
            next(klines)","for i in range(0, 300):
    first_res.append(row)","first_res = [row for i in range(0, 300)]","first_res = [row for i in range(0, 300)]",1,,,,,robosuite
SOLO,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SOLO/tools/analyze_logs.py,https://github.com/WXinlong/SOLO/tree/master/tools/analyze_logs.py,,cal_train_time$10,"def cal_train_time(log_dicts, args):
    for (i, log_dict) in enumerate(log_dicts):
        print('{}Analyze train time of {}{}'.format('-' * 5, args.json_logs[i], '-' * 5))
        all_times = []
        for epoch in log_dict.keys():
            if args.include_outliers:
                all_times.append(log_dict[epoch]['time'])
            else:
                all_times.append(log_dict[epoch]['time'][1:])
        all_times = np.array(all_times)
        epoch_ave_time = all_times.mean(-1)
        slowest_epoch = epoch_ave_time.argmax()
        fastest_epoch = epoch_ave_time.argmin()
        std_over_epoch = epoch_ave_time.std()
        print('slowest epoch {}, average time is {:.4f}'.format(slowest_epoch + 1, epoch_ave_time[slowest_epoch]))
        print('fastest epoch {}, average time is {:.4f}'.format(fastest_epoch + 1, epoch_ave_time[fastest_epoch]))
        print('time std over epochs is {:.4f}'.format(std_over_epoch))
        print('average iter time: {:.4f} s/iter'.format(np.mean(all_times)))
        print()","for epoch in log_dict.keys():
    if args.include_outliers:
        all_times.append(log_dict[epoch]['time'])
    else:
        all_times.append(log_dict[epoch]['time'][1:])",all_times = [log_dict[epoch]['time'] if args.include_outliers else log_dict[epoch]['time'][1:] for epoch in log_dict.keys()],all_times = [log_dict[epoch]['time'] if args.include_outliers else log_dict[epoch]['time'][1:] for epoch in log_dict.keys()],1,,,,,robosuite
not-youtube-dl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tagesschau.py,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tagesschau.py,TagesschauIE,_real_extract$265,"def _real_extract(self, url):
    mobj = re.match(self._VALID_URL, url)
    video_id = mobj.group('id') or mobj.group('path')
    display_id = video_id.lstrip('-')
    webpage = self._download_webpage(url, display_id)
    title = self._html_search_regex('<span[^>]*class=""headline""[^>]*>(.+?)</span>', webpage, 'title', default=None) or self._og_search_title(webpage)
    DOWNLOAD_REGEX = '(?s)<p>Wir bieten dieses (?P<kind>Video|Audio) in folgenden Formaten zum Download an:</p>\\s*<div class=""controls"">(?P<links>.*?)</div>\\s*<p>'
    webpage_type = self._og_search_property('type', webpage, default=None)
    if webpage_type == 'website':
        entries = []
        for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1):
            entries.append({'id': '%s-%d' % (display_id, num), 'title': '%s' % entry_title, 'formats': self._extract_formats(download_text, media_kind)})
        if len(entries) > 1:
            return self.playlist_result(entries, display_id, title)
        formats = entries[0]['formats']
    else:
        download_text = self._search_regex(DOWNLOAD_REGEX, webpage, 'download links', group='links')
        media_kind = self._search_regex(DOWNLOAD_REGEX, webpage, 'media kind', default='Video', group='kind')
        formats = self._extract_formats(download_text, media_kind)
    thumbnail = self._og_search_thumbnail(webpage)
    description = self._html_search_regex('(?s)<p class=""teasertext"">(.*?)</p>', webpage, 'description', default=None)
    self._sort_formats(formats)
    return {'id': display_id, 'title': title, 'thumbnail': thumbnail, 'formats': formats, 'description': description}","for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1):
    entries.append({'id': '%s-%d' % (display_id, num), 'title': '%s' % entry_title, 'formats': self._extract_formats(download_text, media_kind)})","entries = [{'id': '%s-%d' % (display_id, num), 'title': '%s' % entry_title, 'formats': self._extract_formats(download_text, media_kind)} for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1)]","entries = [{'id': '%s-%d' % (display_id, num), 'title': '%s' % entry_title, 'formats': self._extract_formats(download_text, media_kind)} for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1)]",1,,,,,robosuite
dataprep,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dataprep/dataprep/eda/distribution/render.py,https://github.com/sfu-db/dataprep/tree/master/dataprep/eda/distribution/render.py,,geo_viz$661,"def geo_viz(df: pd.DataFrame, plot_width: int, y: Optional[str]=None) -> Panel:
    """"""
    Render a geo plot visualization
    """"""
    minimum = min(df[y])
    maximum = max(df[y])
    value = {}
    names = NAME_DICT.keys()
    for i in range(df[y].shape[0]):
        if df.index[i].lower().strip() in names:
            value[NAME_DICT[df.index[i].lower().strip()]] = df[y][i]
    temp_list = []
    for itr in range(len(MAPS['name'])):
        temp_list.append(value.get(MAPS['fip'][itr], 'unknown'))
    MAPS['value'] = temp_list
    mapper = LinearColorMapper(palette=YlGnBu[33:233], low=minimum, high=maximum, nan_color='#cccccc')
    tools = 'pan,wheel_zoom,box_zoom,reset,hover'
    fig = Figure(plot_width=plot_width, plot_height=plot_width // 10 * 7, tools=tools, tooltips=[('Name', '@name'), (y, '@value'), ('(Long, Lat)', '($x, $y)')])
    fig.grid.grid_line_color = None
    fig.hover.point_policy = 'follow_mouse'
    fig.background_fill_color = 'white'
    fig.x_range = Range1d(start=-180, end=180)
    fig.y_range = Range1d(start=-90, end=90)
    fig.patches('xs', 'ys', line_color='white', source=MAPS, fill_color={'field': 'value', 'transform': mapper}, line_width=0.5)
    color_bar = ColorBar(color_mapper=mapper, major_label_text_font_size='7px', ticker=BasicTicker(desired_num_ticks=11), formatter=PrintfTickFormatter(format='%10.2f'), label_standoff=6, border_line_color=None, location=(0, 0))
    if minimum < maximum:
        fig.add_layout(color_bar, 'right')
    return Panel(child=row(fig), title='World Map')","for itr in range(len(MAPS['name'])):
    temp_list.append(value.get(MAPS['fip'][itr], 'unknown'))","temp_list = [value.get(MAPS['fip'][itr], 'unknown') for itr in range(len(MAPS['name']))]","temp_list = [value.get(MAPS['fip'][itr], 'unknown') for itr in range(len(MAPS['name']))]",1,,,,,robosuite
pycorrector,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycorrector/pycorrector/macbert/preprocess.py,https://github.com/shibing624/pycorrector/tree/master/pycorrector/macbert/preprocess.py,,proc_item$33,"def proc_item(item):
    """"""
    婢跺嫮鎮婄拋缂佸啯鏆熼幑闂
    Args:
        item:
    Returns:
        list
    """"""
    root = etree.XML(item)
    passages = dict()
    mistakes = []
    for passage in root.xpath('/ESSAY/TEXT/PASSAGE'):
        passages[passage.get('id')] = traditional2simplified(passage.text)
    for mistake in root.xpath('/ESSAY/MISTAKE'):
        mistakes.append({'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())})
    rst_items = dict()

    def get_passages_by_id(pgs, _id):
        p = pgs.get(_id)
        if p:
            return p
        _id = _id[:-1] + str(int(_id[-1]) + 1)
        p = pgs.get(_id)
        if p:
            return p
        raise ValueError(f'passage not found by {_id}')
    for mistake in mistakes:
        if mistake['id'] not in rst_items.keys():
            rst_items[mistake['id']] = {'original_text': get_passages_by_id(passages, mistake['id']), 'wrong_ids': [], 'correct_text': get_passages_by_id(passages, mistake['id'])}
        ori_text = rst_items[mistake['id']]['original_text']
        cor_text = rst_items[mistake['id']]['correct_text']
        if len(ori_text) == len(cor_text):
            if ori_text[mistake['location']] in mistake['wrong']:
                rst_items[mistake['id']]['wrong_ids'].append(mistake['location'])
                wrong_char_idx = mistake['wrong'].index(ori_text[mistake['location']])
                start = mistake['location'] - wrong_char_idx
                end = start + len(mistake['wrong'])
                rst_items[mistake['id']]['correct_text'] = f""{cor_text[:start]}{mistake['correction']}{cor_text[end:]}""
        else:
            print(f""error line:\n{mistake['id']}\n{ori_text}\n{cor_text}"")
    rst = []
    for k in rst_items.keys():
        if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']):
            rst.append({'id': k, **rst_items[k]})
        else:
            text = rst_items[k]['correct_text']
            rst.append({'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []})
    return rst","for mistake in root.xpath('/ESSAY/MISTAKE'):
    mistakes.append({'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())})","mistakes = [{'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())} for mistake in root.xpath('/ESSAY/MISTAKE')]","mistakes = [{'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())} for mistake in root.xpath('/ESSAY/MISTAKE')]",1,,,,,robosuite
pycorrector,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycorrector/pycorrector/macbert/preprocess.py,https://github.com/shibing624/pycorrector/tree/master/pycorrector/macbert/preprocess.py,,proc_item$33,"def proc_item(item):
    """"""
    婢跺嫮鎮婄拋缂佸啯鏆熼幑闂
    Args:
        item:
    Returns:
        list
    """"""
    root = etree.XML(item)
    passages = dict()
    mistakes = []
    for passage in root.xpath('/ESSAY/TEXT/PASSAGE'):
        passages[passage.get('id')] = traditional2simplified(passage.text)
    for mistake in root.xpath('/ESSAY/MISTAKE'):
        mistakes.append({'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())})
    rst_items = dict()

    def get_passages_by_id(pgs, _id):
        p = pgs.get(_id)
        if p:
            return p
        _id = _id[:-1] + str(int(_id[-1]) + 1)
        p = pgs.get(_id)
        if p:
            return p
        raise ValueError(f'passage not found by {_id}')
    for mistake in mistakes:
        if mistake['id'] not in rst_items.keys():
            rst_items[mistake['id']] = {'original_text': get_passages_by_id(passages, mistake['id']), 'wrong_ids': [], 'correct_text': get_passages_by_id(passages, mistake['id'])}
        ori_text = rst_items[mistake['id']]['original_text']
        cor_text = rst_items[mistake['id']]['correct_text']
        if len(ori_text) == len(cor_text):
            if ori_text[mistake['location']] in mistake['wrong']:
                rst_items[mistake['id']]['wrong_ids'].append(mistake['location'])
                wrong_char_idx = mistake['wrong'].index(ori_text[mistake['location']])
                start = mistake['location'] - wrong_char_idx
                end = start + len(mistake['wrong'])
                rst_items[mistake['id']]['correct_text'] = f""{cor_text[:start]}{mistake['correction']}{cor_text[end:]}""
        else:
            print(f""error line:\n{mistake['id']}\n{ori_text}\n{cor_text}"")
    rst = []
    for k in rst_items.keys():
        if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']):
            rst.append({'id': k, **rst_items[k]})
        else:
            text = rst_items[k]['correct_text']
            rst.append({'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []})
    return rst","for k in rst_items.keys():
    if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']):
        rst.append({'id': k, **rst_items[k]})
    else:
        text = rst_items[k]['correct_text']
        rst.append({'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []})","rst += [{'id': k, **rst_items[k]} if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']) else {'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []} for (k, text) in [(k, rst_items[k]['correct_text']) for k in rst_items.keys()]]",Cannot refactor,-1,1,,,,robosuite
xlnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xlnet/tpu_estimator.py,https://github.com/zihangdai/xlnet/tree/master//tpu_estimator.py,TPUEstimator,_call_model_fn_for_inference$2275,"def _call_model_fn_for_inference(self, features, labels, mode, config):
    """"""Wraps `_call_model_fn` for `export_savedmodel`.""""""
    if mode != _REWRITE_FOR_INFERENCE_MODE:
        raise ValueError('mode must be {}; got {}.'.format(_REWRITE_FOR_INFERENCE_MODE, mode))
    capture = _CapturedObject()

    def computation():
        """"""Compute tpu tensors used in export_outputs.

      Passed to rewrite_for_inference so that model_fn will be called under
      the rewriting contexts. Only tpu tensors are returned, but export_outputs
      and scaffold are captured.

      Returns:
         A list of Tensors used in export_outputs and not marked for
         outside_compilation.
      """"""
        mode = model_fn_lib.ModeKeys.PREDICT
        estimator_spec = self._call_model_fn(features, labels, mode, config)
        tensors_dict = collections.OrderedDict(((k, _export_output_to_tensors(v)) for (k, v) in six.iteritems(estimator_spec.export_outputs)))
        tensors = nest.flatten(tensors_dict)
        tpu_tensors = [t for t in tensors if t is not None]
        capture.capture((estimator_spec, tensors_dict, tensors))
        return tpu_tensors
    tpu_tensors_on_cpu = tpu.rewrite_for_inference(computation)
    (estimator_spec, tensors_dict, tensors) = capture.get()
    new_tensors = []
    for t in tensors:
        if t is None:
            new_tensors.append(None)
        else:
            new_tensors.append(tpu_tensors_on_cpu.pop(0))
    new_tensors_dict = nest.pack_sequence_as(tensors_dict, new_tensors)
    export_outputs = estimator_spec.export_outputs
    new_export_outputs = collections.OrderedDict(((k, _clone_export_output_with_tensors(export_outputs[k], v)) for (k, v) in six.iteritems(new_tensors_dict)))
    return estimator_spec._replace(export_outputs=new_export_outputs)","for t in tensors:
    if t is None:
        new_tensors.append(None)
    else:
        new_tensors.append(tpu_tensors_on_cpu.pop(0))",new_tensors = [tpu_tensors_on_cpu.pop(0) if t is not None else None for t in tensors],new_tensors = [None if t is None else tpu_tensors_on_cpu.pop(0) for t in tensors],0,1,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/ansiblegate.py,https://github.com/saltstack/salt/tree/master/salt/modules/ansiblegate.py,,call$175,"def call(module, *args, **kwargs):
    """"""
    Call an Ansible module by invoking it.

    :param module: the name of the module.
    :param args: Arguments to pass to the module
    :param kwargs: keywords to pass to the module

    CLI Example:

    .. code-block:: bash

        salt * ansible.call ping data=foobar
    """"""
    module_args = []
    for arg in args:
        module_args.append(salt.utils.json.dumps(arg))
    _kwargs = {}
    for _kw in kwargs.get('__pub_arg', []):
        if isinstance(_kw, dict):
            _kwargs = _kw
            break
    else:
        _kwargs = {k: v for (k, v) in kwargs.items() if not k.startswith('__pub')}
    for (key, value) in _kwargs.items():
        module_args.append('{}={}'.format(key, salt.utils.json.dumps(value)))
    with NamedTemporaryFile(mode='w') as inventory:
        ansible_binary_path = salt.utils.path.which('ansible')
        log.debug('Calling ansible module %r', module)
        try:
            proc_exc = subprocess.run([ansible_binary_path, 'localhost', '--limit', '127.0.0.1', '-m', module, '-a', ' '.join(module_args), '-i', inventory.name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=__opts__.get('ansible_timeout', DEFAULT_TIMEOUT), universal_newlines=True, check=True, shell=False)
            original_output = proc_exc.stdout
            proc_out = original_output.splitlines()
            if proc_out[0].endswith('{'):
                proc_out[0] = '{'
                try:
                    out = salt.utils.json.loads('\n'.join(proc_out))
                except ValueError as exc:
                    out = {'Error': proc_exc.stderr or str(exc), 'Output': original_output}
                    return out
            elif proc_out[0].endswith('>>'):
                out = {'output': '\n'.join(proc_out[1:])}
            else:
                out = {'output': original_output}
        except subprocess.CalledProcessError as exc:
            out = {'Exitcode': exc.returncode, 'Error': exc.stderr or str(exc)}
            if exc.stdout:
                out['Given JSON output'] = exc.stdout
            return out
    for key in ('invocation', 'changed'):
        out.pop(key, None)
    return out","for arg in args:
    module_args.append(salt.utils.json.dumps(arg))",module_args = [salt.utils.json.dumps(arg) for arg in args],module_args = [salt.utils.json.dumps(arg) for arg in args],1,,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/ansiblegate.py,https://github.com/saltstack/salt/tree/master/salt/modules/ansiblegate.py,,call$175,"def call(module, *args, **kwargs):
    """"""
    Call an Ansible module by invoking it.

    :param module: the name of the module.
    :param args: Arguments to pass to the module
    :param kwargs: keywords to pass to the module

    CLI Example:

    .. code-block:: bash

        salt * ansible.call ping data=foobar
    """"""
    module_args = []
    for arg in args:
        module_args.append(salt.utils.json.dumps(arg))
    _kwargs = {}
    for _kw in kwargs.get('__pub_arg', []):
        if isinstance(_kw, dict):
            _kwargs = _kw
            break
    else:
        _kwargs = {k: v for (k, v) in kwargs.items() if not k.startswith('__pub')}
    for (key, value) in _kwargs.items():
        module_args.append('{}={}'.format(key, salt.utils.json.dumps(value)))
    with NamedTemporaryFile(mode='w') as inventory:
        ansible_binary_path = salt.utils.path.which('ansible')
        log.debug('Calling ansible module %r', module)
        try:
            proc_exc = subprocess.run([ansible_binary_path, 'localhost', '--limit', '127.0.0.1', '-m', module, '-a', ' '.join(module_args), '-i', inventory.name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=__opts__.get('ansible_timeout', DEFAULT_TIMEOUT), universal_newlines=True, check=True, shell=False)
            original_output = proc_exc.stdout
            proc_out = original_output.splitlines()
            if proc_out[0].endswith('{'):
                proc_out[0] = '{'
                try:
                    out = salt.utils.json.loads('\n'.join(proc_out))
                except ValueError as exc:
                    out = {'Error': proc_exc.stderr or str(exc), 'Output': original_output}
                    return out
            elif proc_out[0].endswith('>>'):
                out = {'output': '\n'.join(proc_out[1:])}
            else:
                out = {'output': original_output}
        except subprocess.CalledProcessError as exc:
            out = {'Exitcode': exc.returncode, 'Error': exc.stderr or str(exc)}
            if exc.stdout:
                out['Given JSON output'] = exc.stdout
            return out
    for key in ('invocation', 'changed'):
        out.pop(key, None)
    return out","for (key, value) in _kwargs.items():
    module_args.append('{}={}'.format(key, salt.utils.json.dumps(value)))","module_args += ['{}={}'.format(key, salt.utils.json.dumps(value)) for (key, value) in _kwargs.items()]",Cannot refactor,-1,1,,,,robosuite
open-paperless,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open-paperless/mayan/apps/navigation/templatetags/navigation_tags.py,https://github.com/zhoubear/open-paperless/tree/master/mayan/apps/navigation/templatetags/navigation_tags.py,,get_menus_links$17,"def get_menus_links(context, names, source=None):
    result = []
    for name in names.split(','):
        for links in Menu.get(name=name).resolve(context=context):
            if links:
                result.append(links)
    return result","for name in names.split(','):
    for links in Menu.get(name=name).resolve(context=context):
        if links:
            result.append(links)","result = [links for name in names.split(',') for links in Menu.get(name=name).resolve(context=context) if links]","result = [links for name in names.split(',') for links in Menu.get(name=name).resolve(context=context) if links]",1,,,,,robosuite
aiortc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aiortc/src/aiortc/rtcrtpreceiver.py,https://github.com/aiortc/aiortc/tree/master/src/aiortc/rtcrtpreceiver.py,RTCRtpReceiver,getSynchronizationSources$328,"def getSynchronizationSources(self) -> List[RTCRtpSynchronizationSource]:
    """"""
        Returns a :class:`RTCRtpSynchronizationSource` for each unique SSRC identifier
        received in the last 10 seconds.
        """"""
    cutoff = clock.current_datetime() - datetime.timedelta(seconds=10)
    sources = []
    for (source, timestamp) in self.__active_ssrc.items():
        if timestamp >= cutoff:
            sources.append(RTCRtpSynchronizationSource(source=source, timestamp=timestamp))
    return sources","for (source, timestamp) in self.__active_ssrc.items():
    if timestamp >= cutoff:
        sources.append(RTCRtpSynchronizationSource(source=source, timestamp=timestamp))","sources = [RTCRtpSynchronizationSource(source=source, timestamp=timestamp) for (source, timestamp) in self.__active_ssrc.items() if timestamp >= cutoff]","sources = [RTCRtpSynchronizationSource(source=source, timestamp=timestamp) for (source, timestamp) in self.__active_ssrc.items() if timestamp >= cutoff]",1,,,,,robosuite
OWOD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OWOD/detectron2/evaluation/cityscapes_evaluation.py,https://github.com/JosephKJ/OWOD/tree/master/detectron2/evaluation/cityscapes_evaluation.py,CityscapesInstanceEvaluator,evaluate$88,"def evaluate(self):
    """"""
        Returns:
            dict: has a key ""segm"", whose value is a dict of ""AP"" and ""AP50"".
        """"""
    comm.synchronize()
    if comm.get_rank() > 0:
        return
    import cityscapesscripts.evaluation.evalInstanceLevelSemanticLabeling as cityscapes_eval
    self._logger.info('Evaluating results under {} ...'.format(self._temp_dir))
    cityscapes_eval.args.predictionPath = os.path.abspath(self._temp_dir)
    cityscapes_eval.args.predictionWalk = None
    cityscapes_eval.args.JSONOutput = False
    cityscapes_eval.args.colorized = False
    cityscapes_eval.args.gtInstancesFile = os.path.join(self._temp_dir, 'gtInstances.json')
    gt_dir = PathManager.get_local_path(self._metadata.gt_dir)
    groundTruthImgList = glob.glob(os.path.join(gt_dir, '*', '*_gtFine_instanceIds.png'))
    assert len(groundTruthImgList), 'Cannot find any ground truth images to use for evaluation. Searched for: {}'.format(cityscapes_eval.args.groundTruthSearch)
    predictionImgList = []
    for gt in groundTruthImgList:
        predictionImgList.append(cityscapes_eval.getPrediction(gt, cityscapes_eval.args))
    results = cityscapes_eval.evaluateImgLists(predictionImgList, groundTruthImgList, cityscapes_eval.args)['averages']
    ret = OrderedDict()
    ret['segm'] = {'AP': results['allAp'] * 100, 'AP50': results['allAp50%'] * 100}
    self._working_dir.cleanup()
    return ret","for gt in groundTruthImgList:
    predictionImgList.append(cityscapes_eval.getPrediction(gt, cityscapes_eval.args))","predictionImgList = [cityscapes_eval.getPrediction(gt, cityscapes_eval.args) for gt in groundTruthImgList]","predictionImgList = [cityscapes_eval.getPrediction(gt, cityscapes_eval.args) for gt in groundTruthImgList]",1,,,,,robosuite
pysolr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pysolr/tests/test_client.py,https://github.com/django-haystack/pysolr/tree/master/tests/test_client.py,SolrTestCase,test_field_update_set$848,"def test_field_update_set(self):
    originalDocs = self.solr.search('doc')
    updated_popularity = 10
    self.assertEqual(len(originalDocs), 3)
    updateList = []
    for doc in originalDocs:
        updateList.append({'id': doc['id'], 'popularity': updated_popularity})
    self.solr.add(updateList, fieldUpdates={'popularity': 'set'}, commit=True)
    updatedDocs = self.solr.search('doc')
    self.assertEqual(len(updatedDocs), 3)
    for (originalDoc, updatedDoc) in zip(originalDocs, updatedDocs):
        self.assertEqual(len(updatedDoc.keys()), len(originalDoc.keys()))
        self.assertEqual(updatedDoc['popularity'], updated_popularity)
        self.assertTrue(all((updatedDoc[k] == originalDoc[k] for k in updatedDoc.keys() if k not in ['_version_', 'popularity'])))","for doc in originalDocs:
    updateList.append({'id': doc['id'], 'popularity': updated_popularity})","updateList = [{'id': doc['id'], 'popularity': updated_popularity} for doc in originalDocs]","updateList = [{'id': doc['id'], 'popularity': updated_popularity} for doc in originalDocs]",1,,,,,robosuite
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/circuits/trotter_exp_to_qgates.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/circuits/trotter_exp_to_qgates.py,,trotter_operator_grouping$67,"def trotter_operator_grouping(hamiltonian, trotter_number=1, trotter_order=1, term_ordering=None, k_exp=1.0):
    """"""Trotter-decomposes operators into groups without exponentiating.

    Operators are still Hermitian at the end of this method but have been
        multiplied by k_exp.

    Note:
        The default term_ordering is simply the ordered keys of
        the QubitOperators.terms dict.

    Args:
        hamiltonian (QubitOperator): full hamiltonian
        trotter_number (int): optional number of trotter steps -
            default is 1
        trotter_order (int): optional order of trotterization as
            an integer from 1-3 - default is 1
        term_ordering (list of (tuples of tuples)): optional list
            of QubitOperator terms dictionary keys that specifies
            order of terms when trotterizing
        k_exp (float): optional exponential factor
            to all terms when trotterizing

    Yields:
        QubitOperator generator

    Raises:
        ValueError if order > 3 or order <= 0,
        TypeError for incorrect types
    """"""
    if trotter_order > 3 or trotter_order <= 0:
        raise ValueError('Invalid trotter order: ' + str(trotter_order))
    if not isinstance(hamiltonian, QubitOperator):
        raise TypeError('Hamiltonian must be a QubitOperator.')
    if len(hamiltonian.terms) == 0:
        raise TypeError('Hamiltonian must be a non-empty QubitOperator.')
    if term_ordering is None:
        term_ordering = sorted(list(hamiltonian.terms.keys()))
    if len(term_ordering) == 0:
        raise TypeError('term_ordering must None or non-empty list.')
    k_exp = float(k_exp)
    if trotter_order == 1:
        for _ in range(trotter_number):
            for op in term_ordering:
                yield QubitOperator(op, hamiltonian.terms[op] * k_exp / trotter_number)
    elif trotter_order == 2:
        if len(term_ordering) < 2:
            raise ValueError('Not enough terms in the Hamiltonian to do ' + 'second order trotterization')
        for _ in range(trotter_number):
            for op in term_ordering[:-1]:
                yield QubitOperator(op, hamiltonian.terms[op] * k_exp / (2.0 * trotter_number))
            yield QubitOperator(term_ordering[-1], hamiltonian.terms[term_ordering[-1]] * k_exp / trotter_number)
            for op in reversed(term_ordering[:-1]):
                yield QubitOperator(op, hamiltonian.terms[op] * k_exp / (2.0 * trotter_number))
    elif trotter_order == 3:
        if len(term_ordering) < 2:
            raise ValueError('Not enough terms in the Hamiltonian to do ' + 'third order trotterization')
        ham = hamiltonian * k_exp / float(trotter_number)
        ham_temp = []
        for term in term_ordering:
            ham_temp.append(QubitOperator(term, ham.terms[term]))
        for _ in range(trotter_number):
            for returned_op in _third_order_trotter_helper(ham_temp):
                yield returned_op","for term in term_ordering:
    ham_temp.append(QubitOperator(term, ham.terms[term]))","ham_temp = [QubitOperator(term, ham.terms[term]) for term in term_ordering]","ham_temp = [QubitOperator(term, ham.terms[term]) for term in term_ordering]",1,,,,,robosuite
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/1268_Search_Suggestions_System.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/1268_Search_Suggestions_System.py,Solution,suggestedProducts$5,"def suggestedProducts(self, products, searchWord):
    """"""
        :type products: List[str]
        :type searchWord: str
        :rtype: List[List[str]]
        """"""
    products.sort()
    (result, prefix, startIdx) = ([], '', 0)
    for char in searchWord:
        prefix += char
        startIdx = bisect.bisect_left(products, prefix, startIdx)
        currnetSearchRes = []
        for product in products[startIdx:startIdx + 3]:
            if product.startswith(prefix):
                currnetSearchRes.append(product)
        result.append(currnetSearchRes)
    return result","for char in searchWord:
    prefix += char
    startIdx = bisect.bisect_left(products, prefix, startIdx)
    currnetSearchRes = []
    for product in products[startIdx:startIdx + 3]:
        if product.startswith(prefix):
            currnetSearchRes.append(product)
    result.append(currnetSearchRes)","result = [[product for product in products[bisect.bisect_left(products, prefix, startIdx):bisect.bisect_left(products, prefix, startIdx) + 3] if product.startswith(prefix)] for prefix in [searchWord[:i] for i in range(1, len(searchWord) + 1)]]",Cannot refactor,-1,1,,,,robosuite
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/1268_Search_Suggestions_System.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/1268_Search_Suggestions_System.py,Solution,suggestedProducts$5,"def suggestedProducts(self, products, searchWord):
    """"""
        :type products: List[str]
        :type searchWord: str
        :rtype: List[List[str]]
        """"""
    products.sort()
    (result, prefix, startIdx) = ([], '', 0)
    for char in searchWord:
        prefix += char
        startIdx = bisect.bisect_left(products, prefix, startIdx)
        currnetSearchRes = []
        for product in products[startIdx:startIdx + 3]:
            if product.startswith(prefix):
                currnetSearchRes.append(product)
        result.append(currnetSearchRes)
    return result","for product in products[startIdx:startIdx + 3]:
    if product.startswith(prefix):
        currnetSearchRes.append(product)",currnetSearchRes = [product for product in products[startIdx:startIdx + 3] if product.startswith(prefix)],currnetSearchRes = [product for product in products[startIdx:startIdx + 3] if product.startswith(prefix)],1,,,,,robosuite
pgadmin4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/utils/sqlautocomplete/autocomplete.py,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/utils/sqlautocomplete/autocomplete.py,SQLAutoComplete,fetch_foreign_keys$1288,"def fetch_foreign_keys(self, schemas):
    """"""
        This function is used to fetch the foreign_keys for the given
        schema name
        :param schemas:
        :return:
        """"""
    data = []
    query = render_template('/'.join([self.sql_path, 'foreign_keys.sql']), schema_names=schemas)
    if self.conn.connected():
        (status, res) = self.conn.execute_dict(query)
        if status:
            for row in res['rows']:
                data.append(ForeignKey(row['parentschema'], row['parenttable'], row['parentcolumn'], row['childschema'], row['childtable'], row['childcolumn']))
    return data","for row in res['rows']:
    data.append(ForeignKey(row['parentschema'], row['parenttable'], row['parentcolumn'], row['childschema'], row['childtable'], row['childcolumn']))","data = [ForeignKey(row['parentschema'], row['parenttable'], row['parentcolumn'], row['childschema'], row['childtable'], row['childcolumn']) for row in res['rows']]","data = [ForeignKey(row['parentschema'], row['parenttable'], row['parentcolumn'], row['childschema'], row['childtable'], row['childcolumn']) for row in res['rows']]",1,,,,,robosuite
FlowNetPytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FlowNetPytorch/main.py,https://github.com/ClementPinard/FlowNetPytorch/tree/master//main.py,,main$95,"def main():
    global args, best_EPE
    args = parser.parse_args()
    save_path = '{},{},{}epochs{},b{},lr{}'.format(args.arch, args.solver, args.epochs, ',epochSize' + str(args.epoch_size) if args.epoch_size > 0 else '', args.batch_size, args.lr)
    if not args.no_date:
        timestamp = datetime.datetime.now().strftime('%m-%d-%H:%M')
        save_path = os.path.join(timestamp, save_path)
    save_path = os.path.join(args.dataset, save_path)
    print('=> will save everything to {}'.format(save_path))
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    if args.seed_split is not None:
        np.random.seed(args.seed_split)
    train_writer = SummaryWriter(os.path.join(save_path, 'train'))
    test_writer = SummaryWriter(os.path.join(save_path, 'test'))
    output_writers = []
    for i in range(3):
        output_writers.append(SummaryWriter(os.path.join(save_path, 'test', str(i))))
    input_transform = transforms.Compose([flow_transforms.ArrayToTensor(), transforms.Normalize(mean=[0, 0, 0], std=[255, 255, 255]), transforms.Normalize(mean=[0.45, 0.432, 0.411], std=[1, 1, 1])])
    target_transform = transforms.Compose([flow_transforms.ArrayToTensor(), transforms.Normalize(mean=[0, 0], std=[args.div_flow, args.div_flow])])
    if 'KITTI' in args.dataset:
        args.sparse = True
    if args.sparse:
        co_transform = flow_transforms.Compose([flow_transforms.RandomCrop((320, 448)), flow_transforms.RandomVerticalFlip(), flow_transforms.RandomHorizontalFlip()])
    else:
        co_transform = flow_transforms.Compose([flow_transforms.RandomTranslate(10), flow_transforms.RandomRotate(10, 5), flow_transforms.RandomCrop((320, 448)), flow_transforms.RandomVerticalFlip(), flow_transforms.RandomHorizontalFlip()])
    print(""=> fetching img pairs in '{}'"".format(args.data))
    (train_set, test_set) = datasets.__dict__[args.dataset](args.data, transform=input_transform, target_transform=target_transform, co_transform=co_transform, split=args.split_file if args.split_file else args.split_value)
    print('{} samples found, {} train samples and {} test samples '.format(len(test_set) + len(train_set), len(train_set), len(test_set)))
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, num_workers=args.workers, pin_memory=True, shuffle=True)
    val_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, num_workers=args.workers, pin_memory=True, shuffle=False)
    if args.pretrained:
        network_data = torch.load(args.pretrained)
        args.arch = network_data['arch']
        print(""=> using pre-trained model '{}'"".format(args.arch))
    else:
        network_data = None
        print(""=> creating model '{}'"".format(args.arch))
    model = models.__dict__[args.arch](network_data).to(device)
    assert args.solver in ['adam', 'sgd']
    print('=> setting {} solver'.format(args.solver))
    param_groups = [{'params': model.bias_parameters(), 'weight_decay': args.bias_decay}, {'params': model.weight_parameters(), 'weight_decay': args.weight_decay}]
    if device.type == 'cuda':
        model = torch.nn.DataParallel(model).cuda()
        cudnn.benchmark = True
    if args.solver == 'adam':
        optimizer = torch.optim.Adam(param_groups, args.lr, betas=(args.momentum, args.beta))
    elif args.solver == 'sgd':
        optimizer = torch.optim.SGD(param_groups, args.lr, momentum=args.momentum)
    if args.evaluate:
        best_EPE = validate(val_loader, model, 0, output_writers)
        return
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=0.5)
    for epoch in range(args.start_epoch, args.epochs):
        scheduler.step()
        (train_loss, train_EPE) = train(train_loader, model, optimizer, epoch, train_writer)
        train_writer.add_scalar('mean EPE', train_EPE, epoch)
        with torch.no_grad():
            EPE = validate(val_loader, model, epoch, output_writers)
        test_writer.add_scalar('mean EPE', EPE, epoch)
        if best_EPE < 0:
            best_EPE = EPE
        is_best = EPE < best_EPE
        best_EPE = min(EPE, best_EPE)
        save_checkpoint({'epoch': epoch + 1, 'arch': args.arch, 'state_dict': model.module.state_dict(), 'best_EPE': best_EPE, 'div_flow': args.div_flow}, is_best, save_path)","for i in range(3):
    output_writers.append(SummaryWriter(os.path.join(save_path, 'test', str(i))))","output_writers = [SummaryWriter(os.path.join(save_path, 'test', str(i))) for i in range(3)]","output_writers = [SummaryWriter(os.path.join(save_path, 'test', str(i))) for i in range(3)]",1,,,,,robosuite
smd,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/smd/deezer.py,https://github.com/artyshko/smd/tree/master//deezer.py,Deezer,getPlaylist$68,"def getPlaylist(self, id):
    try:
        response = requests.get(f'{self.__url}/playlist/{id}').json()
        alb = {'name': response['title']}
        tracks = []
        for item in response['tracks']['data']:
            tracks.append({'uri': f""D{item['id']}T"", 'name': item['title'], 'artist': [item['artist']['name']], 'album': item['album']['title'], 'image': item['album']['cover_xl'], 'preview_url': item['preview'], 'duration_ms': item['duration']})
        alb.setdefault('tracks', tracks)
        return alb
    except:
        return None","for item in response['tracks']['data']:
    tracks.append({'uri': f""D{item['id']}T"", 'name': item['title'], 'artist': [item['artist']['name']], 'album': item['album']['title'], 'image': item['album']['cover_xl'], 'preview_url': item['preview'], 'duration_ms': item['duration']})","tracks += [{'uri': f""D{item['id']}T"", 'name': item['title'], 'artist': [item['artist']['name']], 'album': item['album']['title'], 'image': item['album']['cover_xl'], 'preview_url': item['preview'], 'duration_ms': item['duration']} for item in response['tracks']['data']]","tracks = [{'uri': f""D{item['id']}T"", 'name': item['title'], 'artist': [item['artist']['name']], 'album': item['album']['title'], 'image': item['album']['cover_xl'], 'preview_url': item['preview'], 'duration_ms': item['duration']} for item in response['tracks']['data']]",0,1,,,,robosuite
dcc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcc/dex2c/compiler.py,https://github.com/amimo/dcc/tree/master/dex2c/compiler.py,IrBuilder,hack_polymorphic_constant$247,"def hack_polymorphic_constant(self):
    nodes = self.graph.compute_block_order()
    todo_list = []
    for node in nodes:
        for ins in node.get_instr_list():
            if isinstance(ins, LoadConstant) and ins.get_value_type() is None:
                todo_list.append(ins)
    while todo_list:
        ins = todo_list.pop(0)
        bb = ins.parent
        for user in ins.get_users():
            new_val = self.write_variable(ins.get_value().get_register())
            new_ins = LoadConstant(new_val, ins.get_cst())
            bb.add_ins_before(new_ins, ins)
            user.replase_use_of_with(ins.get_value(), new_val)
        ins.parent.remove_ins(ins)","for node in nodes:
    for ins in node.get_instr_list():
        if isinstance(ins, LoadConstant) and ins.get_value_type() is None:
            todo_list.append(ins)","todo_list = [ins for node in nodes for ins in node.get_instr_list() if isinstance(ins, LoadConstant) and ins.get_value_type() is None]","todo_list = [ins for node in nodes for ins in node.get_instr_list() if isinstance(ins, LoadConstant) and ins.get_value_type() is None]",1,,,,,robosuite
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/domains/cpp.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/domains/cpp.py,ASTTemplateDeclarationPrefix,get_id_except_requires_clause_in_last$3925,"def get_id_except_requires_clause_in_last(self, version: int) -> str:
    assert version >= 2
    res = []
    lastIndex = len(self.templates) - 1
    for (i, t) in enumerate(self.templates):
        if isinstance(t, ASTTemplateParams):
            res.append(t.get_id(version, excludeRequires=i == lastIndex))
        else:
            res.append(t.get_id(version))
    return ''.join(res)","for (i, t) in enumerate(self.templates):
    if isinstance(t, ASTTemplateParams):
        res.append(t.get_id(version, excludeRequires=i == lastIndex))
    else:
        res.append(t.get_id(version))","res = [t.get_id(version, excludeRequires=i == lastIndex) if isinstance(t, ASTTemplateParams) else t.get_id(version) for (i, t) in enumerate(self.templates)]","res = [t.get_id(version, excludeRequires=i == lastIndex) if isinstance(t, ASTTemplateParams) else t.get_id(version) for (i, t) in enumerate(self.templates)]",1,,,,,robosuite
bcloud,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bcloud/bcloud/IconWindow.py,https://github.com/XuShaohua/bcloud/tree/master/bcloud/IconWindow.py,IconWindow,on_trash_activated$652,"def on_trash_activated(self, menu_item):
    tree_paths = self.iconview.get_selected_items()
    if not tree_paths:
        return
    path_list = []
    for tree_path in tree_paths:
        path_list.append(self.liststore[tree_path][PATH_COL])
    gutil.async_call(pcs.delete_files, self.app.cookie, self.app.tokens, path_list, callback=self.parent.reload)
    self.app.blink_page(self.app.trash_page)
    self.app.trash_page.reload()","for tree_path in tree_paths:
    path_list.append(self.liststore[tree_path][PATH_COL])",path_list = [self.liststore[tree_path][PATH_COL] for tree_path in tree_paths],path_list = [self.liststore[tree_path][PATH_COL] for tree_path in tree_paths],1,,,,,robosuite
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/pillar/ec2_pillar.py,https://github.com/saltstack/salt/tree/master/salt/pillar/ec2_pillar.py,,ext_pillar$107,"def ext_pillar(minion_id, pillar, use_grain=False, minion_ids=None, tag_match_key=None, tag_match_value='asis', tag_list_key=None, tag_list_sep=';'):
    """"""
    Execute a command and read the output as YAML
    """"""
    valid_tag_match_value = ['uqdn', 'asis']
    grain_instance_id = __grains__.get('meta-data', {}).get('instance-id', None)
    if not grain_instance_id:
        grain_instance_id = __grains__.get('dynamic', {}).get('instance-identity', {}).get('document', {}).get('instance-id', None)
    if grain_instance_id and re.search('^i-([0-9a-z]{17}|[0-9a-z]{8})$', grain_instance_id) is None:
        log.error(""External pillar %s, instance-id '%s' is not valid for '%s'"", __name__, grain_instance_id, minion_id)
        grain_instance_id = None
    if tag_match_key and re.match('[\\w=.:/@-]+$', tag_match_key) is None:
        log.error(""External pillar %s, tag_match_key '%s' is not valid "", __name__, tag_match_key if isinstance(tag_match_key, str) else 'non-string')
        return {}
    if tag_match_key and tag_match_value not in valid_tag_match_value:
        log.error(""External pillar %s, tag_value '%s' is not valid must be one of %s"", __name__, tag_match_value, ' '.join(valid_tag_match_value))
        return {}
    if not tag_match_key:
        base_msg = (""External pillar %s, querying EC2 tags for minion id '%s' against instance-id"", __name__, minion_id)
    else:
        base_msg = (""External pillar %s, querying EC2 tags for minion id '%s' against instance-id or '%s' against '%s'"", __name__, minion_id, tag_match_key, tag_match_value)
    log.debug(base_msg)
    find_filter = None
    find_id = None
    if re.search('^i-([0-9a-z]{17}|[0-9a-z]{8})$', minion_id) is not None:
        find_filter = None
        find_id = minion_id
    elif tag_match_key:
        if tag_match_value == 'uqdn':
            find_filter = {'tag:{}'.format(tag_match_key): minion_id.split('.', 1)[0]}
        else:
            find_filter = {'tag:{}'.format(tag_match_key): minion_id}
        if grain_instance_id:
            find_filter.update({'instance-id': grain_instance_id})
    if not find_filter and (not find_id) and use_grain:
        if not grain_instance_id:
            log.debug('Minion-id is not in AWS instance-id formation, and there is no instance-id grain for minion %s', minion_id)
            return {}
        if minion_ids is not None and minion_id not in minion_ids:
            log.debug('Minion-id is not in AWS instance ID format, and minion_ids is set in the ec2_pillar configuration, but minion %s is not in the list of allowed minions %s', minion_id, minion_ids)
            return {}
        find_id = grain_instance_id
    if not (find_filter or find_id):
        log.debug(""External pillar %s, querying EC2 tags for minion id '%s' against instance-id or '%s' against '%s' noughthing to match against"", __name__, minion_id, tag_match_key, tag_match_value)
        return {}
    myself = boto.utils.get_instance_metadata(timeout=0.1, num_retries=1)
    if len(myself.keys()) < 1:
        log.info('%s: salt master not an EC2 instance, skipping', __name__)
        return {}
    (_, region) = _get_instance_info()
    if use_grain:
        region = __grains__.get('ec2', {}).get('region', region)
    try:
        conn = boto.ec2.connect_to_region(region)
    except boto.exception.AWSConnectionError as exc:
        log.error('%s: invalid AWS credentials, %s', __name__, exc)
        return {}
    if conn is None:
        log.error('%s: Could not connect to region %s', __name__, region)
        return {}
    try:
        if find_id:
            instance_data = conn.get_only_instances(instance_ids=[find_id], dry_run=False)
        else:
            instance_data = conn.get_only_instances(filters=find_filter, dry_run=False)
    except boto.exception.EC2ResponseError as exc:
        log.error(""%s failed with '%s'"", base_msg, exc)
        return {}
    if not instance_data:
        log.debug(""%s no match using '%s'"", base_msg, find_id if find_id else find_filter)
        return {}
    active_inst = []
    for (idx, inst_data) in enumerate(instance_data):
        if inst_data.state not in ['terminated', 'stopped']:
            active_inst.append(idx)
    valid_inst = len(active_inst)
    if not valid_inst:
        log.debug(""%s match found but not active '%s'"", base_msg, find_id if find_id else find_filter)
        return {}
    if valid_inst > 1:
        log.error(""%s multiple matches, ignored, using '%s'"", base_msg, find_id if find_id else find_filter)
        return {}
    instance = instance_data[active_inst[0]]
    if instance.tags:
        ec2_tags = instance.tags
        ec2_tags_list = {}
        log.debug(""External pillar %s, for minion id '%s', tags: %s"", __name__, minion_id, instance.tags)
        if tag_list_key and isinstance(tag_list_key, list):
            for item in tag_list_key:
                if item in ec2_tags:
                    ec2_tags_list[item] = ec2_tags[item].split(tag_list_sep)
                    del ec2_tags[item]
                else:
                    ec2_tags_list[item] = []
        return {'ec2_tags': ec2_tags, 'ec2_tags_list': ec2_tags_list}
    return {}","for (idx, inst_data) in enumerate(instance_data):
    if inst_data.state not in ['terminated', 'stopped']:
        active_inst.append(idx)","active_inst = [idx for (idx, inst_data) in enumerate(instance_data) if inst_data.state not in ['terminated', 'stopped']]","active_inst = [idx for (idx, inst_data) in enumerate(instance_data) if inst_data.state not in ['terminated', 'stopped']]",1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,ToolPanelManager,add_to_shed_tool_config$25,"def add_to_shed_tool_config(self, shed_tool_conf_dict, elem_list):
    """"""
        ""A tool shed repository is being installed so change the shed_tool_conf file.  Parse the
        config file to generate the entire list of config_elems instead of using the in-memory list
        since it will be a subset of the entire list if one or more repositories have been deactivated.
        """"""
    if not elem_list:
        return
    old_toolbox = self.app.toolbox
    shed_tool_conf = shed_tool_conf_dict['config_filename']
    tool_cache_data_dir = shed_tool_conf_dict.get('tool_cache_data_dir')
    tool_path = shed_tool_conf_dict['tool_path']
    config_elems = []
    try:
        (tree, error_message) = parse_xml(shed_tool_conf, check_exists=False)
    except OSError as exc:
        if exc.errno == errno.ENOENT and shed_tool_conf_dict.get('create', None) is not None:
            log.info('Creating shed tool config with default contents: %s', shed_tool_conf)
            with open(shed_tool_conf, 'w') as fh:
                fh.write(shed_tool_conf_dict['create'])
            (tree, error_message) = parse_xml(shed_tool_conf)
        else:
            log.error('Unable to load shed tool config: %s', shed_tool_conf)
            raise
    if tree:
        root = tree.getroot()
        for elem in root:
            config_elems.append(elem)
        for elem_entry in elem_list:
            if elem_entry.tag == 'section':
                for existing_elem in config_elems:
                    if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                        for child in elem_entry:
                            existing_elem.append(child)
                        break
                else:
                    config_elems.append(elem_entry)
            else:
                config_elems.append(elem_entry)
        self.config_elems_to_xml_file(config_elems, shed_tool_conf, tool_path, tool_cache_data_dir)
        self.app.wait_for_toolbox_reload(old_toolbox)
    else:
        log.error(error_message)","for elem in root:
    config_elems.append(elem)",config_elems = [elem for elem in root],config_elems = [elem for elem in root],1,,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,ToolPanelManager,add_to_shed_tool_config$25,"def add_to_shed_tool_config(self, shed_tool_conf_dict, elem_list):
    """"""
        ""A tool shed repository is being installed so change the shed_tool_conf file.  Parse the
        config file to generate the entire list of config_elems instead of using the in-memory list
        since it will be a subset of the entire list if one or more repositories have been deactivated.
        """"""
    if not elem_list:
        return
    old_toolbox = self.app.toolbox
    shed_tool_conf = shed_tool_conf_dict['config_filename']
    tool_cache_data_dir = shed_tool_conf_dict.get('tool_cache_data_dir')
    tool_path = shed_tool_conf_dict['tool_path']
    config_elems = []
    try:
        (tree, error_message) = parse_xml(shed_tool_conf, check_exists=False)
    except OSError as exc:
        if exc.errno == errno.ENOENT and shed_tool_conf_dict.get('create', None) is not None:
            log.info('Creating shed tool config with default contents: %s', shed_tool_conf)
            with open(shed_tool_conf, 'w') as fh:
                fh.write(shed_tool_conf_dict['create'])
            (tree, error_message) = parse_xml(shed_tool_conf)
        else:
            log.error('Unable to load shed tool config: %s', shed_tool_conf)
            raise
    if tree:
        root = tree.getroot()
        for elem in root:
            config_elems.append(elem)
        for elem_entry in elem_list:
            if elem_entry.tag == 'section':
                for existing_elem in config_elems:
                    if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                        for child in elem_entry:
                            existing_elem.append(child)
                        break
                else:
                    config_elems.append(elem_entry)
            else:
                config_elems.append(elem_entry)
        self.config_elems_to_xml_file(config_elems, shed_tool_conf, tool_path, tool_cache_data_dir)
        self.app.wait_for_toolbox_reload(old_toolbox)
    else:
        log.error(error_message)","for elem_entry in elem_list:
    if elem_entry.tag == 'section':
        for existing_elem in config_elems:
            if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                for child in elem_entry:
                    existing_elem.append(child)
                break
        else:
            config_elems.append(elem_entry)
    else:
        config_elems.append(elem_entry)","config_elems += [existing_elem.append(child) if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None) else config_elems.append(elem_entry) if elem_entry.tag == 'section' else config_elems.append(elem_entry) for elem_entry in elem_list for existing_elem in config_elems if elem_entry.tag == 'section' and existing_elem.tag == 'section' and (existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None))]",Cannot refactor,-1,0,,,,robosuite
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,ToolPanelManager,add_to_shed_tool_config$25,"def add_to_shed_tool_config(self, shed_tool_conf_dict, elem_list):
    """"""
        ""A tool shed repository is being installed so change the shed_tool_conf file.  Parse the
        config file to generate the entire list of config_elems instead of using the in-memory list
        since it will be a subset of the entire list if one or more repositories have been deactivated.
        """"""
    if not elem_list:
        return
    old_toolbox = self.app.toolbox
    shed_tool_conf = shed_tool_conf_dict['config_filename']
    tool_cache_data_dir = shed_tool_conf_dict.get('tool_cache_data_dir')
    tool_path = shed_tool_conf_dict['tool_path']
    config_elems = []
    try:
        (tree, error_message) = parse_xml(shed_tool_conf, check_exists=False)
    except OSError as exc:
        if exc.errno == errno.ENOENT and shed_tool_conf_dict.get('create', None) is not None:
            log.info('Creating shed tool config with default contents: %s', shed_tool_conf)
            with open(shed_tool_conf, 'w') as fh:
                fh.write(shed_tool_conf_dict['create'])
            (tree, error_message) = parse_xml(shed_tool_conf)
        else:
            log.error('Unable to load shed tool config: %s', shed_tool_conf)
            raise
    if tree:
        root = tree.getroot()
        for elem in root:
            config_elems.append(elem)
        for elem_entry in elem_list:
            if elem_entry.tag == 'section':
                for existing_elem in config_elems:
                    if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                        for child in elem_entry:
                            existing_elem.append(child)
                        break
                else:
                    config_elems.append(elem_entry)
            else:
                config_elems.append(elem_entry)
        self.config_elems_to_xml_file(config_elems, shed_tool_conf, tool_path, tool_cache_data_dir)
        self.app.wait_for_toolbox_reload(old_toolbox)
    else:
        log.error(error_message)","for child in elem_entry:
    existing_elem.append(child)",existing_elem += [child for child in elem_entry],Cannot refactor,-1,1,,,,robosuite
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/datasets/kitti/kitti_common.py,https://github.com/poodarchu/Det3D/tree/master/det3d/datasets/kitti/kitti_common.py,,add_difficulty_to_annos$772,"def add_difficulty_to_annos(info):
    min_height = [40, 25, 25]
    max_occlusion = [0, 1, 2]
    max_trunc = [0.15, 0.3, 0.5]
    annos = info['annos']
    dims = annos['dimensions']
    bbox = annos['bbox']
    height = bbox[:, 3] - bbox[:, 1]
    occlusion = annos['occluded']
    truncation = annos['truncated']
    diff = []
    easy_mask = np.ones((len(dims),), dtype=np.bool)
    moderate_mask = np.ones((len(dims),), dtype=np.bool)
    hard_mask = np.ones((len(dims),), dtype=np.bool)
    i = 0
    for (h, o, t) in zip(height, occlusion, truncation):
        if o > max_occlusion[0] or h <= min_height[0] or t > max_trunc[0]:
            easy_mask[i] = False
        if o > max_occlusion[1] or h <= min_height[1] or t > max_trunc[1]:
            moderate_mask[i] = False
        if o > max_occlusion[2] or h <= min_height[2] or t > max_trunc[2]:
            hard_mask[i] = False
        i += 1
    is_easy = easy_mask
    is_moderate = np.logical_xor(easy_mask, moderate_mask)
    is_hard = np.logical_xor(hard_mask, moderate_mask)
    for i in range(len(dims)):
        if is_easy[i]:
            diff.append(0)
        elif is_moderate[i]:
            diff.append(1)
        elif is_hard[i]:
            diff.append(2)
        else:
            diff.append(-1)
    annos['difficulty'] = np.array(diff, np.int32)
    return diff","for i in range(len(dims)):
    if is_easy[i]:
        diff.append(0)
    elif is_moderate[i]:
        diff.append(1)
    elif is_hard[i]:
        diff.append(2)
    else:
        diff.append(-1)",diff = [0 if is_easy[i] else 1 if is_moderate[i] else 2 if is_hard[i] else -1 for i in range(len(dims))],diff = [0 if is_easy[i] else 1 if is_moderate[i] else 2 if is_hard[i] else -1 for i in range(len(dims))],1,,,,,robosuite
bili2.0,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bili2.0/substance/substance_raffle_sql.py,https://github.com/yjqiang/bili2.0/tree/master/substance/substance_raffle_sql.py,SubstanceRaffleJoinedTable,select_all$138,"def select_all(self):
    results = []
    for row in self.conn.execute('SELECT * FROM substanceraffle_joined'):
        results.append(self.as_bili_data(row))
    return results","for row in self.conn.execute('SELECT * FROM substanceraffle_joined'):
    results.append(self.as_bili_data(row))",results = [self.as_bili_data(row) for row in self.conn.execute('SELECT * FROM substanceraffle_joined')],results = [self.as_bili_data(row) for row in self.conn.execute('SELECT * FROM substanceraffle_joined')],1,,,,,robosuite
mlrun,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mlrun/mlrun/api/db/sqldb/db.py,https://github.com/mlrun/mlrun/tree/master/mlrun/api/db/sqldb/db.py,SQLDB,list_artifact_tags$1006,"def list_artifact_tags(self, session, project, category: schemas.ArtifactCategories=None) -> typing.List[typing.Tuple[str, str, str]]:
    """"""
        :return: a list of Tuple of (project, artifact.key, tag)
        """"""
    artifacts = self.list_artifacts(session, project=project, tag='*', category=category)
    results = []
    for artifact in artifacts:
        if is_legacy_artifact(artifact):
            results.append((project, artifact.get('db_key'), artifact.get('tag')))
        else:
            results.append((project, artifact['spec'].get('db_key'), artifact['metadata'].get('tag')))
    return results","for artifact in artifacts:
    if is_legacy_artifact(artifact):
        results.append((project, artifact.get('db_key'), artifact.get('tag')))
    else:
        results.append((project, artifact['spec'].get('db_key'), artifact['metadata'].get('tag')))","results = [(project, artifact.get('db_key'), artifact.get('tag')) if is_legacy_artifact(artifact) else (project, artifact['spec'].get('db_key'), artifact['metadata'].get('tag')) for artifact in artifacts]","results = [(project, artifact.get('db_key'), artifact.get('tag')) if is_legacy_artifact(artifact) else (project, artifact['spec'].get('db_key'), artifact['metadata'].get('tag')) for artifact in artifacts]",1,,,,,robosuite
dulwich,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dulwich/dulwich/objectspec.py,https://github.com/dulwich/dulwich/tree/master/dulwich/objectspec.py,,scan_for_short_id$194,"def scan_for_short_id(object_store, prefix):
    """"""Scan an object store for a short id.""""""
    ret = []
    for object_id in object_store:
        if object_id.startswith(prefix):
            ret.append(object_store[object_id])
    if not ret:
        raise KeyError(prefix)
    if len(ret) == 1:
        return ret[0]
    raise AmbiguousShortId(prefix, ret)","for object_id in object_store:
    if object_id.startswith(prefix):
        ret.append(object_store[object_id])",ret = [object_store[object_id] for object_id in object_store if object_id.startswith(prefix)],ret = [object_store[object_id] for object_id in object_store if object_id.startswith(prefix)],1,,,,,robosuite
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/tools/x2seg.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/tools/x2seg.py,JingLing2Seg,json2png$170,"def json2png(self, image_dir, json_dir, png_dir):
    color_map = self.get_color_map_list(256)
    for img_name in os.listdir(image_dir):
        img_name_part = osp.splitext(img_name)[0]
        json_file = osp.join(json_dir, img_name_part + '.json')
        if not osp.exists(json_file):
            os.remove(osp.join(image_dir, img_name))
            continue
        with open(json_file, mode='r', encoding=get_encoding(json_file)) as j:
            json_info = json.load(j)
            data_shapes = []
            if 'outputs' in json_info:
                for output in json_info['outputs']['object']:
                    if 'polygon' in output.keys():
                        polygon = output['polygon']
                        name = output['name']
                        points = []
                        for i in range(1, int(len(polygon) / 2) + 1):
                            points.append([polygon['x' + str(i)], polygon['y' + str(i)]])
                        shape = {'label': name, 'points': points, 'shape_type': 'polygon'}
                        data_shapes.append(shape)
            if 'size' not in json_info:
                continue
        img_shape = (json_info['size']['height'], json_info['size']['width'], json_info['size']['depth'])
        (lbl, _) = self.shapes_to_label(img_shape=img_shape, shapes=data_shapes, label_name_to_value=self.labels2ids)
        out_png_file = osp.join(png_dir, img_name_part + '.png')
        if lbl.min() >= 0 and lbl.max() <= 255:
            lbl_pil = PIL.Image.fromarray(lbl.astype(np.uint8), mode='P')
            lbl_pil.putpalette(color_map)
            lbl_pil.save(out_png_file)
        else:
            raise ValueError('[%s] Cannot save the pixel-wise class label as PNG. Please consider using the .npy format.' % out_png_file)","for output in json_info['outputs']['object']:
    if 'polygon' in output.keys():
        polygon = output['polygon']
        name = output['name']
        points = []
        for i in range(1, int(len(polygon) / 2) + 1):
            points.append([polygon['x' + str(i)], polygon['y' + str(i)]])
        shape = {'label': name, 'points': points, 'shape_type': 'polygon'}
        data_shapes.append(shape)","data_shapes = [{'label': output['name'], 'points': [[output['polygon']['x' + str(i)], output['polygon']['y' + str(i)]] for i in range(1, int(len(output['polygon']) / 2) + 1)], 'shape_type': 'polygon'} for output in json_info['outputs']['object'] if 'polygon' in output.keys()]",Cannot refactor,-1,1,,,,robosuite
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/tools/x2seg.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/tools/x2seg.py,JingLing2Seg,json2png$170,"def json2png(self, image_dir, json_dir, png_dir):
    color_map = self.get_color_map_list(256)
    for img_name in os.listdir(image_dir):
        img_name_part = osp.splitext(img_name)[0]
        json_file = osp.join(json_dir, img_name_part + '.json')
        if not osp.exists(json_file):
            os.remove(osp.join(image_dir, img_name))
            continue
        with open(json_file, mode='r', encoding=get_encoding(json_file)) as j:
            json_info = json.load(j)
            data_shapes = []
            if 'outputs' in json_info:
                for output in json_info['outputs']['object']:
                    if 'polygon' in output.keys():
                        polygon = output['polygon']
                        name = output['name']
                        points = []
                        for i in range(1, int(len(polygon) / 2) + 1):
                            points.append([polygon['x' + str(i)], polygon['y' + str(i)]])
                        shape = {'label': name, 'points': points, 'shape_type': 'polygon'}
                        data_shapes.append(shape)
            if 'size' not in json_info:
                continue
        img_shape = (json_info['size']['height'], json_info['size']['width'], json_info['size']['depth'])
        (lbl, _) = self.shapes_to_label(img_shape=img_shape, shapes=data_shapes, label_name_to_value=self.labels2ids)
        out_png_file = osp.join(png_dir, img_name_part + '.png')
        if lbl.min() >= 0 and lbl.max() <= 255:
            lbl_pil = PIL.Image.fromarray(lbl.astype(np.uint8), mode='P')
            lbl_pil.putpalette(color_map)
            lbl_pil.save(out_png_file)
        else:
            raise ValueError('[%s] Cannot save the pixel-wise class label as PNG. Please consider using the .npy format.' % out_png_file)","for i in range(1, int(len(polygon) / 2) + 1):
    points.append([polygon['x' + str(i)], polygon['y' + str(i)]])","points = [[polygon['x' + str(i)], polygon['y' + str(i)]] for i in range(1, int(len(polygon) / 2) + 1)]","points = [[polygon['x' + str(i)], polygon['y' + str(i)]] for i in range(1, int(len(polygon) / 2) + 1)]",1,,,,,robosuite
fuxi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuxi/fuxi/core/tasks/discovery/subdomain_task.py,https://github.com/jeffzh3ng/fuxi/tree/master/fuxi/core/tasks/discovery/subdomain_task.py,,t_subdomain_task$94,"def t_subdomain_task(task_id, res_return=False):
    data = []
    try:
        _item = DBSubdomainTask.find_by_id(task_id)
        target = _item['target']
        brute = _item['brute']
        threads = _item['threads']
        get_info = _item['info']
        DBSubdomainTask.update_by_id(task_id, {'status': 'running'})
        try:
            for domain in target:
                temp_save_data = []
                subdomain_list = subdomain_scanner(domain, brute, threads)
                for item in subdomain_list:
                    temp_save_data.append({'task_id': str(task_id), 'domain': domain, 'subdomain': item['subdomain'], 'title': '-', 'ip': item['ip'], 'response': 0})
                try:
                    DBSubdomainResult.add_multiple(temp_save_data)
                except Exception as e:
                    logger.warning('save subdomain failed: {}'.format(e))
                if get_info:
                    subdomain_domain_list = [subdomain['subdomain'] for subdomain in subdomain_list]
                    subdomain_web_info(subdomain_domain_list, threads)
        except Exception as e:
            logger.warning('start whatweb failed: {}'.format(e))
        DBSubdomainTask.update_by_id(task_id, {'status': 'completed', 'end_date': int(time.time())})
        logger.success('subdomain: {} the task completed'.format(task_id))
    except Exception as e:
        logger.warning('{} start subdomain task failed: {}'.format(task_id, e))
    return data if res_return else []","for item in subdomain_list:
    temp_save_data.append({'task_id': str(task_id), 'domain': domain, 'subdomain': item['subdomain'], 'title': '-', 'ip': item['ip'], 'response': 0})","temp_save_data = [{'task_id': str(task_id), 'domain': domain, 'subdomain': item['subdomain'], 'title': '-', 'ip': item['ip'], 'response': 0} for item in subdomain_list]","temp_save_data = [{'task_id': str(task_id), 'domain': domain, 'subdomain': item['subdomain'], 'title': '-', 'ip': item['ip'], 'response': 0} for item in subdomain_list]",1,,,,,robosuite
milk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/milk/milk/ext/jugparallel.py,https://github.com/luispedro/milk/tree/master/milk/ext/jugparallel.py,,kmeans_select_best$95,"def kmeans_select_best(features, ks, repeats=1, method='AIC', R=None, **kwargs):
    """"""
    assignments_centroids = kmeans_select_best(features, ks, repeats=1, method='AIC', R=None, **kwargs)

    Perform ``repeats`` calls to ``kmeans`` for each ``k`` in ``ks``, select
    the best one according to ``method.``

    Note that, unlike a raw ``kmeans`` call, this is *always deterministic*
    even if ``R=None`` (which is interpreted as being equivalent to setting it
    to a fixed value). Otherwise, the jug paradigm would be broken as different
    runs would give different results.

    Parameters
    ----------
    features : array-like
        2D array
    ks : sequence of integers
        These will be the values of ``k`` to try
    repeats : integer, optional
        How many times to attempt each k (default: 1).
    method : str, optional
        Which method to use. Must be one of 'AIC' (default) or 'BIC'.
    R : random number source, optional
        Even you do not pass a value, the result will be deterministic. This is
        different from the typical behaviour of ``R``, but, when using jug,
        reproducibility is often but, when using jug, reproducibility is often
        a desired feature.
    kwargs : other options
        These are passed transparently to ``kmeans``

    Returns
    -------
    assignments_centroids : jug.Task
        jug.Task which is the result of the best (as measured by ``method``)
        kmeans clustering.
    """"""
    from milk import kmeans
    from milk.utils import get_pyrandom
    kmeans = TaskGenerator(kmeans)
    if R is not None:
        start = get_pyrandom(R).randint(0, 1024 * 1024)
    else:
        start = 7
    results = []
    for (ki, k) in enumerate(ks):
        for i in range(repeats):
            results.append(kmeans(features, k, R=start + 7 * repeats * ki + i, **kwargs))
    return _select_best(features, results, method)[1]","for (ki, k) in enumerate(ks):
    for i in range(repeats):
        results.append(kmeans(features, k, R=start + 7 * repeats * ki + i, **kwargs))","results = [kmeans(features, k, R=start + 7 * repeats * ki + i, **kwargs) for (ki, k) in enumerate(ks) for i in range(repeats)]","results = [kmeans(features, k, R=start + 7 * repeats * ki + i, **kwargs) for (ki, k) in enumerate(ks) for i in range(repeats)]",1,,,,,robosuite
DG-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DG-Net/reIDmodel.py,https://github.com/NVlabs/DG-Net/tree/master//reIDmodel.py,PCB,forward$219,"def forward(self, x):
    x = self.model.conv1(x)
    x = self.model.bn1(x)
    x = self.model.relu(x)
    x = self.model.maxpool(x)
    x = self.model.layer1(x)
    x = self.model.layer2(x)
    x = self.model.layer3(x)
    x = self.model.layer4(x)
    x = self.avgpool(x)
    f = x
    f = f.view(f.size(0), f.size(1) * self.part)
    x = self.dropout(x)
    part = {}
    predict = {}
    for i in range(self.part):
        part[i] = x[:, :, i].contiguous()
        part[i] = part[i].view(x.size(0), x.size(1))
        name = 'classifier' + str(i)
        c = getattr(self, name)
        predict[i] = c(part[i])
    y = []
    for i in range(self.part):
        y.append(predict[i])
    return (f, y)","for i in range(self.part):
    y.append(predict[i])",y = [predict[i] for i in range(self.part)],y = [predict[i] for i in range(self.part)],1,,,,,robosuite
evalml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evalml/evalml/tests/automl_tests/test_automl.py,https://github.com/alteryx/evalml/tree/master/evalml/tests/automl_tests/test_automl.py,,test_automl_drop_index_columns$4650,"def test_automl_drop_index_columns(AutoMLTestEnv, X_y_binary):
    (X, y) = X_y_binary
    X = pd.DataFrame(X)
    X['index_col'] = pd.Series(range(len(X)))
    X.ww.init(index='index_col')
    automl = AutoMLSearch(X_train=X, y_train=y, problem_type='binary', optimize_thresholds=False, max_batches=2)
    env = AutoMLTestEnv('binary')
    with env.test_context(score_return_value={automl.objective.name: 1.0}):
        automl.search()
    for pipeline in automl.allowed_pipelines:
        assert pipeline.get_component('Drop Columns Transformer')
        assert 'Drop Columns Transformer' in pipeline.parameters
        assert pipeline.parameters['Drop Columns Transformer'] == {'columns': ['index_col']}
    all_drop_column_params = []
    for (_, row) in automl.full_rankings.iterrows():
        if 'Baseline' not in row.pipeline_name:
            all_drop_column_params.append(row.parameters['Drop Columns Transformer']['columns'])
    assert all((param == ['index_col'] for param in all_drop_column_params))","for (_, row) in automl.full_rankings.iterrows():
    if 'Baseline' not in row.pipeline_name:
        all_drop_column_params.append(row.parameters['Drop Columns Transformer']['columns'])","all_drop_column_params = [row.parameters['Drop Columns Transformer']['columns'] for (_, row) in automl.full_rankings.iterrows() if 'Baseline' not in row.pipeline_name]","all_drop_column_params = [row.parameters['Drop Columns Transformer']['columns'] for (_, row) in automl.full_rankings.iterrows() if 'Baseline' not in row.pipeline_name]",1,,,,,robosuite
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/server.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/server.py,RotkehlchenServer,__init__$17,"def __init__(self) -> None:
    """"""Initializes the backend server
        May raise:
        - SystemPermissionError due to the given args containing a datadir
        that does not have the correct permissions
        """"""
    arg_parser = app_args(prog='rotki', description='rotki, the portfolio tracker and accounting tool that respects your privacy')
    self.args = arg_parser.parse_args()
    add_logging_level('TRACE', TRACE)
    configure_logging(self.args)
    self.rotkehlchen = Rotkehlchen(self.args)
    self.stop_event = gevent.event.Event()
    domain_list = []
    if self.args.api_cors:
        if ',' in self.args.api_cors:
            for domain in self.args.api_cors.split(','):
                domain_list.append(str(domain))
        else:
            domain_list.append(str(self.args.api_cors))
    self.api_server = APIServer(rest_api=RestAPI(rotkehlchen=self.rotkehlchen), ws_notifier=self.rotkehlchen.rotki_notifier, cors_domain_list=domain_list)","for domain in self.args.api_cors.split(','):
    domain_list.append(str(domain))","domain_list = [str(domain) for domain in self.args.api_cors.split(',')]","domain_list = [str(domain) for domain in self.args.api_cors.split(',')]",1,,,,,robosuite
flexx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flexx/flexx/ui/layouts/_hv.py,https://github.com/flexxui/flexx/tree/master/flexx/ui/layouts/_hv.py,HVLayout,set_from_flex_values$327,"def set_from_flex_values(self):
    """""" Set the divider positions corresponding to the children's flex values.
        Only has a visual effect in split-mode.
        """"""
    sizes = []
    dim = 0 if 'h' in self.orientation else 1
    for widget in self.children:
        sizes.append(widget.flex[dim])
    size_sum = 0 if len(sizes) == 0 else sum(sizes)
    if size_sum == 0:
        sizes = [1 / len(sizes) for i in sizes]
    else:
        sizes = [i / size_sum for i in sizes]
    positions = []
    pos = 0
    for i in range(len(sizes) - 1):
        pos = pos + sizes[i]
        positions.append(pos)
    self._mutate_splitter_positions(positions)","for widget in self.children:
    sizes.append(widget.flex[dim])",sizes = [widget.flex[dim] for widget in self.children],sizes = [widget.flex[dim] for widget in self.children],1,,,,,robosuite
flexx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flexx/flexx/ui/layouts/_hv.py,https://github.com/flexxui/flexx/tree/master/flexx/ui/layouts/_hv.py,HVLayout,set_from_flex_values$327,"def set_from_flex_values(self):
    """""" Set the divider positions corresponding to the children's flex values.
        Only has a visual effect in split-mode.
        """"""
    sizes = []
    dim = 0 if 'h' in self.orientation else 1
    for widget in self.children:
        sizes.append(widget.flex[dim])
    size_sum = 0 if len(sizes) == 0 else sum(sizes)
    if size_sum == 0:
        sizes = [1 / len(sizes) for i in sizes]
    else:
        sizes = [i / size_sum for i in sizes]
    positions = []
    pos = 0
    for i in range(len(sizes) - 1):
        pos = pos + sizes[i]
        positions.append(pos)
    self._mutate_splitter_positions(positions)","for i in range(len(sizes) - 1):
    pos = pos + sizes[i]
    positions.append(pos)",positions = [pos + sizes[i] for i in range(len(sizes) - 1)],Cannot refactor,-1,0,,,,robosuite
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_bbox$110,"def may_augment_bbox(self, aug, ori_shape, bboxes):
    imgaug_bboxes = []
    for bbox in bboxes:
        (x1, y1, x2, y2) = bbox
        imgaug_bboxes.append(imgaug.BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2))
    imgaug_bboxes = aug.augment_bounding_boxes([imgaug.BoundingBoxesOnImage(imgaug_bboxes, shape=ori_shape)])[0].clip_out_of_image()
    new_bboxes = []
    for box in imgaug_bboxes.bounding_boxes:
        new_bboxes.append(np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32))
    return new_bboxes","for bbox in bboxes:
    (x1, y1, x2, y2) = bbox
    imgaug_bboxes.append(imgaug.BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2))","imgaug_bboxes = [imgaug.BoundingBox(x1=bbox[0], y1=bbox[1], x2=bbox[2], y2=bbox[3]) for bbox in bboxes]",Cannot refactor,-1,1,,,,robosuite
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_bbox$110,"def may_augment_bbox(self, aug, ori_shape, bboxes):
    imgaug_bboxes = []
    for bbox in bboxes:
        (x1, y1, x2, y2) = bbox
        imgaug_bboxes.append(imgaug.BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2))
    imgaug_bboxes = aug.augment_bounding_boxes([imgaug.BoundingBoxesOnImage(imgaug_bboxes, shape=ori_shape)])[0].clip_out_of_image()
    new_bboxes = []
    for box in imgaug_bboxes.bounding_boxes:
        new_bboxes.append(np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32))
    return new_bboxes","for box in imgaug_bboxes.bounding_boxes:
    new_bboxes.append(np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32))","new_bboxes = [np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32) for box in imgaug_bboxes.bounding_boxes]","new_bboxes = [np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32) for box in imgaug_bboxes.bounding_boxes]",1,,,,,robosuite
pygatt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygatt/pygatt/backends/bgapi/bgapi.py,https://github.com/peplin/pygatt/tree/master/pygatt/backends/bgapi/bgapi.py,BGAPIBackend,scan$323,"def scan(self, timeout=10, scan_interval=75, scan_window=50, active=True, discover_mode=constants.gap_discover_mode['observation'], scan_cb=None, **kwargs):
    """"""
        Perform a scan to discover BLE devices.

        timeout -- the number of seconds this scan should last.
        scan_interval -- the number of milliseconds until scanning is restarted.
        scan_window -- the number of milliseconds the scanner will listen on one
                     frequency for advertisement packets.
        active -- True --> ask sender for scan response data. False --> don't.
        discover_mode -- one of the gap_discover_mode constants.
        scan_cb -- This callback function is called whenever a new BLE
                   advertising packet is received.
                   The function takes three parameters:
                       devices, addr, packet_type
                   If the function returns True, the scan is aborted
        """"""
    self._scan_cb = scan_cb
    parameters = 1 if active else 0
    self.send_command(CommandBuilder.gap_set_scan_parameters(scan_interval, scan_window, parameters))
    self.expect(ResponsePacketType.gap_set_scan_parameters)
    log.info('Starting an %s scan', 'active' if active else 'passive')
    self.send_command(CommandBuilder.gap_discover(discover_mode))
    self.expect(ResponsePacketType.gap_discover)
    log.info('Pausing for maximum %ds to allow scan to complete', timeout)
    self._evt.set()
    start_time = time.time()
    while self._evt.is_set():
        try:
            self.expect(EventPacketType.gap_scan_response, timeout=timeout)
        except ExpectedResponseTimeout:
            pass
        if _timed_out(start_time, timeout):
            break
    log.info('Stopping scan')
    self.send_command(CommandBuilder.gap_end_procedure())
    self.expect(ResponsePacketType.gap_end_procedure)
    devices = []
    for (address, info) in self._devices_discovered.items():
        devices.append({'address': address, 'name': info.name, 'rssi': info.rssi, 'packet_data': info.packet_data})
    log.info('Discovered %d devices: %s', len(devices), devices)
    self._devices_discovered = {}
    return devices","for (address, info) in self._devices_discovered.items():
    devices.append({'address': address, 'name': info.name, 'rssi': info.rssi, 'packet_data': info.packet_data})","devices += [{'address': address, 'name': info.name, 'rssi': info.rssi, 'packet_data': info.packet_data} for (address, info) in self._devices_discovered.items()]","devices = [{'address': address, 'name': info.name, 'rssi': info.rssi, 'packet_data': info.packet_data} for (address, info) in self._devices_discovered.items()]",0,1,,,,robosuite
Metis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Metis/app/dao/time_series_detector/train_op.py,https://github.com/Tencent/Metis/tree/master/app/dao/time_series_detector/train_op.py,TrainOperation,query_train_source$87,"def query_train_source(self):
    command = 'select distinct source from train_task'
    num = self.__cur.execute(command)
    source_list = []
    query_res = self.__cur.fetchmany(num)
    for row in query_res:
        source_list.append(row[0])
    return (OP_SUCCESS, {'source': source_list})","for row in query_res:
    source_list.append(row[0])",source_list = [row[0] for row in query_res],source_list = [row[0] for row in query_res],1,,,,,robosuite
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/controllers/print_settings.py,https://github.com/frappe/erpnext/tree/master/erpnext/controllers/print_settings.py,,format_columns$38,"def format_columns(display_columns, compact_fields):
    compact_fields = compact_fields + ['image', 'item_code', 'item_name']
    final_columns = []
    for column in display_columns:
        if column not in compact_fields:
            final_columns.append(column)
    return final_columns","for column in display_columns:
    if column not in compact_fields:
        final_columns.append(column)",final_columns = [column for column in display_columns if column not in compact_fields],final_columns = [column for column in display_columns if column not in compact_fields],1,,,,,robosuite
hamster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hamster/waflib/TaskGen.py,https://github.com/projecthamster/hamster/tree/master/waflib/TaskGen.py,task_gen,__repr__$101,"def __repr__(self):
    """"""Debugging helper""""""
    lst = []
    for x in self.__dict__:
        if x not in ('env', 'bld', 'compiled_tasks', 'tasks'):
            lst.append('%s=%s' % (x, repr(getattr(self, x))))
    return 'bld(%s) in %s' % (', '.join(lst), self.path.abspath())","for x in self.__dict__:
    if x not in ('env', 'bld', 'compiled_tasks', 'tasks'):
        lst.append('%s=%s' % (x, repr(getattr(self, x))))","lst = ['%s=%s' % (x, repr(getattr(self, x))) for x in self.__dict__ if x not in ('env', 'bld', 'compiled_tasks', 'tasks')]","lst = ['%s=%s' % (x, repr(getattr(self, x))) for x in self.__dict__ if x not in ('env', 'bld', 'compiled_tasks', 'tasks')]",1,,,,,robosuite
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/reassembler.py,https://github.com/angr/angr/tree/master/angr/analyses/reassembler.py,Reassembler,remove_cgc_attachments$2113,"def remove_cgc_attachments(self):
    """"""
        Remove CGC attachments.

        :return: True if CGC attachments are found and removed, False otherwise
        :rtype: bool
        """"""
    cgc_package_list = None
    cgc_extended_application = None
    for data in self.data:
        if data.sort == 'cgc-package-list':
            cgc_package_list = data
        elif data.sort == 'cgc-extended-application':
            cgc_extended_application = data
    if not cgc_package_list or not cgc_extended_application:
        return False
    if cgc_package_list.skip or cgc_extended_application.skip:
        return True
    cgcpl_memory_data = self.cfg.memory_data.get(cgc_package_list.addr, None)
    cgcea_memory_data = self.cfg.memory_data.get(cgc_extended_application.addr, None)
    refs = self.cfg.kb.xrefs
    if cgcpl_memory_data is None or cgcea_memory_data is None:
        return False
    if len(refs.get_xrefs_by_dst(cgcpl_memory_data.addr)) != 1:
        return False
    if len(refs.get_xrefs_by_dst(cgcea_memory_data.addr)) != 1:
        return False
    if next(iter(refs.get_xrefs_by_dst(cgcpl_memory_data.addr))).block_addr != next(iter(refs.get_xrefs_by_dst(cgcea_memory_data.addr))).block_addr:
        return False
    insn_addr = next(iter(refs.get_xrefs_by_dst(cgcpl_memory_data.addr))).ins_addr
    cfg_node = self.cfg.model.get_any_node(insn_addr, anyaddr=True)
    if not cfg_node:
        return False
    func_addr = cfg_node.function_address
    sub_func_addr = None
    if func_addr not in self.cfg.functions:
        return False
    function = self.cfg.functions[func_addr]
    calling_targets = []
    for (_, dst, data) in function.transition_graph.edges(data=True):
        if 'type' in data and data['type'] == 'call':
            calling_targets.append(dst.addr)
    if len(calling_targets) != 1:
        return False
    sub_func_addr = calling_targets[0]
    proc = next((p for p in self.procedures if p.addr == func_addr), None)
    if proc is None:
        return False
    subproc = next((p for p in self.procedures if p.addr == sub_func_addr), None)
    if subproc is None:
        return False
    has_label = True
    lowest_address = min(cgc_package_list.addr, cgc_extended_application.addr)
    for obj in (cgc_package_list, cgc_extended_application):
        labels = obj.labels
        for (addr, label) in labels:
            if addr != lowest_address:
                label.base_addr = lowest_address
    if has_label:
        data = next((d for d in self.data if d.addr is not None and d.addr + d.size == lowest_address), None)
        if data is None:
            pass
        else:
            lbl = self.symbol_manager.addr_to_label[lowest_address][0]
            if lbl not in data.end_labels:
                data.end_labels.append(lbl)
    proc.asm_code = '\tret\n'
    subproc.asm_code = '\tret\n'
    cgc_package_list.skip = True
    cgc_extended_application.skip = True
    l.info('CGC attachments are removed.')
    return True","for (_, dst, data) in function.transition_graph.edges(data=True):
    if 'type' in data and data['type'] == 'call':
        calling_targets.append(dst.addr)","calling_targets = [dst.addr for (_, dst, data) in function.transition_graph.edges(data=True) if 'type' in data and data['type'] == 'call']","calling_targets = [dst.addr for (_, dst, data) in function.transition_graph.edges(data=True) if 'type' in data and data['type'] == 'call']",1,,,,,robosuite
PIME,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PIME/python/python3/tornado/httputil.py,https://github.com/EasyIME/PIME/tree/master/python/python3/tornado/httputil.py,HTTPHeaders,__str__$238,"def __str__(self) -> str:
    lines = []
    for (name, value) in self.get_all():
        lines.append('%s: %s\n' % (name, value))
    return ''.join(lines)","for (name, value) in self.get_all():
    lines.append('%s: %s\n' % (name, value))","lines = ['%s: %s\n' % (name, value) for (name, value) in self.get_all()]","lines = ['%s: %s\n' % (name, value) for (name, value) in self.get_all()]",1,,,,,robosuite
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/servicefabric/custom.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/servicefabric/custom.py,,add_client_cert$306,"def add_client_cert(cmd, client, resource_group_name, cluster_name, is_admin=False, thumbprint=None, certificate_common_name=None, certificate_issuer_thumbprint=None, admin_client_thumbprints=None, readonly_client_thumbprints=None, client_certificate_common_names=None):
    cli_ctx = cmd.cli_ctx
    if thumbprint:
        if certificate_common_name or certificate_issuer_thumbprint or admin_client_thumbprints or readonly_client_thumbprints or client_certificate_common_names:
            raise CLIError('--thumbprint can only specified alone or with --is-admin')
    if certificate_common_name or certificate_issuer_thumbprint:
        if certificate_issuer_thumbprint is None or certificate_common_name is None:
            raise CLIError(""Both '--certificate-common-name' and '--certificate-issuer-thumbprint should not be None'"")
        if thumbprint or admin_client_thumbprints or readonly_client_thumbprints or client_certificate_common_names or is_admin:
            raise CLIError(""Only '--certificate-common-name' and '--certificate-issuer-thumbprint' can be specified together"")
    if admin_client_thumbprints or readonly_client_thumbprints:
        if thumbprint or certificate_common_name or certificate_issuer_thumbprint or client_certificate_common_names or is_admin:
            raise CLIError(""Only '--admin-client-thumbprints' and '--readonly-client-thumbprints' can be specified together"")
    if client_certificate_common_names:
        if is_admin or thumbprint or certificate_common_name or certificate_issuer_thumbprint or admin_client_thumbprints or readonly_client_thumbprints:
            raise CLIError(""'--client-certificate-commonNames' can only be specified alone"")
    cluster = client.get(resource_group_name, cluster_name)

    def _add_thumbprint(cluster, is_admin, thumbprint):
        remove = []
        for t in cluster.client_certificate_thumbprints:
            if t.certificate_thumbprint.lower() == thumbprint.lower():
                remove.append(t)
        for t in remove:
            cluster.client_certificate_thumbprints.remove(t)
        cluster.client_certificate_thumbprints.append(ClientCertificateThumbprint(is_admin, thumbprint))

    def _add_common_name(cluster, is_admin, certificate_common_name, certificate_issuer_thumbprint):
        for t in cluster.client_certificate_common_names:
            if t.certificate_common_name.lower() == certificate_common_name.lower() and t.certificate_issuer_thumbprint.lower() == certificate_issuer_thumbprint.lower():
                remove = t
        if remove:
            cluster.client_certificate_common_names.remove(remove)
        cluster.client_certificate_common_names.add(ClientCertificateCommonName(is_admin, certificate_common_name, certificate_issuer_thumbprint))
        return cluster.client_certificate_common_names
    if thumbprint:
        _add_thumbprint(cluster, is_admin, thumbprint)
    if admin_client_thumbprints or readonly_client_thumbprints:
        if admin_client_thumbprints:
            for t in admin_client_thumbprints:
                _add_thumbprint(cluster, True, t)
        if readonly_client_thumbprints:
            for t in readonly_client_thumbprints:
                _add_thumbprint(cluster, False, t)
    if certificate_common_name:
        _add_common_name(cluster, is_admin, certificate_common_name, certificate_issuer_thumbprint)
    if client_certificate_common_names:
        for common_name in client_certificate_common_names:
            if 'certificateCommonName' in common_name and 'certificateIssuerThumbprint' in common_name and ('isAdmin' in common_name):
                cluster.client_certificate_common_names = _add_common_name(cluster, common_name['isAdmin'], common_name['certificateCommonName'], common_name['certificateIssuerThumbprint'])
            else:
                raise CLIError('client_certificate_common_names is invalid')
    patch_request = ClusterUpdateParameters(client_certificate_thumbprints=cluster.client_certificate_thumbprints, client_certificate_common_names=cluster.client_certificate_common_names)
    update_cluster_poll = client.begin_update(resource_group_name, cluster_name, patch_request)
    return LongRunningOperation(cli_ctx)(update_cluster_poll)","for t in cluster.client_certificate_thumbprints:
    if t.certificate_thumbprint.lower() == thumbprint.lower():
        remove.append(t)",remove = [t for t in cluster.client_certificate_thumbprints if t.certificate_thumbprint.lower() == thumbprint.lower()],remove = [t for t in cluster.client_certificate_thumbprints if t.certificate_thumbprint.lower() == thumbprint.lower()],1,,,,,robosuite
dm-haiku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm-haiku/haiku/_src/nets/resnet.py,https://github.com/deepmind/dm-haiku/tree/master/haiku/_src/nets/resnet.py,BlockGroup,__init__$203,"def __init__(self, channels: int, num_blocks: int, stride: Union[int, Sequence[int]], bn_config: Mapping[str, FloatStrOrBool], resnet_v2: bool, bottleneck: bool, use_projection: bool, name: Optional[str]=None):
    super().__init__(name=name)
    block_cls = BlockV2 if resnet_v2 else BlockV1
    self.blocks = []
    for i in range(num_blocks):
        self.blocks.append(block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i))","for i in range(num_blocks):
    self.blocks.append(block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i))","self.blocks = [block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i) for i in range(num_blocks)]","self.blocks = [block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i) for i in range(num_blocks)]",1,,,,,robosuite
horovod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horovod/horovod/ray/strategy.py,https://github.com/horovod/horovod/tree/master/horovod/ray/strategy.py,ColocatedStrategy,create_workers$88,"def create_workers(self):
    (self.placement_group, bundles) = create_placement_group(resources_per_bundle=self._resources_per_host(), num_bundles=self.num_hosts, pg_timeout=self.settings.placement_group_timeout_s, pg_strategy='STRICT_SPREAD')
    self.workers = []
    for bundle_index in range(len(bundles)):
        gpu_id_futures = []
        curr_node_workers = []
        remote_cls = ray.remote(BaseHorovodWorker)
        for i in range(self.num_workers_per_host):
            remote_cls_with_options = remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index)
            worker = remote_cls_with_options.remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)
            if self.use_gpu:
                gpu_id_futures.append(worker.get_gpu_ids.remote())
            self.workers.append(worker)
            curr_node_workers.append(worker)
        if len(gpu_id_futures) > 0:
            gpu_ids = sum(ray.get(gpu_id_futures), [])
            assert len(gpu_ids) == len(set(gpu_ids)) == self.num_workers_per_host, gpu_ids
            all_ids = ','.join([str(gpu_id) for gpu_id in gpu_ids])
            futures = []
            for worker in curr_node_workers:
                futures.append(worker.update_env_vars.remote({'CUDA_VISIBLE_DEVICES': all_ids}))
            ray.get(futures)
    return (self.workers, self.get_node_workers(self.workers))","for i in range(self.num_workers_per_host):
    remote_cls_with_options = remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index)
    worker = remote_cls_with_options.remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)
    if self.use_gpu:
        gpu_id_futures.append(worker.get_gpu_ids.remote())
    self.workers.append(worker)
    curr_node_workers.append(worker)","curr_node_workers = [self.workers.append(remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index).remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)) and (worker.get_gpu_ids.remote() if self.use_gpu else None) for i in range(self.num_workers_per_host)]",Cannot refactor,-1,,,,,robosuite
horovod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horovod/horovod/ray/strategy.py,https://github.com/horovod/horovod/tree/master/horovod/ray/strategy.py,ColocatedStrategy,create_workers$88,"def create_workers(self):
    (self.placement_group, bundles) = create_placement_group(resources_per_bundle=self._resources_per_host(), num_bundles=self.num_hosts, pg_timeout=self.settings.placement_group_timeout_s, pg_strategy='STRICT_SPREAD')
    self.workers = []
    for bundle_index in range(len(bundles)):
        gpu_id_futures = []
        curr_node_workers = []
        remote_cls = ray.remote(BaseHorovodWorker)
        for i in range(self.num_workers_per_host):
            remote_cls_with_options = remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index)
            worker = remote_cls_with_options.remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)
            if self.use_gpu:
                gpu_id_futures.append(worker.get_gpu_ids.remote())
            self.workers.append(worker)
            curr_node_workers.append(worker)
        if len(gpu_id_futures) > 0:
            gpu_ids = sum(ray.get(gpu_id_futures), [])
            assert len(gpu_ids) == len(set(gpu_ids)) == self.num_workers_per_host, gpu_ids
            all_ids = ','.join([str(gpu_id) for gpu_id in gpu_ids])
            futures = []
            for worker in curr_node_workers:
                futures.append(worker.update_env_vars.remote({'CUDA_VISIBLE_DEVICES': all_ids}))
            ray.get(futures)
    return (self.workers, self.get_node_workers(self.workers))","for worker in curr_node_workers:
    futures.append(worker.update_env_vars.remote({'CUDA_VISIBLE_DEVICES': all_ids}))",futures += [worker.update_env_vars.remote({'CUDA_VISIBLE_DEVICES': all_ids}) for worker in curr_node_workers],futures = [worker.update_env_vars.remote({'CUDA_VISIBLE_DEVICES': all_ids}) for worker in curr_node_workers],0,1,,,,robosuite
dino,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dino/utils.py,https://github.com/facebookresearch/dino/tree/master//utils.py,MetricLogger,__str__$333,"def __str__(self):
    loss_str = []
    for (name, meter) in self.meters.items():
        loss_str.append('{}: {}'.format(name, str(meter)))
    return self.delimiter.join(loss_str)","for (name, meter) in self.meters.items():
    loss_str.append('{}: {}'.format(name, str(meter)))","loss_str = ['{}: {}'.format(name, str(meter)) for (name, meter) in self.meters.items()]","loss_str = ['{}: {}'.format(name, str(meter)) for (name, meter) in self.meters.items()]",1,,,,,robosuite
py-ipfs-http-client,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/py-ipfs-http-client/ipfshttpclient/client/base.py,https://github.com/ipfs-shipyard/py-ipfs-http-client/tree/master/ipfshttpclient/client/base.py,ResponseBase,__repr__$102,"def __repr__(self) -> str:
    attr_str_parts = []
    for name in type(self)._repr_attr_display:
        attr_str_parts.append('{0}={1!r}'.format(name, getattr(self, name)))
    json_hidden = type(self)._repr_json_hidden
    attr_json_parts = []
    for (name, value) in filter(lambda i: i[0] not in json_hidden, self._raw.items()):
        attr_json_parts.append('{0!r}: {1!r}'.format(name, value))
    if attr_str_parts and attr_json_parts:
        arg_str = '{0}, **{{{1}}}'.format(', '.join(attr_str_parts), ', '.join(attr_json_parts))
    elif attr_str_parts:
        arg_str = ', '.join(attr_str_parts)
    else:
        arg_str = '{{{0}}}'.format(', '.join(attr_json_parts))
    return '<{0.__module__}.{0.__qualname__}: {1}>'.format(type(self), arg_str)","for name in type(self)._repr_attr_display:
    attr_str_parts.append('{0}={1!r}'.format(name, getattr(self, name)))","attr_str_parts = ['{0}={1!r}'.format(name, getattr(self, name)) for name in type(self)._repr_attr_display]","attr_str_parts = ['{0}={1!r}'.format(name, getattr(self, name)) for name in type(self)._repr_attr_display]",1,,,,,robosuite
py-ipfs-http-client,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/py-ipfs-http-client/ipfshttpclient/client/base.py,https://github.com/ipfs-shipyard/py-ipfs-http-client/tree/master/ipfshttpclient/client/base.py,ResponseBase,__repr__$102,"def __repr__(self) -> str:
    attr_str_parts = []
    for name in type(self)._repr_attr_display:
        attr_str_parts.append('{0}={1!r}'.format(name, getattr(self, name)))
    json_hidden = type(self)._repr_json_hidden
    attr_json_parts = []
    for (name, value) in filter(lambda i: i[0] not in json_hidden, self._raw.items()):
        attr_json_parts.append('{0!r}: {1!r}'.format(name, value))
    if attr_str_parts and attr_json_parts:
        arg_str = '{0}, **{{{1}}}'.format(', '.join(attr_str_parts), ', '.join(attr_json_parts))
    elif attr_str_parts:
        arg_str = ', '.join(attr_str_parts)
    else:
        arg_str = '{{{0}}}'.format(', '.join(attr_json_parts))
    return '<{0.__module__}.{0.__qualname__}: {1}>'.format(type(self), arg_str)","for (name, value) in filter(lambda i: i[0] not in json_hidden, self._raw.items()):
    attr_json_parts.append('{0!r}: {1!r}'.format(name, value))","attr_json_parts = ['{0!r}: {1!r}'.format(name, value) for (name, value) in filter(lambda i: i[0] not in json_hidden, self._raw.items())]","attr_json_parts = ['{0!r}: {1!r}'.format(name, value) for (name, value) in filter(lambda i: i[0] not in json_hidden, self._raw.items())]",1,,,,,robosuite
rosshow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rosshow/rosshow/termgraphics.py,https://github.com/dheera/rosshow/tree/master/rosshow/termgraphics.py,,if_main_my$346,"if __name__ == '__main__':
    g = TermGraphics()
    for j in range(10):
        g.clear()
        points = []
        for i in range(300):
            points.append((i, int(i * j / 5)))
        g.points(points)
        g.text('hello', (10, 10))
        g.draw()
        time.sleep(0.1)","for i in range(300):
    points.append((i, int(i * j / 5)))","points = [(i, int(i * j / 5)) for i in range(300)]","points = [(i, int(i * j / 5)) for i in range(300)]",1,,,,,robosuite
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/path_chooser_common.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/path_chooser_common.py,,get_completion_paths$38,"def get_completion_paths(args):
    """"""
    Takes a path value and returns the available completions.
    If the path_value is a valid path, return all sub-directories.
    If the path_value is not a valid path, remove the basename from the
    path and return all sub-directories of path that start with basename.

    :param args: options
    :type args: dict
    :returns: the args argument containing the available completions for the completion_text
    :rtype: list

    """"""
    args['paths'] = []
    path_value = args['completion_text']
    hidden_files = args['show_hidden_files']

    def get_subdirs(dirname):
        try:
            if PY2:
                return os.walk(dirname).__next__[1]
            else:
                return next(os.walk(dirname))[1]
        except StopIteration:
            return []
    dirname = os.path.dirname(path_value)
    basename = os.path.basename(path_value)
    dirs = get_subdirs(dirname)
    if not dirs:
        return args
    if not basename:
        if not hidden_files:
            old_dirs = dirs
            dirs = []
            for d in old_dirs:
                if not is_hidden(os.path.join(dirname, d)):
                    dirs.append(d)
    matching_dirs = []
    for s in dirs:
        if s.startswith(basename):
            p = os.path.join(dirname, s)
            if not p.endswith(os.path.sep):
                p += os.path.sep
            matching_dirs.append(p)
    args['paths'] = sorted(matching_dirs)
    return args","for s in dirs:
    if s.startswith(basename):
        p = os.path.join(dirname, s)
        if not p.endswith(os.path.sep):
            p += os.path.sep
        matching_dirs.append(p)","matching_dirs = [os.path.join(dirname, s) + os.path.sep for s in dirs if s.startswith(basename) and (not os.path.join(dirname, s).endswith(os.path.sep))]",Cannot refactor,-1,0,,2,1,robosuite
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/path_chooser_common.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/path_chooser_common.py,,get_completion_paths$38,"def get_completion_paths(args):
    """"""
    Takes a path value and returns the available completions.
    If the path_value is a valid path, return all sub-directories.
    If the path_value is not a valid path, remove the basename from the
    path and return all sub-directories of path that start with basename.

    :param args: options
    :type args: dict
    :returns: the args argument containing the available completions for the completion_text
    :rtype: list

    """"""
    args['paths'] = []
    path_value = args['completion_text']
    hidden_files = args['show_hidden_files']

    def get_subdirs(dirname):
        try:
            if PY2:
                return os.walk(dirname).__next__[1]
            else:
                return next(os.walk(dirname))[1]
        except StopIteration:
            return []
    dirname = os.path.dirname(path_value)
    basename = os.path.basename(path_value)
    dirs = get_subdirs(dirname)
    if not dirs:
        return args
    if not basename:
        if not hidden_files:
            old_dirs = dirs
            dirs = []
            for d in old_dirs:
                if not is_hidden(os.path.join(dirname, d)):
                    dirs.append(d)
    matching_dirs = []
    for s in dirs:
        if s.startswith(basename):
            p = os.path.join(dirname, s)
            if not p.endswith(os.path.sep):
                p += os.path.sep
            matching_dirs.append(p)
    args['paths'] = sorted(matching_dirs)
    return args","for d in old_dirs:
    if not is_hidden(os.path.join(dirname, d)):
        dirs.append(d)","dirs += [d for d in old_dirs if not is_hidden(os.path.join(dirname, d))]","dirs = [d for d in old_dirs if not is_hidden(os.path.join(dirname, d))]",0,1,,,,robosuite
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/learner_progress_services.py,https://github.com/oppia/oppia/tree/master/core/domain/learner_progress_services.py,,get_topics_and_stories_progress$1977,"def get_topics_and_stories_progress(user_id: str) -> Tuple[learner_progress_domain.LearnerProgressInTopicsAndStories, Dict[str, int]]:
    """"""Returns the progress of the learners - the stories and learnt_topics
    completed by the user and those in progress.

    Args:
        user_id: str. The id of the learner.

    Returns:
        (LearnerProgressInTopicsAndStories, dict).
        The first return value is the learner progress in topics and stories
        domain object corresponding to the particular learner.
        The second return value is the numbers of the activities that are
        no longer present. It contains three keys:
            - partially_learnt_topics: int. The number of partially learnt
                topics no longer present.
            - completed_stories: int. The number of completed stories no
                longer present.
            - learnt_topics: int. The number of learnt topics no
                longer present.
            - topics_to_learn: int. The number of topics marked to learn.
    """"""
    activity_ids_in_learner_dashboard = get_learner_dashboard_activities(user_id)
    completed_story_ids = activity_ids_in_learner_dashboard.completed_story_ids
    learnt_topic_ids = activity_ids_in_learner_dashboard.learnt_topic_ids
    partially_learnt_topic_ids = activity_ids_in_learner_dashboard.partially_learnt_topic_ids
    topic_ids_to_learn = activity_ids_in_learner_dashboard.topic_ids_to_learn
    all_topic_ids = activity_ids_in_learner_dashboard.all_topic_ids
    untracked_topic_ids = activity_ids_in_learner_dashboard.untracked_topic_ids
    unique_topic_ids = list(set(partially_learnt_topic_ids + learnt_topic_ids + topic_ids_to_learn + all_topic_ids + untracked_topic_ids))
    activity_models = datastore_services.fetch_multiple_entities_by_ids_and_models([('TopicSummaryModel', unique_topic_ids), ('StorySummaryModel', completed_story_ids)])
    topic_id_to_model_dict: Dict[str, topic_domain.TopicSummary] = {}
    for model in activity_models[0]:
        if model is not None:
            assert isinstance(model, topic_models.TopicSummaryModel)
            topic_id_to_model_dict[model.id] = topic_fetchers.get_topic_summary_from_model(model)
    completed_story_models = activity_models[1]
    completed_story_summaries: List[Optional[story_domain.StorySummary]] = []
    for model in completed_story_models:
        if model is not None:
            assert isinstance(model, story_models.StorySummaryModel)
            completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
        else:
            completed_story_summaries.append(None)
    partially_learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in partially_learnt_topic_ids]
    learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in learnt_topic_ids]
    topics_to_learn_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in topic_ids_to_learn]
    all_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in all_topic_ids]
    untracked_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in untracked_topic_ids]
    (filtered_completed_story_summaries, nonexistent_completed_story_ids, completed_to_incomplete_story_summaries) = _get_filtered_completed_story_summaries(user_id, completed_story_summaries, completed_story_ids)
    completed_to_incomplete_story_titles = []
    for story_summary in completed_to_incomplete_story_summaries:
        completed_to_incomplete_story_titles.append(story_summary.title)
    (filtered_learnt_topic_summaries, nonexistent_learnt_topic_ids, learnt_to_partially_learnt_topic_summaries) = _get_filtered_learnt_topic_summaries(user_id, learnt_topic_summaries, learnt_topic_ids)
    learnt_to_partially_learnt_topic_titles = []
    for topic_summary in learnt_to_partially_learnt_topic_summaries:
        partially_learnt_topic_summaries.append(topic_summary)
        learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
        partially_learnt_topic_ids.append(topic_summary.id)
    (filtered_partially_learnt_topic_summaries, nonexistent_partially_learnt_topic_ids) = _get_filtered_partially_learnt_topic_summaries(partially_learnt_topic_summaries, partially_learnt_topic_ids)
    (filtered_topics_to_learn_summaries, nonexistent_topic_ids_to_learn) = _get_filtered_topics_to_learn_summaries(user_id, topics_to_learn_summaries, topic_ids_to_learn)
    filtered_all_topic_summaries = _get_filtered_all_topic_summaries(all_topic_summaries, all_topic_ids)
    filtered_untracked_topic_summaries = _get_filtered_untracked_topic_summaries(untracked_topic_summaries, untracked_topic_ids)
    number_of_nonexistent_topics_and_stories = {'partially_learnt_topics': len(nonexistent_partially_learnt_topic_ids), 'completed_stories': len(nonexistent_completed_story_ids), 'learnt_topics': len(nonexistent_learnt_topic_ids), 'topics_to_learn': len(nonexistent_topic_ids_to_learn)}
    _remove_activity_ids_from_incomplete_list(user_id, exploration_ids=[], collection_ids=[], partially_learnt_topic_ids=nonexistent_partially_learnt_topic_ids)
    _remove_activity_ids_from_completed_list(user_id, [], [], nonexistent_completed_story_ids, nonexistent_learnt_topic_ids)
    learner_goals_services.remove_topics_from_learn_goal(user_id, nonexistent_topic_ids_to_learn)
    learner_progress_in_topics_and_stories = learner_progress_domain.LearnerProgressInTopicsAndStories(filtered_partially_learnt_topic_summaries, filtered_completed_story_summaries, filtered_learnt_topic_summaries, filtered_topics_to_learn_summaries, filtered_all_topic_summaries, filtered_untracked_topic_summaries, completed_to_incomplete_story_titles, learnt_to_partially_learnt_topic_titles)
    return (learner_progress_in_topics_and_stories, number_of_nonexistent_topics_and_stories)","for model in completed_story_models:
    if model is not None:
        assert isinstance(model, story_models.StorySummaryModel)
        completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
    else:
        completed_story_summaries.append(None)","completed_story_summaries = [story_fetchers.get_story_summary_from_model(model) if model is not None and isinstance(model, story_models.StorySummaryModel) else None for model in completed_story_models]",Cannot refactor,-1,0,,,,robosuite
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/learner_progress_services.py,https://github.com/oppia/oppia/tree/master/core/domain/learner_progress_services.py,,get_topics_and_stories_progress$1977,"def get_topics_and_stories_progress(user_id: str) -> Tuple[learner_progress_domain.LearnerProgressInTopicsAndStories, Dict[str, int]]:
    """"""Returns the progress of the learners - the stories and learnt_topics
    completed by the user and those in progress.

    Args:
        user_id: str. The id of the learner.

    Returns:
        (LearnerProgressInTopicsAndStories, dict).
        The first return value is the learner progress in topics and stories
        domain object corresponding to the particular learner.
        The second return value is the numbers of the activities that are
        no longer present. It contains three keys:
            - partially_learnt_topics: int. The number of partially learnt
                topics no longer present.
            - completed_stories: int. The number of completed stories no
                longer present.
            - learnt_topics: int. The number of learnt topics no
                longer present.
            - topics_to_learn: int. The number of topics marked to learn.
    """"""
    activity_ids_in_learner_dashboard = get_learner_dashboard_activities(user_id)
    completed_story_ids = activity_ids_in_learner_dashboard.completed_story_ids
    learnt_topic_ids = activity_ids_in_learner_dashboard.learnt_topic_ids
    partially_learnt_topic_ids = activity_ids_in_learner_dashboard.partially_learnt_topic_ids
    topic_ids_to_learn = activity_ids_in_learner_dashboard.topic_ids_to_learn
    all_topic_ids = activity_ids_in_learner_dashboard.all_topic_ids
    untracked_topic_ids = activity_ids_in_learner_dashboard.untracked_topic_ids
    unique_topic_ids = list(set(partially_learnt_topic_ids + learnt_topic_ids + topic_ids_to_learn + all_topic_ids + untracked_topic_ids))
    activity_models = datastore_services.fetch_multiple_entities_by_ids_and_models([('TopicSummaryModel', unique_topic_ids), ('StorySummaryModel', completed_story_ids)])
    topic_id_to_model_dict: Dict[str, topic_domain.TopicSummary] = {}
    for model in activity_models[0]:
        if model is not None:
            assert isinstance(model, topic_models.TopicSummaryModel)
            topic_id_to_model_dict[model.id] = topic_fetchers.get_topic_summary_from_model(model)
    completed_story_models = activity_models[1]
    completed_story_summaries: List[Optional[story_domain.StorySummary]] = []
    for model in completed_story_models:
        if model is not None:
            assert isinstance(model, story_models.StorySummaryModel)
            completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
        else:
            completed_story_summaries.append(None)
    partially_learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in partially_learnt_topic_ids]
    learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in learnt_topic_ids]
    topics_to_learn_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in topic_ids_to_learn]
    all_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in all_topic_ids]
    untracked_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in untracked_topic_ids]
    (filtered_completed_story_summaries, nonexistent_completed_story_ids, completed_to_incomplete_story_summaries) = _get_filtered_completed_story_summaries(user_id, completed_story_summaries, completed_story_ids)
    completed_to_incomplete_story_titles = []
    for story_summary in completed_to_incomplete_story_summaries:
        completed_to_incomplete_story_titles.append(story_summary.title)
    (filtered_learnt_topic_summaries, nonexistent_learnt_topic_ids, learnt_to_partially_learnt_topic_summaries) = _get_filtered_learnt_topic_summaries(user_id, learnt_topic_summaries, learnt_topic_ids)
    learnt_to_partially_learnt_topic_titles = []
    for topic_summary in learnt_to_partially_learnt_topic_summaries:
        partially_learnt_topic_summaries.append(topic_summary)
        learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
        partially_learnt_topic_ids.append(topic_summary.id)
    (filtered_partially_learnt_topic_summaries, nonexistent_partially_learnt_topic_ids) = _get_filtered_partially_learnt_topic_summaries(partially_learnt_topic_summaries, partially_learnt_topic_ids)
    (filtered_topics_to_learn_summaries, nonexistent_topic_ids_to_learn) = _get_filtered_topics_to_learn_summaries(user_id, topics_to_learn_summaries, topic_ids_to_learn)
    filtered_all_topic_summaries = _get_filtered_all_topic_summaries(all_topic_summaries, all_topic_ids)
    filtered_untracked_topic_summaries = _get_filtered_untracked_topic_summaries(untracked_topic_summaries, untracked_topic_ids)
    number_of_nonexistent_topics_and_stories = {'partially_learnt_topics': len(nonexistent_partially_learnt_topic_ids), 'completed_stories': len(nonexistent_completed_story_ids), 'learnt_topics': len(nonexistent_learnt_topic_ids), 'topics_to_learn': len(nonexistent_topic_ids_to_learn)}
    _remove_activity_ids_from_incomplete_list(user_id, exploration_ids=[], collection_ids=[], partially_learnt_topic_ids=nonexistent_partially_learnt_topic_ids)
    _remove_activity_ids_from_completed_list(user_id, [], [], nonexistent_completed_story_ids, nonexistent_learnt_topic_ids)
    learner_goals_services.remove_topics_from_learn_goal(user_id, nonexistent_topic_ids_to_learn)
    learner_progress_in_topics_and_stories = learner_progress_domain.LearnerProgressInTopicsAndStories(filtered_partially_learnt_topic_summaries, filtered_completed_story_summaries, filtered_learnt_topic_summaries, filtered_topics_to_learn_summaries, filtered_all_topic_summaries, filtered_untracked_topic_summaries, completed_to_incomplete_story_titles, learnt_to_partially_learnt_topic_titles)
    return (learner_progress_in_topics_and_stories, number_of_nonexistent_topics_and_stories)","for story_summary in completed_to_incomplete_story_summaries:
    completed_to_incomplete_story_titles.append(story_summary.title)",completed_to_incomplete_story_titles = [story_summary.title for story_summary in completed_to_incomplete_story_summaries],completed_to_incomplete_story_titles = [story_summary.title for story_summary in completed_to_incomplete_story_summaries],1,,,,,robosuite
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/learner_progress_services.py,https://github.com/oppia/oppia/tree/master/core/domain/learner_progress_services.py,,get_topics_and_stories_progress$1977,"def get_topics_and_stories_progress(user_id: str) -> Tuple[learner_progress_domain.LearnerProgressInTopicsAndStories, Dict[str, int]]:
    """"""Returns the progress of the learners - the stories and learnt_topics
    completed by the user and those in progress.

    Args:
        user_id: str. The id of the learner.

    Returns:
        (LearnerProgressInTopicsAndStories, dict).
        The first return value is the learner progress in topics and stories
        domain object corresponding to the particular learner.
        The second return value is the numbers of the activities that are
        no longer present. It contains three keys:
            - partially_learnt_topics: int. The number of partially learnt
                topics no longer present.
            - completed_stories: int. The number of completed stories no
                longer present.
            - learnt_topics: int. The number of learnt topics no
                longer present.
            - topics_to_learn: int. The number of topics marked to learn.
    """"""
    activity_ids_in_learner_dashboard = get_learner_dashboard_activities(user_id)
    completed_story_ids = activity_ids_in_learner_dashboard.completed_story_ids
    learnt_topic_ids = activity_ids_in_learner_dashboard.learnt_topic_ids
    partially_learnt_topic_ids = activity_ids_in_learner_dashboard.partially_learnt_topic_ids
    topic_ids_to_learn = activity_ids_in_learner_dashboard.topic_ids_to_learn
    all_topic_ids = activity_ids_in_learner_dashboard.all_topic_ids
    untracked_topic_ids = activity_ids_in_learner_dashboard.untracked_topic_ids
    unique_topic_ids = list(set(partially_learnt_topic_ids + learnt_topic_ids + topic_ids_to_learn + all_topic_ids + untracked_topic_ids))
    activity_models = datastore_services.fetch_multiple_entities_by_ids_and_models([('TopicSummaryModel', unique_topic_ids), ('StorySummaryModel', completed_story_ids)])
    topic_id_to_model_dict: Dict[str, topic_domain.TopicSummary] = {}
    for model in activity_models[0]:
        if model is not None:
            assert isinstance(model, topic_models.TopicSummaryModel)
            topic_id_to_model_dict[model.id] = topic_fetchers.get_topic_summary_from_model(model)
    completed_story_models = activity_models[1]
    completed_story_summaries: List[Optional[story_domain.StorySummary]] = []
    for model in completed_story_models:
        if model is not None:
            assert isinstance(model, story_models.StorySummaryModel)
            completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
        else:
            completed_story_summaries.append(None)
    partially_learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in partially_learnt_topic_ids]
    learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in learnt_topic_ids]
    topics_to_learn_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in topic_ids_to_learn]
    all_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in all_topic_ids]
    untracked_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in untracked_topic_ids]
    (filtered_completed_story_summaries, nonexistent_completed_story_ids, completed_to_incomplete_story_summaries) = _get_filtered_completed_story_summaries(user_id, completed_story_summaries, completed_story_ids)
    completed_to_incomplete_story_titles = []
    for story_summary in completed_to_incomplete_story_summaries:
        completed_to_incomplete_story_titles.append(story_summary.title)
    (filtered_learnt_topic_summaries, nonexistent_learnt_topic_ids, learnt_to_partially_learnt_topic_summaries) = _get_filtered_learnt_topic_summaries(user_id, learnt_topic_summaries, learnt_topic_ids)
    learnt_to_partially_learnt_topic_titles = []
    for topic_summary in learnt_to_partially_learnt_topic_summaries:
        partially_learnt_topic_summaries.append(topic_summary)
        learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
        partially_learnt_topic_ids.append(topic_summary.id)
    (filtered_partially_learnt_topic_summaries, nonexistent_partially_learnt_topic_ids) = _get_filtered_partially_learnt_topic_summaries(partially_learnt_topic_summaries, partially_learnt_topic_ids)
    (filtered_topics_to_learn_summaries, nonexistent_topic_ids_to_learn) = _get_filtered_topics_to_learn_summaries(user_id, topics_to_learn_summaries, topic_ids_to_learn)
    filtered_all_topic_summaries = _get_filtered_all_topic_summaries(all_topic_summaries, all_topic_ids)
    filtered_untracked_topic_summaries = _get_filtered_untracked_topic_summaries(untracked_topic_summaries, untracked_topic_ids)
    number_of_nonexistent_topics_and_stories = {'partially_learnt_topics': len(nonexistent_partially_learnt_topic_ids), 'completed_stories': len(nonexistent_completed_story_ids), 'learnt_topics': len(nonexistent_learnt_topic_ids), 'topics_to_learn': len(nonexistent_topic_ids_to_learn)}
    _remove_activity_ids_from_incomplete_list(user_id, exploration_ids=[], collection_ids=[], partially_learnt_topic_ids=nonexistent_partially_learnt_topic_ids)
    _remove_activity_ids_from_completed_list(user_id, [], [], nonexistent_completed_story_ids, nonexistent_learnt_topic_ids)
    learner_goals_services.remove_topics_from_learn_goal(user_id, nonexistent_topic_ids_to_learn)
    learner_progress_in_topics_and_stories = learner_progress_domain.LearnerProgressInTopicsAndStories(filtered_partially_learnt_topic_summaries, filtered_completed_story_summaries, filtered_learnt_topic_summaries, filtered_topics_to_learn_summaries, filtered_all_topic_summaries, filtered_untracked_topic_summaries, completed_to_incomplete_story_titles, learnt_to_partially_learnt_topic_titles)
    return (learner_progress_in_topics_and_stories, number_of_nonexistent_topics_and_stories)","for topic_summary in learnt_to_partially_learnt_topic_summaries:
    partially_learnt_topic_summaries.append(topic_summary)
    learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
    partially_learnt_topic_ids.append(topic_summary.id)",partially_learnt_topic_ids.extend([topic_summary.id for topic_summary in learnt_to_partially_learnt_topic_summaries]),Cannot refactor,-1,,,,,robosuite
CTGAN,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CTGAN/ctgan/data_sampler.py,https://github.com/sdv-dev/CTGAN/tree/master/ctgan/data_sampler.py,DataSampler,__init__$9,"def __init__(self, data, output_info, log_frequency):
    self._data = data

    def is_discrete_column(column_info):
        return len(column_info) == 1 and column_info[0].activation_fn == 'softmax'
    n_discrete_columns = sum([1 for column_info in output_info if is_discrete_column(column_info)])
    self._discrete_column_matrix_st = np.zeros(n_discrete_columns, dtype='int32')
    self._rid_by_cat_cols = []
    st = 0
    for column_info in output_info:
        if is_discrete_column(column_info):
            span_info = column_info[0]
            ed = st + span_info.dim
            rid_by_cat = []
            for j in range(span_info.dim):
                rid_by_cat.append(np.nonzero(data[:, st + j])[0])
            self._rid_by_cat_cols.append(rid_by_cat)
            st = ed
        else:
            st += sum([span_info.dim for span_info in column_info])
    assert st == data.shape[1]
    max_category = max([column_info[0].dim for column_info in output_info if is_discrete_column(column_info)], default=0)
    self._discrete_column_cond_st = np.zeros(n_discrete_columns, dtype='int32')
    self._discrete_column_n_category = np.zeros(n_discrete_columns, dtype='int32')
    self._discrete_column_category_prob = np.zeros((n_discrete_columns, max_category))
    self._n_discrete_columns = n_discrete_columns
    self._n_categories = sum([column_info[0].dim for column_info in output_info if is_discrete_column(column_info)])
    st = 0
    current_id = 0
    current_cond_st = 0
    for column_info in output_info:
        if is_discrete_column(column_info):
            span_info = column_info[0]
            ed = st + span_info.dim
            category_freq = np.sum(data[:, st:ed], axis=0)
            if log_frequency:
                category_freq = np.log(category_freq + 1)
            category_prob = category_freq / np.sum(category_freq)
            self._discrete_column_category_prob[current_id, :span_info.dim] = category_prob
            self._discrete_column_cond_st[current_id] = current_cond_st
            self._discrete_column_n_category[current_id] = span_info.dim
            current_cond_st += span_info.dim
            current_id += 1
            st = ed
        else:
            st += sum([span_info.dim for span_info in column_info])","for j in range(span_info.dim):
    rid_by_cat.append(np.nonzero(data[:, st + j])[0])","rid_by_cat = [np.nonzero(data[:, st + j])[0] for j in range(span_info.dim)]","rid_by_cat = [np.nonzero(data[:, st + j])[0] for j in range(span_info.dim)]",1,,,,,robosuite
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/functions/elementary/hyperbolic.py,https://github.com/sympy/sympy/tree/master/sympy/functions/elementary/hyperbolic.py,tanh,_eval_expand_trig$641,"def _eval_expand_trig(self, **hints):
    arg = self.args[0]
    if arg.is_Add:
        from sympy.polys.specialpolys import symmetric_poly
        n = len(arg.args)
        TX = [tanh(x, evaluate=False)._eval_expand_trig() for x in arg.args]
        p = [0, 0]
        for i in range(n + 1):
            p[i % 2] += symmetric_poly(i, TX)
        return p[1] / p[0]
    elif arg.is_Mul:
        from sympy.functions.combinatorial.numbers import nC
        (coeff, terms) = arg.as_coeff_Mul()
        if coeff.is_Integer and coeff > 1:
            n = []
            d = []
            T = tanh(terms)
            for k in range(1, coeff + 1, 2):
                n.append(nC(range(coeff), k) * T ** k)
            for k in range(0, coeff + 1, 2):
                d.append(nC(range(coeff), k) * T ** k)
            return Add(*n) / Add(*d)
    return tanh(arg)","for k in range(1, coeff + 1, 2):
    n.append(nC(range(coeff), k) * T ** k)","n += [nC(range(coeff), k) * T ** k for k in range(1, coeff + 1, 2)]","n = [nC(range(coeff), k) * T ** k for k in range(1, coeff + 1, 2)]",0,1,,,,robosuite
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/functions/elementary/hyperbolic.py,https://github.com/sympy/sympy/tree/master/sympy/functions/elementary/hyperbolic.py,tanh,_eval_expand_trig$641,"def _eval_expand_trig(self, **hints):
    arg = self.args[0]
    if arg.is_Add:
        from sympy.polys.specialpolys import symmetric_poly
        n = len(arg.args)
        TX = [tanh(x, evaluate=False)._eval_expand_trig() for x in arg.args]
        p = [0, 0]
        for i in range(n + 1):
            p[i % 2] += symmetric_poly(i, TX)
        return p[1] / p[0]
    elif arg.is_Mul:
        from sympy.functions.combinatorial.numbers import nC
        (coeff, terms) = arg.as_coeff_Mul()
        if coeff.is_Integer and coeff > 1:
            n = []
            d = []
            T = tanh(terms)
            for k in range(1, coeff + 1, 2):
                n.append(nC(range(coeff), k) * T ** k)
            for k in range(0, coeff + 1, 2):
                d.append(nC(range(coeff), k) * T ** k)
            return Add(*n) / Add(*d)
    return tanh(arg)","for k in range(0, coeff + 1, 2):
    d.append(nC(range(coeff), k) * T ** k)","d += [nC(range(coeff), k) * T ** k for k in range(0, coeff + 1, 2)]","d = [nC(range(coeff), k) * T ** k for k in range(0, coeff + 1, 2)]",0,1,,,,robosuite
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/lib/mysql/connector/cursor.py,https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/cursor.py,MySQLCursorBufferedDict,fetchall$1423,"def fetchall(self):
    """"""Returns all rows of a query result set
        """"""
    if self._executed is None or self._rows is None:
        raise errors.InterfaceError(ERR_NO_RESULT_TO_FETCH)
    res = []
    for row in self._rows[self._next_row:]:
        res.append(self._row_to_python(row, self.description))
    self._next_row = len(self._rows)
    return res","for row in self._rows[self._next_row:]:
    res.append(self._row_to_python(row, self.description))","res = [self._row_to_python(row, self.description) for row in self._rows[self._next_row:]]","res = [self._row_to_python(row, self.description) for row in self._rows[self._next_row:]]",1,,,,,robosuite
PyBitmessage,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyBitmessage/src/tests/test_process.py,https://github.com/Bitmessage/PyBitmessage/tree/master/src/tests/test_process.py,TestProcessProto,_test_threads$137,"def _test_threads(self):
    """"""Test number and names of threads""""""
    self.longMessage = True
    try:
        thread_names = subprocess.check_output(['ps', '-L', '-o', 'comm=', '--pid', str(self.process.pid)]).split()
    except subprocess.CalledProcessError:
        thread_names = []
    except:
        thread_names = []
    running_threads = len(thread_names)
    if 0 < running_threads < 30:
        extra_threads = []
        missing_threads = []
        for thread_name in thread_names:
            if thread_name not in self._threads_names:
                extra_threads.append(thread_name)
        for thread_name in self._threads_names:
            if thread_name not in thread_names:
                missing_threads.append(thread_name)
        msg = 'Missing threads: {}, Extra threads: {}'.format(','.join(missing_threads), ','.join(extra_threads))
    else:
        running_threads = self.process.num_threads()
        if sys.platform.startswith('win'):
            running_threads -= 1
        msg = 'Unexpected running thread count'
    self.assertGreaterEqual(running_threads, self._threads_count_min, msg)
    self.assertLessEqual(running_threads, self._threads_count_max, msg)","for thread_name in thread_names:
    if thread_name not in self._threads_names:
        extra_threads.append(thread_name)",extra_threads = [thread_name for thread_name in thread_names if thread_name not in self._threads_names],extra_threads = [thread_name for thread_name in thread_names if thread_name not in self._threads_names],1,,,,,robosuite
PyBitmessage,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyBitmessage/src/tests/test_process.py,https://github.com/Bitmessage/PyBitmessage/tree/master/src/tests/test_process.py,TestProcessProto,_test_threads$137,"def _test_threads(self):
    """"""Test number and names of threads""""""
    self.longMessage = True
    try:
        thread_names = subprocess.check_output(['ps', '-L', '-o', 'comm=', '--pid', str(self.process.pid)]).split()
    except subprocess.CalledProcessError:
        thread_names = []
    except:
        thread_names = []
    running_threads = len(thread_names)
    if 0 < running_threads < 30:
        extra_threads = []
        missing_threads = []
        for thread_name in thread_names:
            if thread_name not in self._threads_names:
                extra_threads.append(thread_name)
        for thread_name in self._threads_names:
            if thread_name not in thread_names:
                missing_threads.append(thread_name)
        msg = 'Missing threads: {}, Extra threads: {}'.format(','.join(missing_threads), ','.join(extra_threads))
    else:
        running_threads = self.process.num_threads()
        if sys.platform.startswith('win'):
            running_threads -= 1
        msg = 'Unexpected running thread count'
    self.assertGreaterEqual(running_threads, self._threads_count_min, msg)
    self.assertLessEqual(running_threads, self._threads_count_max, msg)","for thread_name in self._threads_names:
    if thread_name not in thread_names:
        missing_threads.append(thread_name)",missing_threads = [thread_name for thread_name in self._threads_names if thread_name not in thread_names],missing_threads = [thread_name for thread_name in self._threads_names if thread_name not in thread_names],1,,,,,robosuite
skll,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/skll/tests/test_featureset.py,https://github.com/EducationalTestingService/skll/tree/master/tests/test_featureset.py,,featureset_creation_from_dataframe_helper$953,"def featureset_creation_from_dataframe_helper(with_labels, use_feature_hasher):
    """"""
    Helper function for the two unit tests for FeatureSet.from_data_frame().
    Since labels are optional, run two tests, one with, one without.
    """"""
    (X, y) = make_classification(n_samples=100, n_features=4, n_informative=4, n_redundant=0, n_classes=3, random_state=1234567890)
    ids = list(range(100, 200))
    featureset_name = 'test'
    feature_bins = 4
    vectorizer = FeatureHasher(n_features=feature_bins) if use_feature_hasher else None
    feature_names = [f'f{n}' for n in range(1, 5)]
    features = []
    for row in X:
        features.append(dict(zip(feature_names, row)))
    if with_labels:
        expected = FeatureSet(featureset_name, ids, features=features, labels=y, vectorizer=vectorizer)
    else:
        expected = FeatureSet(featureset_name, ids, features=features, vectorizer=vectorizer)
    df = pd.DataFrame(features, index=ids)
    if with_labels:
        df['y'] = y
        current = FeatureSet.from_data_frame(df, featureset_name, labels_column='y', vectorizer=vectorizer)
    else:
        current = FeatureSet.from_data_frame(df, featureset_name, vectorizer=vectorizer)
    return (expected, current)","for row in X:
    features.append(dict(zip(feature_names, row)))","features = [dict(zip(feature_names, row)) for row in X]","features = [dict(zip(feature_names, row)) for row in X]",1,,,,,robosuite
dpkt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dpkt/dpkt/netbios.py,https://github.com/kbandla/dpkt/tree/master/dpkt/netbios.py,,decode_name$76,"def decode_name(nbname):
    """"""
    Return the NetBIOS first-level decoded nbname.

    """"""
    if len(nbname) != 32:
        return nbname
    l_ = []
    for i in range(0, 32, 2):
        l_.append(chr(ord(nbname[i]) - 65 << 4 | ord(nbname[i + 1]) - 65 & 15))
    return ''.join(l_).split('\x00', 1)[0]","for i in range(0, 32, 2):
    l_.append(chr(ord(nbname[i]) - 65 << 4 | ord(nbname[i + 1]) - 65 & 15))","l_ = [chr(ord(nbname[i]) - 65 << 4 | ord(nbname[i + 1]) - 65 & 15) for i in range(0, 32, 2)]","l_ = [chr(ord(nbname[i]) - 65 << 4 | ord(nbname[i + 1]) - 65 & 15) for i in range(0, 32, 2)]",1,,,,,robosuite
specter-desktop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/specter-desktop/src/cryptoadvance/specter/rpc.py,https://github.com/cryptoadvance/specter-desktop/tree/master/src/cryptoadvance/specter/rpc.py,,detect_rpc_confs$116,"def detect_rpc_confs(config=None, datadir=get_default_datadir()):
    rpcconfs = get_configs(config, datadir)
    rpc_arr = []
    for conf in rpcconfs:
        rpc_arr.append(conf)
    return rpc_arr","for conf in rpcconfs:
    rpc_arr.append(conf)",rpc_arr = [conf for conf in rpcconfs],rpc_arr = [conf for conf in rpcconfs],1,,,,,robosuite
writehat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/writehat/writehat/views.py,https://github.com/blacklanternsecurity/writehat/tree/master/writehat/views.py,,engagementFgroupList$1376,"def engagementFgroupList(request, uuid):
    fgroupsDict = {}
    log.debug(f'engagementFgroupList called for UUID {uuid}; request.method: {request.method}')
    CVSSFGroupList = []
    CVSSFgroups = CVSSFindingGroup.objects.filter(engagementParent=uuid)
    log.debug(list(CVSSFgroups))
    for i in CVSSFgroups:
        CVSSFGroupList.append({'id': str(i.id), 'name': str(i.name)})
    fgroupsDict['CVSS'] = CVSSFGroupList
    DreadFGroupList = []
    DreadFgroups = DREADFindingGroup.objects.filter(engagementParent=uuid)
    log.debug(list(DreadFgroups))
    for i in DreadFgroups:
        DreadFGroupList.append({'id': str(i.id), 'name': str(i.name)})
    fgroupsDict['DREAD'] = DreadFGroupList
    return JsonResponse(fgroupsDict)","for i in CVSSFgroups:
    CVSSFGroupList.append({'id': str(i.id), 'name': str(i.name)})","CVSSFGroupList = [{'id': str(i.id), 'name': str(i.name)} for i in CVSSFgroups]","CVSSFGroupList = [{'id': str(i.id), 'name': str(i.name)} for i in CVSSFgroups]",1,,,,,robosuite
writehat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/writehat/writehat/views.py,https://github.com/blacklanternsecurity/writehat/tree/master/writehat/views.py,,engagementFgroupList$1376,"def engagementFgroupList(request, uuid):
    fgroupsDict = {}
    log.debug(f'engagementFgroupList called for UUID {uuid}; request.method: {request.method}')
    CVSSFGroupList = []
    CVSSFgroups = CVSSFindingGroup.objects.filter(engagementParent=uuid)
    log.debug(list(CVSSFgroups))
    for i in CVSSFgroups:
        CVSSFGroupList.append({'id': str(i.id), 'name': str(i.name)})
    fgroupsDict['CVSS'] = CVSSFGroupList
    DreadFGroupList = []
    DreadFgroups = DREADFindingGroup.objects.filter(engagementParent=uuid)
    log.debug(list(DreadFgroups))
    for i in DreadFgroups:
        DreadFGroupList.append({'id': str(i.id), 'name': str(i.name)})
    fgroupsDict['DREAD'] = DreadFGroupList
    return JsonResponse(fgroupsDict)","for i in DreadFgroups:
    DreadFGroupList.append({'id': str(i.id), 'name': str(i.name)})","DreadFGroupList = [{'id': str(i.id), 'name': str(i.name)} for i in DreadFgroups]","DreadFGroupList = [{'id': str(i.id), 'name': str(i.name)} for i in DreadFgroups]",1,,,,,robosuite
audio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/audio/test/torchaudio_unittest/example/souce_sepration/sdr_reference.py,https://github.com/pytorch/audio/tree/master/test/torchaudio_unittest/example/souce_sepration/sdr_reference.py,,batch_SDR_torch$52,"def batch_SDR_torch(estimation, origin, mask=None):
    """"""
    batch-wise SDR caculation for multiple audio files.
    estimation: (batch, nsource, nsample)
    origin: (batch, nsource, nsample)
    mask: optional, (batch, nsample), binary
    """"""
    (batch_size_est, nsource_est, nsample_est) = estimation.size()
    (batch_size_ori, nsource_ori, nsample_ori) = origin.size()
    assert batch_size_est == batch_size_ori, 'Estimation and original sources should have same shape.'
    assert nsource_est == nsource_ori, 'Estimation and original sources should have same shape.'
    assert nsample_est == nsample_ori, 'Estimation and original sources should have same shape.'
    assert nsource_est < nsample_est, 'Axis 1 should be the number of sources, and axis 2 should be the signal.'
    batch_size = batch_size_est
    nsource = nsource_est
    nsample = nsample_est
    estimation = estimation - torch.mean(estimation, 2, keepdim=True).expand_as(estimation)
    origin = origin - torch.mean(origin, 2, keepdim=True).expand_as(estimation)
    perm = list(set(permutations(np.arange(nsource))))
    SDR = torch.zeros((batch_size, nsource, nsource)).type(estimation.type())
    for i in range(nsource):
        for j in range(nsource):
            SDR[:, i, j] = calc_sdr_torch(estimation[:, i], origin[:, j], mask)
    SDR_max = []
    SDR_perm = []
    for permute in perm:
        sdr = []
        for idx in range(len(permute)):
            sdr.append(SDR[:, idx, permute[idx]].view(batch_size, -1))
        sdr = torch.sum(torch.cat(sdr, 1), 1)
        SDR_perm.append(sdr.view(batch_size, 1))
    SDR_perm = torch.cat(SDR_perm, 1)
    (SDR_max, _) = torch.max(SDR_perm, dim=1)
    return SDR_max / nsource","for permute in perm:
    sdr = []
    for idx in range(len(permute)):
        sdr.append(SDR[:, idx, permute[idx]].view(batch_size, -1))
    sdr = torch.sum(torch.cat(sdr, 1), 1)
    SDR_perm.append(sdr.view(batch_size, 1))","SDR_perm = [torch.sum(torch.cat([SDR[:, idx, permute[idx]].view(batch_size, -1) for idx in range(len(permute))], 1), 1).view(batch_size, 1) for permute in perm]",Cannot refactor,-1,1,,,,robosuite
audio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/audio/test/torchaudio_unittest/example/souce_sepration/sdr_reference.py,https://github.com/pytorch/audio/tree/master/test/torchaudio_unittest/example/souce_sepration/sdr_reference.py,,batch_SDR_torch$52,"def batch_SDR_torch(estimation, origin, mask=None):
    """"""
    batch-wise SDR caculation for multiple audio files.
    estimation: (batch, nsource, nsample)
    origin: (batch, nsource, nsample)
    mask: optional, (batch, nsample), binary
    """"""
    (batch_size_est, nsource_est, nsample_est) = estimation.size()
    (batch_size_ori, nsource_ori, nsample_ori) = origin.size()
    assert batch_size_est == batch_size_ori, 'Estimation and original sources should have same shape.'
    assert nsource_est == nsource_ori, 'Estimation and original sources should have same shape.'
    assert nsample_est == nsample_ori, 'Estimation and original sources should have same shape.'
    assert nsource_est < nsample_est, 'Axis 1 should be the number of sources, and axis 2 should be the signal.'
    batch_size = batch_size_est
    nsource = nsource_est
    nsample = nsample_est
    estimation = estimation - torch.mean(estimation, 2, keepdim=True).expand_as(estimation)
    origin = origin - torch.mean(origin, 2, keepdim=True).expand_as(estimation)
    perm = list(set(permutations(np.arange(nsource))))
    SDR = torch.zeros((batch_size, nsource, nsource)).type(estimation.type())
    for i in range(nsource):
        for j in range(nsource):
            SDR[:, i, j] = calc_sdr_torch(estimation[:, i], origin[:, j], mask)
    SDR_max = []
    SDR_perm = []
    for permute in perm:
        sdr = []
        for idx in range(len(permute)):
            sdr.append(SDR[:, idx, permute[idx]].view(batch_size, -1))
        sdr = torch.sum(torch.cat(sdr, 1), 1)
        SDR_perm.append(sdr.view(batch_size, 1))
    SDR_perm = torch.cat(SDR_perm, 1)
    (SDR_max, _) = torch.max(SDR_perm, dim=1)
    return SDR_max / nsource","for idx in range(len(permute)):
    sdr.append(SDR[:, idx, permute[idx]].view(batch_size, -1))","sdr = [SDR[:, idx, permute[idx]].view(batch_size, -1) for idx in range(len(permute))]","sdr = [SDR[:, idx, permute[idx]].view(batch_size, -1) for idx in range(len(permute))]",1,,,,,robosuite
3d-vehicle-tracking,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/3d-vehicle-tracking/3d-tracking/motion_lstm.py,https://github.com/ucbdrive/3d-vehicle-tracking/tree/master/3d-tracking/motion_lstm.py,Dataset,sample$343,"def sample(self, data, seq_len=10):
    datakey = []
    for key in list(data):
        if len(data[key]) > seq_len:
            datakey.append(key)
    return datakey","for key in list(data):
    if len(data[key]) > seq_len:
        datakey.append(key)",datakey = [key for key in list(data) if len(data[key]) > seq_len],datakey = [key for key in list(data) if len(data[key]) > seq_len],1,,,,,robosuite
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for i in range(len(params)):
    formatted_and_starred = []
    for (v, pv) in zip(params.values[i], pvalues[i]):
        formatted_and_starred.append(add_star(_str(v), pv, self._stars))
    params_fmt.append(formatted_and_starred)
    precision_fmt = []
    for v in precision.values[i]:
        v_str = _str(v)
        v_str = '({0})'.format(v_str) if v_str.strip() else v_str
        precision_fmt.append(v_str)
    params_fmt.append(precision_fmt)
    params_stub.append(params.index[i])
    params_stub.append(' ')",params_stub = [params.index[i] + ' ' for i in range(len(params))],Cannot refactor,-1,0,,,,robosuite
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for key in self._results:
    res = self._results[key]
    all_instr.append(res.model.instruments.cols)",all_instr = [self._results[key].model.instruments.cols for key in self._results],Cannot refactor,-1,1,,,,robosuite
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for (v, pv) in zip(params.values[i], pvalues[i]):
    formatted_and_starred.append(add_star(_str(v), pv, self._stars))","formatted_and_starred = [add_star(_str(v), pv, self._stars) for (v, pv) in zip(params.values[i], pvalues[i])]","formatted_and_starred = [add_star(_str(v), pv, self._stars) for (v, pv) in zip(params.values[i], pvalues[i])]",1,,,,,robosuite
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for v in precision.values[i]:
    v_str = _str(v)
    v_str = '({0})'.format(v_str) if v_str.strip() else v_str
    precision_fmt.append(v_str)",precision_fmt = ['({0})'.format(_str(v)) if _str(v).strip() else _str(v) for v in precision.values[i]],Cannot refactor,-1,1,,,,robosuite
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_settings.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_settings.py,,settings_action_import$598,"def settings_action_import(form):
    """"""
    Receive an action module file, check it for errors, add it to Mycodo controller list
    """"""
    action = '{action} {controller}'.format(action=gettext('Import'), controller=TRANSLATIONS['actions']['title'])
    error = []
    action_info = None
    try:
        install_dir = os.path.abspath(INSTALL_DIRECTORY)
        tmp_directory = os.path.join(install_dir, 'mycodo/actions/tmp_actions')
        assure_path_exists(tmp_directory)
        assure_path_exists(PATH_ACTIONS_CUSTOM)
        tmp_name = 'tmp_action_testing.py'
        full_path_tmp = os.path.join(tmp_directory, tmp_name)
        if not form.import_action_file.data:
            error.append('No file present')
        elif form.import_action_file.data.filename == '':
            error.append('No file name')
        else:
            form.import_action_file.data.save(full_path_tmp)
        try:
            (action_info, status) = load_module_from_file(full_path_tmp, 'actions')
            if not action_info or not hasattr(action_info, 'ACTION_INFORMATION'):
                error.append('Could not load ACTION_INFORMATION dictionary from the uploaded action module')
        except Exception:
            error.append('Could not load uploaded file as a python module:\n{}'.format(traceback.format_exc()))
        dict_actions = parse_action_information()
        list_actions = []
        for each_key in dict_actions.keys():
            list_actions.append(each_key.lower())
        if not error:
            if 'name_unique' not in action_info.ACTION_INFORMATION:
                error.append(""'name_unique' not found in ACTION_INFORMATION dictionary"")
            elif action_info.ACTION_INFORMATION['name_unique'] == '':
                error.append(""'name_unique' is empty"")
            elif action_info.ACTION_INFORMATION['name_unique'].lower() in list_actions:
                error.append(""'name_unique' is not unique, there is already an action with that name ({})"".format(action_info.ACTION_INFORMATION['name_unique'].lower()))
            if 'name' not in action_info.ACTION_INFORMATION:
                error.append(""'name' not found in ACTION_INFORMATION dictionary"")
            elif action_info.ACTION_INFORMATION['name'] == '':
                error.append(""'name' is empty"")
            if 'dependencies_module' in action_info.ACTION_INFORMATION:
                if not isinstance(action_info.ACTION_INFORMATION['dependencies_module'], list):
                    error.append(""'dependencies_module' must be a list of tuples"")
                else:
                    for each_dep in action_info.ACTION_INFORMATION['dependencies_module']:
                        if not isinstance(each_dep, tuple):
                            error.append(""'dependencies_module' must be a list of tuples"")
                        elif len(each_dep) != 3:
                            error.append(""'dependencies_module': tuples in list must have 3 items"")
                        elif not each_dep[0] or not each_dep[1] or (not each_dep[2]):
                            error.append(""'dependencies_module': tuples in list must not be empty"")
                        elif each_dep[0] not in ['internal', 'pip-pypi', 'apt']:
                            error.append(""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"")
        if not error:
            unique_name = '{}.py'.format(action_info.ACTION_INFORMATION['name_unique'].lower())
            full_path_final = os.path.join(PATH_ACTIONS_CUSTOM, unique_name)
            os.rename(full_path_tmp, full_path_final)
            cmd = '{path}/mycodo/scripts/mycodo_wrapper frontend_reload 2>&1'.format(path=install_dir)
            subprocess.Popen(cmd, shell=True)
            flash('Frontend reloaded to scan for new Action Modules', 'success')
    except Exception as err:
        logger.exception('Action Import')
        error.append('Exception: {}'.format(err))
    flash_success_errors(error, action, url_for('routes_settings.settings_action'))","for each_key in dict_actions.keys():
    list_actions.append(each_key.lower())",list_actions = [each_key.lower() for each_key in dict_actions.keys()],list_actions = [each_key.lower() for each_key in dict_actions.keys()],1,,,,,robosuite
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_settings.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_settings.py,,settings_action_import$598,"def settings_action_import(form):
    """"""
    Receive an action module file, check it for errors, add it to Mycodo controller list
    """"""
    action = '{action} {controller}'.format(action=gettext('Import'), controller=TRANSLATIONS['actions']['title'])
    error = []
    action_info = None
    try:
        install_dir = os.path.abspath(INSTALL_DIRECTORY)
        tmp_directory = os.path.join(install_dir, 'mycodo/actions/tmp_actions')
        assure_path_exists(tmp_directory)
        assure_path_exists(PATH_ACTIONS_CUSTOM)
        tmp_name = 'tmp_action_testing.py'
        full_path_tmp = os.path.join(tmp_directory, tmp_name)
        if not form.import_action_file.data:
            error.append('No file present')
        elif form.import_action_file.data.filename == '':
            error.append('No file name')
        else:
            form.import_action_file.data.save(full_path_tmp)
        try:
            (action_info, status) = load_module_from_file(full_path_tmp, 'actions')
            if not action_info or not hasattr(action_info, 'ACTION_INFORMATION'):
                error.append('Could not load ACTION_INFORMATION dictionary from the uploaded action module')
        except Exception:
            error.append('Could not load uploaded file as a python module:\n{}'.format(traceback.format_exc()))
        dict_actions = parse_action_information()
        list_actions = []
        for each_key in dict_actions.keys():
            list_actions.append(each_key.lower())
        if not error:
            if 'name_unique' not in action_info.ACTION_INFORMATION:
                error.append(""'name_unique' not found in ACTION_INFORMATION dictionary"")
            elif action_info.ACTION_INFORMATION['name_unique'] == '':
                error.append(""'name_unique' is empty"")
            elif action_info.ACTION_INFORMATION['name_unique'].lower() in list_actions:
                error.append(""'name_unique' is not unique, there is already an action with that name ({})"".format(action_info.ACTION_INFORMATION['name_unique'].lower()))
            if 'name' not in action_info.ACTION_INFORMATION:
                error.append(""'name' not found in ACTION_INFORMATION dictionary"")
            elif action_info.ACTION_INFORMATION['name'] == '':
                error.append(""'name' is empty"")
            if 'dependencies_module' in action_info.ACTION_INFORMATION:
                if not isinstance(action_info.ACTION_INFORMATION['dependencies_module'], list):
                    error.append(""'dependencies_module' must be a list of tuples"")
                else:
                    for each_dep in action_info.ACTION_INFORMATION['dependencies_module']:
                        if not isinstance(each_dep, tuple):
                            error.append(""'dependencies_module' must be a list of tuples"")
                        elif len(each_dep) != 3:
                            error.append(""'dependencies_module': tuples in list must have 3 items"")
                        elif not each_dep[0] or not each_dep[1] or (not each_dep[2]):
                            error.append(""'dependencies_module': tuples in list must not be empty"")
                        elif each_dep[0] not in ['internal', 'pip-pypi', 'apt']:
                            error.append(""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"")
        if not error:
            unique_name = '{}.py'.format(action_info.ACTION_INFORMATION['name_unique'].lower())
            full_path_final = os.path.join(PATH_ACTIONS_CUSTOM, unique_name)
            os.rename(full_path_tmp, full_path_final)
            cmd = '{path}/mycodo/scripts/mycodo_wrapper frontend_reload 2>&1'.format(path=install_dir)
            subprocess.Popen(cmd, shell=True)
            flash('Frontend reloaded to scan for new Action Modules', 'success')
    except Exception as err:
        logger.exception('Action Import')
        error.append('Exception: {}'.format(err))
    flash_success_errors(error, action, url_for('routes_settings.settings_action'))","for each_dep in action_info.ACTION_INFORMATION['dependencies_module']:
    if not isinstance(each_dep, tuple):
        error.append(""'dependencies_module' must be a list of tuples"")
    elif len(each_dep) != 3:
        error.append(""'dependencies_module': tuples in list must have 3 items"")
    elif not each_dep[0] or not each_dep[1] or (not each_dep[2]):
        error.append(""'dependencies_module': tuples in list must not be empty"")
    elif each_dep[0] not in ['internal', 'pip-pypi', 'apt']:
        error.append(""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"")","error += [""'dependencies_module' must be a list of tuples"" if not isinstance(each_dep, tuple) else ""'dependencies_module': tuples in list must have 3 items"" if len(each_dep) != 3 else ""'dependencies_module': tuples in list must not be empty"" if not each_dep[0] or not each_dep[1] or (not each_dep[2]) else ""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"" if each_dep[0] not in ['internal', 'pip-pypi', 'apt'] else None for each_dep in action_info.ACTION_INFORMATION['dependencies_module']]",Cannot refactor,-1,0,,,,robosuite
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/scripts/joinmarket-qt.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/scripts/joinmarket-qt.py,JMMainWindow,exportPrivkeysJson$1805,"def exportPrivkeysJson(self):
    if not self.wallet_service:
        JMQtMessageBox(self, 'No wallet loaded.', mbtype='crit', title='Error')
        return
    d = QDialog(self)
    d.setWindowTitle('Private keys')
    d.setMinimumSize(850, 300)
    vbox = QVBoxLayout(d)
    msg = '%s\n%s\n%s' % ('WARNING: ALL your private keys are secret.', 'Exposing a single private key can compromise your entire wallet!', ""In particular, DO NOT use 'redeem private key' services proposed by third parties."")
    vbox.addWidget(QLabel(msg))
    e = QTextEdit()
    e.setReadOnly(True)
    vbox.addWidget(e)
    b = OkButton(d, 'Export')
    b.setEnabled(False)
    vbox.addLayout(Buttons(CancelButton(d), b))
    private_keys = {}
    rows = get_wallet_printout(self.wallet_service)
    addresses = []
    for forchange in rows[0]:
        for mixdepth in forchange:
            for addr_info in mixdepth:
                if float(addr_info[2]) > 0:
                    addresses.append(addr_info[0])
    done = False

    def privkeys_thread():
        get_blockchain_interface_instance(jm_single().config)
        for addr in addresses:
            time.sleep(0.1)
            if done:
                break
            priv = self.wallet_service.get_key_from_addr(addr)
            private_keys[addr] = BTCEngine.privkey_to_wif(priv)
            self.computing_privkeys_signal.emit()
        self.show_privkeys_signal.emit()

    def show_privkeys():
        s = '\n'.join(map(lambda x: x[0] + '\t' + x[1], private_keys.items()))
        e.setText(s)
        b.setEnabled(True)
    self.computing_privkeys_signal.connect(lambda : e.setText('Please wait... %d/%d' % (len(private_keys), len(addresses))))
    self.show_privkeys_signal.connect(show_privkeys)
    threading.Thread(target=privkeys_thread).start()
    if not d.exec_():
        done = True
        return
    privkeys_fn_base = 'joinmarket-private-keys'
    i = 0
    privkeys_fn = privkeys_fn_base
    while os.path.isfile(os.path.join(jm_single().datadir, privkeys_fn + '.json')):
        i += 1
        privkeys_fn = privkeys_fn_base + str(i)
    try:
        with open(os.path.join(jm_single().datadir, privkeys_fn + '.json'), 'wb') as f:
            for (addr, pk) in private_keys.items():
                (rawpriv, _) = BTCEngine.wif_to_privkey(pk)
                if not addr == self.wallet_service._ENGINE.privkey_to_address(rawpriv):
                    JMQtMessageBox(None, 'Failed to create privkey export -' + ' critical error in key parsing.', mbtype='crit')
                    return
            f.write(json.dumps(private_keys, indent=4).encode('utf-8'))
    except (IOError, os.error) as reason:
        export_error_label = 'JoinmarketQt was unable to produce a private key-export.'
        JMQtMessageBox(None, export_error_label + '\n' + str(reason), mbtype='crit', title='Unable to create json file')
    except Exception as er:
        JMQtMessageBox(self, str(er), mbtype='crit', title='Error')
        return
    JMQtMessageBox(self, 'Private keys exported to: ' + os.path.join(jm_single().datadir, privkeys_fn) + '.json', title='Success')","for forchange in rows[0]:
    for mixdepth in forchange:
        for addr_info in mixdepth:
            if float(addr_info[2]) > 0:
                addresses.append(addr_info[0])",addresses = [addr_info[0] for forchange in rows[0] for mixdepth in forchange for addr_info in mixdepth if float(addr_info[2]) > 0],addresses = [addr_info[0] for forchange in rows[0] for mixdepth in forchange for addr_info in mixdepth if float(addr_info[2]) > 0],1,,,,,robosuite
shuup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup_tests/supplier_prices/test_category_detail_filter.py,https://github.com/shuup/shuup/tree/master/shuup_tests/supplier_prices/test_category_detail_filter.py,,test_category_detail_filters$80,"def test_category_detail_filters(client, reindex_catalog):
    shop = factories.get_default_shop()
    assert ThemeSettings.objects.count() == 1
    theme_settings = ThemeSettings.objects.first()
    theme_settings.update_settings({'show_supplier_info': True})
    category = factories.get_default_category()
    set_configuration(shop=shop, data={'filter_products_by_supplier': True, 'filter_products_by_supplier_ordering': 1})
    product_data = [('laptop', 1500), ('keyboard', 150), ('mouse', 150)]
    products = []
    for (sku, price_value) in product_data:
        products.append(factories.create_product(sku, shop=shop, default_price=price_value))
    supplier_data = [('Johnny Inc', 0.5), ('Mike Inc', 0.9), ('Simon Inc', 0.8)]
    for (name, percentage_from_original_price) in supplier_data:
        supplier = factories.get_supplier('simple_supplier', shop, name=name)
        for product in products:
            shop_product = product.get_shop_instance(shop)
            shop_product.suppliers.add(supplier)
            shop_product.primary_category = category
            shop_product.categories.add(category)
            shop_product.save()
            supplier_price = percentage_from_original_price * [price for (sku, price) in product_data if product.sku == sku][0]
            SupplierPrice.objects.create(supplier=supplier, shop=shop, product=product, amount_value=supplier_price)
    strategy = 'shuup.testing.supplier_pricing.supplier_strategy:CheapestSupplierPriceSupplierStrategy'
    with override_settings(SHUUP_PRICING_MODULE='supplier_pricing', SHUUP_SHOP_PRODUCT_SUPPLIERS_STRATEGY=strategy):
        with override_current_theme_class(ClassicGrayTheme, shop):
            reindex_catalog()
            laptop = [product for product in products if product.sku == 'laptop'][0]
            keyboard = [product for product in products if product.sku == 'keyboard'][0]
            mouse = [product for product in products if product.sku == 'mouse'][0]
            supplier_johnny = Supplier.objects.filter(name='Johnny Inc').first()
            soup = _get_category_detail_soup(client, category, supplier_johnny.pk)
            laptop_product_box = soup.find('div', {'id': 'product-%s' % laptop.pk})
            _assert_supplier_info(laptop_product_box, 'Johnny Inc')
            _assert_product_price(laptop_product_box, 750)
            _assert_product_url(laptop_product_box, supplier_johnny, laptop)
            keyboard_product_box = soup.find('div', {'id': 'product-%s' % keyboard.pk})
            _assert_supplier_info(keyboard_product_box, 'Johnny Inc')
            _assert_product_price(keyboard_product_box, 75)
            _assert_product_url(keyboard_product_box, supplier_johnny, keyboard)
            mike_supplier = Supplier.objects.filter(name='Mike Inc').first()
            soup = _get_category_detail_soup(client, category, mike_supplier.pk)
            keyboard_product_box = soup.find('div', {'id': 'product-%s' % keyboard.pk})
            _assert_supplier_info(keyboard_product_box, 'Mike Inc')
            _assert_product_price(keyboard_product_box, 135)
            _assert_product_url(keyboard_product_box, mike_supplier, keyboard)
            simon_supplier = Supplier.objects.filter(name='Simon Inc').first()
            soup = _get_category_detail_soup(client, category, simon_supplier.pk)
            mouse_product_box = soup.find('div', {'id': 'product-%s' % mouse.pk})
            _assert_supplier_info(mouse_product_box, 'Simon Inc')
            _assert_product_price(mouse_product_box, 120)
            _assert_product_url(mouse_product_box, simon_supplier, mouse)","for (sku, price_value) in product_data:
    products.append(factories.create_product(sku, shop=shop, default_price=price_value))","products = [factories.create_product(sku, shop=shop, default_price=price_value) for (sku, price_value) in product_data]","products = [factories.create_product(sku, shop=shop, default_price=price_value) for (sku, price_value) in product_data]",1,,,,,robosuite
yt-dlc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/aes.py,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/aes.py,,shift_rows_inv$342,"def shift_rows_inv(data):
    data_shifted = []
    for column in range(4):
        for row in range(4):
            data_shifted.append(data[(column - row & 3) * 4 + row])
    return data_shifted","for column in range(4):
    for row in range(4):
        data_shifted.append(data[(column - row & 3) * 4 + row])",data_shifted = [data[(column - row & 3) * 4 + row] for column in range(4) for row in range(4)],data_shifted = [data[(column - row & 3) * 4 + row] for column in range(4) for row in range(4)],1,,,,,robosuite
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/environment/collectors/toctree.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/environment/collectors/toctree.py,TocTreeCollector,assign_figure_numbers$240,"def assign_figure_numbers(self, env: BuildEnvironment) -> List[str]:
    """"""Assign a figure number to each figure under a numbered toctree.""""""
    generated_docnames = frozenset(env.domains['std']._virtual_doc_names)
    rewrite_needed = []
    assigned: Set[str] = set()
    old_fignumbers = env.toc_fignumbers
    env.toc_fignumbers = {}
    fignum_counter: Dict[str, Dict[Tuple[int, ...], int]] = {}

    def get_figtype(node: Node) -> Optional[str]:
        for domain in env.domains.values():
            figtype = domain.get_enumerable_node_type(node)
            if domain.name == 'std' and (not domain.get_numfig_title(node)):
                continue
            if figtype:
                return figtype
        return None

    def get_section_number(docname: str, section: nodes.section) -> Tuple[int, ...]:
        anchorname = '#' + section['ids'][0]
        secnumbers = env.toc_secnumbers.get(docname, {})
        if anchorname in secnumbers:
            secnum = secnumbers.get(anchorname)
        else:
            secnum = secnumbers.get('')
        return secnum or ()

    def get_next_fignumber(figtype: str, secnum: Tuple[int, ...]) -> Tuple[int, ...]:
        counter = fignum_counter.setdefault(figtype, {})
        secnum = secnum[:env.config.numfig_secnum_depth]
        counter[secnum] = counter.get(secnum, 0) + 1
        return secnum + (counter[secnum],)

    def register_fignumber(docname: str, secnum: Tuple[int, ...], figtype: str, fignode: Element) -> None:
        env.toc_fignumbers.setdefault(docname, {})
        fignumbers = env.toc_fignumbers[docname].setdefault(figtype, {})
        figure_id = fignode['ids'][0]
        fignumbers[figure_id] = get_next_fignumber(figtype, secnum)

    def _walk_doctree(docname: str, doctree: Element, secnum: Tuple[int, ...]) -> None:
        nonlocal generated_docnames
        for subnode in doctree.children:
            if isinstance(subnode, nodes.section):
                next_secnum = get_section_number(docname, subnode)
                if next_secnum:
                    _walk_doctree(docname, subnode, next_secnum)
                else:
                    _walk_doctree(docname, subnode, secnum)
            elif isinstance(subnode, addnodes.toctree):
                for (_title, subdocname) in subnode['entries']:
                    if url_re.match(subdocname) or subdocname == 'self':
                        continue
                    if subdocname in generated_docnames:
                        continue
                    _walk_doc(subdocname, secnum)
            elif isinstance(subnode, nodes.Element):
                figtype = get_figtype(subnode)
                if figtype and subnode['ids']:
                    register_fignumber(docname, secnum, figtype, subnode)
                _walk_doctree(docname, subnode, secnum)

    def _walk_doc(docname: str, secnum: Tuple[int, ...]) -> None:
        if docname not in assigned:
            assigned.add(docname)
            doctree = env.get_doctree(docname)
            _walk_doctree(docname, doctree, secnum)
    if env.config.numfig:
        _walk_doc(env.config.root_doc, ())
        for (docname, fignums) in env.toc_fignumbers.items():
            if fignums != old_fignumbers.get(docname):
                rewrite_needed.append(docname)
    return rewrite_needed","for (docname, fignums) in env.toc_fignumbers.items():
    if fignums != old_fignumbers.get(docname):
        rewrite_needed.append(docname)","rewrite_needed = [docname for (docname, fignums) in env.toc_fignumbers.items() if fignums != old_fignumbers.get(docname)]","rewrite_needed = [docname for (docname, fignums) in env.toc_fignumbers.items() if fignums != old_fignumbers.get(docname)]",1,,,,,robosuite
RATDecoders,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RATDecoders/malwareconfig/yarascanner.py,https://github.com/kevthehermit/RATDecoders/tree/master/malwareconfig/yarascanner.py,YaraScanner,yara_scan$17,"def yara_scan(self, raw_data):
    matches = self.compiled_rules.match(data=raw_data)
    rule_list = []
    for match in matches:
        rule_list.append(match.rule)
    self.rule_list = rule_list","for match in matches:
    rule_list.append(match.rule)",rule_list = [match.rule for match in matches],rule_list = [match.rule for match in matches],1,,,,,robosuite
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/external_project_access_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/external_project_access_scanner.py,ExternalProjectAccessScanner,_flatten_violations$204,"def _flatten_violations(violations):
    """"""Flatten RuleViolations into a dict for each RuleViolation member.

        Args:
            violations (list): The RuleViolations to flatten.

        Yields:
            dict: Iterator of RuleViolations as a dict per member.
        """"""
    for violation in violations:
        rule_ancestors_names = []
        for ancestor in violation.rule_data['ancestor_resources']:
            rule_ancestors_names.append(ancestor.name)
        violation_data = {'full_name': violation.full_name, 'member': violation.member, 'rule_ancestors': rule_ancestors_names}
        yield {'resource_id': violation.resource_id, 'resource_type': violation.resource_type, 'full_name': violation.full_name, 'rule_index': violation.rule_index, 'rule_name': violation.rule_name, 'violation_type': violation.violation_type, 'violation_data': violation_data, 'resource_data': violation.resource_data}","for ancestor in violation.rule_data['ancestor_resources']:
    rule_ancestors_names.append(ancestor.name)",rule_ancestors_names = [ancestor.name for ancestor in violation.rule_data['ancestor_resources']],rule_ancestors_names = [ancestor.name for ancestor in violation.rule_data['ancestor_resources']],1,,,,,robosuite
orator,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orator/orator/dbal/table.py,https://github.com/sdispater/orator/tree/master/orator/dbal/table.py,Table,_add_index$343,"def _add_index(self, index):
    """"""
        Adds an index to the table.

        :param index: The index to add
        :type index: Index

        :rtype: Table
        """"""
    index_name = index.get_name()
    index_name = self._normalize_identifier(index_name)
    replaced_implicit_indexes = []
    for (name, implicit_index) in self._implicit_indexes.items():
        if implicit_index.is_fullfilled_by(index) and name in self._indexes:
            replaced_implicit_indexes.append(name)
    already_exists = index_name in self._indexes and index_name not in replaced_implicit_indexes or (self._primary_key_name is not False and index.is_primary())
    if already_exists:
        raise IndexAlreadyExists(index_name, self._name)
    for name in replaced_implicit_indexes:
        del self._indexes[name]
        del self._implicit_indexes[name]
    if index.is_primary():
        self._primary_key_name = index_name
    self._indexes[index_name] = index
    return self","for (name, implicit_index) in self._implicit_indexes.items():
    if implicit_index.is_fullfilled_by(index) and name in self._indexes:
        replaced_implicit_indexes.append(name)","replaced_implicit_indexes = [name for (name, implicit_index) in self._implicit_indexes.items() if implicit_index.is_fullfilled_by(index) and name in self._indexes]","replaced_implicit_indexes = [name for (name, implicit_index) in self._implicit_indexes.items() if implicit_index.is_fullfilled_by(index) and name in self._indexes]",1,,,,,robosuite
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/views.py,https://github.com/apache/airflow/tree/master/airflow/www/views.py,Airflow,xcom$1542,"def xcom(self, session=None):
    """"""Retrieve XCOM.""""""
    dag_id = request.args.get('dag_id')
    task_id = request.args.get('task_id')
    execution_date = request.args.get('execution_date')
    dttm = timezone.parse(execution_date)
    form = DateTimeForm(data={'execution_date': dttm})
    root = request.args.get('root', '')
    ti_db = models.TaskInstance
    dag = DagModel.get_dagmodel(dag_id)
    ti = session.query(ti_db).filter(and_(ti_db.dag_id == dag_id, ti_db.task_id == task_id)).first()
    if not ti:
        flash(f""Task [{dag_id}.{task_id}] doesn't seem to exist at the moment"", 'error')
        return redirect(url_for('Airflow.index'))
    xcomlist = session.query(XCom).filter(XCom.dag_id == dag_id, XCom.task_id == task_id, XCom.execution_date == dttm).all()
    attributes = []
    for xcom in xcomlist:
        if not xcom.key.startswith('_'):
            attributes.append((xcom.key, xcom.value))
    title = 'XCom'
    return self.render_template('airflow/xcom.html', attributes=attributes, task_id=task_id, execution_date=execution_date, form=form, root=root, dag=dag, title=title)","for xcom in xcomlist:
    if not xcom.key.startswith('_'):
        attributes.append((xcom.key, xcom.value))","attributes = [(xcom.key, xcom.value) for xcom in xcomlist if not xcom.key.startswith('_')]","attributes = [(xcom.key, xcom.value) for xcom in xcomlist if not xcom.key.startswith('_')]",1,,,,,robosuite
django-haystack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-haystack/haystack/backends/solr_backend.py,https://github.com/django-haystack/django-haystack/tree/master/haystack/backends/solr_backend.py,SolrSearchBackend,extract_spelling_suggestions$592,"def extract_spelling_suggestions(self, raw_results):
    collations = raw_results.spellcheck.get('collations', None)
    suggestions = raw_results.spellcheck.get('suggestions', None)
    spelling_suggestions = []
    if collations:
        if isinstance(collations, dict):
            collation_values = collations['collation']
            if isinstance(collation_values, str):
                collation_values = [collation_values]
            elif isinstance(collation_values, dict):
                collation_values = [collation_values['collationQuery']]
        elif isinstance(collations[1], dict):
            collation_values = collations
        else:
            collation_values = collations[-1:]
        for i in collation_values:
            spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)
    elif suggestions:
        if isinstance(suggestions, dict):
            for i in suggestions.values():
                for j in i['suggestion']:
                    if isinstance(j, dict):
                        spelling_suggestions.append(j['word'])
                    else:
                        spelling_suggestions.append(j)
        elif isinstance(suggestions[0], str) and isinstance(suggestions[1], dict):
            for suggestion in suggestions:
                if isinstance(suggestion, dict):
                    for i in suggestion['suggestion']:
                        if isinstance(i, dict):
                            spelling_suggestions.append(i['word'])
                        else:
                            spelling_suggestions.append(i)
        else:
            spelling_suggestions.append(suggestions[-1])
    return spelling_suggestions","for i in collation_values:
    spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)","spelling_suggestions = [i['collationQuery'] if isinstance(i, dict) else i for i in collation_values]","spelling_suggestions = [i['collationQuery'] if isinstance(i, dict) else i for i in collation_values]",1,,,,,robosuite
django-haystack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-haystack/haystack/backends/solr_backend.py,https://github.com/django-haystack/django-haystack/tree/master/haystack/backends/solr_backend.py,SolrSearchBackend,extract_spelling_suggestions$592,"def extract_spelling_suggestions(self, raw_results):
    collations = raw_results.spellcheck.get('collations', None)
    suggestions = raw_results.spellcheck.get('suggestions', None)
    spelling_suggestions = []
    if collations:
        if isinstance(collations, dict):
            collation_values = collations['collation']
            if isinstance(collation_values, str):
                collation_values = [collation_values]
            elif isinstance(collation_values, dict):
                collation_values = [collation_values['collationQuery']]
        elif isinstance(collations[1], dict):
            collation_values = collations
        else:
            collation_values = collations[-1:]
        for i in collation_values:
            spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)
    elif suggestions:
        if isinstance(suggestions, dict):
            for i in suggestions.values():
                for j in i['suggestion']:
                    if isinstance(j, dict):
                        spelling_suggestions.append(j['word'])
                    else:
                        spelling_suggestions.append(j)
        elif isinstance(suggestions[0], str) and isinstance(suggestions[1], dict):
            for suggestion in suggestions:
                if isinstance(suggestion, dict):
                    for i in suggestion['suggestion']:
                        if isinstance(i, dict):
                            spelling_suggestions.append(i['word'])
                        else:
                            spelling_suggestions.append(i)
        else:
            spelling_suggestions.append(suggestions[-1])
    return spelling_suggestions","for i in suggestions.values():
    for j in i['suggestion']:
        if isinstance(j, dict):
            spelling_suggestions.append(j['word'])
        else:
            spelling_suggestions.append(j)","spelling_suggestions += [j['word'] if isinstance(j, dict) else j for i in suggestions.values() for j in i['suggestion']]",Cannot refactor,-1,1,,,,robosuite
django-haystack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-haystack/haystack/backends/solr_backend.py,https://github.com/django-haystack/django-haystack/tree/master/haystack/backends/solr_backend.py,SolrSearchBackend,extract_spelling_suggestions$592,"def extract_spelling_suggestions(self, raw_results):
    collations = raw_results.spellcheck.get('collations', None)
    suggestions = raw_results.spellcheck.get('suggestions', None)
    spelling_suggestions = []
    if collations:
        if isinstance(collations, dict):
            collation_values = collations['collation']
            if isinstance(collation_values, str):
                collation_values = [collation_values]
            elif isinstance(collation_values, dict):
                collation_values = [collation_values['collationQuery']]
        elif isinstance(collations[1], dict):
            collation_values = collations
        else:
            collation_values = collations[-1:]
        for i in collation_values:
            spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)
    elif suggestions:
        if isinstance(suggestions, dict):
            for i in suggestions.values():
                for j in i['suggestion']:
                    if isinstance(j, dict):
                        spelling_suggestions.append(j['word'])
                    else:
                        spelling_suggestions.append(j)
        elif isinstance(suggestions[0], str) and isinstance(suggestions[1], dict):
            for suggestion in suggestions:
                if isinstance(suggestion, dict):
                    for i in suggestion['suggestion']:
                        if isinstance(i, dict):
                            spelling_suggestions.append(i['word'])
                        else:
                            spelling_suggestions.append(i)
        else:
            spelling_suggestions.append(suggestions[-1])
    return spelling_suggestions","for suggestion in suggestions:
    if isinstance(suggestion, dict):
        for i in suggestion['suggestion']:
            if isinstance(i, dict):
                spelling_suggestions.append(i['word'])
            else:
                spelling_suggestions.append(i)","spelling_suggestions += [i['word'] if isinstance(i, dict) else i for suggestion in suggestions for i in suggestion['suggestion']]",Cannot refactor,-1,1,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/config.py,https://github.com/phaethon/kamene/tree/master/kamene/config.py,LayersList,__repr__$141,"def __repr__(self):
    s = []
    for l in self:
        s.append('%-20s: %s' % (l.__name__, l.name))
    return '\n'.join(s)","for l in self:
    s.append('%-20s: %s' % (l.__name__, l.name))","s += ['%-20s: %s' % (l.__name__, l.name) for l in self]","s = ['%-20s: %s' % (l.__name__, l.name) for l in self]",0,1,,,,robosuite
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/redshift/models.py,https://github.com/spulec/moto/tree/master/moto/redshift/models.py,RedshiftBackend,describe_cluster_snapshots$816,"def describe_cluster_snapshots(self, cluster_identifier=None, snapshot_identifier=None):
    if cluster_identifier:
        cluster_snapshots = []
        for snapshot in self.snapshots.values():
            if snapshot.cluster.cluster_identifier == cluster_identifier:
                cluster_snapshots.append(snapshot)
        if cluster_snapshots:
            return cluster_snapshots
    if snapshot_identifier:
        if snapshot_identifier in self.snapshots:
            return [self.snapshots[snapshot_identifier]]
        raise ClusterSnapshotNotFoundError(snapshot_identifier)
    return self.snapshots.values()","for snapshot in self.snapshots.values():
    if snapshot.cluster.cluster_identifier == cluster_identifier:
        cluster_snapshots.append(snapshot)",cluster_snapshots = [snapshot for snapshot in self.snapshots.values() if snapshot.cluster.cluster_identifier == cluster_identifier],cluster_snapshots = [snapshot for snapshot in self.snapshots.values() if snapshot.cluster.cluster_identifier == cluster_identifier],1,,,,,robosuite
linux-cli-community,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linux-cli-community/protonvpn_cli/connection.py,https://github.com/Rafficer/linux-cli-community/tree/master/protonvpn_cli/connection.py,,fastest$142,"def fastest(protocol=None):
    """"""Connect to the fastest server available.""""""
    logger.debug('Starting fastest connect')
    if not protocol:
        protocol = get_config_value('USER', 'default_protocol')
    disconnect(passed=True)
    pull_server_data(force=True)
    servers = get_servers()
    excluded_features = [1, 2]
    server_pool = []
    for server in servers:
        if server['Features'] not in excluded_features:
            server_pool.append(server)
    fastest_server = get_fastest_server(server_pool)
    openvpn_connect(fastest_server, protocol)","for server in servers:
    if server['Features'] not in excluded_features:
        server_pool.append(server)",server_pool = [server for server in servers if server['Features'] not in excluded_features],server_pool = [server for server in servers if server['Features'] not in excluded_features],1,,,,,robosuite
amundsen,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/amundsen/metadata/metadata_service/proxy/atlas_proxy.py,https://github.com/amundsen-io/amundsen/tree/master/metadata/metadata_service/proxy/atlas_proxy.py,AtlasProxy,_serialize_badges$402,"def _serialize_badges(self, entity: AtlasEntityWithExtInfo) -> List[Badge]:
    """"""
        Return list of Badges for entity. Badges in Amundsen <> Atlas integration are based on Atlas Classification.

        :param entity: entity for which badges should be collected
        :return : List of Amundsen Badge objects.
        """"""
    result = []
    classifications = entity.get('classifications')
    for classification in classifications or list():
        result.append(Badge(badge_name=classification.get('typeName'), category='default'))
    return result","for classification in classifications or list():
    result.append(Badge(badge_name=classification.get('typeName'), category='default'))","result = [Badge(badge_name=classification.get('typeName'), category='default') for classification in classifications or list()]","result = [Badge(badge_name=classification.get('typeName'), category='default') for classification in classifications or list()]",1,,,,,robosuite
Listed-company-news-crawl-and-text-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Listed-company-news-crawl-and-text-analysis/Crawler/crawler_sina.py,https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/tree/master/Crawler/crawler_sina.py,WebCrawlFromSina,coroutine_run$294,"def coroutine_run(self):
    """"""Coroutines running.
        """"""
    jobs = []
    page_ranges_lst = self.GenPagesLst()
    for page_range in page_ranges_lst:
        jobs.append(gevent.spawn(self.CrawlHistoryCompanyNews, page_range[0], page_range[1]))
    gevent.joinall(jobs)","for page_range in page_ranges_lst:
    jobs.append(gevent.spawn(self.CrawlHistoryCompanyNews, page_range[0], page_range[1]))","jobs = [gevent.spawn(self.CrawlHistoryCompanyNews, page_range[0], page_range[1]) for page_range in page_ranges_lst]","jobs = [gevent.spawn(self.CrawlHistoryCompanyNews, page_range[0], page_range[1]) for page_range in page_ranges_lst]",1,,,,,robosuite
jupyter-dash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jupyter-dash/jupyter_dash/jupyter_app.py,https://github.com/plotly/jupyter-dash/tree/master/jupyter_dash/jupyter_app.py,,_custom_formatargvalues$493,"def _custom_formatargvalues(args, varargs, varkw, locals, formatarg=str, formatvarargs=lambda name: '*' + name, formatvarkw=lambda name: '**' + name, formatvalue=lambda value: '=' + repr(value)):
    """"""Copied from inspect.formatargvalues, modified to place function
    arguments on separate lines""""""

    def convert(name, locals=locals, formatarg=formatarg, formatvalue=formatvalue):
        return formatarg(name) + formatvalue(locals[name])
    specs = []
    for i in range(len(args)):
        specs.append(convert(args[i]))
    if varargs:
        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))
    if varkw:
        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))
    result = '(' + ', '.join(specs) + ')'
    if len(result) < 40:
        return result
    else:
        return '(\n    ' + ',\n    '.join(specs) + '\n)'","for i in range(len(args)):
    specs.append(convert(args[i]))",specs = [convert(arg) for arg in args],specs = [convert(args[i]) for i in range(len(args))],0,1,,,,robosuite
diff-match-patch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diff-match-patch/python3/diff_match_patch.py,https://github.com/google/diff-match-patch/tree/master/python3/diff_match_patch.py,diff_match_patch,diff_charsToLines$444,"def diff_charsToLines(self, diffs, lineArray):
    """"""Rehydrate the text in a diff from a string of line hashes to real lines
    of text.

    Args:
      diffs: Array of diff tuples.
      lineArray: Array of unique strings.
    """"""
    for i in range(len(diffs)):
        text = []
        for char in diffs[i][1]:
            text.append(lineArray[ord(char)])
        diffs[i] = (diffs[i][0], ''.join(text))","for char in diffs[i][1]:
    text.append(lineArray[ord(char)])",text = [lineArray[ord(char)] for char in diffs[i][1]],text = [lineArray[ord(char)] for char in diffs[i][1]],1,,,,,robosuite
mifthtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mifthtools/blender/addons/2.7/super_grouper/grouper_main.py,https://github.com/mifth/mifthtools/tree/master/blender/addons/2.7/super_grouper/grouper_main.py,,generate_id$351,"def generate_id():
    other_ids = []
    for scene in bpy.data.scenes:
        if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False:
            for s_group in scene.super_groups:
                other_ids.append(s_group.unique_id)
    while True:
        uni_numb = None
        uniq_id_temp = ''.join((random.choice(string.ascii_uppercase + string.digits) for _ in range(10)))
        if uniq_id_temp not in other_ids:
            uni_numb = uniq_id_temp
            break
    other_ids = None
    return uni_numb","for scene in bpy.data.scenes:
    if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False:
        for s_group in scene.super_groups:
            other_ids.append(s_group.unique_id)",other_ids = [s_group.unique_id for scene in bpy.data.scenes if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False for s_group in scene.super_groups],other_ids = [s_group.unique_id for scene in bpy.data.scenes if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False for s_group in scene.super_groups],1,,,,,robosuite
rally,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/tests/metrics_test.py,https://github.com/elastic/rally/tree/master/tests/metrics_test.py,TestEsClient,test_retries_on_various_transport_errors$207,"def test_retries_on_various_transport_errors(self):

    @mock.patch('random.random')
    @mock.patch('esrally.time.sleep')
    def test_transport_error_retries(side_effect, expected_logging_calls, expected_sleep_calls, mocked_sleep, mocked_random):
        operation = mock.Mock(side_effect=side_effect)
        mocked_random.return_value = 0
        client = metrics.EsClient(self.ClientMock([{'host': '127.0.0.1', 'port': '9243'}]))
        logger = logging.getLogger('esrally.metrics')
        with mock.patch.object(logger, 'debug') as mocked_debug_logger:
            test_result = client.guarded(operation)
            mocked_sleep.assert_has_calls(expected_sleep_calls)
            mocked_debug_logger.assert_has_calls(expected_logging_calls, any_order=True)
            assert test_result == 'success'
    max_retry = 10
    all_err_codes = TransportErrors.err_return_codes
    transport_errors = TransportErrors(max_err_responses=max_retry)
    rnd_err_codes = transport_errors.code_list
    rnd_side_effects = transport_errors.side_effects
    rnd_mocked_logger_calls = []
    sleep_slots = [float(2 ** i) for i in range(0, max_retry)]
    mocked_sleep_calls = [mock.call(sleep_slots[i]) for i in range(0, max_retry)]
    for (rnd_err_idx, rnd_err_code) in enumerate(rnd_err_codes):
        rnd_mocked_logger_calls.append(mock.call('%s (code: %d) in attempt [%d/%d]. Sleeping for [%f] seconds.', all_err_codes[rnd_err_code], rnd_err_code, rnd_err_idx + 1, max_retry + 1, sleep_slots[rnd_err_idx]))
    test_transport_error_retries(rnd_side_effects, rnd_mocked_logger_calls, mocked_sleep_calls)","for (rnd_err_idx, rnd_err_code) in enumerate(rnd_err_codes):
    rnd_mocked_logger_calls.append(mock.call('%s (code: %d) in attempt [%d/%d]. Sleeping for [%f] seconds.', all_err_codes[rnd_err_code], rnd_err_code, rnd_err_idx + 1, max_retry + 1, sleep_slots[rnd_err_idx]))","rnd_mocked_logger_calls = [mock.call('%s (code: %d) in attempt [%d/%d]. Sleeping for [%f] seconds.', all_err_codes[rnd_err_code], rnd_err_code, rnd_err_idx + 1, max_retry + 1, sleep_slots[rnd_err_idx]) for (rnd_err_idx, rnd_err_code) in enumerate(rnd_err_codes)]","rnd_mocked_logger_calls = [mock.call('%s (code: %d) in attempt [%d/%d]. Sleeping for [%f] seconds.', all_err_codes[rnd_err_code], rnd_err_code, rnd_err_idx + 1, max_retry + 1, sleep_slots[rnd_err_idx]) for (rnd_err_idx, rnd_err_code) in enumerate(rnd_err_codes)]",1,,,,,robosuite
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/libvirt/driver.py,https://github.com/openstack/nova/tree/master/nova/virt/libvirt/driver.py,LibvirtDriver,_get_guest_config_meta$5424,"def _get_guest_config_meta(self, instance, network_info):
    """"""Get metadata config for guest.""""""
    meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
    meta.package = version.version_string_with_package()
    meta.name = instance.display_name
    meta.creationTime = time.time()
    if instance.image_ref not in ('', None):
        meta.roottype = 'image'
        meta.rootid = instance.image_ref
    system_meta = instance.system_metadata
    ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
    ometa.userid = instance.user_id
    ometa.username = system_meta.get('owner_user_name', 'N/A')
    ometa.projectid = instance.project_id
    ometa.projectname = system_meta.get('owner_project_name', 'N/A')
    meta.owner = ometa
    fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
    flavor = instance.flavor
    fmeta.name = flavor.name
    fmeta.memory = flavor.memory_mb
    fmeta.vcpus = flavor.vcpus
    fmeta.ephemeral = flavor.ephemeral_gb
    fmeta.disk = flavor.root_gb
    fmeta.swap = flavor.swap
    meta.flavor = fmeta
    ports = []
    for vif in network_info:
        ips = []
        for subnet in vif.get('network', {}).get('subnets', []):
            for ip in subnet.get('ips', []):
                ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')))
        ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=ips))
    meta.ports = vconfig.LibvirtConfigGuestMetaNovaPorts(ports)
    return meta","for vif in network_info:
    ips = []
    for subnet in vif.get('network', {}).get('subnets', []):
        for ip in subnet.get('ips', []):
            ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')))
    ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=ips))","ports = [vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=[vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')) for subnet in vif.get('network', {}).get('subnets', []) for ip in subnet.get('ips', [])]) for vif in network_info]",Cannot refactor,-1,1,,,,robosuite
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/libvirt/driver.py,https://github.com/openstack/nova/tree/master/nova/virt/libvirt/driver.py,LibvirtDriver,_get_guest_config_meta$5424,"def _get_guest_config_meta(self, instance, network_info):
    """"""Get metadata config for guest.""""""
    meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
    meta.package = version.version_string_with_package()
    meta.name = instance.display_name
    meta.creationTime = time.time()
    if instance.image_ref not in ('', None):
        meta.roottype = 'image'
        meta.rootid = instance.image_ref
    system_meta = instance.system_metadata
    ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
    ometa.userid = instance.user_id
    ometa.username = system_meta.get('owner_user_name', 'N/A')
    ometa.projectid = instance.project_id
    ometa.projectname = system_meta.get('owner_project_name', 'N/A')
    meta.owner = ometa
    fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
    flavor = instance.flavor
    fmeta.name = flavor.name
    fmeta.memory = flavor.memory_mb
    fmeta.vcpus = flavor.vcpus
    fmeta.ephemeral = flavor.ephemeral_gb
    fmeta.disk = flavor.root_gb
    fmeta.swap = flavor.swap
    meta.flavor = fmeta
    ports = []
    for vif in network_info:
        ips = []
        for subnet in vif.get('network', {}).get('subnets', []):
            for ip in subnet.get('ips', []):
                ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')))
        ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=ips))
    meta.ports = vconfig.LibvirtConfigGuestMetaNovaPorts(ports)
    return meta","for subnet in vif.get('network', {}).get('subnets', []):
    for ip in subnet.get('ips', []):
        ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')))","ips = [vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')) for subnet in vif.get('network', {}).get('subnets', []) for ip in subnet.get('ips', [])]","ips = [vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')) for subnet in vif.get('network', {}).get('subnets', []) for ip in subnet.get('ips', [])]",1,,,,,robosuite
GFM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GFM/core/util.py,https://github.com/JizhiziLi/GFM/tree/master/core/util.py,,listdir_nohidden$27,"def listdir_nohidden(path):
    new_list = []
    for f in os.listdir(path):
        if not f.startswith('.'):
            new_list.append(f)
    new_list.sort()
    return new_list","for f in os.listdir(path):
    if not f.startswith('.'):
        new_list.append(f)",new_list = [f for f in os.listdir(path) if not f.startswith('.')],new_list = [f for f in os.listdir(path) if not f.startswith('.')],1,,,,,robosuite
oomox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oomox/oomox_gui/theme_model.py,https://github.com/themix-project/oomox/tree/master/oomox_gui/theme_model.py,,get_theme_options_by_key$579,"def get_theme_options_by_key(key, fallback: 'Optional[ThemeModelValue]'=None) -> 'List[ThemeModelValue]':
    result = []
    for (_section_id, section) in get_theme_model().items():
        for theme_option in section:
            if key == theme_option.get('key'):
                result.append(theme_option)
    if not result and fallback:
        return [fallback]
    return result","for (_section_id, section) in get_theme_model().items():
    for theme_option in section:
        if key == theme_option.get('key'):
            result.append(theme_option)","result = [theme_option for (_section_id, section) in get_theme_model().items() for theme_option in section if key == theme_option.get('key')]","result = [theme_option for (_section_id, section) in get_theme_model().items() for theme_option in section if key == theme_option.get('key')]",1,,,,,robosuite
leafmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/leafmap/leafmap/foliumap.py,https://github.com/giswqs/leafmap/tree/master/leafmap/foliumap.py,Map,add_cog_mosaic_from_file$816,"def add_cog_mosaic_from_file(self, filepath, skip_rows=0, name='Untitled', attribution='.', opacity=1.0, shown=True, titiler_endpoint='https://api.cogeo.xyz/', username='anonymous', overwrite=False, show_footprints=False, verbose=True, **kwargs):
    """"""Add a virtual mosaic of COGs to the map.

        Args:
            filepath (str): Local path or HTTP URL to the csv/txt file containing COG URLs.
            skip_rows (int, optional): The number of rows to skip in the file. Defaults to 0.
            name (str, optional): The layer name to use for the layer. Defaults to 'Untitled'.
            attribution (str, optional): The attribution to use. Defaults to '.'.
            opacity (float, optional): The opacity of the layer. Defaults to 1.
            shown (bool, optional): A flag indicating whether the layer should be on by default. Defaults to True.
            titiler_endpoint (str, optional): Titiler endpoint. Defaults to ""https://api.cogeo.xyz/"".
            username (str, optional): The username to create mosaic using the titiler endpoint. Defaults to 'anonymous'.
            overwrite (bool, optional): Whether or not to replace existing layer with the same layer name. Defaults to False.
            show_footprints (bool, optional): Whether or not to show footprints of COGs. Defaults to False.
            verbose (bool, optional): Whether or not to print descriptions. Defaults to True.
        """"""
    import urllib
    layername = name.replace(' ', '_')
    links = []
    if filepath.startswith('http'):
        data = urllib.request.urlopen(filepath)
        for line in data:
            links.append(line.decode('utf-8').strip())
    else:
        with open(filepath) as f:
            links = [line.strip() for line in f.readlines()]
    links = links[skip_rows:]
    tile = cog_mosaic(links, titiler_endpoint=titiler_endpoint, username=username, layername=layername, overwrite=overwrite, verbose=verbose)
    self.add_tile_layer(tile, name, attribution, opacity, shown)
    if show_footprints:
        if verbose:
            print(f'Generating footprints of {len(links)} COGs. This might take a while ...')
        coords = []
        for link in links:
            coord = cog_bounds(link)
            if coord is not None:
                coords.append(coord)
        fc = coords_to_geojson(coords)
        folium.GeoJson(data=fc, name=name + '_footprints').add_to(self)
        center = get_center(fc)
        if verbose:
            print('The footprint layer has been added.')
    else:
        center = cog_center(links[0], titiler_endpoint)
    self.set_center(center[0], center[1], zoom=6)","for line in data:
    links.append(line.decode('utf-8').strip())",links = [line.decode('utf-8').strip() for line in data],links = [line.decode('utf-8').strip() for line in data],1,,,,,robosuite
leafmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/leafmap/leafmap/foliumap.py,https://github.com/giswqs/leafmap/tree/master/leafmap/foliumap.py,Map,add_cog_mosaic_from_file$816,"def add_cog_mosaic_from_file(self, filepath, skip_rows=0, name='Untitled', attribution='.', opacity=1.0, shown=True, titiler_endpoint='https://api.cogeo.xyz/', username='anonymous', overwrite=False, show_footprints=False, verbose=True, **kwargs):
    """"""Add a virtual mosaic of COGs to the map.

        Args:
            filepath (str): Local path or HTTP URL to the csv/txt file containing COG URLs.
            skip_rows (int, optional): The number of rows to skip in the file. Defaults to 0.
            name (str, optional): The layer name to use for the layer. Defaults to 'Untitled'.
            attribution (str, optional): The attribution to use. Defaults to '.'.
            opacity (float, optional): The opacity of the layer. Defaults to 1.
            shown (bool, optional): A flag indicating whether the layer should be on by default. Defaults to True.
            titiler_endpoint (str, optional): Titiler endpoint. Defaults to ""https://api.cogeo.xyz/"".
            username (str, optional): The username to create mosaic using the titiler endpoint. Defaults to 'anonymous'.
            overwrite (bool, optional): Whether or not to replace existing layer with the same layer name. Defaults to False.
            show_footprints (bool, optional): Whether or not to show footprints of COGs. Defaults to False.
            verbose (bool, optional): Whether or not to print descriptions. Defaults to True.
        """"""
    import urllib
    layername = name.replace(' ', '_')
    links = []
    if filepath.startswith('http'):
        data = urllib.request.urlopen(filepath)
        for line in data:
            links.append(line.decode('utf-8').strip())
    else:
        with open(filepath) as f:
            links = [line.strip() for line in f.readlines()]
    links = links[skip_rows:]
    tile = cog_mosaic(links, titiler_endpoint=titiler_endpoint, username=username, layername=layername, overwrite=overwrite, verbose=verbose)
    self.add_tile_layer(tile, name, attribution, opacity, shown)
    if show_footprints:
        if verbose:
            print(f'Generating footprints of {len(links)} COGs. This might take a while ...')
        coords = []
        for link in links:
            coord = cog_bounds(link)
            if coord is not None:
                coords.append(coord)
        fc = coords_to_geojson(coords)
        folium.GeoJson(data=fc, name=name + '_footprints').add_to(self)
        center = get_center(fc)
        if verbose:
            print('The footprint layer has been added.')
    else:
        center = cog_center(links[0], titiler_endpoint)
    self.set_center(center[0], center[1], zoom=6)","for link in links:
    coord = cog_bounds(link)
    if coord is not None:
        coords.append(coord)",coords = [coord for link in links if (coord := cog_bounds(link)) is not None],Cannot refactor,-1,1,,,,robosuite
myscan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/myscan/myscan/lib/parse/dictdata_parser.py,https://github.com/amcai/myscan/tree/master/myscan/lib/parse/dictdata_parser.py,dictdata_parser,getperfolders$46,"def getperfolders(self):
    """"""
        return list 閿涘當very folder will endwith /
        """"""
    folders = []
    url = self.url.get('url').split('?')[0]
    if url.count('/') == 3:
        return ['/'.join(url.split('/')[:3]) + '/']
    elif url.count('/') > 3:
        for x in range(3, url.count('/') + 1):
            folders.append('/'.join(url.split('/')[:x]) + '/')
        return folders
    else:
        return []","for x in range(3, url.count('/') + 1):
    folders.append('/'.join(url.split('/')[:x]) + '/')","folders = ['/'.join(url.split('/')[:x]) + '/' for x in range(3, url.count('/') + 1)]","folders = ['/'.join(url.split('/')[:x]) + '/' for x in range(3, url.count('/') + 1)]",1,,,,,robosuite
albert_zh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/albert_zh/modeling_google.py,https://github.com/brightmart/albert_zh/tree/master//modeling_google.py,,get_shape_list$997,"def get_shape_list(tensor, expected_rank=None, name=None):
    """"""Returns a list of the shape of tensor, preferring static dimensions.
  Args:
    tensor: A tf.Tensor object to find the shape of.
    expected_rank: (optional) int. The expected rank of `tensor`. If this is
      specified and the `tensor` has a different rank, and exception will be
      thrown.
    name: Optional name of the tensor for the error message.
  Returns:
    A list of dimensions of the shape of tensor. All static dimensions will
    be returned as python integers, and dynamic dimensions will be returned
    as tf.Tensor scalars.
  """"""
    if name is None:
        name = tensor.name
    if expected_rank is not None:
        assert_rank(tensor, expected_rank, name)
    shape = tensor.shape.as_list()
    non_static_indexes = []
    for (index, dim) in enumerate(shape):
        if dim is None:
            non_static_indexes.append(index)
    if not non_static_indexes:
        return shape
    dyn_shape = tf.shape(tensor)
    for index in non_static_indexes:
        shape[index] = dyn_shape[index]
    return shape","for (index, dim) in enumerate(shape):
    if dim is None:
        non_static_indexes.append(index)","non_static_indexes = [index for (index, dim) in enumerate(shape) if dim is None]","non_static_indexes = [index for (index, dim) in enumerate(shape) if dim is None]",1,,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/contrib/gsm_um.py,https://github.com/phaethon/kamene/tree/master/kamene/contrib/gsm_um.py,BearerCapabilityHdr,post_build$6006,"def post_build(self, p, pay):
    aList = []
    a = []
    i = 0
    for i in range(0, len(self.fields_desc)):
        aList.append(self.fields_desc[i].name)
    for i in aList:
        a.append(getattr(self, i))
    res = adapt(3, 15, a, self.fields_desc)
    if res[0] is not 0:
        p = p[:-res[0]]
    if len(p) is 5:
        p = p[:-2]
    if self.lengthBC is None:
        print('len von a %s' % (len(p),))
        p = p[:1] + struct.pack('>B', len(p) - 3) + p[2:]
    return p + pay","for i in range(0, len(self.fields_desc)):
    aList.append(self.fields_desc[i].name)","aList = [self.fields_desc[i].name for i in range(0, len(self.fields_desc))]","aList = [self.fields_desc[i].name for i in range(0, len(self.fields_desc))]",1,,,,,robosuite
kamene,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/contrib/gsm_um.py,https://github.com/phaethon/kamene/tree/master/kamene/contrib/gsm_um.py,BearerCapabilityHdr,post_build$6006,"def post_build(self, p, pay):
    aList = []
    a = []
    i = 0
    for i in range(0, len(self.fields_desc)):
        aList.append(self.fields_desc[i].name)
    for i in aList:
        a.append(getattr(self, i))
    res = adapt(3, 15, a, self.fields_desc)
    if res[0] is not 0:
        p = p[:-res[0]]
    if len(p) is 5:
        p = p[:-2]
    if self.lengthBC is None:
        print('len von a %s' % (len(p),))
        p = p[:1] + struct.pack('>B', len(p) - 3) + p[2:]
    return p + pay","for i in aList:
    a.append(getattr(self, i))","a += [getattr(self, i) for i in aList]","a = [getattr(self, i) for i in aList]",0,1,,,,robosuite
FARM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FARM/farm/data_handler/processor.py,https://github.com/deepset-ai/FARM/tree/master/farm/data_handler/processor.py,BertStyleLMProcessor,dataset_from_dicts$1364,"def dataset_from_dicts(self, dicts, indices=None, return_baskets=False):
    dicts = [d['doc'] for d in dicts]
    if self.next_sent_pred:
        assert len(dicts) > 1, 'Need at least 2 documents to sample random sentences from'
        if self.next_sent_pred_style == 'sentence':
            samples = self._create_sequence_pairs_by_line(dicts)
        elif self.next_sent_pred_style == 'bert-style':
            samples = self._create_sequence_pairs_bert_style(dicts)
        else:
            raise NotImplementedError(""next_sent_pred_style has to be 'sentence' or 'bert-style'"")
    else:
        samples = self._create_sequence_pairs_no_next_sent(dicts)
    features = []
    vocab_length = len(self.tokenizer.vocab) - 1
    for sample in samples:
        features.append(self._create_labels(sample=sample, vocab_length=vocab_length))
    (dataset, tensor_names) = convert_features_to_dataset(features=features)
    return (dataset, tensor_names, set())","for sample in samples:
    features.append(self._create_labels(sample=sample, vocab_length=vocab_length))","features = [self._create_labels(sample=sample, vocab_length=vocab_length) for sample in samples]","features = [self._create_labels(sample=sample, vocab_length=vocab_length) for sample in samples]",1,,,,,robosuite
alibi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alibi/alibi/explainers/shap_wrappers.py,https://github.com/SeldonIO/alibi/tree/master/alibi/explainers/shap_wrappers.py,TreeShap,_build_explanation$1476,"def _build_explanation(self, X: Union[np.ndarray, pd.DataFrame, 'catboost.Pool'], shap_output: List[np.ndarray], expected_value: List[float], **kwargs) -> Explanation:
    """"""
        Create an explanation object. If output summarisation is required and all inputs necessary for this operation
        are passed, the raw shap values are summed first so that a single shap value is returned for each categorical
        variable, as opposed to a shap value per dimension of categorical variable encoding. Similarly, the
        shap interaction values are summed such that they represent the interaction between categorical variables as
        opposed to levels of categorical variables. If the interaction option has been specified during `explain`,
        this method computes the shap values given the interactions prior to creating the response.

        Parameters
        ----------
        X
            Instances to be explained.
        shap_output
            If `explain` is callled with ``interactions=True`` then the list contains arrays of dimensionality
            `n_instances x n_features x n_features` of shap interaction values. Otherwise, it contains arrays of
            dimension `n_instances x n_features` representing shap values. The length of the list equals the number of
            model outputs.
        expected_value
            A list containing the expected value of the prediction for each class. Its length is equal to that of
            `shap_output`.

        Returns
        -------
        explanation
            An `Explanation` object containing the shap values and prediction in the `data` field, along with a
            `meta` field containing additional data. See usage at `TreeSHAP examples`_ for details.

            .. _TreeSHAP examples:
               https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html

        """"""
    y = kwargs.get('y')
    if y is None:
        y = np.array([])
    cat_vars_start_idx = kwargs.get('cat_vars_start_idx', ())
    cat_vars_enc_dim = kwargs.get('cat_vars_enc_dim', ())
    summarise_result = kwargs.get('summarise_result', False)
    if len(shap_output[0].shape) == 3:
        shap_interaction_values = shap_output
        shap_values = [interactions.sum(axis=2) for interactions in shap_output]
    else:
        shap_interaction_values = [np.array([])]
        shap_values = shap_output
    if summarise_result:
        self._check_result_summarisation(summarise_result, cat_vars_start_idx, cat_vars_enc_dim)
    if self.summarise_result:
        summarised_shap = []
        for shap_array in shap_values:
            summarised_shap.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))
        shap_values = summarised_shap
        if shap_interaction_values[0].size != 0:
            summarised_shap_interactions = []
            for shap_array in shap_interaction_values:
                summarised_shap_interactions.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))
            shap_interaction_values = summarised_shap_interactions
    if self.model_output == 'log_loss':
        loss = self._explainer.model.predict(X, y, tree_limit=self.tree_limit)
        raw_predictions = []
    else:
        loss = []
        raw_predictions = self._explainer.model.predict(X, tree_limit=self.tree_limit)
        if raw_predictions.shape[-1] == 1:
            raw_predictions = raw_predictions.squeeze(-1)
    argmax_pred = []
    if self.task != 'regression':
        if not isinstance(raw_predictions, list):
            if self.scalar_output:
                if self.model_output == 'raw':
                    probas = expit(raw_predictions)
                else:
                    probas = raw_predictions
                argmax_pred = (probas > 0.5).astype(int)
            else:
                argmax_pred = np.argmax(np.atleast_2d(raw_predictions), axis=1)
    importances = rank_by_importance(shap_values, feature_names=self.feature_names)
    if self._explainer.model.model_type == 'catboost':
        import catboost
        if isinstance(X, catboost.Pool):
            X = X.get_features()
    data = copy.deepcopy(DEFAULT_DATA_TREE_SHAP)
    data.update(shap_values=shap_values, shap_interaction_values=shap_interaction_values, expected_value=expected_value, categorical_names=self.categorical_names, feature_names=self.feature_names)
    data['raw'].update(raw_prediction=raw_predictions, loss=loss, prediction=argmax_pred, instances=np.array(X), labels=y, importances=importances)
    self._update_metadata({'summarise_result': self.summarise_result}, params=True)
    return Explanation(meta=copy.deepcopy(self.meta), data=data)","for shap_array in shap_values:
    summarised_shap.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))","summarised_shap = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_values]","summarised_shap = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_values]",1,,,,,robosuite
alibi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alibi/alibi/explainers/shap_wrappers.py,https://github.com/SeldonIO/alibi/tree/master/alibi/explainers/shap_wrappers.py,TreeShap,_build_explanation$1476,"def _build_explanation(self, X: Union[np.ndarray, pd.DataFrame, 'catboost.Pool'], shap_output: List[np.ndarray], expected_value: List[float], **kwargs) -> Explanation:
    """"""
        Create an explanation object. If output summarisation is required and all inputs necessary for this operation
        are passed, the raw shap values are summed first so that a single shap value is returned for each categorical
        variable, as opposed to a shap value per dimension of categorical variable encoding. Similarly, the
        shap interaction values are summed such that they represent the interaction between categorical variables as
        opposed to levels of categorical variables. If the interaction option has been specified during `explain`,
        this method computes the shap values given the interactions prior to creating the response.

        Parameters
        ----------
        X
            Instances to be explained.
        shap_output
            If `explain` is callled with ``interactions=True`` then the list contains arrays of dimensionality
            `n_instances x n_features x n_features` of shap interaction values. Otherwise, it contains arrays of
            dimension `n_instances x n_features` representing shap values. The length of the list equals the number of
            model outputs.
        expected_value
            A list containing the expected value of the prediction for each class. Its length is equal to that of
            `shap_output`.

        Returns
        -------
        explanation
            An `Explanation` object containing the shap values and prediction in the `data` field, along with a
            `meta` field containing additional data. See usage at `TreeSHAP examples`_ for details.

            .. _TreeSHAP examples:
               https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html

        """"""
    y = kwargs.get('y')
    if y is None:
        y = np.array([])
    cat_vars_start_idx = kwargs.get('cat_vars_start_idx', ())
    cat_vars_enc_dim = kwargs.get('cat_vars_enc_dim', ())
    summarise_result = kwargs.get('summarise_result', False)
    if len(shap_output[0].shape) == 3:
        shap_interaction_values = shap_output
        shap_values = [interactions.sum(axis=2) for interactions in shap_output]
    else:
        shap_interaction_values = [np.array([])]
        shap_values = shap_output
    if summarise_result:
        self._check_result_summarisation(summarise_result, cat_vars_start_idx, cat_vars_enc_dim)
    if self.summarise_result:
        summarised_shap = []
        for shap_array in shap_values:
            summarised_shap.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))
        shap_values = summarised_shap
        if shap_interaction_values[0].size != 0:
            summarised_shap_interactions = []
            for shap_array in shap_interaction_values:
                summarised_shap_interactions.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))
            shap_interaction_values = summarised_shap_interactions
    if self.model_output == 'log_loss':
        loss = self._explainer.model.predict(X, y, tree_limit=self.tree_limit)
        raw_predictions = []
    else:
        loss = []
        raw_predictions = self._explainer.model.predict(X, tree_limit=self.tree_limit)
        if raw_predictions.shape[-1] == 1:
            raw_predictions = raw_predictions.squeeze(-1)
    argmax_pred = []
    if self.task != 'regression':
        if not isinstance(raw_predictions, list):
            if self.scalar_output:
                if self.model_output == 'raw':
                    probas = expit(raw_predictions)
                else:
                    probas = raw_predictions
                argmax_pred = (probas > 0.5).astype(int)
            else:
                argmax_pred = np.argmax(np.atleast_2d(raw_predictions), axis=1)
    importances = rank_by_importance(shap_values, feature_names=self.feature_names)
    if self._explainer.model.model_type == 'catboost':
        import catboost
        if isinstance(X, catboost.Pool):
            X = X.get_features()
    data = copy.deepcopy(DEFAULT_DATA_TREE_SHAP)
    data.update(shap_values=shap_values, shap_interaction_values=shap_interaction_values, expected_value=expected_value, categorical_names=self.categorical_names, feature_names=self.feature_names)
    data['raw'].update(raw_prediction=raw_predictions, loss=loss, prediction=argmax_pred, instances=np.array(X), labels=y, importances=importances)
    self._update_metadata({'summarise_result': self.summarise_result}, params=True)
    return Explanation(meta=copy.deepcopy(self.meta), data=data)","for shap_array in shap_interaction_values:
    summarised_shap_interactions.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))","summarised_shap_interactions = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_interaction_values]","summarised_shap_interactions = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_interaction_values]",1,,,,,robosuite
SOLO,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SOLO/tools/analyze_logs.py,https://github.com/WXinlong/SOLO/tree/master/tools/analyze_logs.py,,plot_curve$34,"def plot_curve(log_dicts, args):
    if args.backend is not None:
        plt.switch_backend(args.backend)
    sns.set_style(args.style)
    legend = args.legend
    if legend is None:
        legend = []
        for json_log in args.json_logs:
            for metric in args.keys:
                legend.append('{}_{}'.format(json_log, metric))
    assert len(legend) == len(args.json_logs) * len(args.keys)
    metrics = args.keys
    num_metrics = len(metrics)
    for (i, log_dict) in enumerate(log_dicts):
        epochs = list(log_dict.keys())
        for (j, metric) in enumerate(metrics):
            print('plot curve of {}, metric is {}'.format(args.json_logs[i], metric))
            if metric not in log_dict[epochs[0]]:
                raise KeyError('{} does not contain metric {}'.format(args.json_logs[i], metric))
            if 'mAP' in metric:
                xs = np.arange(1, max(epochs) + 1)
                ys = []
                for epoch in epochs:
                    ys += log_dict[epoch][metric]
                ax = plt.gca()
                ax.set_xticks(xs)
                plt.xlabel('epoch')
                plt.plot(xs, ys, label=legend[i * num_metrics + j], marker='o')
            else:
                xs = []
                ys = []
                num_iters_per_epoch = log_dict[epochs[0]]['iter'][-1]
                for epoch in epochs:
                    iters = log_dict[epoch]['iter']
                    if log_dict[epoch]['mode'][-1] == 'val':
                        iters = iters[:-1]
                    xs.append(np.array(iters) + (epoch - 1) * num_iters_per_epoch)
                    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))
                xs = np.concatenate(xs)
                ys = np.concatenate(ys)
                plt.xlabel('iter')
                plt.plot(xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)
            plt.legend()
        if args.title is not None:
            plt.title(args.title)
    if args.out is None:
        plt.show()
    else:
        print('save curve to: {}'.format(args.out))
        plt.savefig(args.out)
        plt.cla()","for json_log in args.json_logs:
    for metric in args.keys:
        legend.append('{}_{}'.format(json_log, metric))","legend = ['{}_{}'.format(json_log, metric) for json_log in args.json_logs for metric in args.keys]","legend = ['{}_{}'.format(json_log, metric) for json_log in args.json_logs for metric in args.keys]",1,,,,,robosuite
SOLO,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SOLO/tools/analyze_logs.py,https://github.com/WXinlong/SOLO/tree/master/tools/analyze_logs.py,,plot_curve$34,"def plot_curve(log_dicts, args):
    if args.backend is not None:
        plt.switch_backend(args.backend)
    sns.set_style(args.style)
    legend = args.legend
    if legend is None:
        legend = []
        for json_log in args.json_logs:
            for metric in args.keys:
                legend.append('{}_{}'.format(json_log, metric))
    assert len(legend) == len(args.json_logs) * len(args.keys)
    metrics = args.keys
    num_metrics = len(metrics)
    for (i, log_dict) in enumerate(log_dicts):
        epochs = list(log_dict.keys())
        for (j, metric) in enumerate(metrics):
            print('plot curve of {}, metric is {}'.format(args.json_logs[i], metric))
            if metric not in log_dict[epochs[0]]:
                raise KeyError('{} does not contain metric {}'.format(args.json_logs[i], metric))
            if 'mAP' in metric:
                xs = np.arange(1, max(epochs) + 1)
                ys = []
                for epoch in epochs:
                    ys += log_dict[epoch][metric]
                ax = plt.gca()
                ax.set_xticks(xs)
                plt.xlabel('epoch')
                plt.plot(xs, ys, label=legend[i * num_metrics + j], marker='o')
            else:
                xs = []
                ys = []
                num_iters_per_epoch = log_dict[epochs[0]]['iter'][-1]
                for epoch in epochs:
                    iters = log_dict[epoch]['iter']
                    if log_dict[epoch]['mode'][-1] == 'val':
                        iters = iters[:-1]
                    xs.append(np.array(iters) + (epoch - 1) * num_iters_per_epoch)
                    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))
                xs = np.concatenate(xs)
                ys = np.concatenate(ys)
                plt.xlabel('iter')
                plt.plot(xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)
            plt.legend()
        if args.title is not None:
            plt.title(args.title)
    if args.out is None:
        plt.show()
    else:
        print('save curve to: {}'.format(args.out))
        plt.savefig(args.out)
        plt.cla()","for epoch in epochs:
    iters = log_dict[epoch]['iter']
    if log_dict[epoch]['mode'][-1] == 'val':
        iters = iters[:-1]
    xs.append(np.array(iters) + (epoch - 1) * num_iters_per_epoch)
    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))","ys += [np.array(log_dict[epoch][metric][:len(iters)]) for (epoch, iters) in zip(epochs, xs)]",Cannot refactor,-1,0,,2,1,robosuite
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/lib/ansible_test/_util/controller/sanity/validate-modules/validate_modules/main.py,https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_util/controller/sanity/validate-modules/validate_modules/main.py,ModuleValidator,_validate_argument_spec$1376,"def _validate_argument_spec(self, docs, spec, kwargs, context=None, last_context_spec=None):
    if not self.analyze_arg_spec:
        return
    if docs is None:
        docs = {}
    if context is None:
        context = []
    if last_context_spec is None:
        last_context_spec = kwargs
    try:
        if not context:
            add_fragments(docs, self.object_path, fragment_loader=fragment_loader, is_module=True)
    except Exception:
        return
    module = NoArgsAnsibleModule({})
    self._validate_list_of_module_args('mutually_exclusive', last_context_spec.get('mutually_exclusive'), spec, context)
    self._validate_list_of_module_args('required_together', last_context_spec.get('required_together'), spec, context)
    self._validate_list_of_module_args('required_one_of', last_context_spec.get('required_one_of'), spec, context)
    self._validate_required_if(last_context_spec.get('required_if'), spec, context, module)
    self._validate_required_by(last_context_spec.get('required_by'), spec, context)
    provider_args = set()
    args_from_argspec = set()
    deprecated_args_from_argspec = set()
    doc_options = docs.get('options', {})
    if doc_options is None:
        doc_options = {}
    for (arg, data) in spec.items():
        restricted_argument_names = ('message', 'syslog_facility')
        if arg.lower() in restricted_argument_names:
            msg = ""Argument '%s' in argument_spec "" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += 'must not be one of %s as it is used internally by Ansible Core Engine' % ','.join(restricted_argument_names)
            self.reporter.error(path=self.object_path, code='invalid-argument-name', msg=msg)
            continue
        if 'aliases' in data:
            for al in data['aliases']:
                if al.lower() in restricted_argument_names:
                    msg = ""Argument alias '%s' in argument_spec "" % al
                    if context:
                        msg += ' found in %s' % ' -> '.join(context)
                    msg += 'must not be one of %s as it is used internally by Ansible Core Engine' % ','.join(restricted_argument_names)
                    self.reporter.error(path=self.object_path, code='invalid-argument-name', msg=msg)
                    continue
        if all((data.get('no_log') is None, is_potential_secret_option(arg), data.get('type') not in ('path', 'bool'), data.get('choices') is None)):
            msg = ""Argument '%s' in argument_spec could be a secret, though doesn't have `no_log` set"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            self.reporter.error(path=self.object_path, code='no-log-needed', msg=msg)
        if not isinstance(data, dict):
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' must be a dictionary/hash when used'
            self.reporter.error(path=self.object_path, code='invalid-argument-spec', msg=msg)
            continue
        removed_at_date = data.get('removed_at_date', None)
        if removed_at_date is not None:
            try:
                if parse_isodate(removed_at_date, allow_date=False) < datetime.date.today():
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += ' found in %s' % ' -> '.join(context)
                    msg += "" has a removed_at_date '%s' before today"" % removed_at_date
                    self.reporter.error(path=self.object_path, code='deprecated-date', msg=msg)
            except ValueError:
                pass
        deprecated_aliases = data.get('deprecated_aliases', None)
        if deprecated_aliases is not None:
            for deprecated_alias in deprecated_aliases:
                if 'name' in deprecated_alias and 'date' in deprecated_alias:
                    try:
                        date = deprecated_alias['date']
                        if parse_isodate(date, allow_date=False) < datetime.date.today():
                            msg = ""Argument '%s' in argument_spec"" % arg
                            if context:
                                msg += ' found in %s' % ' -> '.join(context)
                            msg += "" has deprecated aliases '%s' with removal date '%s' before today"" % (deprecated_alias['name'], deprecated_alias['date'])
                            self.reporter.error(path=self.object_path, code='deprecated-date', msg=msg)
                    except ValueError:
                        pass
        has_version = False
        if self.collection and self.collection_version is not None:
            compare_version = self.collection_version
            version_of_what = 'this collection (%s)' % self.collection_version_str
            code_prefix = 'collection'
            has_version = True
        elif not self.collection:
            compare_version = LOOSE_ANSIBLE_VERSION
            version_of_what = 'Ansible (%s)' % ansible_version
            code_prefix = 'ansible'
            has_version = True
        removed_in_version = data.get('removed_in_version', None)
        if removed_in_version is not None:
            try:
                collection_name = data.get('removed_from_collection')
                removed_in = self._create_version(str(removed_in_version), collection_name=collection_name)
                if has_version and collection_name == self.collection_name and (compare_version >= removed_in):
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += ' found in %s' % ' -> '.join(context)
                    msg += ' has a deprecated removed_in_version %r,' % removed_in_version
                    msg += ' i.e. the version is less than or equal to the current version of %s' % version_of_what
                    self.reporter.error(path=self.object_path, code=code_prefix + '-deprecated-version', msg=msg)
            except ValueError as e:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' has an invalid removed_in_version number %r: %s' % (removed_in_version, e)
                self.reporter.error(path=self.object_path, code='invalid-deprecated-version', msg=msg)
            except TypeError:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' has an invalid removed_in_version number %r: ' % (removed_in_version,)
                msg += ' error while comparing to version of %s' % version_of_what
                self.reporter.error(path=self.object_path, code='invalid-deprecated-version', msg=msg)
        if deprecated_aliases is not None:
            for deprecated_alias in deprecated_aliases:
                if 'name' in deprecated_alias and 'version' in deprecated_alias:
                    try:
                        collection_name = deprecated_alias.get('collection_name')
                        version = self._create_version(str(deprecated_alias['version']), collection_name=collection_name)
                        if has_version and collection_name == self.collection_name and (compare_version >= version):
                            msg = ""Argument '%s' in argument_spec"" % arg
                            if context:
                                msg += ' found in %s' % ' -> '.join(context)
                            msg += "" has deprecated aliases '%s' with removal in version %r,"" % (deprecated_alias['name'], deprecated_alias['version'])
                            msg += ' i.e. the version is less than or equal to the current version of %s' % version_of_what
                            self.reporter.error(path=self.object_path, code=code_prefix + '-deprecated-version', msg=msg)
                    except ValueError as e:
                        msg = ""Argument '%s' in argument_spec"" % arg
                        if context:
                            msg += ' found in %s' % ' -> '.join(context)
                        msg += "" has deprecated aliases '%s' with invalid removal version %r: %s"" % (deprecated_alias['name'], deprecated_alias['version'], e)
                        self.reporter.error(path=self.object_path, code='invalid-deprecated-version', msg=msg)
                    except TypeError:
                        msg = ""Argument '%s' in argument_spec"" % arg
                        if context:
                            msg += ' found in %s' % ' -> '.join(context)
                        msg += "" has deprecated aliases '%s' with invalid removal version %r:"" % (deprecated_alias['name'], deprecated_alias['version'])
                        msg += ' error while comparing to version of %s' % version_of_what
                        self.reporter.error(path=self.object_path, code='invalid-deprecated-version', msg=msg)
        aliases = data.get('aliases', [])
        if arg in aliases:
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' is specified as its own alias'
            self.reporter.error(path=self.object_path, code='parameter-alias-self', msg=msg)
        if len(aliases) > len(set(aliases)):
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' has at least one alias specified multiple times in aliases'
            self.reporter.error(path=self.object_path, code='parameter-alias-repeated', msg=msg)
        if not context and arg == 'state':
            bad_states = set(['list', 'info', 'get']) & set(data.get('choices', set()))
            for bad_state in bad_states:
                self.reporter.error(path=self.object_path, code='parameter-state-invalid-choice', msg=""Argument 'state' includes the value '%s' as a choice"" % bad_state)
        if not data.get('removed_in_version', None) and (not data.get('removed_at_date', None)):
            args_from_argspec.add(arg)
            args_from_argspec.update(aliases)
        else:
            deprecated_args_from_argspec.add(arg)
            deprecated_args_from_argspec.update(aliases)
        if arg == 'provider' and self.object_path.startswith('lib/ansible/modules/network/'):
            if data.get('options') is not None and (not isinstance(data.get('options'), Mapping)):
                self.reporter.error(path=self.object_path, code='invalid-argument-spec-options', msg=""Argument 'options' in argument_spec['provider'] must be a dictionary/hash when used"")
            elif data.get('options'):
                for (provider_arg, provider_data) in data.get('options', {}).items():
                    provider_args.add(provider_arg)
                    provider_args.update(provider_data.get('aliases', []))
        if data.get('required') and data.get('default', object) != object:
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' is marked as required but specifies a default. Arguments with a default should not be marked as required'
            self.reporter.error(path=self.object_path, code='no-default-for-required-parameter', msg=msg)
        if arg in provider_args:
            continue
        _type = data.get('type', 'str')
        if callable(_type):
            _type_checker = _type
        else:
            _type_checker = DEFAULT_TYPE_VALIDATORS.get(_type)
        _elements = data.get('elements')
        if _type == 'list' and (not _elements):
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' defines type as list but elements is not defined'
            self.reporter.error(path=self.object_path, code='parameter-list-no-elements', msg=msg)
        if _elements:
            if not callable(_elements):
                DEFAULT_TYPE_VALIDATORS.get(_elements)
            if _type != 'list':
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' defines elements as %s but it is valid only when value of parameter type is list' % _elements
                self.reporter.error(path=self.object_path, code='parameter-invalid-elements', msg=msg)
        arg_default = None
        if 'default' in data and (not is_empty(data['default'])):
            try:
                with CaptureStd():
                    arg_default = _type_checker(data['default'])
            except (Exception, SystemExit):
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' defines default as (%r) but this is incompatible with parameter type %r' % (data['default'], _type)
                self.reporter.error(path=self.object_path, code='incompatible-default-type', msg=msg)
                continue
        doc_options_args = []
        for alias in sorted(set([arg] + list(aliases))):
            if alias in doc_options:
                doc_options_args.append(alias)
        if len(doc_options_args) == 0:
            doc_options_arg = {}
        else:
            doc_options_arg = doc_options[doc_options_args[0]]
            if len(doc_options_args) > 1:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' with aliases %s is documented multiple times, namely as %s' % (', '.join([""'%s'"" % alias for alias in aliases]), ', '.join([""'%s'"" % alias for alias in doc_options_args]))
                self.reporter.error(path=self.object_path, code='parameter-documented-multiple-times', msg=msg)
        try:
            doc_default = None
            if 'default' in doc_options_arg and (not is_empty(doc_options_arg['default'])):
                with CaptureStd():
                    doc_default = _type_checker(doc_options_arg['default'])
        except (Exception, SystemExit):
            msg = ""Argument '%s' in documentation"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' defines default as (%r) but this is incompatible with parameter type %r' % (doc_options_arg.get('default'), _type)
            self.reporter.error(path=self.object_path, code='doc-default-incompatible-type', msg=msg)
            continue
        if arg_default != doc_default:
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' defines default as (%r) but documentation defines default as (%r)' % (arg_default, doc_default)
            self.reporter.error(path=self.object_path, code='doc-default-does-not-match-spec', msg=msg)
        doc_type = doc_options_arg.get('type')
        if 'type' in data and data['type'] is not None:
            if doc_type is None:
                if not arg.startswith('_'):
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += ' found in %s' % ' -> '.join(context)
                    msg += "" defines type as %r but documentation doesn't define type"" % data['type']
                    self.reporter.error(path=self.object_path, code='parameter-type-not-in-doc', msg=msg)
            elif data['type'] != doc_type:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' defines type as %r but documentation defines type as %r' % (data['type'], doc_type)
                self.reporter.error(path=self.object_path, code='doc-type-does-not-match-spec', msg=msg)
        elif doc_type is None:
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += "" uses default type ('str') but documentation doesn't define type""
            self.reporter.error(path=self.object_path, code='doc-missing-type', msg=msg)
        elif doc_type != 'str':
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += "" implies type as 'str' but documentation defines as %r"" % doc_type
            self.reporter.error(path=self.object_path, code='implied-parameter-type-mismatch', msg=msg)
        doc_choices = []
        try:
            for choice in doc_options_arg.get('choices', []):
                try:
                    with CaptureStd():
                        doc_choices.append(_type_checker(choice))
                except (Exception, SystemExit):
                    msg = ""Argument '%s' in documentation"" % arg
                    if context:
                        msg += ' found in %s' % ' -> '.join(context)
                    msg += ' defines choices as (%r) but this is incompatible with argument type %r' % (choice, _type)
                    self.reporter.error(path=self.object_path, code='doc-choices-incompatible-type', msg=msg)
                    raise StopIteration()
        except StopIteration:
            continue
        arg_choices = []
        try:
            for choice in data.get('choices', []):
                try:
                    with CaptureStd():
                        arg_choices.append(_type_checker(choice))
                except (Exception, SystemExit):
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += ' found in %s' % ' -> '.join(context)
                    msg += ' defines choices as (%r) but this is incompatible with argument type %r' % (choice, _type)
                    self.reporter.error(path=self.object_path, code='incompatible-choices', msg=msg)
                    raise StopIteration()
        except StopIteration:
            continue
        if not compare_unordered_lists(arg_choices, doc_choices):
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' defines choices as (%r) but documentation defines choices as (%r)' % (arg_choices, doc_choices)
            self.reporter.error(path=self.object_path, code='doc-choices-do-not-match-spec', msg=msg)
        doc_required = doc_options_arg.get('required', False)
        data_required = data.get('required', False)
        if (doc_required or data_required) and (not (doc_required and data_required)):
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            if doc_required:
                msg += ' is not required, but is documented as being required'
            else:
                msg += ' is required, but is not documented as being required'
            self.reporter.error(path=self.object_path, code='doc-required-mismatch', msg=msg)
        doc_elements = doc_options_arg.get('elements', None)
        doc_type = doc_options_arg.get('type', 'str')
        data_elements = data.get('elements', None)
        if doc_elements and (not doc_type == 'list'):
            msg = ""Argument '%s' "" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' defines parameter elements as %s but it is valid only when value of parameter type is list' % doc_elements
            self.reporter.error(path=self.object_path, code='doc-elements-invalid', msg=msg)
        if (doc_elements or data_elements) and (not doc_elements == data_elements):
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            if data_elements:
                msg += ' specifies elements as %s,' % data_elements
            else:
                msg += ' does not specify elements,'
            if doc_elements:
                msg += 'but elements is documented as being %s' % doc_elements
            else:
                msg += 'but elements is not documented'
            self.reporter.error(path=self.object_path, code='doc-elements-mismatch', msg=msg)
        spec_suboptions = data.get('options')
        doc_suboptions = doc_options_arg.get('suboptions', {})
        if spec_suboptions:
            if not doc_suboptions:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += ' found in %s' % ' -> '.join(context)
                msg += ' has sub-options but documentation does not define it'
                self.reporter.error(path=self.object_path, code='missing-suboption-docs', msg=msg)
            self._validate_argument_spec({'options': doc_suboptions}, spec_suboptions, kwargs, context=context + [arg], last_context_spec=data)
    for arg in args_from_argspec:
        if not str(arg).isidentifier():
            msg = ""Argument '%s' in argument_spec"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' is not a valid python identifier'
            self.reporter.error(path=self.object_path, code='parameter-invalid', msg=msg)
    if docs:
        args_from_docs = set()
        for (arg, data) in doc_options.items():
            args_from_docs.add(arg)
            args_from_docs.update(data.get('aliases', []))
        args_missing_from_docs = args_from_argspec.difference(args_from_docs)
        docs_missing_from_args = args_from_docs.difference(args_from_argspec | deprecated_args_from_argspec)
        for arg in args_missing_from_docs:
            if arg in provider_args:
                continue
            msg = ""Argument '%s'"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' is listed in the argument_spec, but not documented in the module documentation'
            self.reporter.error(path=self.object_path, code='undocumented-parameter', msg=msg)
        for arg in docs_missing_from_args:
            msg = ""Argument '%s'"" % arg
            if context:
                msg += ' found in %s' % ' -> '.join(context)
            msg += ' is listed in DOCUMENTATION.options, but not accepted by the module argument_spec'
            self.reporter.error(path=self.object_path, code='nonexistent-parameter-documented', msg=msg)","for alias in sorted(set([arg] + list(aliases))):
    if alias in doc_options:
        doc_options_args.append(alias)",doc_options_args = [alias for alias in sorted(set([arg] + list(aliases))) if alias in doc_options],doc_options_args = [alias for alias in sorted(set([arg] + list(aliases))) if alias in doc_options],1,,,,,robosuite
PaddleHub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleHub/modules/image/text_recognition/chinese_text_detection_db_server/processor.py,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/image/text_recognition/chinese_text_detection_db_server/processor.py,DBPostProcess,__call__$177,"def __call__(self, predictions, ratio_list):
    pred = predictions[:, 0, :, :]
    segmentation = pred > self.thresh
    boxes_batch = []
    for batch_index in range(pred.shape[0]):
        (height, width) = pred.shape[-2:]
        (tmp_boxes, tmp_scores) = self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)
        boxes = []
        for k in range(len(tmp_boxes)):
            if tmp_scores[k] > self.box_thresh:
                boxes.append(tmp_boxes[k])
        if len(boxes) > 0:
            boxes = np.array(boxes)
            (ratio_h, ratio_w) = ratio_list[batch_index]
            boxes[:, :, 0] = boxes[:, :, 0] / ratio_w
            boxes[:, :, 1] = boxes[:, :, 1] / ratio_h
        boxes_batch.append(boxes)
    return boxes_batch","for batch_index in range(pred.shape[0]):
    (height, width) = pred.shape[-2:]
    (tmp_boxes, tmp_scores) = self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)
    boxes = []
    for k in range(len(tmp_boxes)):
        if tmp_scores[k] > self.box_thresh:
            boxes.append(tmp_boxes[k])
    if len(boxes) > 0:
        boxes = np.array(boxes)
        (ratio_h, ratio_w) = ratio_list[batch_index]
        boxes[:, :, 0] = boxes[:, :, 0] / ratio_w
        boxes[:, :, 1] = boxes[:, :, 1] / ratio_h
    boxes_batch.append(boxes)","boxes_batch = [np.array([box / ratio_list[batch_index][1], box / ratio_list[batch_index][0]]) for batch_index in range(pred.shape[0]) for (box, score) in zip(*self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], pred.shape[-1], pred.shape[-2])) if score > self.box_thresh]",Cannot refactor,-1,0,,2,1,robosuite
PaddleHub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleHub/modules/image/text_recognition/chinese_text_detection_db_server/processor.py,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/image/text_recognition/chinese_text_detection_db_server/processor.py,DBPostProcess,__call__$177,"def __call__(self, predictions, ratio_list):
    pred = predictions[:, 0, :, :]
    segmentation = pred > self.thresh
    boxes_batch = []
    for batch_index in range(pred.shape[0]):
        (height, width) = pred.shape[-2:]
        (tmp_boxes, tmp_scores) = self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)
        boxes = []
        for k in range(len(tmp_boxes)):
            if tmp_scores[k] > self.box_thresh:
                boxes.append(tmp_boxes[k])
        if len(boxes) > 0:
            boxes = np.array(boxes)
            (ratio_h, ratio_w) = ratio_list[batch_index]
            boxes[:, :, 0] = boxes[:, :, 0] / ratio_w
            boxes[:, :, 1] = boxes[:, :, 1] / ratio_h
        boxes_batch.append(boxes)
    return boxes_batch","for k in range(len(tmp_boxes)):
    if tmp_scores[k] > self.box_thresh:
        boxes.append(tmp_boxes[k])",boxes += [tmp_boxes[k] for k in range(len(tmp_boxes)) if tmp_scores[k] > self.box_thresh],boxes = [tmp_boxes[k] for k in range(len(tmp_boxes)) if tmp_scores[k] > self.box_thresh],0,1,,,,robosuite
yellowbrick,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yellowbrick/docs/api/features/pcoords_benchmark.py,https://github.com/DistrictDataLabs/yellowbrick/tree/master/docs/api/features/pcoords_benchmark.py,,plot_speedup$9,"def plot_speedup(trials=5, factors=np.arange(1, 11)):

    def pcoords_time(X, y, fast=True):
        (_, ax) = plt.subplots()
        oz = ParallelCoordinates(fast=fast, ax=ax)
        start = time.time()
        oz.fit_transform(X, y)
        delta = time.time() - start
        plt.cla()
        plt.clf()
        plt.close('all')
        return delta

    def pcoords_speedup(X, y):
        fast_time = pcoords_time(X, y, fast=True)
        slow_time = pcoords_time(X, y, fast=False)
        return slow_time / fast_time
    data = load_iris()
    speedups = []
    variance = []
    for factor in factors:
        X = np.repeat(data.data, factor, axis=0)
        y = np.repeat(data.target, factor, axis=0)
        local_speedups = []
        for trial in range(trials):
            local_speedups.append(pcoords_speedup(X, y))
        local_speedups = np.array(local_speedups)
        speedups.append(local_speedups.mean())
        variance.append(local_speedups.std())
    speedups = np.array(speedups)
    variance = np.array(variance)
    series = pd.Series(speedups, index=factors)
    (_, ax) = plt.subplots(figsize=(9, 6))
    series.plot(ax=ax, marker='o', label='speedup factor', color='b')
    ax.fill_between(factors, speedups - variance, speedups + variance, alpha=0.25, color='b')
    ax.set_ylabel('speedup factor')
    ax.set_xlabel('dataset size (number of repeats in Iris dataset)')
    ax.set_title('Speed Improvement of Fast Parallel Coordinates')
    plt.savefig('images/fast_parallel_coordinates_speedup_benchmark.png')","for factor in factors:
    X = np.repeat(data.data, factor, axis=0)
    y = np.repeat(data.target, factor, axis=0)
    local_speedups = []
    for trial in range(trials):
        local_speedups.append(pcoords_speedup(X, y))
    local_speedups = np.array(local_speedups)
    speedups.append(local_speedups.mean())
    variance.append(local_speedups.std())","variance = [np.array([pcoords_speedup(np.repeat(data.data, factor, axis=0), np.repeat(data.target, factor, axis=0)) for trial in range(trials)]).std() for factor in factors]",Cannot refactor,-1,0,,2,1,robosuite
yellowbrick,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yellowbrick/docs/api/features/pcoords_benchmark.py,https://github.com/DistrictDataLabs/yellowbrick/tree/master/docs/api/features/pcoords_benchmark.py,,plot_speedup$9,"def plot_speedup(trials=5, factors=np.arange(1, 11)):

    def pcoords_time(X, y, fast=True):
        (_, ax) = plt.subplots()
        oz = ParallelCoordinates(fast=fast, ax=ax)
        start = time.time()
        oz.fit_transform(X, y)
        delta = time.time() - start
        plt.cla()
        plt.clf()
        plt.close('all')
        return delta

    def pcoords_speedup(X, y):
        fast_time = pcoords_time(X, y, fast=True)
        slow_time = pcoords_time(X, y, fast=False)
        return slow_time / fast_time
    data = load_iris()
    speedups = []
    variance = []
    for factor in factors:
        X = np.repeat(data.data, factor, axis=0)
        y = np.repeat(data.target, factor, axis=0)
        local_speedups = []
        for trial in range(trials):
            local_speedups.append(pcoords_speedup(X, y))
        local_speedups = np.array(local_speedups)
        speedups.append(local_speedups.mean())
        variance.append(local_speedups.std())
    speedups = np.array(speedups)
    variance = np.array(variance)
    series = pd.Series(speedups, index=factors)
    (_, ax) = plt.subplots(figsize=(9, 6))
    series.plot(ax=ax, marker='o', label='speedup factor', color='b')
    ax.fill_between(factors, speedups - variance, speedups + variance, alpha=0.25, color='b')
    ax.set_ylabel('speedup factor')
    ax.set_xlabel('dataset size (number of repeats in Iris dataset)')
    ax.set_title('Speed Improvement of Fast Parallel Coordinates')
    plt.savefig('images/fast_parallel_coordinates_speedup_benchmark.png')","for trial in range(trials):
    local_speedups.append(pcoords_speedup(X, y))","local_speedups = [pcoords_speedup(X, y) for trial in range(trials)]","local_speedups = [pcoords_speedup(X, y) for trial in range(trials)]",1,,,,,robosuite
monoid,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/monoid/Scripts/fontbuilder.py,https://github.com/larsenwork/monoid/tree/master/Scripts/fontbuilder.py,,SwapLookup$135,"def SwapLookup(lookup):
    """"""Swaps the places of glyphs based on an OpenType lookup table""""""

    def swaplookup_op(fnt):
        lookups = [i for i in fnt.gsub_lookups if fnt.getLookupInfo(i)[2][0][0] == lookup]
        subtables = []
        for lookup in lookups:
            for subtable in f.getLookupSubtables(lookup):
                subtables.append(subtable)
        for glyph in fnt.glyphs():
            subbed = False
            for subtable in subtables:
                posSub = glyph.getPosSub(subtable)
                if not subbed and posSub and (posSub[0][1] == 'Substitution'):
                    subbed = True
                    sub = posSub[0][2]
                    swp = glyph.foreground
                    glyph.foreground = fnt[sub].foreground
                    fnt[sub].foreground = swp
    return swaplookup_op","for lookup in lookups:
    for subtable in f.getLookupSubtables(lookup):
        subtables.append(subtable)",subtables = [subtable for lookup in lookups for subtable in f.getLookupSubtables(lookup)],subtables = [subtable for lookup in lookups for subtable in f.getLookupSubtables(lookup)],1,,,,,robosuite
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/upload_testcase.py,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/upload_testcase.py,,get_result$80,"def get_result():
    """"""Get the result.""""""
    params = dict(request.iterparams())
    page = helpers.cast(request.get('page') or 1, int, ""'page' is not an int."")
    query = datastore_query.Query(data_types.TestcaseUploadMetadata)
    query.order('timestamp', is_desc=True)
    if not access.has_access(need_privileged_access=True):
        query.filter('uploader_email', helpers.get_user_email())
        params['permission'] = {'uploaderEmail': helpers.get_user_email()}
    (entities, total_pages, total_items, has_more) = query.fetch_page(page=page, page_size=PAGE_SIZE, projection=None, more_limit=MORE_LIMIT)
    items = []
    for entity in entities:
        items.append({'timestamp': utils.utc_datetime_to_timestamp(entity.timestamp), 'testcaseId': entity.testcase_id, 'uploaderEmail': entity.uploader_email, 'filename': entity.filename, 'bundled': entity.bundled, 'pathInArchive': entity.path_in_archive, 'status': entity.status})
    attach_testcases(items)
    result = {'hasMore': has_more, 'items': items, 'page': page, 'pageSize': PAGE_SIZE, 'totalItems': total_items, 'totalPages': total_pages}
    return (result, params)","for entity in entities:
    items.append({'timestamp': utils.utc_datetime_to_timestamp(entity.timestamp), 'testcaseId': entity.testcase_id, 'uploaderEmail': entity.uploader_email, 'filename': entity.filename, 'bundled': entity.bundled, 'pathInArchive': entity.path_in_archive, 'status': entity.status})","items = [{'timestamp': utils.utc_datetime_to_timestamp(entity.timestamp), 'testcaseId': entity.testcase_id, 'uploaderEmail': entity.uploader_email, 'filename': entity.filename, 'bundled': entity.bundled, 'pathInArchive': entity.path_in_archive, 'status': entity.status} for entity in entities]","items = [{'timestamp': utils.utc_datetime_to_timestamp(entity.timestamp), 'testcaseId': entity.testcase_id, 'uploaderEmail': entity.uploader_email, 'filename': entity.filename, 'bundled': entity.bundled, 'pathInArchive': entity.path_in_archive, 'status': entity.status} for entity in entities]",1,,,,,robosuite
chainer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chainer/chainer/distributions/multivariate_normal.py,https://github.com/chainer/chainer/tree/master/chainer/distributions/multivariate_normal.py,,_batch_triangular_inv$95,"def _batch_triangular_inv(x, lower=True):
    n = len(x)
    y = []
    for i in range(n):
        y.append(_triangular_inv(x[i]))
    return stack.stack(y)","for i in range(n):
    y.append(_triangular_inv(x[i]))",y = [_triangular_inv(x[i]) for i in range(n)],y = [_triangular_inv(x[i]) for i in range(n)],1,,,,,robosuite
alot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alot/alot/widgets/thread.py,https://github.com/pazz/alot/tree/master/alot/widgets/thread.py,TextlinesList,__init__$94,"def __init__(self, content, attr=None, attr_focus=None):
    """"""
        :class:`SimpleTree` that contains a list of all-level-0 Text widgets
        for each line in content.
        """"""
    structure = []
    if settings.get('thread_focus_linewise'):
        for line in content.splitlines():
            structure.append((ANSIText(line, attr, attr_focus, ANSI_BACKGROUND), None))
    else:
        structure.append((ANSIText(content, attr, attr_focus, ANSI_BACKGROUND), None))
    SimpleTree.__init__(self, structure)","for line in content.splitlines():
    structure.append((ANSIText(line, attr, attr_focus, ANSI_BACKGROUND), None))","structure = [(ANSIText(line, attr, attr_focus, ANSI_BACKGROUND), None) for line in content.splitlines()]","structure = [(ANSIText(line, attr, attr_focus, ANSI_BACKGROUND), None) for line in content.splitlines()]",1,,,,,robosuite
robosuite,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/robosuite/robosuite/models/objects/generated_objects.py,https://github.com/ARISE-Initiative/robosuite/tree/master/robosuite/models/objects/generated_objects.py,CompositeBodyObject,_remove_joints$238,"def _remove_joints(body):
    """"""
        Helper function to strip all joints directly appended to the specified @body.

        Args:
            body (ET.Element): Body to strip joints from
        """"""
    children_to_remove = []
    for child in body:
        if child.tag == 'joint':
            children_to_remove.append(child)
    for child in children_to_remove:
        body.remove(child)","for child in body:
    if child.tag == 'joint':
        children_to_remove.append(child)",children_to_remove = [child for child in body if child.tag == 'joint'],children_to_remove = [child for child in body if child.tag == 'joint'],1,,,,,robosuite
NOFOUND,,,,,,,,,,,,,,
pingouin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pingouin/pingouin/regression.py,https://github.com/raphaelvallat/pingouin/tree/master/pingouin/regression.py,,linear_regression$17,"def linear_regression(X, y, add_intercept=True, weights=None, coef_only=False, alpha=0.05, as_dataframe=True, remove_na=False, relimp=False):
    """"""(Multiple) Linear regression.

    Parameters
    ----------
    X : array_like
        Predictor(s), of shape *(n_samples, n_features)* or *(n_samples)*.
    y : array_like
        Dependent variable, of shape *(n_samples)*.
    add_intercept : bool
        If False, assume that the data are already centered. If True, add a
        constant term to the model. In this case, the first value in the
        output dict is the intercept of the model.

        .. note:: It is generally recommended to include a constant term
            (intercept) to the model to limit the bias and force the residual
            mean to equal zero. The intercept coefficient and p-values
            are however rarely meaningful.
    weights : array_like
        An optional vector of sample weights to be used in the fitting
        process, of shape *(n_samples)*. Missing or negative weights are not
        allowed. If not null, a weighted least squares is calculated.

        .. versionadded:: 0.3.5
    coef_only : bool
        If True, return only the regression coefficients.
    alpha : float
        Alpha value used for the confidence intervals.
        :math:`\\text{CI} = [\\alpha / 2 ; 1 - \\alpha / 2]`
    as_dataframe : bool
        If True, returns a pandas DataFrame. If False, returns a dictionnary.
    remove_na : bool
        If True, apply a listwise deletion of missing values (i.e. the entire
        row is removed). Default is False, which will raise an error if missing
        values are present in either the predictor(s) or dependent
        variable.
    relimp : bool
        If True, returns the relative importance (= contribution) of
        predictors. This is irrelevant when the predictors are uncorrelated:
        the total :math:`R^2` of the model is simply the sum of each univariate
        regression :math:`R^2`-values. However, this does not apply when
        predictors are correlated. Instead, the total :math:`R^2` of the model
        is partitioned by averaging over all combinations of predictors,
        as done in the `relaimpo
        <https://cran.r-project.org/web/packages/relaimpo/relaimpo.pdf>`_
        R package (``calc.relimp(type=""lmg"")``).

        .. warning:: The computation time roughly doubles for each
            additional predictor and therefore this can be extremely slow for
            models with more than 12-15 predictors.

        .. versionadded:: 0.3.0

    Returns
    -------
    stats : :py:class:`pandas.DataFrame` or dict
        Linear regression summary:

        * ``'names'``: name of variable(s) in the model (e.g. x1, x2...)
        * ``'coef'``: regression coefficients
        * ``'se'``: standard errors
        * ``'T'``: T-values
        * ``'pval'``: p-values
        * ``'r2'``: coefficient of determination (:math:`R^2`)
        * ``'adj_r2'``: adjusted :math:`R^2`
        * ``'CI[2.5%]'``: lower confidence intervals
        * ``'CI[97.5%]'``: upper confidence intervals
        * ``'relimp'``: relative contribution of each predictor to the final                        :math:`R^2` (only if ``relimp=True``).
        * ``'relimp_perc'``: percent relative contribution

        In addition, the output dataframe comes with hidden attributes such as
        the residuals, and degrees of freedom of the model and residuals, which
        can be accessed as follow, respectively:

        >>> lm = pg.linear_regression() # doctest: +SKIP
        >>> lm.residuals_, lm.df_model_, lm.df_resid_ # doctest: +SKIP

        Note that to follow scikit-learn convention, these hidden atributes end
        with an ""_"". When ``as_dataframe=False`` however, these attributes
        are no longer hidden and can be accessed as any other keys in the
        output dictionary.

        >>> lm = pg.linear_regression() # doctest: +SKIP
        >>> lm['residuals'], lm['df_model'], lm['df_resid'] # doctest: +SKIP

        When ``as_dataframe=False`` the dictionary also contains the
        processed ``X`` and ``y`` arrays (i.e, with NaNs removed if
        ``remove_na=True``) and the model's predicted values ``pred``.

        >>> lm['X'], lm['y'], lm['pred'] # doctest: +SKIP

        For a weighted least squares fit, the weighted ``Xw`` and ``yw``
        arrays are included in the dictionary.

        >>> lm['Xw'], lm['yw'] # doctest: +SKIP

    See also
    --------
    logistic_regression, mediation_analysis, corr

    Notes
    -----
    The :math:`\\beta` coefficients are estimated using an ordinary least
    squares (OLS) regression, as implemented in the
    :py:func:`scipy.linalg.lstsq` function. The OLS method minimizes
    the sum of squared residuals, and leads to a closed-form expression for
    the estimated :math:`\\beta`:

    .. math:: \\hat{\\beta} = (X^TX)^{-1} X^Ty

    It is generally recommended to include a constant term (intercept) to the
    model to limit the bias and force the residual mean to equal zero.
    Note that intercept coefficient and p-values are however rarely meaningful.

    The standard error of the estimates is a measure of the accuracy of the
    prediction defined as:

    .. math:: \\sigma = \\sqrt{\\text{MSE} \\cdot (X^TX)^{-1}}

    where :math:`\\text{MSE}` is the mean squared error,

    .. math::

        \\text{MSE} = \\frac{SS_{\\text{resid}}}{n - p - 1}
         = \\frac{\\sum{(\\text{true} - \\text{pred})^2}}{n - p - 1}

    :math:`p` is the total number of predictor variables in the model
    (excluding the intercept) and :math:`n` is the sample size.

    Using the :math:`\\beta` coefficients and the standard errors,
    the T-values can be obtained:

    .. math:: T = \\frac{\\beta}{\\sigma}

    and the p-values approximated using a T-distribution with
    :math:`n - p - 1` degrees of freedom.

    The coefficient of determination (:math:`R^2`) is defined as:

    .. math:: R^2 = 1 - (\\frac{SS_{\\text{resid}}}{SS_{\\text{total}}})

    The adjusted :math:`R^2` is defined as:

    .. math:: \\overline{R}^2 = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1}

    The relative importance (``relimp``) column is a partitioning of the
    total :math:`R^2` of the model into individual :math:`R^2` contribution.
    This is calculated by taking the average over average contributions in
    models of different sizes. For more details, please refer to
    `Groemping et al. 2006 <http://dx.doi.org/10.18637/jss.v017.i01>`_
    and the R package `relaimpo
    <https://cran.r-project.org/web/packages/relaimpo/relaimpo.pdf>`_.

    Note that Pingouin will automatically remove any duplicate columns
    from :math:`X`, as well as any column with only one unique value
    (constant), excluding the intercept.

    Results have been compared against sklearn, R, statsmodels and JASP.

    Examples
    --------
    1. Simple linear regression using columns of a pandas dataframe

    In this first example, we'll use the tips dataset to see how well we
    can predict the waiter's tip (in dollars) based on the total bill (also
    in dollars).

    >>> import numpy as np
    >>> import pingouin as pg
    >>> df = pg.read_dataset('tips')
    >>> # Let's predict the tip ($) based on the total bill (also in $)
    >>> lm = pg.linear_regression(df['total_bill'], df['tip'])
    >>> lm.round(2)
            names  coef    se      T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0   Intercept  0.92  0.16   5.76   0.0  0.46    0.45      0.61       1.23
    1  total_bill  0.11  0.01  14.26   0.0  0.46    0.45      0.09       0.12

    It comes as no surprise that total bill is indeed a significant predictor
    of the waiter's tip (T=14.26, p<0.05). The :math:`R^2` of the model is 0.46
    and the adjusted :math:`R^2` is 0.45, which means that our model roughly
    explains ~45% of the total variance in the tip amount.

    2. Multiple linear regression

    We can also have more than one predictor and run a multiple linear
    regression. Below, we add the party size as a second predictor of tip.

    >>> # We'll add a second predictor: the party size
    >>> lm = pg.linear_regression(df[['total_bill', 'size']], df['tip'])
    >>> lm.round(2)
            names  coef    se      T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0   Intercept  0.67  0.19   3.46  0.00  0.47    0.46      0.29       1.05
    1  total_bill  0.09  0.01  10.17  0.00  0.47    0.46      0.07       0.11
    2        size  0.19  0.09   2.26  0.02  0.47    0.46      0.02       0.36

    The party size is also a significant predictor of tip (T=2.26, p=0.02).
    Note that adding this new predictor however only improved the :math:`R^2`
    of our model by ~1%.

    This function also works with numpy arrays:

    >>> X = df[['total_bill', 'size']].to_numpy()
    >>> y = df['tip'].to_numpy()
    >>> pg.linear_regression(X, y).round(2)
           names  coef    se      T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0  Intercept  0.67  0.19   3.46  0.00  0.47    0.46      0.29       1.05
    1         x1  0.09  0.01  10.17  0.00  0.47    0.46      0.07       0.11
    2         x2  0.19  0.09   2.26  0.02  0.47    0.46      0.02       0.36

    3. Get the residuals

    >>> # For clarity, only display the first 9 values
    >>> np.round(lm.residuals_, 2)[:9]
    array([-1.62, -0.55,  0.31,  0.06, -0.11,  0.93,  0.13, -0.81, -0.49])

    Using pandas, we can show a summary of the distribution of the residuals:

    >>> import pandas as pd
    >>> pd.Series(lm.residuals_).describe().round(2)
    count    244.00
    mean      -0.00
    std        1.01
    min       -2.93
    25%       -0.55
    50%       -0.09
    75%        0.51
    max        4.04
    dtype: float64

    5. No intercept and return only the regression coefficients

    Sometimes it may be useful to remove the constant term from the regression,
    or to only return the regression coefficients without calculating the
    standard errors or p-values. This latter can potentially save you a lot of
    time if you need to calculate hundreds of regression and only care about
    the coefficients!

    >>> pg.linear_regression(X, y, add_intercept=False, coef_only=True)
    array([0.1007119 , 0.36209717])

    6. Return a dictionnary instead of a dataframe

    >>> lm_dict = pg.linear_regression(X, y, as_dataframe=False)
    >>> lm_dict.keys()
    dict_keys(['names', 'coef', 'se', 'T', 'pval', 'r2', 'adj_r2', 'CI[2.5%]',
               'CI[97.5%]', 'df_model', 'df_resid', 'residuals', 'X', 'y',
               'pred'])

    7. Remove missing values

    >>> X[4, 1] = np.nan
    >>> y[7] = np.nan
    >>> pg.linear_regression(X, y, remove_na=True, coef_only=True)
    array([0.65749955, 0.09262059, 0.19927529])

    8. Get the relative importance of predictors

    >>> lm = pg.linear_regression(X, y, remove_na=True, relimp=True)
    >>> lm[['names', 'relimp', 'relimp_perc']]
           names    relimp  relimp_perc
    0  Intercept       NaN          NaN
    1         x1  0.342503    73.045583
    2         x2  0.126386    26.954417

    The ``relimp`` column is a partitioning of the total :math:`R^2` of the
    model into individual contribution. Therefore, it sums to the :math:`R^2`
    of the full model. The ``relimp_perc`` is normalized to sum to 100%. See
    `Groemping 2006 <https://www.jstatsoft.org/article/view/v017i01>`_
    for more details.

    >>> lm[['relimp', 'relimp_perc']].sum()
    relimp           0.468889
    relimp_perc    100.000000
    dtype: float64

    9. Weighted linear regression

    >>> X = [1, 2, 3, 4, 5, 6]
    >>> y = [10, 22, 11, 13, 13, 16]
    >>> w = [1, 0.1, 1, 1, 0.5, 1]  # Array of weights. Must be >= 0.
    >>> lm = pg.linear_regression(X, y, weights=w)
    >>> lm.round(2)
           names  coef    se     T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0  Intercept  9.00  2.03  4.42  0.01  0.51    0.39      3.35      14.64
    1         x1  1.04  0.50  2.06  0.11  0.51    0.39     -0.36       2.44
    """"""
    if isinstance(X, pd.DataFrame):
        names = X.keys().tolist()
    elif isinstance(X, pd.Series):
        names = [X.name]
    else:
        names = []
    X = np.asarray(X)
    y = np.asarray(y)
    assert y.ndim == 1, 'y must be one-dimensional.'
    assert 0 < alpha < 1
    if X.ndim == 1:
        X = X[..., np.newaxis]
    if remove_na:
        (X, y) = rm_na(X, y[..., np.newaxis], paired=True, axis='rows')
        y = np.squeeze(y)
    y_gd = np.isfinite(y).all()
    X_gd = np.isfinite(X).all()
    assert y_gd, 'Target (y) contains NaN or Inf. Please remove them manually or use remove_na=True.'
    assert X_gd, 'Predictors (X) contain NaN or Inf. Please remove them manually or use remove_na=True.'
    assert y.shape[0] == X.shape[0], 'X and y must have same number of samples'
    if not names:
        names = ['x' + str(i + 1) for i in range(X.shape[1])]
    if add_intercept:
        X = np.column_stack((np.ones(X.shape[0]), X))
        names.insert(0, 'Intercept')
    n_nonzero = np.count_nonzero(X, axis=0)
    idx_zero = np.flatnonzero(n_nonzero == 0)
    if len(idx_zero):
        X = np.delete(X, idx_zero, 1)
        names = np.delete(names, idx_zero)
    idx_unique = np.where(np.all(X == X[0, :], axis=0))[0]
    if len(idx_unique) > 1:
        X = np.delete(X, idx_unique[1:], 1)
        names = np.delete(names, idx_unique[1:])
    constant = 1 if len(idx_unique) > 0 else 0
    if X.shape[1] > 1:
        idx_duplicate = []
        for pair in itertools.combinations(range(X.shape[1]), 2):
            if np.array_equal(X[:, pair[0]], X[:, pair[1]]):
                idx_duplicate.append(pair[1])
        if len(idx_duplicate):
            X = np.delete(X, idx_duplicate, 1)
            names = np.delete(names, idx_duplicate)
    (n, p) = (X.shape[0], X.shape[1])
    assert n >= 3, 'At least three valid samples are required in X.'
    assert p >= 1, 'X must have at least one valid column.'
    if weights is not None:
        if relimp:
            raise ValueError('relimp = True is not supported when using weights.')
        w = np.asarray(weights)
        assert w.ndim == 1, 'weights must be a 1D array.'
        assert w.size == n, 'weights must be of shape n_samples.'
        assert not np.isnan(w).any(), 'Missing weights are not accepted.'
        assert not (w < 0).any(), 'Negative weights are not accepted.'
        n = np.count_nonzero(w)
        wts = np.diag(np.sqrt(w))
        Xw = wts @ X
        yw = wts @ y
    else:
        w = np.ones(n)
        Xw = X
        yw = y
    (coef, ss_res, rank, _) = lstsq(Xw, yw, cond=None)
    ss_res = ss_res[0] if ss_res.shape == (1,) else ss_res
    if coef_only:
        return coef
    calc_ss_res = False
    if rank < Xw.shape[1]:
        warnings.warn(f'Design matrix supplied with `X` parameter is rank deficient (rank {rank} with {Xw.shape[1]} columns). That means that one or more of the columns in `X` are a linear combination of one of more of the other columns.')
        calc_ss_res = True
    df_model = rank - constant
    df_resid = n - rank
    pred = Xw @ coef
    resid = yw - pred
    if calc_ss_res:
        ss_res = (resid ** 2).sum()
    ss_tot = yw @ yw
    ss_wtot = np.sum(w * (y - np.average(y, weights=w)) ** 2)
    if constant:
        r2 = 1 - ss_res / ss_wtot
    else:
        r2 = 1 - ss_res / ss_tot
    adj_r2 = 1 - (1 - r2) * (n - constant) / df_resid
    mse = ss_res / df_resid
    beta_var = mse * np.linalg.pinv(Xw.T @ Xw).diagonal()
    beta_se = np.sqrt(beta_var)
    T = coef / beta_se
    pval = 2 * t.sf(np.fabs(T), df_resid)
    crit = t.ppf(1 - alpha / 2, df_resid)
    marg_error = crit * beta_se
    ll = coef - marg_error
    ul = coef + marg_error
    ll_name = 'CI[%.1f%%]' % (100 * alpha / 2)
    ul_name = 'CI[%.1f%%]' % (100 * (1 - alpha / 2))
    stats = {'names': names, 'coef': coef, 'se': beta_se, 'T': T, 'pval': pval, 'r2': r2, 'adj_r2': adj_r2, ll_name: ll, ul_name: ul}
    if relimp:
        data = pd.concat([pd.DataFrame(y, columns=['y']), pd.DataFrame(X, columns=names)], sort=False, axis=1)
        if 'Intercept' in names:
            reli = _relimp(data.drop(columns=['Intercept']).cov())
            reli['names'] = ['Intercept'] + reli['names']
            reli['relimp'] = np.insert(reli['relimp'], 0, np.nan)
            reli['relimp_perc'] = np.insert(reli['relimp_perc'], 0, np.nan)
        else:
            reli = _relimp(data.cov())
        stats.update(reli)
    if as_dataframe:
        stats = _postprocess_dataframe(pd.DataFrame(stats))
        stats.df_model_ = df_model
        stats.df_resid_ = df_resid
        stats.residuals_ = 0
        stats.residuals_ = resid
    else:
        stats['df_model'] = df_model
        stats['df_resid'] = df_resid
        stats['residuals'] = resid
        stats['X'] = X
        stats['y'] = y
        stats['pred'] = pred
        if weights is not None:
            stats['yw'] = yw
            stats['Xw'] = Xw
    return stats","for pair in itertools.combinations(range(X.shape[1]), 2):
    if np.array_equal(X[:, pair[0]], X[:, pair[1]]):
        idx_duplicate.append(pair[1])","idx_duplicate = [pair[1] for pair in itertools.combinations(range(X.shape[1]), 2) if np.array_equal(X[:, pair[0]], X[:, pair[1]])]",0,,,,,,
pixelsort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pixelsort/pixelsort/sorter.py,https://github.com/satyarth/pixelsort/tree/master/pixelsort/sorter.py,,sort_image$4,"def sort_image(size, image_data, mask_data, intervals, randomness, sorting_function):
    sorted_pixels = []
    for y in range(size[1]):
        row = []
        x_min = 0
        for x_max in intervals[y] + [size[0]]:
            interval = []
            for x in range(x_min, x_max):
                if mask_data[x, y]:
                    interval.append(image_data[x, y])
            if random.random() < randomness / 100:
                row += interval
            else:
                row += sort_interval(interval, sorting_function)
            x_min = x_max
        sorted_pixels.append(row)
    return sorted_pixels","for x in range(x_min, x_max):
    if mask_data[x, y]:
        interval.append(image_data[x, y])","interval = [image_data[x, y] for x in range(x_min, x_max) if mask_data[x, y]]",0,,,,,,
Minecraft-Overviewer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Minecraft-Overviewer/overviewer.py,https://github.com/overviewer/Minecraft-Overviewer/tree/master//overviewer.py,,main$52,"def main():
    logger.configure()
    if os.name == 'posix':
        if os.geteuid() == 0:
            logging.warning('You are running Overviewer as root. It is recommended that you never do this, as it is dangerous for your system. If you are running into permission errors, fix your file/directory permissions instead. Overviewer does not need access to critical system resources and therefore does not require root access.')
        try:
            with open('/etc/redhat-release', 'r') as release_f:
                rel_contents = release_f.read()
                try:
                    major_rel = re.search('\\d(\\.\\d+)?', rel_contents).group(0).split('.')[0]
                    if major_rel == '6':
                        logging.warning('We will be dropping support for this release of your distribution soon. Please upgrade as soon as possible, or you will not receive future Overviewer updates.')
                except AttributeError:
                    pass
        except IOError:
            pass
    try:
        cpus = multiprocessing.cpu_count()
    except NotImplementedError:
        cpus = 1
    avail_north_dirs = ['lower-left', 'upper-left', 'upper-right', 'lower-right', 'auto']
    parser = ArgumentParser(usage=helptext)
    parser.add_argument('-c', '--config', dest='config', action='store', help='Specify the config file to use.')
    parser.add_argument('-p', '--processes', dest='procs', action='store', type=int, help='The number of local worker processes to spawn. Defaults to the number of CPU cores your computer has.')
    parser.add_argument('--pid', dest='pid', action='store', help='Specify the pid file to use.')
    parser.add_argument('--rendermodes', dest='rendermodes', action='store', help=""If you're not using a config file, specify which rendermodes to render with this option. This is a comma-separated list."")
    parser.add_argument('world', nargs='?', help='Path or name of the world you want to render.')
    parser.add_argument('output', nargs='?', help='Output directory for the rendered map.')
    render_modifiers = parser.add_mutually_exclusive_group()
    render_modifiers.add_argument('--forcerender', dest='forcerender', action='store_true', help='Force re-render the entire map.')
    render_modifiers.add_argument('--check-tiles', dest='checktiles', action='store_true', help='Check each tile on disk and re-render old tiles.')
    render_modifiers.add_argument('--no-tile-checks', dest='notilechecks', action='store_true', help='Only render tiles that come from chunks that have changed since the last render (the default).')
    parser.add_argument('--check-terrain', dest='check_terrain', action='store_true', help='Try to locate the texture files. Useful for debugging texture problems.')
    parser.add_argument('-V', '--version', dest='version', help='Display version information and then exits.', action='store_true')
    parser.add_argument('--check-version', dest='checkversion', help='Fetch information about the latest version of Overviewer.', action='store_true')
    parser.add_argument('--update-web-assets', dest='update_web_assets', action='store_true', help='Update web assets. Will *not* render tiles or update overviewerConfig.js.')
    parser.add_argument('-q', '--quiet', dest='quiet', action='count', default=0, help='Print less output. You can specify this option multiple times.')
    parser.add_argument('-v', '--verbose', dest='verbose', action='count', default=0, help='Print more output. You can specify this option multiple times.')
    parser.add_argument('--simple-output', dest='simple', action='store_true', default=False, help='Use a simple output format, with no colors or progress bars.')
    exegroup = parser.add_argument_group('Other Scripts', 'These scripts may accept different arguments than the ones listed above.')
    exegroup.add_argument('--genpoi', dest='genpoi', action='store_true', help='Run the genPOI script.')
    exegroup.add_argument('--skip-scan', dest='skipscan', action='store_true', help=""When running GenPOI, don't scan for entities."")
    exegroup.add_argument('--skip-players', dest='skipplayers', action='store_true', help=""When running GenPOI, don't scan player data."")
    (args, unknowns) = parser.parse_known_args()
    if len(unknowns) > 0 and args.world and args.output:
        possible_mistakes = []
        for i in range(len(unknowns) + 1):
            possible_mistakes.append(' '.join([args.world, args.output] + unknowns[:i]))
            possible_mistakes.append(' '.join([args.output] + unknowns[:i]))
        for mistake in possible_mistakes:
            if os.path.exists(mistake):
                logging.warning('Looks like you tried to make me use {0} as an argument, but forgot to quote the argument correctly. Try using ""{0}"" instead if the spaces are part of the path.'.format(mistake))
                parser.error('Too many arguments.')
        parser.error('Too many arguments.')
    if args.genpoi:
        sys.argv.remove('--genpoi')
        g = __import__('overviewer_core.aux_files', {}, {}, ['genPOI'])
        g.genPOI.main()
        return 0
    logger.configure(logging.INFO + 10 * args.quiet - 10 * args.verbose, verbose=args.verbose > 0, simple=args.simple)
    if args.version:
        print('Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            import overviewer_core.overviewer_version as overviewer_version
            print('built on %s' % overviewer_version.BUILD_DATE)
            if args.verbose > 0:
                print('Build machine: %s %s' % (overviewer_version.BUILD_PLATFORM, overviewer_version.BUILD_OS))
                print('Read version information from %r' % overviewer_version.__file__)
        except ImportError:
            print('(build info not found)')
        if args.verbose > 0:
            print('Python executable: %r' % sys.executable)
            print(sys.version)
        if not args.checkversion:
            return 0
    if args.checkversion:
        print('Currently running Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            from urllib import request
            import json
            latest_ver = json.loads(request.urlopen('http://overviewer.org/download.json').read())['src']
            print('Latest version of Minecraft Overviewer %s (%s)' % (latest_ver['version'], latest_ver['commit'][:7]))
            print('See https://overviewer.org/downloads for more information.')
        except Exception:
            print('Failed to fetch latest version info.')
            if args.verbose > 0:
                import traceback
                traceback.print_exc()
            else:
                print('Re-run with --verbose for more details.')
            return 1
        return 0
    if args.pid:
        if os.path.exists(args.pid):
            try:
                with open(args.pid, 'r') as fpid:
                    pid = int(fpid.read())
                    if util.pid_exists(pid):
                        print('Overviewer is already running (pid exists) - exiting.')
                        return 0
            except (IOError, ValueError):
                pass
        with open(args.pid, 'w') as f:
            f.write(str(os.getpid()))
    if args.check_terrain and (not args.config):
        import hashlib
        from overviewer_core.textures import Textures
        tex = Textures()
        logging.info('Looking for a few common texture files...')
        try:
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/acacia_planks.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/ancient_debris_top.png', verbose=True)
        except IOError:
            logging.error('Could not find any texture files.')
            return 1
        return 0
    if not (args.world and args.output) and (not args.config):
        if util.is_bare_console():
            print('\n')
            print('The Overviewer is a console program.  Please open a Windows command prompt')
            print('first and run Overviewer from there.   Further documentation is available at')
            print('http://docs.overviewer.org/\n')
            print('\n')
            print('For a quick-start guide on Windows, visit the following URL:\n')
            print('http://docs.overviewer.org/en/latest/win_tut/windowsguide/\n')
        else:
            logging.error('You must either specify --config or give me a world directory and output directory.')
            parser.print_help()
            list_worlds()
        return 1
    if args.config and (args.world and args.output):
        print()
        print('If you specify --config, you need to specify the world to render as well as the destination in the config file, not on the command line.')
        print('Put something like this in your config file:')
        print(""worlds['myworld'] = %r"" % args[0])
        print('outputdir = %r' % (args[1] if len(args) > 1 else '/path/to/output'))
        print()
        logging.error('You cannot specify both --config AND a world + output directory on the command line.')
        parser.print_help()
        return 1
    if not args.config and (args.world or args.output) and (not (args.world and args.output)):
        logging.error('You must specify both the world directory and an output directory')
        parser.print_help()
        return 1
    mw_parser = config_parser.MultiWorldParser()
    if not args.config:
        (worldpath, destdir) = map(os.path.expanduser, [args.world, args.output])
        logging.debug('Using %r as the world directory', worldpath)
        logging.debug('Using %r as the output directory', destdir)
        mw_parser.set_config_item('worlds', {'world': worldpath})
        mw_parser.set_config_item('outputdir', destdir)
        rendermodes = ['lighting']
        if args.rendermodes:
            rendermodes = args.rendermodes.replace('-', '_').split(',')
        renders = OrderedDict()
        for rm in rendermodes:
            renders['world-' + rm] = {'world': 'world', 'title': 'Overviewer Render (%s)' % rm, 'rendermode': rm}
        mw_parser.set_config_item('renders', renders)
    else:
        if args.rendermodes:
            logging.error('You cannot specify --rendermodes if you give a config file. Configure your rendermodes in the config file instead.')
            parser.print_help()
            return 1
        try:
            mw_parser.parse(os.path.expanduser(args.config))
        except config_parser.MissingConfigException as e:
            logging.error(str(e))
            util.nice_exit(1)
    if args.procs:
        mw_parser.set_config_item('processes', args.procs)
    try:
        config = mw_parser.get_validated_config()
    except Exception as ex:
        if args.verbose:
            logging.exception('An error was encountered with your configuration. See the information below.')
        else:
            logging.error('An error was encountered with your configuration.')
            logging.error(str(ex))
        return 1
    if args.check_terrain:
        logging.info('Looking for a few common texture files...')
        for (render_name, render) in config['renders'].items():
            logging.info('Looking at render %r.', render_name)
            texopts = util.dict_subset(render, ['texturepath'])
            tex = textures.Textures(**texopts)
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/oak_planks.png', verbose=True)
        return 0
    logging.info('Welcome to Minecraft Overviewer version %s (%s)!' % (util.findGitVersion(), util.findGitHash()[:7]))
    logging.debug('Current log level: {0}.'.format(logging.getLogger().level))

    def set_renderchecks(checkname, num):
        for (name, render) in config['renders'].items():
            if render.get('renderchecks', 0) == 3:
                logging.warning(checkname + ' ignoring render ' + repr(name) + ' since it\'s marked as ""don\'t render"".')
            else:
                render['renderchecks'] = num
    if args.forcerender:
        logging.info('Forcerender mode activated. ALL tiles will be rendered.')
        set_renderchecks('forcerender', 2)
    elif args.checktiles:
        logging.info('Checking all tiles for updates manually.')
        set_renderchecks('checktiles', 1)
    elif args.notilechecks:
        logging.info('Disabling all tile mtime checks. Only rendering tiles that need updating since last render.')
        set_renderchecks('notilechecks', 0)
    if not config['renders']:
        logging.error(""You must specify at least one render in your config file. Check the documentation at http://docs.overviewer.org if you're having trouble."")
        return 1
    for (rname, render) in config['renders'].items():
        try:
            worldpath = config['worlds'][render['world']]
        except KeyError:
            logging.error(""Render %s's world is '%s', but I could not find a corresponding entry in the worlds dictionary."", rname, render['world'])
            return 1
        render['worldname_orig'] = render['world']
        render['world'] = worldpath
        if render.get('forcerender', False):
            render['renderchecks'] = 2
        if render.get('overlay', []) != []:
            for x in render.get('overlay'):
                if x != rname:
                    try:
                        renderLink = config['renders'][x]
                    except KeyError:
                        logging.error(""Render %s's overlay is '%s', but I could not find a corresponding entry in the renders dictionary."", rname, x)
                        return 1
                else:
                    logging.error(""Render %s's overlay contains itself."", rname)
                    return 1
    destdir = config['outputdir']
    if not destdir:
        logging.error('You must specify the output directory in your config file.')
        logging.error(""e.g. outputdir = '/path/to/outputdir'"")
        return 1
    if not os.path.exists(destdir):
        try:
            os.mkdir(destdir)
        except OSError:
            logging.exception('Could not create the output directory.')
            return 1
    assetMrg = assetmanager.AssetManager(destdir, config.get('customwebassets', None))
    if args.update_web_assets:
        assetMrg.output_noconfig()
        logging.info('Web assets have been updated.')
        return 0
    changelists = {}
    for render in config['renders'].values():
        if 'changelist' in render:
            path = render['changelist']
            if path not in changelists:
                out = open(path, 'w')
                logging.debug('Opening changelist %s (%s).', out, out.fileno())
                changelists[path] = out
            else:
                out = changelists[path]
            render['changelist'] = out.fileno()
    tilesets = []
    worldcache = {}
    texcache = {}
    caches = []
    caches.append(cache.LRUCache(size=100))
    renders = config['renders']
    for (render_name, render) in renders.items():
        logging.debug('Found the following render thing: %r', render)
        try:
            w = worldcache[render['world']]
        except KeyError:
            try:
                w = world.World(render['world'])
            except CorruptNBTError as e:
                logging.error('Failed to open world %r.', render['world'])
                raise e
            except world.UnsupportedVersion as e:
                for ln in str(e).split('\n'):
                    logging.error(ln)
                sys.exit(1)
            worldcache[render['world']] = w
        texopts = util.dict_subset(render, ['texturepath', 'bgcolor', 'northdirection'])
        texopts_key = tuple(texopts.items())
        if texopts_key not in texcache:
            tex = textures.Textures(**texopts)
            logging.info('Generating textures...')
            tex.generate()
            logging.debug('Finished generating textures.')
            texcache[texopts_key] = tex
        else:
            tex = texcache[texopts_key]
        try:
            logging.debug('Asking for regionset %r.' % render['dimension'][1])
            rset = w.get_regionset(render['dimension'][1])
        except IndexError:
            logging.error(""Sorry, I can't find anything to render!  Are you sure there are .mca files in the world directory of %s?"" % render['world'])
            return 1
        if rset is None:
            logging.warning(""Sorry, you requested dimension '%s' for %s, but I couldn't find it."", render['dimension'][0], render_name)
            continue
        rset = world.CachedRegionSet(rset, caches)
        if 'crop' in render:
            rsets = []
            for zone in render['crop']:
                rsets.append(world.CroppedRegionSet(rset, *zone))
        else:
            rsets = [rset]
        if render['northdirection'] > 0:
            newrsets = []
            for r in rsets:
                r = world.RotatedRegionSet(r, render['northdirection'])
                newrsets.append(r)
            rsets = newrsets
        tileset_dir = os.path.abspath(os.path.join(destdir, render_name))
        render['name'] = render_name
        tileSetOpts = util.dict_subset(render, ['name', 'imgformat', 'renderchecks', 'rerenderprob', 'bgcolor', 'defaultzoom', 'imgquality', 'imglossless', 'optimizeimg', 'rendermode', 'worldname_orig', 'title', 'dimension', 'changelist', 'showspawn', 'overlay', 'base', 'poititle', 'maxzoom', 'showlocationmarker', 'minzoom', 'center'])
        tileSetOpts.update({'spawn': w.find_true_spawn()})
        for rset in rsets:
            tset = tileset.TileSet(w, rset, assetMrg, tex, tileSetOpts, tileset_dir)
            tilesets.append(tset)
    if not tilesets:
        logging.error(""There are no tilesets to render! There's nothing to do, so exiting."")
        return 1
    logging.info('Preprocessing...')
    for ts in tilesets:
        ts.do_preprocessing()
    assetMrg.initialize(tilesets)
    if config['processes'] == 1:
        dispatch = dispatcher.Dispatcher()
    else:
        dispatch = dispatcher.MultiprocessingDispatcher(local_procs=config['processes'])
    dispatch.render_all(tilesets, config['observer'])
    dispatch.close()
    assetMrg.finalize(tilesets)
    for out in changelists.values():
        logging.debug('Closing %s (%s).', out, out.fileno())
        out.close()
    if config['processes'] == 1:
        logging.debug('Final cache stats:')
        for c in caches:
            logging.debug('\t%s: %s hits, %s misses', c.__class__.__name__, c.hits, c.misses)
    if args.pid:
        os.remove(args.pid)
    logging.info(""Your render has been written to '%s', open index.html to view it."" % destdir)
    return 0","for zone in render['crop']:
    rsets.append(world.CroppedRegionSet(rset, *zone))","rsets = [world.CroppedRegionSet(rset, *zone) for zone in render['crop']]",0,,,,,,
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/currency_crosses.py,https://github.com/alvarobartt/investpy/tree/master/investpy/currency_crosses.py,,get_currency_cross_recent_data$187,"def get_currency_cross_recent_data(currency_cross, as_json=False, order='ascending', interval='Daily'):
    """"""
    This function retrieves recent historical data from the introduced `currency_cross` as indexed in Investing.com
    via Web Scraping. The resulting data can it either be stored in a :obj:`pandas.DataFrame` or in a
    :obj:`json` file, with `ascending` or `descending` order.

    Args:
        currency_cross (:obj:`str`): name of the currency_cross to retrieve recent historical data from.
        as_json (:obj:`bool`, optional):
            optional argument to determine the format of the output data (:obj:`pandas.DataFrame` or :obj:`json`).
        order (:obj:`str`, optional):
            optional argument to define the order of the retrieved data (`ascending`, `asc` or `descending`, `desc`).
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function returns a either a :obj:`pandas.DataFrame` or a :obj:`json` file containing the retrieved
            recent data from the specified currency_cross via argument. The dataset contains the open, high, low, close,
            volume and currency values for the selected currency_cross on market days.

            The return data is in case we use default arguments will look like::

                Date || Open | High | Low | Close | Currency
                -----||------|------|-----|-------|---------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxxxx

            but if we define `as_json=True`, then the output will be::

                {
                    name: name,
                    recent: [
                        dd/mm/yyyy: {
                            'open': x,
                            'high': x,
                            'low': x,
                            'close': x,
                            'currency' : x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised if any of the introduced arguments was not valid or errored.
        IOError: raised if currency_crosses object/file not found or unable to retrieve.
        RuntimeError: raised introduced currency_cross does not match any of the indexed ones.
        ConnectionError: raised if GET request did not return 200 status code.
        IndexError: raised if currency_cross information was unavailable or not found.

    Examples:
        >>> data = investpy.get_currency_cross_recent_data(currency_cross='EUR/USD')
        >>> data.head()
                      Open    High     Low   Close Currency
        Date
        2019-08-27  1.1101  1.1116  1.1084  1.1091      USD
        2019-08-28  1.1090  1.1099  1.1072  1.1078      USD
        2019-08-29  1.1078  1.1093  1.1042  1.1057      USD
        2019-08-30  1.1058  1.1062  1.0963  1.0991      USD
        2019-09-02  1.0990  1.1000  1.0958  1.0968      USD

    """"""
    if not currency_cross:
        raise ValueError('ERR#0052: currency_cross param is mandatory and should be a str.')
    if not isinstance(currency_cross, str):
        raise ValueError('ERR#0052: currency_cross param is mandatory and should be a str.')
    if not isinstance(as_json, bool):
        raise ValueError('ERR#0002: as_json argument can just be True or False, bool type.')
    if order not in ['ascending', 'asc', 'descending', 'desc']:
        raise ValueError('ERR#0003: order argument can just be ascending (asc) or descending (desc), str type.')
    if not interval:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    if not isinstance(interval, str):
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    interval = interval.lower()
    if interval not in ['daily', 'weekly', 'monthly']:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    resource_package = 'investpy'
    resource_path = '/'.join(('resources', 'currency_crosses.csv'))
    if pkg_resources.resource_exists(resource_package, resource_path):
        currency_crosses = pd.read_csv(pkg_resources.resource_filename(resource_package, resource_path), keep_default_na=False)
    else:
        raise FileNotFoundError('ERR#0060: currency_crosses file not found or errored.')
    if currency_crosses is None:
        raise IOError('ERR#0050: currency_crosses not found or unable to retrieve.')
    currency_cross = unidecode(currency_cross.strip().lower())
    if currency_cross not in list(currency_crosses['name'].apply(unidecode).str.lower()):
        raise RuntimeError('ERR#0054: the introduced currency_cross ' + str(currency_cross) + ' does not exist.')
    id_ = currency_crosses.loc[(currency_crosses['name'].apply(unidecode).str.lower() == currency_cross).idxmax(), 'id']
    name = currency_crosses.loc[(currency_crosses['name'].apply(unidecode).str.lower() == currency_cross).idxmax(), 'name']
    currency = currency_crosses.loc[(currency_crosses['name'].apply(unidecode).str.lower() == currency_cross).idxmax(), 'second']
    header = name + ' Historical Data'
    params = {'curr_id': id_, 'smlID': str(randint(1000000, 99999999)), 'header': header, 'interval_sec': interval.capitalize(), 'sort_col': 'date', 'sort_ord': 'DESC', 'action': 'historical_data'}
    head = {'User-Agent': random_user_agent(), 'X-Requested-With': 'XMLHttpRequest', 'Accept': 'text/html', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive'}
    url = 'https://www.investing.com/instruments/HistoricalDataAjax'
    req = requests.post(url, headers=head, data=params)
    if req.status_code != 200:
        raise ConnectionError('ERR#0015: error ' + str(req.status_code) + ', try again later.')
    root_ = fromstring(req.text)
    path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
    result = list()
    if path_:
        for elements_ in path_:
            if elements_.xpath('.//td')[0].text_content() == 'No results found':
                raise IndexError('ERR#0055: currency_cross information unavailable or not found.')
            info = []
            for nested_ in elements_.xpath('.//td'):
                info.append(nested_.get('data-real-value'))
            currency_cross_date = datetime.strptime(str(datetime.fromtimestamp(int(info[0]), tz=pytz.timezone('GMT')).date()), '%Y-%m-%d')
            currency_cross_close = float(info[1].replace(',', ''))
            currency_cross_open = float(info[2].replace(',', ''))
            currency_cross_high = float(info[3].replace(',', ''))
            currency_cross_low = float(info[4].replace(',', ''))
            result.insert(len(result), Data(currency_cross_date, currency_cross_open, currency_cross_high, currency_cross_low, currency_cross_close, None, currency, None))
        if order in ['ascending', 'asc']:
            result = result[::-1]
        elif order in ['descending', 'desc']:
            result = result
        if as_json is True:
            json_ = {'name': name, 'recent': [value.currency_cross_as_json() for value in result]}
            return json.dumps(json_, sort_keys=False)
        elif as_json is False:
            df = pd.DataFrame.from_records([value.currency_cross_to_dict() for value in result])
            df.set_index('Date', inplace=True)
            return df
    else:
        raise RuntimeError('ERR#0004: data retrieval error while scraping.')","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],0,,,,,,
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/lookups/handlers/ami.py,https://github.com/cloudtools/stacker/tree/master/stacker/lookups/handlers/ami.py,AmiLookup,handle$25,"def handle(cls, value, provider, **kwargs):
    """"""Fetch the most recent AMI Id using a filter
    
        For example:
    
            ${ami [<region>@]owners:self,account,amazon name_regex:serverX-[0-9]+ architecture:x64,i386}
    
            The above fetches the most recent AMI where owner is self
            account or amazon and the ami name matches the regex described,
            the architecture will be either x64 or i386
    
            You can also optionally specify the region in which to perform the
            AMI lookup.
    
            Valid arguments:
    
            owners (comma delimited) REQUIRED ONCE:
                aws_account_id | amazon | self
    
            name_regex (a regex) REQUIRED ONCE:
                e.g. my-ubuntu-server-[0-9]+
    
            executable_users (comma delimited) OPTIONAL ONCE:
                aws_account_id | amazon | self
    
            Any other arguments specified are sent as filters to the aws api
            For example, ""architecture:x86_64"" will add a filter
        """"""
    value = read_value_from_path(value)
    if '@' in value:
        (region, value) = value.split('@', 1)
    else:
        region = provider.region
    ec2 = get_session(region).client('ec2')
    values = {}
    describe_args = {}
    matches = re.findall('([0-9a-zA-z_-]+:[^\\s$]+)', value)
    for match in matches:
        (k, v) = match.split(':', 1)
        values[k] = v
    if not values.get('owners'):
        raise Exception(""'owners' value required when using ami"")
    owners = values.pop('owners').split(',')
    describe_args['Owners'] = owners
    if not values.get('name_regex'):
        raise Exception(""'name_regex' value required when using ami"")
    name_regex = values.pop('name_regex')
    executable_users = None
    if values.get('executable_users'):
        executable_users = values.pop('executable_users').split(',')
        describe_args['ExecutableUsers'] = executable_users
    filters = []
    for (k, v) in values.items():
        filters.append({'Name': k, 'Values': v.split(',')})
    describe_args['Filters'] = filters
    result = ec2.describe_images(**describe_args)
    images = sorted(result['Images'], key=operator.itemgetter('CreationDate'), reverse=True)
    for image in images:
        if re.match('^%s$' % name_regex, image.get('Name', '')):
            return image['ImageId']
    raise ImageNotFound(value)","for (k, v) in values.items():
    filters.append({'Name': k, 'Values': v.split(',')})","filters = [{'Name': k, 'Values': v.split(',')} for (k, v) in values.items()]",0,,,,,,
uncertainty-baselines,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/uncertainty-baselines/baselines/jft/experiments/imagenet21k_vit_base16_finetune_cifar100.py,https://github.com/google/uncertainty-baselines/tree/master/baselines/jft/experiments/imagenet21k_vit_base16_finetune_cifar100.py,,get_config$29,"def get_config():
    """"""Config for training a patch-transformer on JFT.""""""
    config = ml_collections.ConfigDict()
    config.dataset = 'cifar100'
    config.val_split = 'train[98%:]'
    config.train_split = 'train[:98%]'
    config.num_classes = 100
    BATCH_SIZE = 512
    config.batch_size = BATCH_SIZE
    config.total_steps = 10000
    INPUT_RES = 384
    pp_common = '|value_range(-1, 1)'
    pp_common += f'|onehot({config.num_classes}, key=""label"", key_result=""labels"")'
    pp_common += '|keep([""image"", ""labels""])'
    config.pp_train = f'decode|inception_crop({INPUT_RES})|flip_lr' + pp_common
    config.pp_eval = f'decode|resize({INPUT_RES})' + pp_common
    config.ood_datasets = ['cifar10', 'svhn_cropped']
    config.ood_num_classes = [10, 10]
    config.ood_split = 'test'
    config.ood_methods = ['msp', 'entropy', 'maha', 'rmaha']
    pp_eval_ood = []
    for num_classes in config.ood_num_classes:
        if num_classes > config.num_classes:
            pp_eval_ood.append(config.pp_eval.replace(f'onehot({config.num_classes}', f'onehot({num_classes}'))
        else:
            pp_eval_ood.append(config.pp_eval)
    config.pp_eval_ood = pp_eval_ood
    config.shuffle_buffer_size = 50000
    config.log_training_steps = 10
    config.log_eval_steps = 100
    config.checkpoint_steps = 1000
    config.checkpoint_timeout = 1
    config.prefetch_to_device = 2
    config.trial = 0
    config.model_init = '/path/to/pretrained_model_ckpt.npz'
    config.model = ml_collections.ConfigDict()
    config.model.patches = ml_collections.ConfigDict()
    config.model.patches.size = [16, 16]
    config.model.hidden_size = 768
    config.model.transformer = ml_collections.ConfigDict()
    config.model.transformer.attention_dropout_rate = 0.0
    config.model.transformer.dropout_rate = 0.0
    config.model.transformer.mlp_dim = 3072
    config.model.transformer.num_heads = 12
    config.model.transformer.num_layers = 12
    config.model.classifier = 'token'
    config.model.representation_size = None
    config.optim_name = 'Momentum'
    config.optim = ml_collections.ConfigDict()
    config.grad_clip_norm = 1.0
    config.weight_decay = None
    config.loss = 'softmax_xent'
    config.lr = ml_collections.ConfigDict()
    config.lr.base = 0.01
    config.lr.warmup_steps = 500
    config.lr.decay_type = 'cosine'
    return config","for num_classes in config.ood_num_classes:
    if num_classes > config.num_classes:
        pp_eval_ood.append(config.pp_eval.replace(f'onehot({config.num_classes}', f'onehot({num_classes}'))
    else:
        pp_eval_ood.append(config.pp_eval)","pp_eval_ood = [config.pp_eval.replace(f'onehot({config.num_classes}', f'onehot({num_classes}') if num_classes > config.num_classes else config.pp_eval for num_classes in config.ood_num_classes]",0,,,,,,
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/convert/convert.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/convert/convert.py,Converter,forward_function$213,"def forward_function(x):
    node_outputs = {}
    for (index, input) in enumerate(model_proto.graph.input):
        node_outputs[input.name] = x[index]
    for node in nodes:
        inputs = []
        for input in node.input:
            if input in node_outputs.keys():
                inputs.append(node_outputs[input])
        with tf.name_scope(node.name + '/forward'):
            res = tfe_nodes[node.name].forward(inputs)
        for (i, output) in enumerate(node.output):
            node_outputs[output] = res[i]
    res = []
    for output in model_proto.graph.output:
        res.append(node_outputs[output.name])
    return res","for input in node.input:
    if input in node_outputs.keys():
        inputs.append(node_outputs[input])",inputs = [node_outputs[input] for input in node.input if input in node_outputs.keys()],0,,,,,,
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for p in ss(el, '.examples p'):
    examp.append(p.text.strip())","examp = [p.text.strip() for p in ss(el, '.examples p')]",0,,,,,,
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/crypto.py,https://github.com/alvarobartt/investpy/tree/master/investpy/crypto.py,,get_crypto_recent_data$116,"def get_crypto_recent_data(crypto, as_json=False, order='ascending', interval='Daily'):
    """"""
    This function retrieves recent historical data from the introduced crypto from Investing.com. So on, the recent data
    of the introduced crypto will be retrieved and returned as a :obj:`pandas.DataFrame` if the parameters are valid
    and the request to Investing.com succeeds. Note that additionally some optional parameters can be specified: as_json
    and order, which let the user decide if the data is going to be returned as a :obj:`json` or not, and if the historical
    data is going to be ordered ascending or descending (where the index is the date), respectively.

    Args:
        crypto (:obj:`str`): name of the crypto currency to retrieve data from.
        as_json (:obj:`bool`, optional):
            to determine the format of the output data, either a :obj:`pandas.DataFrame` if False and a :obj:`json` if True.
        order (:obj:`str`, optional): to define the order of the retrieved data which can either be ascending or descending.
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function can return either a :obj:`pandas.DataFrame` or a :obj:`json` object, containing the retrieved
            recent data of the specified crypto currency. So on, the resulting dataframe contains the open, high, low,
            close and volume values for the selected crypto on market days and the currency in which those values are presented.

            The resulting recent data, in case that the default parameters were applied, will look like::

                Date || Open | High | Low | Close | Volume | Currency
                -----||------|------|-----|-------|--------|----------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxx | xxxxxxxx

            but in case that as_json parameter was defined as True, then the output will be::

                {
                    name: name,
                    recent: [
                        {
                            date: 'dd/mm/yyyy',
                            open: x,
                            high: x,
                            low: x,
                            close: x,
                            volume: x,
                            currency: x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised whenever any of the introduced arguments is not valid or errored.
        IOError: raised if cryptos object/file was not found or unable to retrieve.
        RuntimeError: raised if the introduced crypto name was not found or did not match any of the existing ones.
        ConnectionError: raised if connection to Investing.com could not be established.
        IndexError: raised if crypto recent data was unavailable or not found in Investing.com.

    Examples:
        >>> data = investpy.get_crypto_recent_data(crypto='bitcoin')
        >>> data.head()
                      Open     High     Low   Close   Volume Currency
        Date
        2019-10-25  7422.8   8697.7  7404.9  8658.3  1177632      USD
        2019-10-26  8658.4  10540.0  8061.8  9230.6  1784005      USD
        2019-10-27  9230.6   9773.2  9081.0  9529.6  1155038      USD
        2019-10-28  9530.1   9866.9  9202.5  9207.2  1039295      USD
        2019-10-29  9206.5   9531.3  9125.3  9411.3   918477      USD

    """"""
    if not crypto:
        raise ValueError('ERR#0083: crypto parameter is mandatory and must be a valid crypto name.')
    if not isinstance(crypto, str):
        raise ValueError('ERR#0084: crypto argument needs to be a str.')
    if not isinstance(as_json, bool):
        raise ValueError('ERR#0002: as_json argument can just be True or False, bool type.')
    if order not in ['ascending', 'asc', 'descending', 'desc']:
        raise ValueError('ERR#0003: order argument can just be ascending (asc) or descending (desc), str type.')
    if not interval:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    if not isinstance(interval, str):
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    interval = interval.lower()
    if interval not in ['daily', 'weekly', 'monthly']:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    resource_package = 'investpy'
    resource_path = '/'.join(('resources', 'cryptos.csv'))
    if pkg_resources.resource_exists(resource_package, resource_path):
        cryptos = pd.read_csv(pkg_resources.resource_filename(resource_package, resource_path), keep_default_na=False)
    else:
        raise FileNotFoundError('ERR#0081: cryptos file not found or errored.')
    if cryptos is None:
        raise IOError('ERR#0082: cryptos not found or unable to retrieve.')
    crypto = unidecode(crypto.strip().lower())
    if crypto not in list(cryptos['name'].apply(unidecode).str.lower()):
        raise RuntimeError('ERR#0085: crypto currency: ' + crypto + ', not found, check if it is correct.')
    status = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'status']
    if status == 'unavailable':
        raise ValueError('ERR#0086: the selected crypto currency is not available for retrieval in Investing.com.')
    crypto_name = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'name']
    crypto_id = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'id']
    crypto_currency = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'currency']
    header = crypto_name + ' Historical Data'
    params = {'curr_id': crypto_id, 'smlID': str(randint(1000000, 99999999)), 'header': header, 'interval_sec': interval.capitalize(), 'sort_col': 'date', 'sort_ord': 'DESC', 'action': 'historical_data'}
    head = {'User-Agent': random_user_agent(), 'X-Requested-With': 'XMLHttpRequest', 'Accept': 'text/html', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive'}
    url = 'https://www.investing.com/instruments/HistoricalDataAjax'
    req = requests.post(url, headers=head, data=params)
    if req.status_code != 200:
        raise ConnectionError('ERR#0015: error ' + str(req.status_code) + ', try again later.')
    root_ = fromstring(req.text)
    path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
    result = list()
    if path_:
        for elements_ in path_:
            if elements_.xpath('.//td')[0].text_content() == 'No results found':
                raise IndexError('ERR#0087: crypto information unavailable or not found.')
            info = []
            for nested_ in elements_.xpath('.//td'):
                info.append(nested_.get('data-real-value'))
            crypto_date = datetime.strptime(str(datetime.fromtimestamp(int(info[0]), tz=pytz.timezone('GMT')).date()), '%Y-%m-%d')
            crypto_close = float(info[1].replace(',', ''))
            crypto_open = float(info[2].replace(',', ''))
            crypto_high = float(info[3].replace(',', ''))
            crypto_low = float(info[4].replace(',', ''))
            crypto_volume = int(info[5])
            result.insert(len(result), Data(crypto_date, crypto_open, crypto_high, crypto_low, crypto_close, crypto_volume, crypto_currency, None))
        if order in ['ascending', 'asc']:
            result = result[::-1]
        elif order in ['descending', 'desc']:
            result = result
        if as_json is True:
            json_ = {'name': crypto_name, 'recent': [value.crypto_as_json() for value in result]}
            return json.dumps(json_, sort_keys=False)
        elif as_json is False:
            df = pd.DataFrame.from_records([value.crypto_to_dict() for value in result])
            df.set_index('Date', inplace=True)
            return df
    else:
        raise RuntimeError('ERR#0004: data retrieval error while scraping.')","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],0,,,,,,
trax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trax/trax/layers/research/efficient_attention.py,https://github.com/google/trax/tree/master/trax/layers/research/efficient_attention.py,LSHSelfAttention,forward_and_or_backward$2261,"def forward_and_or_backward(self, inputs, weights, state, rng, output_grad=None, compute_output=True, update_state=True):
    """"""Performs batched forward and/or backward passes.

    See `forward` for a reference implementation of what this layer does. The
    reference implementation is not very efficient, however, and this method
    provides a more performant version.

    Args:
      inputs: inputs to the attention layer
      weights: weights for the attention layer
      state: state of the attention layer
      rng: PRNG key for the layer (shared across all examples and heads)
      output_grad: gradient of the loss wrt the output of the layer, or None.
          This function performs the backward pass iff `output_grad` is not
          None.
      compute_output: bool: whether to return the output of the forward pass
          (for example, a pure backwards pass does not need to return the
          output).
      update_state: bool: whether to return an updated layer state.

    Returns:
      A tuple (output, new_state, inputs_grad, weights_grad).

      - output is not None iff compute_output is True
      - new_state is not None iff update_state is True
      - inputs_grad & weights_grad are not None iff output_grad is not None
    """"""
    have_single_input = not isinstance(inputs, (tuple, list))
    if have_single_input:
        inputs = (inputs,)
    batch_size = int(inputs[0].shape[0])
    seqlen = inputs[0].shape[-2]
    d_model = inputs[0].shape[-1]
    compute_grad = output_grad is not None
    assert compute_output or compute_grad, 'No work to perform!'
    if not self._incremental:
        forward_unbatched = functools.partial(self.forward_unbatched, rng=rng, update_state=update_state)
    else:
        if update_state:
            (inputs, state, q_start, new_mem, new_mem_end) = self._use_predict_mem(inputs, state)
        else:
            (new_mem_end, inputs, state) = state
            q_start = new_mem_end - seqlen
        forward_unbatched = functools.partial(self._incremental_forward_unbatched, q_start=fastmath.stop_gradient(q_start), q_len=fastmath.stop_gradient(seqlen), rng=rng, update_state=update_state)
    n_parallel_heads = batch_size * self._n_heads
    if self._n_parallel_heads and self._n_parallel_heads < n_parallel_heads:
        n_parallel_heads = self._n_parallel_heads

    def tree_update(tree, indices, new_values):
        return fastmath.nested_map_multiarg(lambda x, y: fastmath.index_update(x, jax.numpy.index_exp[indices], y), tree, new_values)

    def tree_add(tree, indices, new_values):
        return fastmath.nested_map_multiarg(lambda x, y: fastmath.index_add(x, jax.numpy.index_exp[indices], y), tree, new_values)
    if compute_grad:
        inputs_is_differentiable = fastmath.nested_map(lambda x: np.issubdtype(x.dtype, np.inexact), inputs)

        def split_differentiable(xs):
            differentiable_xs = fastmath.nested_map_multiarg(lambda x, is_differentiable: x if is_differentiable else None, xs, inputs_is_differentiable)
            non_differentiable_xs = fastmath.nested_map_multiarg(lambda x, is_differentiable: None if is_differentiable else x, xs, inputs_is_differentiable)
            return (differentiable_xs, non_differentiable_xs)

        def join_differentiable(differentiable_xs, non_differentiable_xs):
            """"""Reconstitute inputs pytree from differentiable/non-d. partitions.""""""
            differentiable_leaves = fastmath.tree_leaves(differentiable_xs)
            non_differentiable_leaves = fastmath.tree_leaves(non_differentiable_xs)
            leaves = []
            for is_differentiable in fastmath.tree_leaves(inputs_is_differentiable):
                if is_differentiable:
                    leaves.append(differentiable_leaves.pop(0))
                else:
                    leaves.append(non_differentiable_leaves.pop(0))
            assert not differentiable_leaves
            assert not non_differentiable_leaves
            (tree, _) = fastmath.tree_unflatten(leaves, inputs)
            return tree

        def vjp(fn, inp, *args, has_aux=False):
            (d_inp, nd_inp) = split_differentiable(inp)

            def fn_closed_over_nd_inp(d_inp, *args):
                inp = join_differentiable(d_inp, nd_inp)
                return fn(inp, *args)
            return fastmath.vjp(fn_closed_over_nd_inp, d_inp, *args, has_aux=has_aux)
    if n_parallel_heads == 1:

        def run_inner(idx, loop_val):
            """"""Runs one slice of attention (for a single head).""""""
            (o_all, s_all, i_ct_all, w_ct_all) = loop_val
            example_idx = idx // self._n_heads
            head_idx = idx % self._n_heads
            i_h = fastmath.nested_map(lambda x: x[example_idx], inputs)
            w_h = fastmath.nested_map(lambda w: w[head_idx], weights)
            s_h = fastmath.nested_map(lambda s: s[idx], state)

            def forward_fn(i_h, w_h):
                return forward_unbatched(*i_h, weights=w_h, state=fastmath.stop_gradient(s_h))
            if compute_grad:
                (o_h, backward_fn, s_h) = vjp(forward_fn, i_h, w_h, has_aux=True)
                ct_h = output_grad[example_idx]
                assert o_h.shape == ct_h.shape
                (i_ct_h, w_ct_h) = backward_fn(ct_h)
            else:
                (o_h, s_h) = forward_fn(i_h, w_h)
            if compute_output:
                o_all = fastmath.index_add(o_all, example_idx, o_h)
            if update_state:
                s_all = tree_update(s_all, idx, s_h)
            if compute_grad:
                i_ct_all = tree_add(i_ct_all, example_idx, i_ct_h)
                w_ct_all = tree_add(w_ct_all, head_idx, w_ct_h)
            return (o_all, s_all, i_ct_all, w_ct_all)
    elif n_parallel_heads < self._n_heads:
        assert self._n_heads % n_parallel_heads == 0

        def run_inner(idx, loop_val):
            """"""Runs one slice of attention (multiple heads, but one example).""""""
            (o_all, s_all, i_ct_all, w_ct_all) = loop_val
            idx = idx * self._n_parallel_heads
            example_idx = idx // self._n_heads
            head_idx_lo = idx % self._n_heads
            head_range = head_idx_lo + np.arange(n_parallel_heads, dtype=np.int32)
            state_range = idx + np.arange(n_parallel_heads, dtype=np.int32)
            i_mh = fastmath.nested_map(lambda x: x[example_idx], inputs)
            w_mh = fastmath.nested_map(lambda w: w[head_range], weights)
            s_mh = fastmath.nested_map(lambda s: s[state_range], state)

            def forward_unbatched_h(i_h, w_h, s_h):
                return forward_unbatched(*i_h, weights=w_h, state=s_h)

            def forward_fn(i_mh, w_mh):
                (o_mh, new_s_mh) = fastmath.vmap(forward_unbatched_h, in_axes=(None, 0, 0), out_axes=0)(i_mh, w_mh, s_mh)
                o_mh = np.sum(o_mh, axis=0)
                return (o_mh, new_s_mh)
            if compute_grad:
                (o_mh, backward_fn, s_mh) = vjp(forward_fn, i_mh, w_mh, has_aux=True)
                ct_mh = output_grad[example_idx]
                assert o_mh.shape == ct_mh.shape
                (i_ct_mh, w_ct_mh) = backward_fn(ct_mh)
            else:
                (o_mh, s_mh) = forward_fn(i_mh, w_mh)
            if compute_output:
                o_all = fastmath.index_add(o_all, example_idx, o_mh)
            if update_state:
                s_all = tree_update(s_all, state_range, s_mh)
            if compute_grad:
                i_ct_all = tree_add(i_ct_all, example_idx, i_ct_mh)
                w_ct_all = tree_add(w_ct_all, head_range, w_ct_mh)
            return (o_all, s_all, i_ct_all, w_ct_all)
    else:
        assert n_parallel_heads % self._n_heads == 0

        def forward_single_example(i_x, w_all, s_x):

            def forward_unbatched_h(i_h, w_h, s_h):
                return forward_unbatched(*i_h, weights=w_h, state=s_h)
            (o_x, s_x) = fastmath.vmap(forward_unbatched_h, in_axes=(None, 0, 0), out_axes=(0, 0))(i_x, w_all, s_x)
            o_x = np.sum(o_x, axis=0)
            return (o_x, s_x)

        def run_inner(idx, loop_val):
            """"""Runs one slice of attention (all heads for one or more examples).""""""
            (o_all, s_all, i_ct_all, w_ct_all) = loop_val
            idx = idx * n_parallel_heads
            example_idx_lo = idx // self._n_heads
            example_range = example_idx_lo + np.arange(n_parallel_heads // self._n_heads, dtype=np.int32)
            state_range = idx + np.arange(n_parallel_heads, dtype=np.int32)
            i_mex = fastmath.nested_map(lambda x: x[example_range], inputs)
            s_mex = fastmath.nested_map(lambda s: np.reshape(s[state_range], (-1, self._n_heads) + s.shape[1:]), state)

            def forward_fn(i_mex, w_all):
                (o_mex, new_s_mex) = fastmath.vmap(forward_single_example, in_axes=(0, None, 0), out_axes=(0, 0))(i_mex, w_all, s_mex)
                new_s_mex = fastmath.nested_map(lambda s: np.reshape(s, (n_parallel_heads,) + s.shape[2:]), new_s_mex)
                return (o_mex.astype(i_mex[0].dtype), new_s_mex)
            if compute_grad:
                (o_mex, backward_fn, s_mex) = vjp(forward_fn, i_mex, weights, has_aux=True)
                ct_mex = output_grad[example_range]
                assert o_mex.shape == ct_mex.shape, str(ct_mex.shape)
                assert o_mex.dtype == ct_mex.dtype, str(ct_mex.dtype)
                (i_ct_mex, w_ct_mex) = backward_fn(ct_mex)
            else:
                (o_mex, s_mex) = forward_fn(i_mex, weights)
            if compute_output:
                o_all = fastmath.index_add(o_all, jax.numpy.index_exp[example_range], o_mex)
            if update_state:
                s_all = tree_update(s_all, state_range, s_mex)
            if compute_grad:
                i_ct_all = tree_update(i_ct_all, example_range, i_ct_mex)
                w_ct_all = fastmath.nested_map_multiarg(lambda old_all, delta_all: old_all + delta_all, w_ct_all, w_ct_mex)
            return (o_all, s_all, i_ct_all, w_ct_all)
    o_all = s_all = i_ct_all = w_ct_all = None
    if compute_output:
        o_all = np.zeros((batch_size, seqlen, d_model), dtype=inputs[0].dtype)
    if update_state:
        s_all = state
    if compute_grad:
        i_ct_all = fastmath.nested_map(np.zeros_like, inputs)
        (i_ct_all, i_nondifferentiable_dummy_ct) = split_differentiable(i_ct_all)
        w_ct_all = fastmath.nested_map(np.zeros_like, weights)
    loop_val = (o_all, s_all, i_ct_all, w_ct_all)
    assert batch_size * self._n_heads % n_parallel_heads == 0
    loop_hi = batch_size * self._n_heads // n_parallel_heads
    if self._use_python_loop or loop_hi == 1:
        for idx in range(loop_hi):
            loop_val = run_inner(idx, loop_val)
    else:
        loop_val = fastmath.fori_loop(0, loop_hi, run_inner, loop_val)
    (o_all, s_all, i_ct_all, w_ct_all) = loop_val
    if compute_grad:
        i_ct_all = join_differentiable(i_ct_all, i_nondifferentiable_dummy_ct)
    if self._incremental and update_state:
        s_all = (new_mem_end, new_mem, s_all)
    if have_single_input and compute_grad:
        assert isinstance(i_ct_all, tuple) and len(i_ct_all) == 1
        return (o_all, s_all, i_ct_all[0], w_ct_all)
    else:
        return (o_all, s_all, i_ct_all, w_ct_all)","for is_differentiable in fastmath.tree_leaves(inputs_is_differentiable):
    if is_differentiable:
        leaves.append(differentiable_leaves.pop(0))
    else:
        leaves.append(non_differentiable_leaves.pop(0))",leaves = [differentiable_leaves.pop(0) if is_differentiable else non_differentiable_leaves.pop(0) for is_differentiable in fastmath.tree_leaves(inputs_is_differentiable)],0,,,,,,
ludwig,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ludwig/ludwig/combiners/combiners.py,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/combiners/combiners.py,ComparatorCombiner,__init__$967,"def __init__(self, input_features: Dict[str, 'InputFeature'], config: ComparatorCombinerConfig=None, **kwargs):
    super().__init__(input_features)
    self.name = 'ComparatorCombiner'
    logger.debug('Entering {}'.format(self.name))
    self.entity_1 = config.entity_1
    self.entity_2 = config.entity_2
    self.required_inputs = set(config.entity_1 + config.entity_2)
    self.fc_size = config.fc_size
    self.fc_stack = None
    fc_layers = config.fc_layers
    if fc_layers is None and config.num_fc_layers is not None:
        fc_layers = []
        for i in range(config.num_fc_layers):
            fc_layers.append({'fc_size': config.fc_size})
    if fc_layers is not None:
        logger.debug('Setting up FCStack')
        self.e1_fc_stack = FCStack(self.get_entity_shape(config.entity_1)[-1], layers=fc_layers, num_layers=config.num_fc_layers, default_fc_size=config.fc_size, default_use_bias=config.use_bias, default_weights_initializer=config.weights_initializer, default_bias_initializer=config.bias_initializer, default_norm=config.norm, default_norm_params=config.norm_params, default_activation=config.activation, default_dropout=config.dropout)
        self.e2_fc_stack = FCStack(self.get_entity_shape(config.entity_2)[-1], layers=fc_layers, num_layers=config.num_fc_layers, default_fc_size=config.fc_size, default_use_bias=config.use_bias, default_weights_initializer=config.weights_initializer, default_bias_initializer=config.bias_initializer, default_norm=config.norm, default_norm_params=config.norm_params, default_activation=config.activation, default_dropout=config.dropout)
    self.last_fc_layer_fc_size = fc_layers[-1]['fc_size']
    self.bilinear_weights = torch.randn([self.last_fc_layer_fc_size, self.last_fc_layer_fc_size], dtype=torch.float32)","for i in range(config.num_fc_layers):
    fc_layers.append({'fc_size': config.fc_size})",fc_layers = [{'fc_size': config.fc_size} for i in range(config.num_fc_layers)],0,,,,,,
DSB2017,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DSB2017/training/classifier/utils.py,https://github.com/lfz/DSB2017/tree/master/training/classifier/utils.py,,combine16$172,"def combine16(output, z, h, w):
    splits = []
    for i in range(len(output)):
        splits.append(output[i])
    output = np.zeros((z, h, w, splits[0].shape[3], splits[0].shape[4]), np.float32)
    z_width = z / 4
    h_width = h / 2
    w_width = w / 2
    splitzstart = splits[0].shape[0] / 2 - z_width / 2
    z_pos = [z * 3 / 8 - z_width / 2, z * 5 / 8 - z_width / 2]
    i = 0
    for (zz, zz2) in zip([[0, z_width], [z_width, z_width * 2], [z_width * 2, z_width * 3], [z_width * 3 - z, None]], [[0, z_width], [splitzstart, z_width + splitzstart], [splitzstart, z_width + splitzstart], [z_width * 3 - z, None]]):
        for hh in [[0, h_width], [h_width - h, None]]:
            for ww in [[0, w_width], [w_width - w, None]]:
                output[zz[0]:zz[1], hh[0]:hh[1], ww[0]:ww[1], :, :] = splits[i][zz2[0]:zz2[1], hh[0]:hh[1], ww[0]:ww[1], :, :]
                i = i + 1
    return output","for i in range(len(output)):
    splits.append(output[i])",splits = [output[i] for i in range(len(output))],0,,,,,,
fairseq,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/tests/test_dictionary.py,https://github.com/pytorch/fairseq/tree/master/tests/test_dictionary.py,TestDictionary,get_ids$53,"def get_ids(dictionary):
    ids = []
    for line in txt:
        ids.append(dictionary.encode_line(line, add_if_not_exist=False))
    return ids","for line in txt:
    ids.append(dictionary.encode_line(line, add_if_not_exist=False))","ids = [dictionary.encode_line(line, add_if_not_exist=False) for line in txt]",0,,,,,,
beets,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/beets/beets/dbcore/db.py,https://github.com/beetbox/beets/tree/master/beets/dbcore/db.py,Database,_make_table$1028,"def _make_table(self, table, fields):
    """"""Set up the schema of the database. `fields` is a mapping
        from field names to `Type`s. Columns are added if necessary.
        """"""
    with self.transaction() as tx:
        rows = tx.query('PRAGMA table_info(%s)' % table)
    current_fields = {row[1] for row in rows}
    field_names = set(fields.keys())
    if current_fields.issuperset(field_names):
        return
    if not current_fields:
        columns = []
        for (name, typ) in fields.items():
            columns.append(f'{name} {typ.sql}')
        setup_sql = 'CREATE TABLE {} ({});\n'.format(table, ', '.join(columns))
    else:
        setup_sql = ''
        for (name, typ) in fields.items():
            if name in current_fields:
                continue
            setup_sql += 'ALTER TABLE {} ADD COLUMN {} {};\n'.format(table, name, typ.sql)
    with self.transaction() as tx:
        tx.script(setup_sql)","for (name, typ) in fields.items():
    columns.append(f'{name} {typ.sql}')","columns = [f'{name} {typ.sql}' for (name, typ) in fields.items()]",0,,,,,,
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/etfs.py,https://github.com/alvarobartt/investpy/tree/master/investpy/etfs.py,,get_etf_historical_data$514,"def get_etf_historical_data(etf, country, from_date, to_date, stock_exchange=None, as_json=False, order='ascending', interval='Daily'):
    """"""
    This function retrieves historical data from the introduced `etf` from Investing.com via Web Scraping on the
    introduced date range. The resulting data can it either be stored in a :obj:`pandas.DataFrame` or in a
    :obj:`json` object with `ascending` or `descending` order.

    Args:
        etf (:obj:`str`): name of the etf to retrieve recent historical data from.
        country (:obj:`str`): name of the country from where the etf is.
        from_date (:obj:`str`): date as `str` formatted as `dd/mm/yyyy`, from where data is going to be retrieved.
        to_date (:obj:`str`): date as `str` formatted as `dd/mm/yyyy`, until where data is going to be retrieved.
        as_json (:obj:`bool`, optional):
            to determine the format of the output data (:obj:`pandas.DataFrame` or :obj:`json`).
        order (:obj:`str`, optional):
            optional argument to define the order of the retrieved data (`ascending`, `asc` or `descending`, `desc`).
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function returns either a :obj:`pandas.DataFrame` or a :obj:`json` file containing the retrieved
            recent data from the specified etf via argument. The dataset contains the open, high, low and close
            values for the selected etf on market days.

            The returned data is case we use default arguments will look like::

                Date || Open | High | Low | Close | Volume | Currency | Exchange
                -----||------|------|-----|-------|--------|----------|---------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxx | xxxxxxxx | xxxxxxxx

            but if we define `as_json=True`, then the output will be::

                {
                    name: name,
                    historical: [
                        {
                            date: dd/mm/yyyy,
                            open: x,
                            high: x,
                            low: x,
                            close: x,
                            volume: x,
                            currency: x,
                            exchange: x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised whenever any of the arguments is not valid or errored.
        IOError: raised if etfs object/file not found or unable to retrieve.
        RuntimeError:raised if the introduced etf does not match any of the indexed ones.
        ConnectionError: raised if GET requests does not return 200 status code.
        IndexError: raised if etf information was unavailable or not found.

    Examples:
        >>> data = investpy.get_etf_historical_data(etf='bbva accion dj eurostoxx 50', country='spain', from_date='01/01/2010', to_date='01/01/2019')
        >>> data.head()
                     Open   High    Low  Close  Volume Currency Exchange
        Date
        2011-12-07  23.70  23.70  23.70  23.62    2000      EUR   Madrid
        2011-12-08  23.53  23.60  23.15  23.04     599      EUR   Madrid
        2011-12-09  23.36  23.60  23.36  23.62    2379      EUR   Madrid
        2011-12-12  23.15  23.26  23.00  22.88   10695      EUR   Madrid
        2011-12-13  22.88  22.88  22.88  22.80      15      EUR   Madrid

    """"""
    if not etf:
        raise ValueError('ERR#0031: etf parameter is mandatory and must be a valid etf name.')
    if not isinstance(etf, str):
        raise ValueError('ERR#0030: etf argument needs to be a str.')
    if country is None:
        raise ValueError('ERR#0039: country can not be None, it should be a str.')
    if country is not None and (not isinstance(country, str)):
        raise ValueError('ERR#0025: specified country value not valid.')
    if stock_exchange is not None and (not isinstance(stock_exchange, str)):
        raise ValueError('ERR#0125: specified stock_exchange value is not valid, it should be a str.')
    if not isinstance(as_json, bool):
        raise ValueError('ERR#0002: as_json argument can just be True or False, bool type.')
    if order not in ['ascending', 'asc', 'descending', 'desc']:
        raise ValueError('ERR#0003: order argument can just be ascending (asc) or descending (desc), str type.')
    if not interval:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    if not isinstance(interval, str):
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    interval = interval.lower()
    if interval not in ['daily', 'weekly', 'monthly']:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    try:
        datetime.strptime(from_date, '%d/%m/%Y')
    except ValueError:
        raise ValueError(""ERR#0011: incorrect data format, it should be 'dd/mm/yyyy'."")
    try:
        datetime.strptime(to_date, '%d/%m/%Y')
    except ValueError:
        raise ValueError(""ERR#0011: incorrect data format, it should be 'dd/mm/yyyy'."")
    start_date = datetime.strptime(from_date, '%d/%m/%Y')
    end_date = datetime.strptime(to_date, '%d/%m/%Y')
    if start_date >= end_date:
        raise ValueError(""ERR#0032: to_date should be greater than from_date, both formatted as 'dd/mm/yyyy'."")
    date_interval = {'intervals': []}
    flag = True
    while flag is True:
        diff = end_date.year - start_date.year
        if diff > 19:
            obj = {'start': start_date.strftime('%m/%d/%Y'), 'end': start_date.replace(year=start_date.year + 19).strftime('%m/%d/%Y')}
            date_interval['intervals'].append(obj)
            start_date = start_date.replace(year=start_date.year + 19) + timedelta(days=1)
        else:
            obj = {'start': start_date.strftime('%m/%d/%Y'), 'end': end_date.strftime('%m/%d/%Y')}
            date_interval['intervals'].append(obj)
            flag = False
    interval_limit = len(date_interval['intervals'])
    interval_counter = 0
    data_flag = False
    resource_package = 'investpy'
    resource_path = '/'.join(('resources', 'etfs.csv'))
    if pkg_resources.resource_exists(resource_package, resource_path):
        etfs = pd.read_csv(pkg_resources.resource_filename(resource_package, resource_path), keep_default_na=False)
    else:
        raise FileNotFoundError('ERR#0058: etfs file not found or errored.')
    if etfs is None:
        raise IOError('ERR#0009: etfs object not found or unable to retrieve.')
    country = unidecode(country.strip().lower())
    if country not in get_etf_countries():
        raise RuntimeError('ERR#0034: country ' + country + ' not found, check if it is correct.')
    etf = unidecode(etf.strip().lower())
    def_exchange = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['def_stock_exchange'] == True)).idxmax()]
    etfs = etfs[etfs['country'].str.lower() == country]
    if etf not in list(etfs['name'].apply(unidecode).str.lower()):
        raise RuntimeError('ERR#0019: etf ' + etf + ' not found, check if it is correct.')
    etfs = etfs[etfs['name'].apply(unidecode).str.lower() == etf]
    if def_exchange['country'] != country:
        warnings.warn('Selected country does not contain the default stock exchange of the introduced ETF. ' + 'Default country is: ""' + def_exchange['country'] + '"" and default stock_exchange: ""' + def_exchange['stock_exchange'] + '"".', Warning)
        if stock_exchange:
            if stock_exchange.lower() not in etfs['stock_exchange'].str.lower():
                raise ValueError('ERR#0126: introduced stock_exchange value does not exists, leave this parameter to None to use default stock_exchange.')
            etf_exchange = etfs.loc[(etfs['stock_exchange'].str.lower() == stock_exchange.lower()).idxmax(), 'stock_exchange']
        else:
            found_etfs = etfs[etfs['name'].apply(unidecode).str.lower() == etf]
            if len(found_etfs) > 1:
                warnings.warn('Note that the displayed information can differ depending on the stock exchange. Available stock_exchange' + ' values for ""' + country + '"" are: ""' + '"", ""'.join(found_etfs['stock_exchange']) + '"".', Warning)
            del found_etfs
            etf_exchange = etfs.loc[(etfs['name'].apply(unidecode).str.lower() == etf).idxmax(), 'stock_exchange']
    elif stock_exchange:
        if stock_exchange.lower() not in etfs['stock_exchange'].str.lower():
            raise ValueError('ERR#0126: introduced stock_exchange value does not exists, leave this parameter to None to use default stock_exchange.')
        if def_exchange['stock_exchange'].lower() != stock_exchange.lower():
            warnings.warn('Selected stock_exchange is not the default one of the introduced ETF. ' + 'Default country is: ""' + def_exchange['country'] + '"" and default stock_exchange: ""' + def_exchange['stock_exchange'].lower() + '"".', Warning)
        etf_exchange = etfs.loc[(etfs['stock_exchange'].str.lower() == stock_exchange.lower()).idxmax(), 'stock_exchange']
    else:
        etf_exchange = def_exchange['stock_exchange']
    symbol = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'symbol']
    id_ = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'id']
    name = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'name']
    etf_currency = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'currency']
    final = list()
    header = symbol + ' Historical Data'
    for index in range(len(date_interval['intervals'])):
        interval_counter += 1
        params = {'curr_id': id_, 'smlID': str(randint(1000000, 99999999)), 'header': header, 'st_date': date_interval['intervals'][index]['start'], 'end_date': date_interval['intervals'][index]['end'], 'interval_sec': interval.capitalize(), 'sort_col': 'date', 'sort_ord': 'DESC', 'action': 'historical_data'}
        head = {'User-Agent': random_user_agent(), 'X-Requested-With': 'XMLHttpRequest', 'Accept': 'text/html', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive'}
        url = 'https://www.investing.com/instruments/HistoricalDataAjax'
        req = requests.post(url, headers=head, data=params)
        if req.status_code != 200:
            raise ConnectionError('ERR#0015: error ' + str(req.status_code) + ', try again later.')
        if not req.text:
            continue
        root_ = fromstring(req.text)
        path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
        result = list()
        if path_:
            for elements_ in path_:
                if elements_.xpath('.//td')[0].text_content() == 'No results found':
                    if interval_counter < interval_limit:
                        data_flag = False
                    else:
                        raise IndexError('ERR#0010: etf information unavailable or not found.')
                else:
                    data_flag = True
                info = []
                for nested_ in elements_.xpath('.//td'):
                    info.append(nested_.get('data-real-value'))
                if data_flag is True:
                    etf_date = datetime.strptime(str(datetime.fromtimestamp(int(info[0]), tz=pytz.timezone('GMT')).date()), '%Y-%m-%d')
                    etf_close = float(info[1].replace(',', ''))
                    etf_open = float(info[2].replace(',', ''))
                    etf_high = float(info[3].replace(',', ''))
                    etf_low = float(info[4].replace(',', ''))
                    etf_volume = int(info[5])
                    result.insert(len(result), Data(etf_date, etf_open, etf_high, etf_low, etf_close, etf_volume, etf_currency, etf_exchange))
            if data_flag is True:
                if order in ['ascending', 'asc']:
                    result = result[::-1]
                elif order in ['descending', 'desc']:
                    result = result
                if as_json is True:
                    json_list = [value.etf_as_json() for value in result]
                    final.append(json_list)
                elif as_json is False:
                    df = pd.DataFrame.from_records([value.etf_to_dict() for value in result])
                    df.set_index('Date', inplace=True)
                    final.append(df)
        else:
            raise RuntimeError('ERR#0004: data retrieval error while scraping.')
    if order in ['descending', 'desc']:
        final.reverse()
    if as_json is True:
        json_ = {'name': name, 'historical': [value for json_list in final for value in json_list]}
        return json.dumps(json_, sort_keys=False)
    elif as_json is False:
        return pd.concat(final)","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],0,,,,,,
dronekit-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dronekit-python/examples/mission_import_export/mission_import_export.py,https://github.com/dronekit/dronekit-python/tree/master/examples/mission_import_export/mission_import_export.py,,download_mission$104,"def download_mission():
    """"""
    Downloads the current mission and returns it in a list.
    It is used in save_mission() to get the file information to save.
    """"""
    print(' Download mission from vehicle')
    missionlist = []
    cmds = vehicle.commands
    cmds.download()
    cmds.wait_ready()
    for cmd in cmds:
        missionlist.append(cmd)
    return missionlist","for cmd in cmds:
    missionlist.append(cmd)",missionlist = [cmd for cmd in cmds],0,,,,,,
wukong-robot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wukong-robot/robot/sdk/unit.py,https://github.com/wzpan/wukong-robot/tree/master/robot/sdk/unit.py,,getSlotWords$158,"def getSlotWords(parsed, intent, name):
    """"""
    閹垫儳鍤閸涙垝鑵戦弻鎰閲滅拠宥喰閻ㄥ嫬鍞寸

    :param parsed: UNIT 鐟欙絾鐎界紒鎾寸亯
    :param intent: 閹板繐娴橀惃鍕鎮曠粔
    :param name: 鐠囧秵蝎閸
    :returns: 閸涙垝鑵戠拠銉ㄧ槤濡茬晫娈戦崐鑲╂畱閸掓勩冮妴
    """"""
    slots = getSlots(parsed, intent)
    words = []
    for slot in slots:
        if slot['name'] == name:
            words.append(slot['normalized_word'])
    return words","for slot in slots:
    if slot['name'] == name:
        words.append(slot['normalized_word'])",words = [slot['normalized_word'] for slot in slots if slot['name'] == name],0,,,,,,
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/dpr/tokenization_dpr.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/dpr/tokenization_dpr.py,CustomDPRReaderTokenizerMixin,__call__$202,"def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:
    if titles is None and texts is None:
        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)
    elif titles is None or texts is None:
        text_pair = titles if texts is None else texts
        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)
    titles = titles if not isinstance(titles, str) else [titles]
    texts = texts if not isinstance(texts, str) else [texts]
    n_passages = len(titles)
    questions = questions if not isinstance(questions, str) else [questions] * n_passages
    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'
    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']
    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']
    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}
    if return_attention_mask is not False:
        attention_mask = []
        for input_ids in encoded_inputs['input_ids']:
            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])
        encoded_inputs['attention_mask'] = attention_mask
    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)","for input_ids in encoded_inputs['input_ids']:
    attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])",attention_mask = [[int(input_id != self.pad_token_id) for input_id in input_ids] for input_ids in encoded_inputs['input_ids']],0,,,,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/support/network-integration/collections/ansible_collections/vyos/vyos/plugins/module_utils/network/vyos/utils/utils.py,https://github.com/ansible/ansible/tree/master/test/support/network-integration/collections/ansible_collections/vyos/vyos/plugins/module_utils/network/vyos/utils/utils.py,,diff_list_of_dicts$61,"def diff_list_of_dicts(want, have):
    diff = []
    set_w = set((tuple(d.items()) for d in want))
    set_h = set((tuple(d.items()) for d in have))
    difference = set_w.difference(set_h)
    for element in difference:
        diff.append(dict(((x, y) for (x, y) in element)))
    return diff","for element in difference:
    diff.append(dict(((x, y) for (x, y) in element)))","diff = [dict(((x, y) for (x, y) in element)) for element in difference]",0,,,,,,
,19涓猲o found,37 refactor error,,536,499,462,0.925851703,,,,,,,
