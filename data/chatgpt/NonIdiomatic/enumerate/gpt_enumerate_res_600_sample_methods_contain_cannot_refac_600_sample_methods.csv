repo_name,file_path,file_html,class_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
detection-rules,https://github.com/elastic/detection-rules/tree/master/detection_rules/devtools.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detection-rules/detection_rules/devtools.py,,"def kibana_commit(ctx, local_repo: str, github_repo: str, ssh: bool, kibana_directory: str, base_branch: str, branch_name: Optional[str], message: Optional[str], push: bool) -> (str, str):
    """"""Prep a commit and push to Kibana.""""""
    package_name = Package.load_configs()['name']
    release_dir = os.path.join(RELEASE_DIR, package_name)
    message = message or f'[Detection Rules] Add {package_name} rules'
    if not os.path.exists(release_dir):
        click.secho(""Release directory doesn't exist."", fg='red', err=True)
        click.echo(f""Run {click.style('python -m detection_rules dev build-release', bold=True)} to populate"", err=True)
        ctx.exit(1)
    git = utils.make_git('-C', local_repo)
    rules_git = utils.make_git('-C', utils.get_path())
    long_commit_hash = rules_git('rev-parse', 'HEAD')
    short_commit_hash = rules_git('rev-parse', '--short', 'HEAD')
    try:
        if not os.path.exists(local_repo):
            click.echo(f""Kibana repository doesn't exist at {local_repo}. Cloning..."")
            url = f'git@github.com:{github_repo}.git' if ssh else f'https://github.com/{github_repo}.git'
            utils.make_git()('clone', url, local_repo, '--depth', '1')
        else:
            git('checkout', base_branch)
        branch_name = branch_name or f'detection-rules/{package_name}-{short_commit_hash}'
        git('checkout', '-b', branch_name, print_output=True)
        git('rm', '-r', kibana_directory)
        source_dir = os.path.join(release_dir, 'rules')
        target_dir = os.path.join(local_repo, kibana_directory)
        os.makedirs(target_dir)
        for name in os.listdir(source_dir):
            (_, ext) = os.path.splitext(name)
            path = os.path.join(source_dir, name)
            if ext in ('.ts', '.json'):
                shutil.copyfile(path, os.path.join(target_dir, name))
        git('add', kibana_directory)
        git('commit', '--no-verify', '-m', message)
        git('status', print_output=True)
        if push:
            git('push', 'origin', branch_name)
        click.echo(f'Kibana repository {local_repo} prepped. Push changes when ready')
        click.secho(f'cd {local_repo}', bold=True)
        return (branch_name, long_commit_hash)
    except subprocess.CalledProcessError as e:
        client_error(str(e), e, ctx=ctx)","for name in os.listdir(source_dir):
    (_, ext) = os.path.splitext(name)
    path = os.path.join(source_dir, name)
    if ext in ('.ts', '.json'):
        shutil.copyfile(path, os.path.join(target_dir, name))","for i,name in enumerate(os.listdir(source_dir)):
    (_, ext) = os.path.splitext(name)
    path = os.path.join(source_dir, name)
    if ext in ('.ts', '.json'):
        shutil.copyfile(path, os.path.join(target_dir, name))",1,,,,,,,,,,
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model","for m_name in name:
    if re.fullmatch('[A-Za-z]+\\d+', m_name):
        l = re.split('(\\d+)', m_name)
    else:
        l = [m_name]
    if l[0] == 'g':
        pointer = getattr(pointer, 'weight')
    elif l[0] == 'b':
        pointer = getattr(pointer, 'bias')
    elif l[0] == 'w':
        pointer = getattr(pointer, 'weight')
    else:
        pointer = getattr(pointer, l[0])
    if len(l) >= 2:
        num = int(l[1])
        pointer = pointer[num]","for i,m_name in enumerate(name):
    if re.fullmatch('[A-Za-z]+\\d+', m_name):
        l = re.split('(\\d+)', m_name)
    else:
        l = [m_name]
    if l[0] == 'g':
        pointer = getattr(pointer, 'weight')
    elif l[0] == 'b':
        pointer = getattr(pointer, 'bias')
    elif l[0] == 'w':
        pointer = getattr(pointer, 'weight')
    else:
        pointer = getattr(pointer, l[0])
    if len(l) >= 2:
        num = int(l[1])
        pointer = pointer[num]",1,,,,,,,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)","for inv_y in (False, True):
    for (vc, result) in results.items():
        net = pp.create_empty_network(sn_mva=16)
        add_network(net, vc)
        try:
            sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
        except Exception as e:
            raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
        check_results(net, vc, result)","for inv_y in (False, True):
    for (vc, result) in results.items():
        net = pp.create_empty_network(sn_mva=16)
        add_network(net, vc)
        try:
            sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
        except Exception as e:
            raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
        check_results(net, vc, result)",1,,,,,,,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)","for (vc, result) in results.items():
    net = pp.create_empty_network(sn_mva=16)
    add_network(net, vc)
    try:
        sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
    except Exception as e:
        raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
    check_results(net, vc, result)","for i, (vc, result) in enumerate(results.items()):
    net = pp.create_empty_network(sn_mva=16)
    add_network(net, vc)
    try:
        sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
    except Exception as e:
        raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
    check_results(net, vc, result)",1,,,,,,,,,,
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","for line in f:
    infos = line.split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])","for line in enumerate(f):
    infos = line[1].split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])",1,,,,,,,,,,
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","for i in range(1, length):
    width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
    height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
    dataSet.append([width, height])","for i in range(1, length):
    info = infos[i].split(',')
    width = int(info[2]) - int(info[0])
    height = int(info[3]) - int(info[1])
    dataSet.append([width, height])",1,,,,,,,,,,
aliyun-openapi-python-sdk,https://github.com/aliyun/aliyun-openapi-python-sdk/tree/master/aliyun-python-sdk-core/aliyunsdkcore/vendored/requests/packages/urllib3/_collections.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aliyun-openapi-python-sdk/aliyun-python-sdk-core/aliyunsdkcore/vendored/requests/packages/urllib3/_collections.py,HTTPHeaderDict,"def __iter__(self):
    for vals in self._container.values():
        yield vals[0]","for vals in self._container.values():
    yield vals[0]","for i, vals in enumerate(self._container.values()):
    yield vals[0]",1,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/ToolsPage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/ToolsPage.py,Tools,"def __init__(self, gcode):
    self.gcode = gcode
    self.inches = False
    self.digits = 4
    self.active = StringVar()
    self.tools = {}
    self.buttons = {}
    self.widget = {}
    self.listbox = None
    for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
        tool = cls(self)
        self.addTool(tool)
    for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
        (name, ext) = os.path.splitext(os.path.basename(f))
        try:
            exec(f'import {name}')
            tool = eval(f'{name}.Tool(self)')
            self.addTool(tool)
        except (ImportError, AttributeError):
            (typ, val, tb) = sys.exc_info()
            traceback.print_exception(typ, val, tb)","for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
    tool = cls(self)
    self.addTool(tool)","for i, cls in enumerate([Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]):
    tool = cls(self)
    self.addTool(tool)",1,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/ToolsPage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/ToolsPage.py,Tools,"def __init__(self, gcode):
    self.gcode = gcode
    self.inches = False
    self.digits = 4
    self.active = StringVar()
    self.tools = {}
    self.buttons = {}
    self.widget = {}
    self.listbox = None
    for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
        tool = cls(self)
        self.addTool(tool)
    for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
        (name, ext) = os.path.splitext(os.path.basename(f))
        try:
            exec(f'import {name}')
            tool = eval(f'{name}.Tool(self)')
            self.addTool(tool)
        except (ImportError, AttributeError):
            (typ, val, tb) = sys.exc_info()
            traceback.print_exception(typ, val, tb)","for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
    (name, ext) = os.path.splitext(os.path.basename(f))
    try:
        exec(f'import {name}')
        tool = eval(f'{name}.Tool(self)')
        self.addTool(tool)
    except (ImportError, AttributeError):
        (typ, val, tb) = sys.exc_info()
        traceback.print_exception(typ, val, tb)","for i,f in enumerate(glob.glob(f'{Utils.prgpath}/plugins/*.py')):
    (name, ext) = os.path.splitext(os.path.basename(f))
    try:
        exec(f'import {name}')
        tool = eval(f'{name}.Tool(self)')
        self.addTool(tool)
    except (ImportError, AttributeError):
        (typ, val, tb) = sys.exc_info()
        traceback.print_exception(typ, val, tb)",1,,,,,,,,,,
HUNT,https://github.com/bugcrowd/HUNT/tree/master/Burp/lib/issues.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HUNT/Burp/lib/issues.py,Issues,"def create_scanner_issues(self, view, callbacks, helpers, vuln_parameters, request_response):
    issues = self.issues
    json = self.json
    for vuln_parameter in vuln_parameters:
        issue_name = vuln_parameter['vuln_name']
        vuln_param = vuln_parameter['vuln_param']
        param_name = vuln_parameter['param']
        param_value = vuln_parameter['value']
        url = helpers.analyzeRequest(request_response).getUrl()
        url = urlparse.urlsplit(str(url))
        hostname = url.hostname
        path = url.path
        url = url.scheme + '://' + url.hostname + url.path
        http_service = request_response.getHttpService()
        http_messages = [callbacks.applyMarkers(request_response, None, None)]
        detail = json['issues'][issue_name]['detail']
        severity = 'Medium'
        scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
        is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
        if is_scanner_issue_dupe:
            continue
        else:
            self.set_scanner_issues(scanner_issue)
        issue_count = self.set_issue_count(issue_name, vuln_param)
        total_count = self.total_count[issue_name]
        view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
        view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)","for vuln_parameter in vuln_parameters:
    issue_name = vuln_parameter['vuln_name']
    vuln_param = vuln_parameter['vuln_param']
    param_name = vuln_parameter['param']
    param_value = vuln_parameter['value']
    url = helpers.analyzeRequest(request_response).getUrl()
    url = urlparse.urlsplit(str(url))
    hostname = url.hostname
    path = url.path
    url = url.scheme + '://' + url.hostname + url.path
    http_service = request_response.getHttpService()
    http_messages = [callbacks.applyMarkers(request_response, None, None)]
    detail = json['issues'][issue_name]['detail']
    severity = 'Medium'
    scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
    is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
    if is_scanner_issue_dupe:
        continue
    else:
        self.set_scanner_issues(scanner_issue)
    issue_count = self.set_issue_count(issue_name, vuln_param)
    total_count = self.total_count[issue_name]
    view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
    view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)","for i,vuln_parameter in enumerate(vuln_parameters):
    issue_name = vuln_parameter['vuln_name']
    vuln_param = vuln_parameter['vuln_param']
    param_name = vuln_parameter['param']
    param_value = vuln_parameter['value']
    url = helpers.analyzeRequest(request_response).getUrl()
    url = urlparse.urlsplit(str(url))
    hostname = url.hostname
    path = url.path
    url = url.scheme + '://' + url.hostname + url.path
    http_service = request_response.getHttpService()
    http_messages = [callbacks.applyMarkers(request_response, None, None)]
    detail = json['issues'][issue_name]['detail']
    severity = 'Medium'
    scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
    is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
    if is_scanner_issue_dupe:
        continue
    else:
        self.set_scanner_issues(scanner_issue)
    issue_count = self.set_issue_count(issue_name, vuln_param)
    total_count = self.total_count[issue_name]
    view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
    view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)",1,,,,,,,,,,
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for item in arguments:
    if item.startswith('choice'):
        if arguments[item][0] != '':
            uuidsplit = item.split('uuid-')
            if len(uuidsplit) > 1:
                choiceitems[uuidsplit[1]] = arguments[item][0]
            else:
                for flagoption in arguments[item]:
                    if len(flagoption) > 0:
                        FlagChoice.create_choice(flag, decode(flagoption))","for i,item in enumerate(arguments):
    if item.startswith('choice'):
        if arguments[item][0] != '':
            uuidsplit = item.split('uuid-')
            if len(uuidsplit) > 1:
                choiceitems[uuidsplit[1]] = arguments[item][0]
            else:
                for flagoption in arguments[item]:
                    if len(flagoption) > 0:
                        FlagChoice.create_choice(flag, decode(flagoption))",1,,,,,,,,,,
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for choice in currentchoices:
    if not choice['uuid'] in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice['uuid'])
        self.dbsession.delete(flagchoice)","for i, choice in enumerate(currentchoices):
    if not choice['uuid'] in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice['uuid'])
        self.dbsession.delete(flagchoice)",1,,,,,,,,,,
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for choice in choiceitems:
    flagchoice = FlagChoice.by_uuid(choice)
    if choiceitems[choice] != flagchoice.choice:
        flagchoice.choice = decode(choiceitems[choice])
        self.dbsession.add(flagchoice)","for i, choice in enumerate(choiceitems):
    flagchoice = FlagChoice.by_uuid(choice)
    if choiceitems[choice] != flagchoice.choice:
        flagchoice.choice = decode(choiceitems[choice])
        self.dbsession.add(flagchoice)",1,,,,,,,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for d in range(1, der + 1):
    if c.shape[1] >= 2:
        c = c[:, 1:maxN - d + 1]
    else:
        c = zeros((ng, 1))
        break
    for k in range(2, maxN - d + 1):
        c[:, k - 1] = c[:, k - 1] * k","for d, _ in enumerate(range(1, der + 1)):
    if c.shape[1] >= 2:
        c = c[:, 1:maxN - d + 1]
    else:
        c = zeros((ng, 1))
        break
    for k, _ in enumerate(range(2, maxN - d + 1)):
        c[:, k - 1] = c[:, k - 1] * k",1,,,,,,,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(2, maxN - d + 1):
    c[:, k - 1] = c[:, k - 1] * k","for i,k in enumerate(range(2, maxN - d + 1)):
    c[:, i] = c[:, i] * k",1,,,,,,,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(1, c.shape[1]):
    f = f + c[:, k] * Pg ** k","for k in enumerate(range(1, c.shape[1])):
    f = f + c[:, k[0]+1] * Pg ** k[0]+1",1,,,,,,,,,,
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-en/d2l/tensorflow.py,,"def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):
    """"""Defined in :numref:`sec_minibatches`""""""
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01), trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)
    (net, loss) = (lambda X: d2l.linreg(X, w, b), d2l.squared_loss)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35])
    (n, timer) = (0, d2l.Timer())
    for _ in range(num_epochs):
        for (X, y) in data_iter:
            with tf.GradientTape() as g:
                l = tf.math.reduce_mean(loss(net(X), y))
            (dw, db) = g.gradient(l, [w, b])
            trainer_fn([w, b], [dw, db], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n / X.shape[0]
                q = p / tf.data.experimental.cardinality(data_iter).numpy()
                r = (d2l.evaluate_loss(net, data_iter, loss),)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch')
    return (timer.cumsum(), animator.Y[0])","for _ in range(num_epochs):
    for (X, y) in data_iter:
        with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))
        (dw, db) = g.gradient(l, [w, b])
        trainer_fn([w, b], [dw, db], states, hyperparams)
        n += X.shape[0]
        if n % 200 == 0:
            timer.stop()
            p = n / X.shape[0]
            q = p / tf.data.experimental.cardinality(data_iter).numpy()
            r = (d2l.evaluate_loss(net, data_iter, loss),)
            animator.add(q, r)
            timer.start()","for epoch in range(num_epochs):
    for (X, y) in data_iter:
        with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))
        (dw, db) = g.gradient(l, [w, b])
        trainer_fn([w, b], [dw, db], states, hyperparams)
        n += X.shape[0]
        if n % 200 == 0:
            timer.stop()
            p = n / X.shape[0]
            q = p / tf.data.experimental.cardinality(data_iter).numpy()
            r = (d2l.evaluate_loss(net, data_iter, loss),)
            animator.add(q, r)
            timer.start()",1,,,,,,,,,,
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-en/d2l/tensorflow.py,,"def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):
    """"""Defined in :numref:`sec_minibatches`""""""
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01), trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)
    (net, loss) = (lambda X: d2l.linreg(X, w, b), d2l.squared_loss)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35])
    (n, timer) = (0, d2l.Timer())
    for _ in range(num_epochs):
        for (X, y) in data_iter:
            with tf.GradientTape() as g:
                l = tf.math.reduce_mean(loss(net(X), y))
            (dw, db) = g.gradient(l, [w, b])
            trainer_fn([w, b], [dw, db], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n / X.shape[0]
                q = p / tf.data.experimental.cardinality(data_iter).numpy()
                r = (d2l.evaluate_loss(net, data_iter, loss),)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch')
    return (timer.cumsum(), animator.Y[0])","for (X, y) in data_iter:
    with tf.GradientTape() as g:
        l = tf.math.reduce_mean(loss(net(X), y))
    (dw, db) = g.gradient(l, [w, b])
    trainer_fn([w, b], [dw, db], states, hyperparams)
    n += X.shape[0]
    if n % 200 == 0:
        timer.stop()
        p = n / X.shape[0]
        q = p / tf.data.experimental.cardinality(data_iter).numpy()
        r = (d2l.evaluate_loss(net, data_iter, loss),)
        animator.add(q, r)
        timer.start()","for i, (X, y) in enumerate(data_iter):
    with tf.GradientTape() as g:
        l = tf.math.reduce_mean(loss(net(X), y))
    (dw, db) = g.gradient(l, [w, b])
    trainer_fn([w, b], [dw, db], states, hyperparams)
    n += X.shape[0]
    if n % 200 == 0:
        timer.stop()
        p = n / X.shape[0]
        q = p / tf.data.experimental.cardinality(data_iter).numpy()
        r = (d2l.evaluate_loss(net, data_iter, loss),)
        animator.add(q, r)
        timer.start()",1,,,,,,,,,,
pygatt,https://github.com/peplin/pygatt/tree/master/pygatt/backends/bgapi/bgapi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygatt/pygatt/backends/bgapi/bgapi.py,BGAPIBackend,"def stop(self):
    connected_devices = list(self._connections.values())
    for device in connected_devices:
        try:
            device.disconnect()
        except NotConnectedError:
            pass
    if self._running:
        if self._running.is_set():
            log.info('Stopping')
        self._running.clear()
    if self._receiver:
        self._receiver.join()
    self._receiver = None
    if self._ser:
        self._ser.close()
        self._ser = None","for device in connected_devices:
    try:
        device.disconnect()
    except NotConnectedError:
        pass","for i, device in enumerate(connected_devices):
    try:
        device.disconnect()
    except NotConnectedError:
        pass",1,,,,,,,,,,
faust,https://github.com/robinhood/faust/tree/master/t/stress/producer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/stress/producer.py,,"def install_produce_command(app) -> None:

    @app.command(option('--max-latency', type=float, default=0.5, envvar='PRODUCE_LATENCY', help='Add delay of (at most) n seconds between publishing.'), option('--max-messages', type=int, default=None, help='Send at most N messages or 0 for infinity.'))
    async def produce(self, max_latency: float, max_messages: int):
        """"""Produce example events.""""""
        prods = {aiter(p(max_messages)) for p in app.stress_producers}
        i = 0
        while not app.should_stop:
            to_remove: Set[Any] = set()
            for producer in prods:
                i += 1
                try:
                    await anext(producer)
                except StopAsyncIteration:
                    to_remove.add(producer)
                if not max_latency:
                    if not i % 10000:
                        self.say(f'+SEND {i}')
                elif not i % 10:
                    self.say(f'+SEND {i}')
            if not prods:
                await asyncio.sleep(1.0)
            if max_latency:
                await asyncio.sleep(random.uniform(0, max_latency))
            for producer in to_remove:
                prods.discard(producer)
        print('No more producers - exiting', file=sys.stderr)","for producer in prods:
    i += 1
    try:
        await anext(producer)
    except StopAsyncIteration:
        to_remove.add(producer)
    if not max_latency:
        if not i % 10000:
            self.say(f'+SEND {i}')
    elif not i % 10:
        self.say(f'+SEND {i}')","to_remove = set()
for i, producer in enumerate(prods, start=1):
    try:
        await anext(producer)
    except StopAsyncIteration:
        to_remove.add(producer)
    if not max_latency:
        if not i % 10000:
            self.say(f'+SEND {i}')
    elif not i % 10:
        self.say(f'+SEND {i}')",1,,,,,,,,,,
faust,https://github.com/robinhood/faust/tree/master/t/stress/producer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/stress/producer.py,,"def install_produce_command(app) -> None:

    @app.command(option('--max-latency', type=float, default=0.5, envvar='PRODUCE_LATENCY', help='Add delay of (at most) n seconds between publishing.'), option('--max-messages', type=int, default=None, help='Send at most N messages or 0 for infinity.'))
    async def produce(self, max_latency: float, max_messages: int):
        """"""Produce example events.""""""
        prods = {aiter(p(max_messages)) for p in app.stress_producers}
        i = 0
        while not app.should_stop:
            to_remove: Set[Any] = set()
            for producer in prods:
                i += 1
                try:
                    await anext(producer)
                except StopAsyncIteration:
                    to_remove.add(producer)
                if not max_latency:
                    if not i % 10000:
                        self.say(f'+SEND {i}')
                elif not i % 10:
                    self.say(f'+SEND {i}')
            if not prods:
                await asyncio.sleep(1.0)
            if max_latency:
                await asyncio.sleep(random.uniform(0, max_latency))
            for producer in to_remove:
                prods.discard(producer)
        print('No more producers - exiting', file=sys.stderr)","for producer in to_remove:
    prods.discard(producer)","for i, producer in enumerate(to_remove):
    prods.discard(producer)",1,,,,,,,,,,
poutyne,https://github.com/GRAAL-Research/poutyne/tree/master/tests/framework/callbacks/test_mlflow_logger.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poutyne/tests/framework/callbacks/test_mlflow_logger.py,MLFlowLoggerTest,"def _assert_has_granularity_calls(self, ml_flow_client_patch):
    for _ in range(1, self.num_epochs):
        for step_number in range(1, self.steps_per_epoch):
            ml_flow_client_step_calls = []
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
            ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for _ in range(1, self.num_epochs):
    for step_number in range(1, self.steps_per_epoch):
        ml_flow_client_step_calls = []
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
        ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for epoch_number in range(1, self.num_epochs):
    for step_number in range(1, self.steps_per_epoch):
        ml_flow_client_step_calls = []
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
        ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)",1,,,,,,,,,,
poutyne,https://github.com/GRAAL-Research/poutyne/tree/master/tests/framework/callbacks/test_mlflow_logger.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poutyne/tests/framework/callbacks/test_mlflow_logger.py,MLFlowLoggerTest,"def _assert_has_granularity_calls(self, ml_flow_client_patch):
    for _ in range(1, self.num_epochs):
        for step_number in range(1, self.steps_per_epoch):
            ml_flow_client_step_calls = []
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
            ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for step_number in range(1, self.steps_per_epoch):
    ml_flow_client_step_calls = []
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
    ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for step_number, _ in enumerate(range(1, self.steps_per_epoch), start=1):
    ml_flow_client_step_calls = []
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
    ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)",1,,,,,,,,,,
curator,https://github.com/elastic/curator/tree/master/curator/indexlist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/curator/curator/indexlist.py,IndexList,"def filter_by_size(self, size_threshold=None, threshold_behavior='greater_than', exclude=False, size_behavior='primary'):
    """"""
        Remove indices from the actionable list based on index size.

        `threshold_behavior`, when set to `greater_than` (default), includes if it the index
        tests to be larger than `size_threshold`. When set to `less_than`, it includes if
        the index is smaller than `size_threshold`

        :arg size_threshold: Filter indices over *n* gigabytes
        :arg threshold_behavior: Size to filter, either ``greater_than`` or ``less_than``. Defaults
            to ``greater_than`` to preserve backwards compatability.
        :arg size_behavior: Size that used to filter, either ``primary`` or ``total``. Defaults to ``primary``
        :arg exclude: If `exclude` is `True`, this filter will remove matching
            indices from `indices`. If `exclude` is `False`, then only matching
            indices will be kept in `indices`.
            Default is `False`
        """"""
    self.loggit.debug('Filtering indices by index size')
    if not size_threshold:
        raise exceptions.MissingArgument('No value for ""size_threshold"" provided')
    if size_behavior not in ['primary', 'total']:
        raise ValueError('Invalid value for ""size_behavior"": {0}'.format(size_behavior))
    if threshold_behavior not in ['greater_than', 'less_than']:
        raise ValueError('Invalid value for ""threshold_behavior"": {0}'.format(threshold_behavior))
    index_size_limit = float(size_threshold) * 2 ** 30
    self.loggit.debug('Cannot get disk usage info from closed indices.  Omitting any closed indices.')
    self.filter_closed()
    working_list = self.working_list()
    for index in working_list:
        if size_behavior == 'primary':
            index_size = self.index_info[index]['primary_size_in_bytes']
        else:
            index_size = self.index_info[index]['size_in_bytes']
        msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
        if threshold_behavior == 'greater_than':
            self.__excludify(index_size > index_size_limit, exclude, index, msg)
        elif threshold_behavior == 'less_than':
            self.__excludify(index_size < index_size_limit, exclude, index, msg)","for index in working_list:
    if size_behavior == 'primary':
        index_size = self.index_info[index]['primary_size_in_bytes']
    else:
        index_size = self.index_info[index]['size_in_bytes']
    msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
    if threshold_behavior == 'greater_than':
        self.__excludify(index_size > index_size_limit, exclude, index, msg)
    elif threshold_behavior == 'less_than':
        self.__excludify(index_size < index_size_limit, exclude, index, msg)","for i,index in enumerate(working_list):
    if size_behavior == 'primary':
        index_size = self.index_info[index]['primary_size_in_bytes']
    else:
        index_size = self.index_info[index]['size_in_bytes']
    msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
    if threshold_behavior == 'greater_than':
        self.__excludify(index_size > index_size_limit, exclude, index, msg)
    elif threshold_behavior == 'less_than':
        self.__excludify(index_size < index_size_limit, exclude, index, msg)",1,,,,,,,,,,
Machine-Learning-Collection,https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Machine-Learning-Collection/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,,"def check_accuracy(loader, model, device='cuda'):
    num_correct = 0
    num_samples = 0
    model.eval()
    with torch.no_grad():
        for (x, y) in loader:
            x = x.to(device=device)
            y = y.to(device=device)
            scores = torch.sigmoid(model(x))
            predictions = (scores > 0.5).float()
            num_correct += (predictions == y).sum()
            num_samples += predictions.shape[0]
        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')
    model.train()","for (x, y) in loader:
    x = x.to(device=device)
    y = y.to(device=device)
    scores = torch.sigmoid(model(x))
    predictions = (scores > 0.5).float()
    num_correct += (predictions == y).sum()
    num_samples += predictions.shape[0]","for i, (x, y) in enumerate(loader):
    x = x.to(device=device)
    y = y.to(device=device)
    scores = torch.sigmoid(model(x))
    predictions = (scores > 0.5).float()
    num_correct += (predictions == y).sum()
    num_samples += predictions.shape[0]",1,,,,,,,,,,
EasyTransfer,https://github.com/alibaba/EasyTransfer/tree/master/scripts/fashion_bert/image_feature_extract.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/scripts/fashion_bert/image_feature_extract.py,PredictorImpl,"def search_pb(self, directory):
    """"""
    search pb file recursively, if multiple pb files exist, exception will be
    raised

    Returns:
      directory contain pb file
    """"""
    dir_list = []
    for (root, dirs, files) in tf.gfile.Walk(directory):
        for f in files:
            (_, ext) = os.path.splitext(f)
            if ext == '.pb':
                dir_list.append(root)
    if len(dir_list) == 0:
        raise ValueError('savedmodel is not found in directory %s' % directory)
    elif len(dir_list) > 1:
        raise ValueError('multiple saved model found in directory %s' % directory)
    return dir_list[0]","for f in files:
    (_, ext) = os.path.splitext(f)
    if ext == '.pb':
        dir_list.append(root)","for i,f in enumerate(files):
    (_, ext) = os.path.splitext(f)
    if ext == '.pb':
        dir_list.append(root)",1,,,,,,,,,,
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_dragonbones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_dragonbones.py,,"def get_bone_index(self, armature, bone_name):
    armature_bones = []
    for bone in armature.data.bones:
        armature_bones.append(bone)
    for (i, bone) in enumerate(armature_bones):
        if bone_name == bone.name:
            return i","for bone in armature.data.bones:
    armature_bones.append(bone)","for i,bone in enumerate(armature.data.bones):
    armature_bones.append(bone)",1,,,,,,,,,,
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for _index in range(len(domain_segments)):
    full_domain = '.'.join(domain_segments)
    potential_domain = Domain.query.filter(Domain.name == full_domain).first()
    if potential_domain in domains:
        domain = potential_domain
        break
    domain_segments.pop(0)","for _index, _ in enumerate(domain_segments):
    full_domain = '.'.join(domain_segments)
    potential_domain = Domain.query.filter(Domain.name == full_domain).first()
    if potential_domain in domains:
        domain = potential_domain
        break
    domain_segments.pop(0)",1,,,,,,,,,,
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for ip in myip_addr or remote_addr:
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()","for i,ip in enumerate(myip_addr or remote_addr):
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()",1,,,,,,,,,,
espresso,https://github.com/freewym/espresso/tree/master/fairseq/criterions/sentence_ranking.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/criterions/sentence_ranking.py,SentenceRankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute ranking loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    assert hasattr(model, 'classification_heads') and self.ranking_head_name in model.classification_heads, 'model must provide sentence ranking head for --criterion=sentence_ranking'
    scores = []
    for idx in range(self.num_classes):
        (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
        scores.append(score)
    logits = torch.cat(scores, dim=1)
    sample_size = logits.size(0)
    if 'target' in sample:
        targets = model.get_targets(sample, [logits]).view(-1)
        lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
        loss = F.nll_loss(lprobs, targets, reduction='sum')
    else:
        targets = None
        loss = torch.tensor(0.0, requires_grad=True)
    if self.prediction_h is not None:
        preds = logits.argmax(dim=1)
        for (i, (id, pred)) in enumerate(zip(sample['id'].tolist(), preds.tolist())):
            if targets is not None:
                label = targets[i].item()
                print('{}\t{}\t{}'.format(id, pred, label), file=self.prediction_h)
            else:
                print('{}\t{}'.format(id, pred), file=self.prediction_h)
    logging_output = {'loss': loss.data, 'ntokens': sample['ntokens'], 'nsentences': sample_size, 'sample_size': sample_size}
    if targets is not None:
        logging_output['ncorrect'] = (logits.argmax(dim=1) == targets).sum()
    return (loss, sample_size, logging_output)","for idx in range(self.num_classes):
    (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
    scores.append(score)","for idx in range(self.num_classes):
    (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
    scores.append(score)",1,,,,,,,,,,
neutron,https://github.com/openstack/neutron/tree/master/neutron/services/trunk/drivers/openvswitch/driver.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/services/trunk/drivers/openvswitch/driver.py,OVSDriver,"def _update_subport_binding(self, context, trunk_id):
    """"""Update the subport binding host""""""
    trunk_obj = self._get_trunk(context, trunk_id)
    trunk_port = self.core_plugin.get_port(context, trunk_obj.port_id)
    trunk_host = trunk_port.get(portbindings.HOST_ID)
    for subport in trunk_obj.sub_ports:
        port = self.core_plugin.update_port(context, subport.port_id, {'port': {portbindings.HOST_ID: trunk_host, 'device_owner': trunk_consts.TRUNK_SUBPORT_OWNER}})
        vif_type = port.get(portbindings.VIF_TYPE)
        if vif_type == portbindings.VIF_TYPE_BINDING_FAILED:
            raise trunk_exc.SubPortBindingError(port_id=subport.port_id, trunk_id=trunk_obj.id)","for subport in trunk_obj.sub_ports:
    port = self.core_plugin.update_port(context, subport.port_id, {'port': {portbindings.HOST_ID: trunk_host, 'device_owner': trunk_consts.TRUNK_SUBPORT_OWNER}})
    vif_type = port.get(portbindings.VIF_TYPE)
    if vif_type == portbindings.VIF_TYPE_BINDING_FAILED:
        raise trunk_exc.SubPortBindingError(port_id=subport.port_id, trunk_id=trunk_obj.id)","for i, subport in enumerate(trunk_obj.sub_ports):
    port = self.core_plugin.update_port(context, subport.port_id, {'port': {portbindings.HOST_ID: trunk_host, 'device_owner': trunk_consts.TRUNK_SUBPORT_OWNER}})
    vif_type = port.get(portbindings.VIF_TYPE)
    if vif_type == portbindings.VIF_TYPE_BINDING_FAILED:
        raise trunk_exc.SubPortBindingError(port_id=subport.port_id, trunk_id=trunk_obj.id)",1,,,,,,,,,,
h,https://github.com/hypothesis/h/tree/master/tests/h/views/activity_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/h/tests/h/views/activity_test.py,TestGroupSearchController,"def test_search_returns_group_moderators_faceted_by(self, controller, pyramid_request, test_user, test_group):
    pyramid_request.params = {'q': 'user:does_not_matter'}
    result = controller.search()
    for moderator in result['group_users_args'][1]:
        assert not moderator['faceted_by']","for moderator in result['group_users_args'][1]:
    assert not moderator['faceted_by']","for i, moderator in enumerate(result['group_users_args'][1]):
    assert not moderator['faceted_by']",1,,,,,,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for j in range(len(y)):
    d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))","for j, _ in enumerate(y):
    d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))",1,,,,,,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for j in range(len(y)):
    for i in range(len(x)):
        d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))","for j, y_val in enumerate(y):
    for i, x_val in enumerate(x):
        d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x_val != y_val), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))",1,,,,,,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for i in range(len(x)):
    d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))","for i, _ in enumerate(x):
    d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))",1,,,,,,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for j in range(16):
    if i + j < l:
        if line[j]:
            col = colorize[(linex[j] != liney[j]) * (doy - dox)]
            print(col('%02X' % line[j][0]), end=' ')
            if linex[j] == liney[j]:
                cl += sane_color(line[j])
            else:
                cl += col(sane(line[j]))
        else:
            print('  ', end=' ')
            cl += ' '
    else:
        print('  ', end=' ')
    if j == 7:
        print('', end=' ')","for j in range(16):
    if i + j < l:
        if line[j]:
            col = colorize[(linex[j] != liney[j]) * (doy - dox)]
            print(col('%02X' % line[j][0]), end=' ')
            if linex[j] == liney[j]:
                cl += sane_color(line[j])
            else:
                cl += col(sane(line[j]))
        else:
            print('  ', end=' ')
            cl += ' '
    else:
        print('  ', end=' ')
    if j == 7:
        print('', end=' ')",1,,,,,,,,,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/dns_data_management.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/dns_data_management.py,IPASystemRecords,"def _get_location_dns_records_for_server(self, zone_obj, hostname, locations, roles=None, include_master_role=True, include_kerberos_realm=True):
    server = self.servers_data[hostname]
    if roles:
        eff_roles = server['roles'] & roles
    else:
        eff_roles = server['roles']
    hostname_abs = DNSName(hostname).make_absolute()
    for location in locations:
        if location == self.servers_data[hostname]['location']:
            priority = self.PRIORITY_HIGH
        else:
            priority = self.PRIORITY_LOW
        if include_kerberos_realm:
            self.__add_kerberos_txt_rec(zone_obj, location)
        if include_master_role:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
            self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
        if 'AD trust controller' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
        if 'NTP server' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)
    return zone_obj","for location in locations:
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)","for i, location in enumerate(locations):
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)",1,,,,,,,,,,
napalm,https://github.com/napalm-automation/napalm/tree/master/napalm/iosxr_netconf/iosxr_netconf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/iosxr_netconf/iosxr_netconf.py,IOSXRNETCONFDriver,"def get_snmp_information(self):
    """"""Return the SNMP configuration.""""""
    snmp_information = {}
    rpc_reply = self.device.get_config(source='running', filter=('subtree', C.SNMP_RPC_REQ_FILTER)).xml
    snmp_result_tree = ETREE.fromstring(rpc_reply)
    _PRIVILEGE_MODE_MAP_ = {'read-only': 'ro', 'read-write': 'rw'}
    snmp_information = {'chassis_id': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:chassis-id', default='', namespaces=C.NS), 'contact': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:contact', default='', namespaces=C.NS), 'location': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:location', default='', namespaces=C.NS), 'community': {}}
    for community in snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS):
        name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
        privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
        acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
        snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}
    return snmp_information","for community in snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS):
    name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
    privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
    acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
    snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}","for i, community in enumerate(snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS)):
    name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
    privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
    acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
    snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}",1,,,,,,,,,,
youtube-dl,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/rtve.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/rtve.py,RTVEALaCartaIE,"def _extract_png_formats(self, video_id):
    png = self._download_webpage('http://www.rtve.es/ztnr/movil/thumbnail/%s/videos/%s.png' % (self._manager, video_id), video_id, 'Downloading url information', query={'q': 'v2'})
    q = qualities(['Media', 'Alta', 'HQ', 'HD_READY', 'HD_FULL'])
    formats = []
    for (quality, video_url) in self._decrypt_url(png):
        ext = determine_ext(video_url)
        if ext == 'm3u8':
            formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
        elif ext == 'mpd':
            formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
        else:
            formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})
    self._sort_formats(formats)
    return formats","for (quality, video_url) in self._decrypt_url(png):
    ext = determine_ext(video_url)
    if ext == 'm3u8':
        formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
    elif ext == 'mpd':
        formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
    else:
        formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})","for i, (quality, video_url) in enumerate(self._decrypt_url(png)):
    ext = determine_ext(video_url)
    if ext == 'm3u8':
        formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
    elif ext == 'mpd':
        formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
    else:
        formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})",1,,,,,,,,,,
ActualVim,https://github.com/lunixbochs/ActualVim/tree/master//edit.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ActualVim//edit.py,EditStep,"def resolve_args(self, view, edit):
    args = []
    for arg in self.args:
        if isinstance(arg, EditFuture):
            arg = arg.resolve(view, edit)
        args.append(arg)
    return args","for arg in self.args:
    if isinstance(arg, EditFuture):
        arg = arg.resolve(view, edit)
    args.append(arg)","for i,arg in enumerate(self.args):
    if isinstance(arg, EditFuture):
        arg = arg.resolve(view, edit)
    args.append(arg)",1,,,,,,,,,,
hmmlearn,https://github.com/hmmlearn/hmmlearn/tree/master/lib/hmmlearn/tests/test_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hmmlearn/lib/hmmlearn/tests/test_base.py,TestMonitor,"def test_report(self, capsys):
    n_iter = 10
    m = ConvergenceMonitor(tol=0.001, n_iter=n_iter, verbose=True)
    for i in reversed(range(n_iter)):
        m.report(-0.01 * i)
    (out, err) = capsys.readouterr()
    assert not out
    assert len(err.splitlines()) == n_iter
    assert len(m.history) == n_iter","for i in reversed(range(n_iter)):
    m.report(-0.01 * i)","for i in enumerate(reversed(range(n_iter))):
    m.report(-0.01 * i[0])",1,,,,,,,,,,
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for subdir in os.listdir(dir1):
    if osp.isdir(osp.join(dir1, subdir)):
        if check_keywords(subdir, keywords):
            dir_path1 = osp.join(dir1, subdir)
            for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                img_list = list()
                for dir in dir_list:
                    dir_path = dir_path1.replace(dir1, dir)
                    img_path = osp.join(dir_path, img_name)
                    img = cv2.imread(img_path)
                    img_list.append(img)
                    if img_path.find(dir1) >= 0:
                        res_img_path = img_path.replace(dir1, res_dir)
                if any([img is None for img in img_list]):
                    continue
                res_img = np.concatenate(img_list, axis=0)
                (h, w) = res_img.shape[:2]
                res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                res_img_path = res_img_path.replace('.png', '.jpg')
                ry_utils.make_subdir(res_img_path)
                cv2.imwrite(res_img_path, res_img)
                print(res_img_path)","for i, subdir in enumerate(os.listdir(dir1)):
    if osp.isdir(osp.join(dir1, subdir)):
        if check_keywords(subdir, keywords):
            dir_path1 = osp.join(dir1, subdir)
            for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                img_list = list()
                for dir in dir_list:
                    dir_path = dir_path1.replace(dir1, dir)
                    img_path = osp.join(dir_path, img_name)
                    img = cv2.imread(img_path)
                    img_list.append(img)
                    if img_path.find(dir1) >= 0:
                        res_img_path = img_path.replace(dir1, res_dir)
                if any([img is None for img in img_list]):
                    continue
                res_img = np.concatenate(img_list, axis=0)
                (h, w) = res_img.shape[:2]
                res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                res_img_path = res_img_path.replace('.png', '.jpg')
                ry_utils.make_subdir(res_img_path)
                cv2.imwrite(res_img_path, res_img)
                print(res_img_path)",1,,,,,,,,,,
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
    img_list = list()
    for dir in dir_list:
        dir_path = dir_path1.replace(dir1, dir)
        img_path = osp.join(dir_path, img_name)
        img = cv2.imread(img_path)
        img_list.append(img)
        if img_path.find(dir1) >= 0:
            res_img_path = img_path.replace(dir1, res_dir)
    if any([img is None for img in img_list]):
        continue
    res_img = np.concatenate(img_list, axis=0)
    (h, w) = res_img.shape[:2]
    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
    res_img_path = res_img_path.replace('.png', '.jpg')
    ry_utils.make_subdir(res_img_path)
    cv2.imwrite(res_img_path, res_img)
    print(res_img_path)","for i,img_name in enumerate(ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only')):
    img_list = list()
    for dir in dir_list:
        dir_path = dir_path1.replace(dir1, dir)
        img_path = osp.join(dir_path, img_name)
        img = cv2.imread(img_path)
        img_list.append(img)
        if img_path.find(dir1) >= 0:
            res_img_path = img_path.replace(dir1, res_dir)
    if any([img is None for img in img_list]):
        continue
    res_img = np.concatenate(img_list, axis=0)
    (h, w) = res_img.shape[:2]
    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
    res_img_path = res_img_path.replace('.png', '.jpg')
    ry_utils.make_subdir(res_img_path)
    cv2.imwrite(res_img_path, res_img)
    print(res_img_path)",1,,,,,,,,,,
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for dir in dir_list:
    dir_path = dir_path1.replace(dir1, dir)
    img_path = osp.join(dir_path, img_name)
    img = cv2.imread(img_path)
    img_list.append(img)
    if img_path.find(dir1) >= 0:
        res_img_path = img_path.replace(dir1, res_dir)","for i, dir in enumerate(dir_list):
    dir_path = dir_path1.replace(dir1, dir)
    img_path = osp.join(dir_path, img_name)
    img = cv2.imread(img_path)
    img_list.append(img)
    if img_path.find(dir1) >= 0:
        res_img_path = img_path.replace(dir1, res_dir)",1,,,,,,,,,,
manuskript,https://github.com/olivierkes/manuskript/tree/master/manuskript/functions/spellchecker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/manuskript/manuskript/functions/spellchecker.py,Spellchecker,"def getDictionary(dictionary):
    if not dictionary:
        dictionary = Spellchecker.getDefaultDictionary()
    if not dictionary:
        return None
    values = dictionary.split(':', 1)
    if len(values) == 1:
        (lib, name) = (Spellchecker.implementations[0].getLibraryName(), dictionary)
        dictionary = Spellchecker.normalizeDictName(lib, name)
    else:
        (lib, name) = values
    try:
        d = Spellchecker.dictionaries.get(dictionary, None)
        if d == None:
            for impl in Spellchecker.implementations:
                if impl.isInstalled() and lib == impl.getLibraryName():
                    d = impl(name)
                    Spellchecker.dictionaries[dictionary] = d
                    break
        return d
    except Exception as e:
        pass
    return None","for impl in Spellchecker.implementations:
    if impl.isInstalled() and lib == impl.getLibraryName():
        d = impl(name)
        Spellchecker.dictionaries[dictionary] = d
        break","for i, impl in enumerate(Spellchecker.implementations):
    if impl.isInstalled() and lib == impl.getLibraryName():
        d = impl(name)
        Spellchecker.dictionaries[dictionary] = d
        break",1,,,,,,,,,,
pootle,https://github.com/translate/pootle/tree/master/tests/views/admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pootle/tests/views/admin.py,,"def test_admin_view_project_delete_tp(english, client, admin):
    user = admin
    project = Project.objects.get(code='project0')
    tp = TranslationProject.objects.create(language=english, project=project)
    project.config['pootle.core.lang_mapping'] = {tp.language.code: 'foo'}
    client.login(username=user.username, password=TEST_USERS['admin']['password'])
    get_response = _admin_view_get(client, project)
    post_data = {}
    formset = get_response.context['formset']
    forms = formset.forms + formset.extra_forms + [formset.management_form]
    for form in forms:
        for field in form.fields:
            post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')
    tp_pk = post_data['form-0-id']
    post_data['form-0-DELETE'] = 'true'
    response = _admin_view_post(client, project, **post_data)
    assert tp_pk not in project.translationproject_set.values_list('pk', flat=True)
    _test_admin_view(response, project)
    assert project.config['pootle.core.lang_mapping'] == {}","for form in forms:
    for field in form.fields:
        post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')","for i, form in enumerate(forms):
    for field in form.fields:
        post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')",1,,,,,,,,,,
pootle,https://github.com/translate/pootle/tree/master/tests/views/admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pootle/tests/views/admin.py,,"def test_admin_view_project_delete_tp(english, client, admin):
    user = admin
    project = Project.objects.get(code='project0')
    tp = TranslationProject.objects.create(language=english, project=project)
    project.config['pootle.core.lang_mapping'] = {tp.language.code: 'foo'}
    client.login(username=user.username, password=TEST_USERS['admin']['password'])
    get_response = _admin_view_get(client, project)
    post_data = {}
    formset = get_response.context['formset']
    forms = formset.forms + formset.extra_forms + [formset.management_form]
    for form in forms:
        for field in form.fields:
            post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')
    tp_pk = post_data['form-0-id']
    post_data['form-0-DELETE'] = 'true'
    response = _admin_view_post(client, project, **post_data)
    assert tp_pk not in project.translationproject_set.values_list('pk', flat=True)
    _test_admin_view(response, project)
    assert project.config['pootle.core.lang_mapping'] == {}","for field in form.fields:
    post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')","for i, field in enumerate(form.fields):
    post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')",1,,,,,,,,,,
nova,https://github.com/openstack/nova/tree/master/nova/virt/hyperv/imagecache.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/hyperv/imagecache.py,ImageCache,"def _age_and_verify_cached_images(self, context, all_instances, base_dir):
    for img in self.originals:
        if img in self.used_images:
            self._update_image_timestamp(img)
        elif CONF.image_cache.remove_unused_base_images:
            self._remove_if_old_image(img)","for img in self.originals:
    if img in self.used_images:
        self._update_image_timestamp(img)
    elif CONF.image_cache.remove_unused_base_images:
        self._remove_if_old_image(img)","for i,img in enumerate(self.originals):
    if img in self.used_images:
        self._update_image_timestamp(img)
    elif CONF.image_cache.remove_unused_base_images:
        self._remove_if_old_image(img)",1,,,,,,,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","for index in self._ops.keys():
    term_strs.append('%s%s' % (self[index], index))","for i,index in enumerate(self._ops.keys()):
    term_strs.append('%s%s' % (self[index], index))",1,,,,,,,,,,
moto,https://github.com/spulec/moto/tree/master/moto/s3/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/s3/models.py,FakeMultipart,"def list_parts(self, part_number_marker, max_parts):
    max_marker = part_number_marker + max_parts
    for part_id in self.partlist[part_number_marker:max_marker]:
        yield self.parts[part_id]","for part_id in self.partlist[part_number_marker:max_marker]:
    yield self.parts[part_id]","for i, part_id in enumerate(self.partlist[part_number_marker:max_marker]):
    yield self.parts[part_id]",1,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","for (model, classes) in models:
    net = model(pretrained=pretrained)
    ctx = mx.cpu()
    if not pretrained:
        net.initialize(ctx=ctx)
    net_params = net.collect_params()
    weight_count = 0
    for param in net_params.values():
        if param.shape is None or not param._differentiable:
            continue
        weight_count += np.prod(param.shape)
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resdropresnet20_cifar10 or weight_count == 272474
    assert model != resdropresnet20_cifar100 or weight_count == 278324
    assert model != resdropresnet20_svhn or weight_count == 272474
    x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
    y = net(x)
    assert y.shape == (14, classes)","for i, (model, classes) in enumerate(models):
    net = model(pretrained=pretrained)
    ctx = mx.cpu()
    if not pretrained:
        net.initialize(ctx=ctx)
    net_params = net.collect_params()
    weight_count = 0
    for param in net_params.values():
        if param.shape is None or not param._differentiable:
            continue
        weight_count += np.prod(param.shape)
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resdropresnet20_cifar10 or weight_count == 272474
    assert model != resdropresnet20_cifar100 or weight_count == 278324
    assert model != resdropresnet20_svhn or weight_count == 272474
    x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
    y = net(x)
    assert y.shape == (14, classes)",1,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","for param in net_params.values():
    if param.shape is None or not param._differentiable:
        continue
    weight_count += np.prod(param.shape)","for i,param in enumerate(net_params.values()):
    if param.shape is None or not param._differentiable:
        continue
    weight_count += np.prod(param.shape)",1,,,,,,,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","for i in range(self.FILE_HEADER.NumberOfSections):
    if i >= MAX_SECTIONS:
        self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
        break
    simultaneous_errors = 0
    section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
    if not section:
        break
    section_offset = offset + section.sizeof() * i
    section.set_file_offset(section_offset)
    section_data = self.__data__[section_offset:section_offset + section.sizeof()]
    if count_zeroes(section_data) == section.sizeof():
        self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
        break
    if not section_data:
        self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
        break
    section.__unpack__(section_data)
    self.__structures__.append(section)
    if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
    if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
    if section.Misc_VirtualSize > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
    if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
    if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
    if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
        self.__warnings.append('Too many warnings parsing section. Aborting.')
        break
    section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
    set_flags(section, section.Characteristics, section_flags)
    if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
        if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
            pass
        else:
            self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
    self.sections.append(section)","for i in range(self.FILE_HEADER.NumberOfSections):
    if i >= MAX_SECTIONS:
        self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
        break
    simultaneous_errors = 0
    section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
    if not section:
        break
    section_offset = offset + section.sizeof() * i
    section.set_file_offset(section_offset)
    section_data = self.__data__[section_offset:section_offset + section.sizeof()]
    if count_zeroes(section_data) == section.sizeof():
        self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
        break
    if not section_data:
        self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
        break
    section.__unpack__(section_data)
    self.__structures__.append(section)
    if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
    if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
    if section.Misc_VirtualSize > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
    if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
    if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
    if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
        self.__warnings.append('Too many warnings parsing section. Aborting.')
        break
    section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
    set_flags(section, section.Characteristics, section_flags)
    if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
        if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
            pass
        else:
            self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
    self.sections.append(section)",1,,,,,,,,,,
contrastive-unpaired-translation,https://github.com/taesungp/contrastive-unpaired-translation/tree/master/datasets/make_dataset_aligned.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/contrastive-unpaired-translation/datasets/make_dataset_aligned.py,,"def align_images(a_file_paths, b_file_paths, target_path):
    if not os.path.exists(target_path):
        os.makedirs(target_path)
    for i in range(len(a_file_paths)):
        img_a = Image.open(a_file_paths[i])
        img_b = Image.open(b_file_paths[i])
        assert img_a.size == img_b.size
        aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
        aligned_image.paste(img_a, (0, 0))
        aligned_image.paste(img_b, (img_a.size[0], 0))
        aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))","for i in range(len(a_file_paths)):
    img_a = Image.open(a_file_paths[i])
    img_b = Image.open(b_file_paths[i])
    assert img_a.size == img_b.size
    aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
    aligned_image.paste(img_a, (0, 0))
    aligned_image.paste(img_b, (img_a.size[0], 0))
    aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))","for i, (a_file_path, b_file_path) in enumerate(zip(a_file_paths, b_file_paths)):
    img_a = Image.open(a_file_path)
    img_b = Image.open(b_file_path)
    assert img_a.size == img_b.size
    aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
    aligned_image.paste(img_a, (0, 0))
    aligned_image.paste(img_b, (img_a.size[0], 0))
    aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))",1,,,,,,,,,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/outputs/base_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/outputs/base_output.py,AbstractOutput,"def check_triggers(self, output_id, amount=None, output_channel=0):
    """"""
        This function is executed whenever an output is turned on or off
        It is responsible for executing Output Triggers
        """"""
    output_channel_dev = db_retrieve_table_daemon(OutputChannel).filter(and_(OutputChannel.output_id == output_id, OutputChannel.channel == output_channel)).first()
    if output_channel_dev is None:
        self.logger.error('Could not find channel in database')
        return
    trigger_output = db_retrieve_table_daemon(Trigger)
    trigger_output = trigger_output.filter(Trigger.trigger_type == 'trigger_output')
    trigger_output = trigger_output.filter(Trigger.unique_id_1 == output_id)
    trigger_output = trigger_output.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output = trigger_output.filter(Trigger.is_activated.is_(True))
    if self.is_on(output_channel):
        trigger_output = trigger_output.filter(or_(Trigger.output_state == 'on_duration_none', Trigger.output_state == 'on_duration_any', Trigger.output_state == 'on_duration_none_any', Trigger.output_state == 'on_duration_equal', Trigger.output_state == 'on_duration_greater_than', Trigger.output_state == 'on_duration_equal_greater_than', Trigger.output_state == 'on_duration_less_than', Trigger.output_state == 'on_duration_equal_less_than'))
        on_duration_none = and_(Trigger.output_state == 'on_duration_none', amount == 0.0)
        on_duration_any = and_(Trigger.output_state == 'on_duration_any', bool(amount))
        on_duration_none_any = Trigger.output_state == 'on_duration_none_any'
        on_duration_equal = and_(Trigger.output_state == 'on_duration_equal', Trigger.output_duration == amount)
        on_duration_greater_than = and_(Trigger.output_state == 'on_duration_greater_than', amount > Trigger.output_duration)
        on_duration_equal_greater_than = and_(Trigger.output_state == 'on_duration_equal_greater_than', amount >= Trigger.output_duration)
        on_duration_less_than = and_(Trigger.output_state == 'on_duration_less_than', amount < Trigger.output_duration)
        on_duration_equal_less_than = and_(Trigger.output_state == 'on_duration_equal_less_than', amount <= Trigger.output_duration)
        trigger_output = trigger_output.filter(or_(on_duration_none, on_duration_any, on_duration_none_any, on_duration_equal, on_duration_greater_than, on_duration_equal_greater_than, on_duration_less_than, on_duration_equal_less_than))
    else:
        trigger_output = trigger_output.filter(Trigger.output_state == 'off')
    for each_trigger in trigger_output.all():
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)
    trigger_output_pwm = db_retrieve_table_daemon(Trigger)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.trigger_type == 'trigger_output_pwm')
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_1 == output_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.is_activated.is_(True))
    for each_trigger in trigger_output_pwm.all():
        trigger_trigger = False
        duty_cycle = self.output_state(output_channel)
        if duty_cycle == 'off':
            if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
                trigger_trigger = True
        elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
            trigger_trigger = True
        if not trigger_trigger:
            continue
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for each_trigger in trigger_output.all():
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for i, each_trigger in enumerate(trigger_output.all()):
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)",1,,,,,,,,,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/outputs/base_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/outputs/base_output.py,AbstractOutput,"def check_triggers(self, output_id, amount=None, output_channel=0):
    """"""
        This function is executed whenever an output is turned on or off
        It is responsible for executing Output Triggers
        """"""
    output_channel_dev = db_retrieve_table_daemon(OutputChannel).filter(and_(OutputChannel.output_id == output_id, OutputChannel.channel == output_channel)).first()
    if output_channel_dev is None:
        self.logger.error('Could not find channel in database')
        return
    trigger_output = db_retrieve_table_daemon(Trigger)
    trigger_output = trigger_output.filter(Trigger.trigger_type == 'trigger_output')
    trigger_output = trigger_output.filter(Trigger.unique_id_1 == output_id)
    trigger_output = trigger_output.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output = trigger_output.filter(Trigger.is_activated.is_(True))
    if self.is_on(output_channel):
        trigger_output = trigger_output.filter(or_(Trigger.output_state == 'on_duration_none', Trigger.output_state == 'on_duration_any', Trigger.output_state == 'on_duration_none_any', Trigger.output_state == 'on_duration_equal', Trigger.output_state == 'on_duration_greater_than', Trigger.output_state == 'on_duration_equal_greater_than', Trigger.output_state == 'on_duration_less_than', Trigger.output_state == 'on_duration_equal_less_than'))
        on_duration_none = and_(Trigger.output_state == 'on_duration_none', amount == 0.0)
        on_duration_any = and_(Trigger.output_state == 'on_duration_any', bool(amount))
        on_duration_none_any = Trigger.output_state == 'on_duration_none_any'
        on_duration_equal = and_(Trigger.output_state == 'on_duration_equal', Trigger.output_duration == amount)
        on_duration_greater_than = and_(Trigger.output_state == 'on_duration_greater_than', amount > Trigger.output_duration)
        on_duration_equal_greater_than = and_(Trigger.output_state == 'on_duration_equal_greater_than', amount >= Trigger.output_duration)
        on_duration_less_than = and_(Trigger.output_state == 'on_duration_less_than', amount < Trigger.output_duration)
        on_duration_equal_less_than = and_(Trigger.output_state == 'on_duration_equal_less_than', amount <= Trigger.output_duration)
        trigger_output = trigger_output.filter(or_(on_duration_none, on_duration_any, on_duration_none_any, on_duration_equal, on_duration_greater_than, on_duration_equal_greater_than, on_duration_less_than, on_duration_equal_less_than))
    else:
        trigger_output = trigger_output.filter(Trigger.output_state == 'off')
    for each_trigger in trigger_output.all():
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)
    trigger_output_pwm = db_retrieve_table_daemon(Trigger)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.trigger_type == 'trigger_output_pwm')
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_1 == output_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.is_activated.is_(True))
    for each_trigger in trigger_output_pwm.all():
        trigger_trigger = False
        duty_cycle = self.output_state(output_channel)
        if duty_cycle == 'off':
            if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
                trigger_trigger = True
        elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
            trigger_trigger = True
        if not trigger_trigger:
            continue
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for each_trigger in trigger_output_pwm.all():
    trigger_trigger = False
    duty_cycle = self.output_state(output_channel)
    if duty_cycle == 'off':
        if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
            trigger_trigger = True
    elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
        trigger_trigger = True
    if not trigger_trigger:
        continue
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for i, each_trigger in enumerate(trigger_output_pwm.all()):
    trigger_trigger = False
    duty_cycle = self.output_state(output_channel)
    if duty_cycle == 'off':
        if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
            trigger_trigger = True
    elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
        trigger_trigger = True
    if not trigger_trigger:
        continue
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)",1,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resnet_cub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/resnet_cub.py,,"def _test():
    import numpy as np
    import chainer
    chainer.global_config.train = False
    pretrained = False
    models = [resnet10_cub, resnet12_cub, resnet14_cub, resnetbc14b_cub, resnet16_cub, resnet18_cub, resnet26_cub, resnetbc26b_cub, resnet34_cub, resnetbc38b_cub, resnet50_cub, resnet50b_cub, resnet101_cub, resnet101b_cub, resnet152_cub, resnet152b_cub, resnet200_cub, resnet200b_cub]
    for model in models:
        net = model(pretrained=pretrained)
        weight_count = net.count_params()
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resnet10_cub or weight_count == 5008392
        assert model != resnet12_cub or weight_count == 5082376
        assert model != resnet14_cub or weight_count == 5377800
        assert model != resnetbc14b_cub or weight_count == 8425736
        assert model != resnet16_cub or weight_count == 6558472
        assert model != resnet18_cub or weight_count == 11279112
        assert model != resnet26_cub or weight_count == 17549832
        assert model != resnetbc26b_cub or weight_count == 14355976
        assert model != resnet34_cub or weight_count == 21387272
        assert model != resnetbc38b_cub or weight_count == 20286216
        assert model != resnet50_cub or weight_count == 23917832
        assert model != resnet50b_cub or weight_count == 23917832
        assert model != resnet101_cub or weight_count == 42909960
        assert model != resnet101b_cub or weight_count == 42909960
        assert model != resnet152_cub or weight_count == 58553608
        assert model != resnet152b_cub or weight_count == 58553608
        assert model != resnet200_cub or weight_count == 63034632
        assert model != resnet200b_cub or weight_count == 63034632
        x = np.zeros((1, 3, 224, 224), np.float32)
        y = net(x)
        assert y.shape == (1, 200)","for model in models:
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)","for i,model in enumerate(models):
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)",1,,,,,,,,,,
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break",1,,,,,,,,,,
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape[1:] != subspace.shape:
        all_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape[1:] != subspace.shape:
        all_vectorized = False
        break",1,,,,,,,,,,
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for i in range(len(newArr)):
    for j in range(len(newArr[i])):
        tempList.append(newArr[i][j])","for i in range(len(newArr)):
    for j, val in enumerate(newArr[i]):
        tempList.append(val)",1,,,,,,,,,,
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for j in range(len(newArr[i])):
    tempList.append(newArr[i][j])","for j, val in enumerate(newArr[i]):
    tempList.append(val)",1,,,,,,,,,,
MAML-Pytorch,https://github.com/dragen1860/MAML-Pytorch/tree/master/backup/csmlv0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MAML-Pytorch/backup/csmlv0.py,,"def inner_train(K, gpuidx, support_x, support_y, query_x, query_y, concepts, Q):
    """"""
    inner-loop train function.
    :param K: train iterations
    :param gpuidx: which gpu to train
    :param support_x:   [b, setsz, c_, h, w]
    :param support_y:   []
    :param query_x:     [b, querysz]
    :param query_y:
    :param concepts:    concepts network
    :param Q:           Queue to receive result
    :return:
    """"""
    assert support_x.size(0) == query_x.size(0)
    support_x = support_x.cuda(gpuidx)
    support_y = support_y.cuda(gpuidx)
    query_x = query_x.cuda(gpuidx)
    query_y = query_y.cuda(gpuidx)
    support_db = TensorDataset(support_x, support_y)
    query_db = TensorDataset(query_x, query_y)
    outlayer = OutLayer().cuda(gpuidx)
    criteon = nn.CrossEntropyLoss().cuda(gpuidx)
    optimizer = optim.Adam(outlayer.parameters(), lr=0.001)
    right = Variable(torch.zeros(1).cuda(gpuidx))
    loss = Variable(torch.zeros(1).cuda(gpuidx))
    for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
        for i in range(K):
            x = concepts[gpuidx](support_xb)
            x = x.detach()
            logits = outlayer(x)
            loss = criteon(logits, support_yb)
            outlayer.zero_grad()
            loss.backward()
            optimizer.step()
        x = concepts[gpuidx](query_xb)
        logits = outlayer(x)
        (_, idx) = logits.max(1)
        pred = idx.long()
        right += torch.eq(pred, query_yb).sum().float()
        loss += criteon(logits, query_yb)
    accuracy = right.data[0] / np.array(query_y.size()).prod()
    print(gpuidx, loss.data[0], accuracy)
    Q.put([gpuidx, loss.data[0], accuracy])
    del outlayer, criteon
    print('removed outlayer and criteon.')","for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
    for i in range(K):
        x = concepts[gpuidx](support_xb)
        x = x.detach()
        logits = outlayer(x)
        loss = criteon(logits, support_yb)
        outlayer.zero_grad()
        loss.backward()
        optimizer.step()
    x = concepts[gpuidx](query_xb)
    logits = outlayer(x)
    (_, idx) = logits.max(1)
    pred = idx.long()
    right += torch.eq(pred, query_yb).sum().float()
    loss += criteon(logits, query_yb)","for i, ((support_xb, support_yb), (query_xb, query_yb)) in enumerate(zip(support_db, query_db)):
    for j in range(K):
        x = concepts[gpuidx](support_xb)
        x = x.detach()
        logits = outlayer(x)
        loss = criteon(logits, support_yb)
        outlayer.zero_grad()
        loss.backward()
        optimizer.step()
    x = concepts[gpuidx](query_xb)
    logits = outlayer(x)
    (_, idx) = logits.max(1)
    pred = idx.long()
    right += torch.eq(pred, query_yb).sum().float()
    loss += criteon(logits, query_yb)",1,,,,,,,,,,
MAML-Pytorch,https://github.com/dragen1860/MAML-Pytorch/tree/master/backup/csmlv0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MAML-Pytorch/backup/csmlv0.py,,"def inner_train(K, gpuidx, support_x, support_y, query_x, query_y, concepts, Q):
    """"""
    inner-loop train function.
    :param K: train iterations
    :param gpuidx: which gpu to train
    :param support_x:   [b, setsz, c_, h, w]
    :param support_y:   []
    :param query_x:     [b, querysz]
    :param query_y:
    :param concepts:    concepts network
    :param Q:           Queue to receive result
    :return:
    """"""
    assert support_x.size(0) == query_x.size(0)
    support_x = support_x.cuda(gpuidx)
    support_y = support_y.cuda(gpuidx)
    query_x = query_x.cuda(gpuidx)
    query_y = query_y.cuda(gpuidx)
    support_db = TensorDataset(support_x, support_y)
    query_db = TensorDataset(query_x, query_y)
    outlayer = OutLayer().cuda(gpuidx)
    criteon = nn.CrossEntropyLoss().cuda(gpuidx)
    optimizer = optim.Adam(outlayer.parameters(), lr=0.001)
    right = Variable(torch.zeros(1).cuda(gpuidx))
    loss = Variable(torch.zeros(1).cuda(gpuidx))
    for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
        for i in range(K):
            x = concepts[gpuidx](support_xb)
            x = x.detach()
            logits = outlayer(x)
            loss = criteon(logits, support_yb)
            outlayer.zero_grad()
            loss.backward()
            optimizer.step()
        x = concepts[gpuidx](query_xb)
        logits = outlayer(x)
        (_, idx) = logits.max(1)
        pred = idx.long()
        right += torch.eq(pred, query_yb).sum().float()
        loss += criteon(logits, query_yb)
    accuracy = right.data[0] / np.array(query_y.size()).prod()
    print(gpuidx, loss.data[0], accuracy)
    Q.put([gpuidx, loss.data[0], accuracy])
    del outlayer, criteon
    print('removed outlayer and criteon.')","for i in range(K):
    x = concepts[gpuidx](support_xb)
    x = x.detach()
    logits = outlayer(x)
    loss = criteon(logits, support_yb)
    outlayer.zero_grad()
    loss.backward()
    optimizer.step()","for i in range(K):
    x = concepts[gpuidx](support_xb)
    x = x.detach()
    logits = outlayer(x)
    loss = criteon(logits, support_yb)
    outlayer.zero_grad()
    loss.backward()
    optimizer.step()",1,,,,,,,,,,
django-postgres-extra,https://github.com/SectorLabs/django-postgres-extra/tree/master/psqlextra/backend/schema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-postgres-extra/psqlextra/backend/schema.py,PostgresSchemaEditor,"def replace_materialized_view_model(self, model: Model) -> None:
    """"""Replaces a materialized view with a newer version.

        This is used to alter the backing query of a materialized view.

        Replacing a materialized view is a lot trickier than a normal view.
        For normal views we can use `CREATE OR REPLACE VIEW`, but for
        materialized views, we have to create the new view, copy all
        indexes and constraints and drop the old one.

        This operation is atomic as it runs in a transaction.
        """"""
    with self.connection.cursor() as cursor:
        constraints = self.introspection.get_constraints(cursor, model._meta.db_table)
    with transaction.atomic():
        self.delete_materialized_view_model(model)
        self.create_materialized_view_model(model)
        for (constraint_name, constraint_options) in constraints.items():
            if not constraint_options['definition']:
                raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
            self.execute(constraint_options['definition'])","for (constraint_name, constraint_options) in constraints.items():
    if not constraint_options['definition']:
        raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
    self.execute(constraint_options['definition'])","for i, (constraint_name, constraint_options) in enumerate(constraints.items()):
    if not constraint_options['definition']:
        raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
    self.execute(constraint_options['definition'])",1,,,,,,,,,,
qutebrowser,https://github.com/qutebrowser/qutebrowser/tree/master/qutebrowser/browser/hints.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutebrowser/qutebrowser/browser/hints.py,WordHinter,"def hint(self, elems: _ElemsType) -> _HintStringsType:
    """"""Produce hint labels based on the html tags.

        Produce hint words based on the link text and random words
        from the words arg as fallback.

        Args:
            elems: The elements to get hint strings for.

        Return:
            A list of hint strings, in the same order as the elements.
        """"""
    self.ensure_initialized()
    hints = []
    used_hints: Set[str] = set()
    words = iter(self.words)
    for elem in elems:
        hint = self.new_hint_for(elem, used_hints, words)
        if not hint:
            raise HintingError('Not enough words in the dictionary.')
        used_hints.add(hint)
        hints.append(hint)
    return hints","for elem in elems:
    hint = self.new_hint_for(elem, used_hints, words)
    if not hint:
        raise HintingError('Not enough words in the dictionary.')
    used_hints.add(hint)
    hints.append(hint)","for i, elem in enumerate(elems):
    hint = self.new_hint_for(elem, used_hints, words)
    if not hint:
        raise HintingError('Not enough words in the dictionary.')
    used_hints.add(hint)
    hints.append(hint)",1,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for j in range(self['ny']):
    x = 0.0
    for i in range(self['nx']):
        if i == 0 and j == 0:
            x += dx
            continue
        undoinfo = []
        newblocks = []
        for bid in blocks:
            undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
            newblocks.append((pos, None))
            pos += 1
        app.addUndo(undoinfo)
        app.gcode.moveLines(newblocks, x, y)
        x += dx
    y += dy","y = 0.0
for j in range(self['ny']):
    x = 0.0
    for i, _ in enumerate(range(self['nx'])):
        if i == 0 and j == 0:
            x += dx
            continue
        undoinfo = []
        newblocks = []
        for bid in blocks:
            undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
            newblocks.append((pos, None))
            pos += 1
        app.addUndo(undoinfo)
        app.gcode.moveLines(newblocks, x, y)
        x += dx
    y += dy",1,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for i in range(self['nx']):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx","for i in range(self['nx']):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx

can be refactored as:

for i, _ in enumerate(range(self['nx'])):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx",1,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for bid in blocks:
    undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
    newblocks.append((pos, None))
    pos += 1","for i,bid in enumerate(blocks):
    undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
    newblocks.append((pos, None))
    pos += 1",1,,,,,,,,,,
FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019,https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/tree/master/tools/benchmark/stat_tree.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/tools/benchmark/stat_tree.py,StatNode,"def inference_memory(self):
    total_inference_memory = self._inference_memory
    for child in self.children:
        total_inference_memory += child.inference_memory
    return total_inference_memory","for child in self.children:
    total_inference_memory += child.inference_memory","for i, child in enumerate(self.children):
    total_inference_memory += child.inference_memory",1,,,,,,,,,,
luigi,https://github.com/spotify/luigi/tree/master/luigi/contrib/scalding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/luigi/contrib/scalding.py,ScaldingJobRunner,"def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    for line in open(source).readlines():
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')","for line in open(source).readlines():
    p = re.search('package\\s+([^\\s\\(]+)', line)
    if p:
        package = p.groups()[0]
    p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
    if p:
        job_class = p.groups()[0]
        if job_class == job_name:
            break","with open(source) as f:
    for i,line in enumerate(f.readlines()):
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break",1,,,,,,,,,,
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/modnas/estim/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/modnas/estim/base.py,,"def build_criterions_all(crit_configs, device_ids=None):
    """"""Build Criterions from configs.""""""
    crits_all = []
    crits_train = []
    crits_eval = []
    crits_valid = []
    for crit_conf in streamline_spec(crit_configs):
        crit = backend.get_criterion(crit_conf, device_ids=device_ids)
        crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
        if not isinstance(crit_mode, list):
            crit_mode = [crit_mode]
        if 'all' in crit_mode:
            crits_all.append(crit)
        if 'train' in crit_mode:
            crits_train.append(crit)
        if 'eval' in crit_mode:
            crits_eval.append(crit)
        if 'valid' in crit_mode:
            crits_valid.append(crit)
    return (crits_all, crits_train, crits_eval, crits_valid)","for crit_conf in streamline_spec(crit_configs):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)","for i,crit_conf in enumerate(streamline_spec(crit_configs)):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)",1,,,,,,,,,,
rdflib,https://github.com/RDFLib/rdflib/tree/master/rdflib/compare.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rdflib/rdflib/compare.py,_TripleCanonicalizer,"def _traces(self, coloring: List[Color], stats: Optional[Stats]=None, depth: List[int]=[0]) -> List[Color]:
    if stats is not None and 'prunings' not in stats:
        stats['prunings'] = 0
    depth[0] += 1
    candidates = self._get_candidates(coloring)
    best: List[List[Color]] = []
    best_score = None
    best_experimental = None
    best_experimental_score = None
    last_coloring = None
    generator: Dict[Node, Set[Node]] = defaultdict(set)
    visited: Set[Node] = set()
    for (candidate, color) in candidates:
        if candidate in generator:
            v = generator[candidate] & visited
            if len(v) > 0:
                visited.add(candidate)
                continue
        visited.add(candidate)
        coloring_copy: List[Color] = []
        color_copy = None
        for c in coloring:
            c_copy = c.copy()
            coloring_copy.append(c_copy)
            if c == color:
                color_copy = c_copy
        new_color = self._individuate(color_copy, candidate)
        coloring_copy.append(new_color)
        refined_coloring = self._refine(coloring_copy, [new_color])
        color_score = tuple([c.key() for c in refined_coloring])
        experimental = self._experimental_path(coloring_copy)
        experimental_score = set([c.key() for c in experimental])
        if last_coloring:
            generator = self._create_generator([last_coloring, experimental], generator)
        last_coloring = experimental
        if best_score is None or best_score < color_score:
            best = [refined_coloring]
            best_score = color_score
            best_experimental_score = experimental_score
        elif best_score > color_score:
            if stats is not None:
                stats['prunings'] += 1
        elif experimental_score != best_experimental_score:
            best.append(refined_coloring)
        elif stats is not None:
            stats['prunings'] += 1
    discrete: List[List[Color]] = [x for x in best if self._discrete(x)]
    if len(discrete) == 0:
        best_score = None
        best_depth = None
        for coloring in best:
            d = [depth[0]]
            new_color = self._traces(coloring, stats=stats, depth=d)
            color_score = tuple([c.key() for c in refined_coloring])
            if best_score is None or color_score > best_score:
                discrete = [new_color]
                best_score = color_score
                best_depth = d[0]
        depth[0] = best_depth
    return discrete[0]","for coloring in best:
    d = [depth[0]]
    new_color = self._traces(coloring, stats=stats, depth=d)
    color_score = tuple([c.key() for c in refined_coloring])
    if best_score is None or color_score > best_score:
        discrete = [new_color]
        best_score = color_score
        best_depth = d[0]","for i,coloring in enumerate(best):
    d = [depth[0]]
    new_color = self._traces(coloring, stats=stats, depth=d)
    color_score = tuple([c.key() for c in refined_coloring])
    if best_score is None or color_score > best_score:
        discrete = [new_color]
        best_score = color_score
        best_depth = d[0]",1,,,,,,,,,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/monitoradds.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/monitoradds.py,,"def delmod(bot, event, *args):
    """"""remove user id(s) from the whitelist of who can add to a hangout""""""
    if not bot.get_config_option('mods'):
        return
    mods = bot.get_config_option('mods')
    mods_new = []
    for mod in mods:
        if args[0] != mod:
            mods_new.append(mod)
    bot.config.set_by_path(['mods'], mods_new)
    bot.config.save()
    html_message = _('<i>Moderators updated: {} removed</i>')
    yield from bot.coro_send_message(event.conv, html_message.format(args[0]))","for mod in mods:
    if args[0] != mod:
        mods_new.append(mod)","for i, mod in enumerate(mods):
    if args[0] != mod:
        mods_new.append(mod)",1,,,,,,,,,,
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/jobs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/jobs.py,DeleteJobHandler,"def post(self):
    """"""Handle a post request.""""""
    key = helpers.get_integer_key(request)
    job = ndb.Key(data_types.Job, key).get()
    if not job:
        raise helpers.EarlyExitException('Job not found.', 400)
    for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
        if job.name in fuzzer.jobs:
            fuzzer.jobs.remove(job.name)
            fuzzer.put()
    query = data_types.FuzzerJob.query()
    query = query.filter(data_types.FuzzerJob.job == job.name)
    for mapping in ndb_utils.get_all_from_query(query):
        mapping.key.delete()
    job.key.delete()
    helpers.log('Deleted job %s' % job.name, helpers.MODIFY_OPERATION)
    return self.redirect('/jobs')","for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
    if job.name in fuzzer.jobs:
        fuzzer.jobs.remove(job.name)
        fuzzer.put()","for i,fuzzer in enumerate(ndb_utils.get_all_from_model(data_types.Fuzzer)):
    if job.name in fuzzer.jobs:
        fuzzer.jobs.remove(job.name)
        fuzzer.put()",1,,,,,,,,,,
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/jobs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/jobs.py,DeleteJobHandler,"def post(self):
    """"""Handle a post request.""""""
    key = helpers.get_integer_key(request)
    job = ndb.Key(data_types.Job, key).get()
    if not job:
        raise helpers.EarlyExitException('Job not found.', 400)
    for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
        if job.name in fuzzer.jobs:
            fuzzer.jobs.remove(job.name)
            fuzzer.put()
    query = data_types.FuzzerJob.query()
    query = query.filter(data_types.FuzzerJob.job == job.name)
    for mapping in ndb_utils.get_all_from_query(query):
        mapping.key.delete()
    job.key.delete()
    helpers.log('Deleted job %s' % job.name, helpers.MODIFY_OPERATION)
    return self.redirect('/jobs')","for mapping in ndb_utils.get_all_from_query(query):
    mapping.key.delete()","for i,mapping in enumerate(ndb_utils.get_all_from_query(query)):
    mapping.key.delete()",1,,,,,,,,,,
videoflow,https://github.com/videoflow/videoflow/tree/master/tests/test_release_resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videoflow/tests/test_release_resources.py,,"def test_bboxannotator_resources():
    for datasetid in BoundingBoxAnnotator.supported_datasets:
        filename = f'labels_{datasetid}.pbtxt'
        url_path = BASE_URL_DETECTION + filename
        get_file(filename, url_path)","for datasetid in BoundingBoxAnnotator.supported_datasets:
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)","for i, datasetid in enumerate(BoundingBoxAnnotator.supported_datasets):
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)",1,,,,,,,,,,
videos,https://github.com/3b1b/videos/tree/master/_2017/nn/network.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/nn/network.py,,"def save_organized_images(n_images_per_number=10):
    (training_data, validation_data, test_data) = load_data_wrapper()
    image_map = dict([(k, []) for k in range(10)])
    for (im, output_arr) in training_data:
        if min(list(map(len, list(image_map.values())))) >= n_images_per_number:
            break
        value = int(np.argmax(output_arr))
        if len(image_map[value]) >= n_images_per_number:
            continue
        image_map[value].append(im)
    data_file = open(IMAGE_MAP_DATA_FILE, mode='wb')
    pickle.dump(image_map, data_file)
    data_file.close()","for (im, output_arr) in training_data:
    if min(list(map(len, list(image_map.values())))) >= n_images_per_number:
        break
    value = int(np.argmax(output_arr))
    if len(image_map[value]) >= n_images_per_number:
        continue
    image_map[value].append(im)","for i, (im, output_arr) in enumerate(training_data):
    if min(list(map(len, list(image_map.values())))) >= n_images_per_number:
        break
    value = int(np.argmax(output_arr))
    if len(image_map[value]) >= n_images_per_number:
        continue
    image_map[value].append(im)",1,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for post_id in post_ids:
    for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if i % 1000000 == 0:
            print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)","for post_index, post_id in enumerate(post_ids):
    for (comment_index, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if comment_index % 1000000 == 0:
            print('read %d lines, found %d' % (comment_index, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)",1,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for name in subreddit_names:
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]","for i,name in enumerate(subreddit_names):
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]",1,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/utils/data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/data.py,,"def traverse_dict_and_list(data, key, default=None, delimiter=DEFAULT_TARGET_DELIM):
    """"""
    Traverse a dict or list using a colon-delimited (or otherwise delimited,
    using the 'delimiter' param) target string. The target 'foo:bar:0' will
    return data['foo']['bar'][0] if this value exists, and will otherwise
    return the dict in the default argument.
    Function will automatically determine the target type.
    The target 'foo:bar:0' will return data['foo']['bar'][0] if data like
    {'foo':{'bar':['baz']}} , if data like {'foo':{'bar':{'0':'baz'}}}
    then return data['foo']['bar']['0']
    """"""
    ptr = data
    if isinstance(key, str):
        key = key.split(delimiter)
    if isinstance(key, int):
        key = [key]
    for each in key:
        if isinstance(ptr, list):
            try:
                idx = int(each)
            except ValueError:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[each]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    return default
            else:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[idx]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    try:
                        ptr = ptr[idx]
                    except IndexError:
                        return default
        else:
            try:
                ptr = ptr[each]
            except KeyError:
                import salt.utils.args
                try:
                    loaded_key = salt.utils.args.yamlify_arg(each)
                except Exception:
                    return default
                if loaded_key == each:
                    return default
                else:
                    try:
                        ptr = ptr[loaded_key]
                    except (KeyError, TypeError):
                        return default
            except TypeError:
                return default
    return ptr","for embedded in (x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass","for i, embedded in enumerate(x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass",1,,,,,,,,,,
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/test_streams.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/tests/test_streams.py,,"def test_all_linked_stream_parameters_owners():
    """"""Test to ensure operations can accept parameters in streams dictionary""""""
    stream_classes = param.concrete_descendents(LinkedStream)
    for stream_class in stream_classes.values():
        for (name, p) in stream_class.param.params().items():
            if name != 'name' and p.owner != stream_class:
                msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
                raise Exception(msg % (stream_class, name, p.owner))","for stream_class in stream_classes.values():
    for (name, p) in stream_class.param.params().items():
        if name != 'name' and p.owner != stream_class:
            msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
            raise Exception(msg % (stream_class, name, p.owner))","for i, stream_class in enumerate(stream_classes.values()):
    for (name, p) in stream_class.param.params().items():
        if name != 'name' and p.owner != stream_class:
            msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
            raise Exception(msg % (stream_class, name, p.owner))",1,,,,,,,,,,
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/test_streams.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/tests/test_streams.py,,"def test_all_linked_stream_parameters_owners():
    """"""Test to ensure operations can accept parameters in streams dictionary""""""
    stream_classes = param.concrete_descendents(LinkedStream)
    for stream_class in stream_classes.values():
        for (name, p) in stream_class.param.params().items():
            if name != 'name' and p.owner != stream_class:
                msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
                raise Exception(msg % (stream_class, name, p.owner))","for (name, p) in stream_class.param.params().items():
    if name != 'name' and p.owner != stream_class:
        msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
        raise Exception(msg % (stream_class, name, p.owner))","for i, (name, p) in enumerate(stream_class.param.params().items()):
    if name != 'name' and p.owner != stream_class:
        msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
        raise Exception(msg % (stream_class, name, p.owner))",1,,,,,,,,,,
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/python-stdlib/quopri/quopri.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/micropython-lib/python-stdlib/quopri/quopri.py,,"def main():
    import sys
    import getopt
    try:
        (opts, args) = getopt.getopt(sys.argv[1:], 'td')
    except getopt.error as msg:
        sys.stdout = sys.stderr
        print(msg)
        print('usage: quopri [-t | -d] [file] ...')
        print('-t: quote tabs')
        print('-d: decode; default encode')
        sys.exit(2)
    deco = 0
    tabs = 0
    for (o, a) in opts:
        if o == '-t':
            tabs = 1
        if o == '-d':
            deco = 1
    if tabs and deco:
        sys.stdout = sys.stderr
        print('-t and -d are mutually exclusive')
        sys.exit(2)
    if not args:
        args = ['-']
    sts = 0
    for file in args:
        if file == '-':
            fp = sys.stdin.buffer
        else:
            try:
                fp = open(file, 'rb')
            except IOError as msg:
                sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
                sts = 1
                continue
        try:
            if deco:
                decode(fp, sys.stdout.buffer)
            else:
                encode(fp, sys.stdout.buffer, tabs)
        finally:
            if file != '-':
                fp.close()
    if sts:
        sys.exit(sts)","for file in args:
    if file == '-':
        fp = sys.stdin.buffer
    else:
        try:
            fp = open(file, 'rb')
        except IOError as msg:
            sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
            sts = 1
            continue
    try:
        if deco:
            decode(fp, sys.stdout.buffer)
        else:
            encode(fp, sys.stdout.buffer, tabs)
    finally:
        if file != '-':
            fp.close()","for i,file in enumerate(args):
    if file == '-':
        fp = sys.stdin.buffer
    else:
        try:
            fp = open(file, 'rb')
        except IOError as msg:
            sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
            sts = 1
            continue
    try:
        if deco:
            decode(fp, sys.stdout.buffer)
        else:
            encode(fp, sys.stdout.buffer, tabs)
    finally:
        if file != '-':
            fp.close()",1,,,,,,,,,,
hydra,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/defaults_list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/defaults_list.py,,"def _update_overrides(defaults_list: List[InputDefault], overrides: Overrides, parent: InputDefault, interpolated_subtree: bool) -> None:
    seen_override = False
    last_override_seen = None
    for d in defaults_list:
        if d.is_self():
            continue
        d.update_parent(parent.get_group_path(), parent.get_final_package())
        legacy_hydra_override = False
        if isinstance(d, GroupDefault):
            assert d.group is not None
            if not version.base_at_least('1.2'):
                legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
        if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
            assert isinstance(last_override_seen, GroupDefault)
            pcp = parent.get_config_path()
            okey = last_override_seen.get_override_key()
            oval = last_override_seen.get_name()
            raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
        if isinstance(d, GroupDefault):
            if legacy_hydra_override:
                d.override = True
                url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
                msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
                deprecation_warning(msg)
            if d.override:
                if not legacy_hydra_override:
                    seen_override = True
                last_override_seen = d
                if interpolated_subtree:
                    raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
                overrides.add_override(parent.get_config_path(), d)","for d in defaults_list:
    if d.is_self():
        continue
    d.update_parent(parent.get_group_path(), parent.get_final_package())
    legacy_hydra_override = False
    if isinstance(d, GroupDefault):
        assert d.group is not None
        if not version.base_at_least('1.2'):
            legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
    if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
        assert isinstance(last_override_seen, GroupDefault)
        pcp = parent.get_config_path()
        okey = last_override_seen.get_override_key()
        oval = last_override_seen.get_name()
        raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
    if isinstance(d, GroupDefault):
        if legacy_hydra_override:
            d.override = True
            url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
            msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
            deprecation_warning(msg)
        if d.override:
            if not legacy_hydra_override:
                seen_override = True
            last_override_seen = d
            if interpolated_subtree:
                raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
            overrides.add_override(parent.get_config_path(), d)","for i,d in enumerate(defaults_list):
    if d.is_self():
        continue
    d.update_parent(parent.get_group_path(), parent.get_final_package())
    legacy_hydra_override = False
    if isinstance(d, GroupDefault):
        assert d.group is not None
        if not version.base_at_least('1.2'):
            legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
    if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
        assert isinstance(last_override_seen, GroupDefault)
        pcp = parent.get_config_path()
        okey = last_override_seen.get_override_key()
        oval = last_override_seen.get_name()
        raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
    if isinstance(d, GroupDefault):
        if legacy_hydra_override:
            d.override = True
            url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
            msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
            deprecation_warning(msg)
        if d.override:
            if not legacy_hydra_override:
                seen_override = True
            last_override_seen = d
            if interpolated_subtree:
                raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
            overrides.add_override(parent.get_config_path(), d)",1,,,,,,,,,,
s3fs,https://github.com/fsspec/s3fs/tree/master/s3fs/tests/test_s3fs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3fs/s3fs/tests/test_s3fs.py,,"def test_s3_big_ls(s3):
    for x in range(1200):
        s3.touch(test_bucket_name + '/thousand/%i.part' % x)
    assert len(s3.find(test_bucket_name)) > 1200
    s3.rm(test_bucket_name + '/thousand/', recursive=True)
    assert len(s3.find(test_bucket_name + '/thousand/')) == 0","for x in range(1200):
    s3.touch(test_bucket_name + '/thousand/%i.part' % x)","for i in range(1200):
    s3.touch(test_bucket_name + '/thousand/%i.part' % i)",1,,,,,,,,,,
git-imerge,https://github.com/mhagger/git-imerge/tree/master//gitimerge.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/git-imerge//gitimerge.py,MergeState,"def find_index(self, commit):
    """"""Return (i1,i2) for the specified commit.

        Raise CommitNotFoundError if it is not known.""""""
    for i2 in range(0, self.len2):
        for i1 in range(0, self.len1):
            if (i1, i2) in self:
                record = self[i1, i2]
                if record.sha1 == commit:
                    return (i1, i2)
    raise CommitNotFoundError(commit)","for i2 in range(0, self.len2):
    for i1 in range(0, self.len1):
        if (i1, i2) in self:
            record = self[i1, i2]
            if record.sha1 == commit:
                return (i1, i2)","for i2, _ in enumerate(range(0, self.len2)):
    for i1 in range(0, self.len1):
        if (i1, i2) in self:
            record = self[i1, i2]
            if record.sha1 == commit:
                return (i1, i2)",1,,,,,,,,,,
torch-points3d,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/base_architectures/backbone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/base_architectures/backbone.py,BackboneBasedModel,"def _flatten_compact_options(self, opt):
    """"""Converts from a dict of lists, to a list of dicts
        """"""
    flattenedOpts = []
    for index in range(int(1000000.0)):
        try:
            flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
        except IndexError:
            break
    return flattenedOpts","for index in range(int(1000000.0)):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break","for index in range(int(1000000.0)):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break

can be refactored as:

for index in enumerate(range(int(1000000.0))):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index[0])))
    except IndexError:
        break",1,,,,,,,,,,
DeepPrivacy,https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/dataset/places2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPrivacy/deep_privacy/dataset/places2.py,Places2Dataset,"def _load_impaths(self):
    relevant_suffixes = ['.png', '.jpg', '.jpeg']
    image_dir = self.dirpath
    image_paths = []
    for (dirpath, dirnames, filenames) in os.walk(image_dir):
        for filename in filenames:
            path = pathlib.Path(dirpath, filename)
            if path.suffix in relevant_suffixes:
                assert path.is_file()
                image_paths.append(path)
    image_paths.sort(key=lambda x: int(x.stem.split('_')[-1]))
    return image_paths","for filename in filenames:
    path = pathlib.Path(dirpath, filename)
    if path.suffix in relevant_suffixes:
        assert path.is_file()
        image_paths.append(path)","for i,filename in enumerate(filenames):
    path = pathlib.Path(dirpath, filename)
    if path.suffix in relevant_suffixes:
        assert path.is_file()
        image_paths.append(path)",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for item in self.itemNet:
    for user in self.itemNet[item]:
        if self.itemNet[item][user] >= 1:
            self.filteredRatings[user].append(item)","for i,item in enumerate(self.itemNet):
    for user in self.itemNet[item]:
        if self.itemNet[item][user] >= 1:
            self.filteredRatings[user].append(self.itemNet[i])",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.filteredRatings:
    s1 = set(self.filteredRatings[user1])
    for user2 in self.filteredRatings:
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight","for i,user1 in enumerate(self.filteredRatings):
    s1 = set(self.filteredRatings[user1])
    for j,user2 in enumerate(self.filteredRatings):
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.CUNet:
    sims = []
    u1 = self.data.user[user1]
    self.W[u1] = model.wv[user1]
    for user2 in self.CUNet:
        if user1 != user2:
            u2 = self.data.user[user2]
            self.W[u2] = model.wv[user2]
            sims.append((user2, cosine(self.W[u1], self.W[u2])))
    self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
    i += 1
    if i % 200 == 0:
        print('progress:', i, '/', len(self.CUNet))","for i, user1 in enumerate(self.CUNet):
    sims = []
    u1 = self.data.user[user1]
    self.W[u1] = model.wv[user1]
    for j, user2 in enumerate(self.CUNet):
        if user1 != user2:
            u2 = self.data.user[user2]
            self.W[u2] = model.wv[user2]
            sims.append((user2, cosine(self.W[u1], self.W[u2])))
    self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
    if (i+1) % 200 == 0:
        print('progress:', i+1, '/', len(self.CUNet))",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user in self.itemNet[item]:
    if self.itemNet[item][user] >= 1:
        self.filteredRatings[user].append(item)","for i,user in enumerate(self.itemNet[item]):
    if self.itemNet[item][user] >= 1:
        self.filteredRatings[user].append(item)",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user2 in self.CUNet:
    if user1 != user2:
        u2 = self.data.user[user2]
        self.W[u2] = model.wv[user2]
        sims.append((user2, cosine(self.W[u1], self.W[u2])))","for i,user2 in enumerate(self.CUNet):
    if user1 != user2:
        u2 = self.data.user[user2]
        self.W[u2] = model.wv[user2]
        sims.append((user2, cosine(self.W[u1], self.W[u2]))))",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for entry in self.data.trainingData:
    (user, item, rating) = entry
    u = self.data.user[user]
    i = self.data.item[item]
    error = rating - self.P[u].dot(self.Q[i])
    self.loss += error ** 2
    p = self.P[u]
    q = self.Q[i]
    self.P[u] += self.lRate * (error * q - self.regU * p)
    self.Q[i] += self.lRate * (error * p - self.regI * q)","for idx, entry in enumerate(self.data.trainingData):
    (user, item, rating) = entry
    u = self.data.user[user]
    i = self.data.item[item]
    error = rating - self.P[u].dot(self.Q[i])
    self.loss += error ** 2
    p = self.P[u]
    q = self.Q[i]
    self.P[u] += self.lRate * (error * q - self.regU * p)
    self.Q[i] += self.lRate * (error * p - self.regI * q)",1,,,,,,,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/loom/loom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/loom/loom.py,Loom,"def _setup_metadata(self):
    """"""Construct the serialized metadata about this loom for the scheduler.""""""
    loom_metadata = loom_pb2.LoomMetadata()
    loom_metadata.max_depth = self._max_depth
    for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
        type_shape_metadata = loom_metadata.type_shape_metadata.add()
        type_shape_metadata.dtype = ts.dtype_enum
        type_shape_metadata.shape.extend(ts.shape)
        type_shape_metadata.tag = ts.tag
        type_shape_metadata.name = str(ts)
        type_shape_metadata.tensor_names.extend(tensor_names)
        type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict
    for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
        op_metadata = loom_metadata.op_metadata.add()
        op_metadata.name = op_name
        op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
        op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))
    self._loom_metadata_str = loom_metadata.SerializeToString()","for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
    type_shape_metadata = loom_metadata.type_shape_metadata.add()
    type_shape_metadata.dtype = ts.dtype_enum
    type_shape_metadata.shape.extend(ts.shape)
    type_shape_metadata.tag = ts.tag
    type_shape_metadata.name = str(ts)
    type_shape_metadata.tensor_names.extend(tensor_names)
    type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict","for i, (ts, tensor_names) in enumerate(zip(self._type_shapes, self._ts_idx_to_tensor_names)):
    type_shape_metadata = loom_metadata.type_shape_metadata.add()
    type_shape_metadata.dtype = ts.dtype_enum
    type_shape_metadata.shape.extend(ts.shape)
    type_shape_metadata.tag = ts.tag
    type_shape_metadata.name = str(ts)
    type_shape_metadata.tensor_names.extend(tensor_names)
    type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict",1,,,,,,,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/loom/loom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/loom/loom.py,Loom,"def _setup_metadata(self):
    """"""Construct the serialized metadata about this loom for the scheduler.""""""
    loom_metadata = loom_pb2.LoomMetadata()
    loom_metadata.max_depth = self._max_depth
    for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
        type_shape_metadata = loom_metadata.type_shape_metadata.add()
        type_shape_metadata.dtype = ts.dtype_enum
        type_shape_metadata.shape.extend(ts.shape)
        type_shape_metadata.tag = ts.tag
        type_shape_metadata.name = str(ts)
        type_shape_metadata.tensor_names.extend(tensor_names)
        type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict
    for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
        op_metadata = loom_metadata.op_metadata.add()
        op_metadata.name = op_name
        op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
        op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))
    self._loom_metadata_str = loom_metadata.SerializeToString()","for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
    op_metadata = loom_metadata.op_metadata.add()
    op_metadata.name = op_name
    op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
    op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))","for i, (op_name, op) in enumerate(zip(self._loom_op_names, self._loom_ops)):
    op_metadata = loom_metadata.op_metadata.add()
    op_metadata.name = op_name
    op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
    op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))",1,,,,,,,,,,
electrum,https://github.com/spesmilo/electrum/tree/master/electrum/lnrater.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/lnrater.py,LNRater,"def _rate_nodes(self):
    """"""Rate nodes by collected statistics.""""""
    max_capacity = 0
    max_num_chan = 0
    min_fee_rate = float('inf')
    for stats in self._node_stats.values():
        max_capacity = max(max_capacity, stats.total_capacity_msat)
        max_num_chan = max(max_num_chan, stats.number_channels)
        min_fee_rate = min(min_fee_rate, stats.mean_fee_rate)
    for (n, stats) in self._node_stats.items():
        heuristics = []
        heuristics_weights = []
        heuristics.append(stats.number_channels / max_num_chan)
        heuristics_weights.append(0.2)
        heuristics.append(stats.total_capacity_msat / max_capacity)
        heuristics_weights.append(0.8)
        fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
        heuristics.append(fees)
        heuristics_weights.append(1.0)
        self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)","for stats in self._node_stats.values():
    max_capacity = max(max_capacity, stats.total_capacity_msat)
    max_num_chan = max(max_num_chan, stats.number_channels)
    min_fee_rate = min(min_fee_rate, stats.mean_fee_rate)","for i, stats in enumerate(self._node_stats.values()):
    max_capacity = max(max_capacity, stats.total_capacity_msat)
    max_num_chan = max(max_num_chan, stats.number_channels)
    min_fee_rate = min(min_fee_rate, stats.mean_fee_rate)",1,,,,,,,,,,
electrum,https://github.com/spesmilo/electrum/tree/master/electrum/lnrater.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/lnrater.py,LNRater,"def _rate_nodes(self):
    """"""Rate nodes by collected statistics.""""""
    max_capacity = 0
    max_num_chan = 0
    min_fee_rate = float('inf')
    for stats in self._node_stats.values():
        max_capacity = max(max_capacity, stats.total_capacity_msat)
        max_num_chan = max(max_num_chan, stats.number_channels)
        min_fee_rate = min(min_fee_rate, stats.mean_fee_rate)
    for (n, stats) in self._node_stats.items():
        heuristics = []
        heuristics_weights = []
        heuristics.append(stats.number_channels / max_num_chan)
        heuristics_weights.append(0.2)
        heuristics.append(stats.total_capacity_msat / max_capacity)
        heuristics_weights.append(0.8)
        fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
        heuristics.append(fees)
        heuristics_weights.append(1.0)
        self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)","for (n, stats) in self._node_stats.items():
    heuristics = []
    heuristics_weights = []
    heuristics.append(stats.number_channels / max_num_chan)
    heuristics_weights.append(0.2)
    heuristics.append(stats.total_capacity_msat / max_capacity)
    heuristics_weights.append(0.8)
    fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
    heuristics.append(fees)
    heuristics_weights.append(1.0)
    self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)","for i, (n, stats) in enumerate(self._node_stats.items()):
    heuristics = []
    heuristics_weights = []
    heuristics.append(stats.number_channels / max_num_chan)
    heuristics_weights.append(0.2)
    heuristics.append(stats.total_capacity_msat / max_capacity)
    heuristics_weights.append(0.8)
    fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
    heuristics.append(fees)
    heuristics_weights.append(1.0)
    self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)",1,,,,,,,,,,
PathPlanning,https://github.com/zhm-real/PathPlanning/tree/master/Sampling_based_Planning/rrt_2D/rrt_star.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPlanning/Sampling_based_Planning/rrt_2D/rrt_star.py,RrtStar,"def rewire(self, node_new, neighbor_index):
    for i in neighbor_index:
        node_neighbor = self.vertex[i]
        if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
            node_neighbor.parent = node_new","for i in neighbor_index:
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new","for index, i in enumerate(neighbor_index):
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new",1,,,,,,,,,,
pulsar,https://github.com/quantmind/pulsar/tree/master/docs/_ext/sphinxtogithub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pulsar/docs/_ext/sphinxtogithub.py,FileHandler,"def process(self):
    text = self.opener(self.name).read()
    for replacer in self.replacers:
        text = replacer.process(text)
    self.opener(self.name, 'w').write(text)","for replacer in self.replacers:
    text = replacer.process(text)","for i,replacer in enumerate(self.replacers):
    text = self.replacers[i].process(text)",1,,,,,,,,,,
dlrm,https://github.com/facebookresearch/dlrm/tree/master//dlrm_data_pytorch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dlrm//dlrm_data_pytorch.py,,"def generate_dist_input_batch(m_den, ln_emb, n, num_indices_per_lookup, num_indices_per_lookup_fixed, rand_data_dist, rand_data_min, rand_data_max, rand_data_mu, rand_data_sigma):
    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))
    lS_emb_offsets = []
    lS_emb_indices = []
    for size in ln_emb:
        lS_batch_offsets = []
        lS_batch_indices = []
        offset = 0
        for _ in range(n):
            if num_indices_per_lookup_fixed:
                sparse_group_size = np.int64(num_indices_per_lookup)
            else:
                r = ra.random(1)
                sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
            if rand_data_dist == 'gaussian':
                if rand_data_mu == -1:
                    rand_data_mu = (rand_data_max + rand_data_min) / 2.0
                r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
                sparse_group = np.clip(r, rand_data_min, rand_data_max)
                sparse_group = np.unique(sparse_group).astype(np.int64)
            elif rand_data_dist == 'uniform':
                r = ra.random(sparse_group_size)
                sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
            else:
                raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
            sparse_group_size = np.int64(sparse_group.size)
            lS_batch_offsets += [offset]
            lS_batch_indices += sparse_group.tolist()
            offset += sparse_group_size
        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
        lS_emb_indices.append(torch.tensor(lS_batch_indices))
    return (Xt, lS_emb_offsets, lS_emb_indices)","for size in ln_emb:
    lS_batch_offsets = []
    lS_batch_indices = []
    offset = 0
    for _ in range(n):
        if num_indices_per_lookup_fixed:
            sparse_group_size = np.int64(num_indices_per_lookup)
        else:
            r = ra.random(1)
            sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
        if rand_data_dist == 'gaussian':
            if rand_data_mu == -1:
                rand_data_mu = (rand_data_max + rand_data_min) / 2.0
            r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
            sparse_group = np.clip(r, rand_data_min, rand_data_max)
            sparse_group = np.unique(sparse_group).astype(np.int64)
        elif rand_data_dist == 'uniform':
            r = ra.random(sparse_group_size)
            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
        else:
            raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
        sparse_group_size = np.int64(sparse_group.size)
        lS_batch_offsets += [offset]
        lS_batch_indices += sparse_group.tolist()
        offset += sparse_group_size
    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
    lS_emb_indices.append(torch.tensor(lS_batch_indices))","for i,size in enumerate(ln_emb):
    lS_batch_offsets = []
    lS_batch_indices = []
    offset = 0
    for _ in range(n):
        if num_indices_per_lookup_fixed:
            sparse_group_size = np.int64(num_indices_per_lookup)
        else:
            r = ra.random(1)
            sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
        if rand_data_dist == 'gaussian':
            if rand_data_mu == -1:
                rand_data_mu = (rand_data_max + rand_data_min) / 2.0
            r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
            sparse_group = np.clip(r, rand_data_min, rand_data_max)
            sparse_group = np.unique(sparse_group).astype(np.int64)
        elif rand_data_dist == 'uniform':
            r = ra.random(sparse_group_size)
            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
        else:
            raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
        sparse_group_size = np.int64(sparse_group.size)
        lS_batch_offsets += [offset]
        lS_batch_indices += sparse_group.tolist()
        offset += sparse_group_size
    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
    lS_emb_indices.append(torch.tensor(lS_batch_indices))",1,,,,,,,,,,
airflow,https://github.com/apache/airflow/tree/master/airflow/www/fab_security/manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/fab_security/manager.py,BaseSecurityManager,"def get_role_permissions(self, role) -> Set[Tuple[str, str]]:
    """"""Get all permissions for a certain role""""""
    result = set()
    if role.name in self.builtin_roles:
        for permission in self.builtin_roles[role.name]:
            result.add((permission[1], permission[0]))
    else:
        for permission in self.get_role_permissions_from_db(role.id):
            result.add((permission.action.name, permission.resource.name))
    return result","for permission in self.builtin_roles[role.name]:
    result.add((permission[1], permission[0]))","for i, permission in enumerate(self.builtin_roles[role.name]):
    result.add((permission[1], permission[0]))",1,,,,,,,,,,
airflow,https://github.com/apache/airflow/tree/master/airflow/www/fab_security/manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/fab_security/manager.py,BaseSecurityManager,"def get_role_permissions(self, role) -> Set[Tuple[str, str]]:
    """"""Get all permissions for a certain role""""""
    result = set()
    if role.name in self.builtin_roles:
        for permission in self.builtin_roles[role.name]:
            result.add((permission[1], permission[0]))
    else:
        for permission in self.get_role_permissions_from_db(role.id):
            result.add((permission.action.name, permission.resource.name))
    return result","for permission in self.get_role_permissions_from_db(role.id):
    result.add((permission.action.name, permission.resource.name))","for i, permission in enumerate(self.get_role_permissions_from_db(role.id)):
    result.add((permission.action.name, permission.resource.name))",1,,,,,,,,,,
slither,https://github.com/crytic/slither/tree/master/slither/printers/call/call_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/slither/slither/printers/call/call_graph.py,PrinterCallGraph,"def output(self, filename):
    """"""
        Output the graph in filename
        Args:
            filename(string)
        """"""
    all_contracts_filename = ''
    if not filename.endswith('.dot'):
        all_contracts_filename = f'{filename}.all_contracts.call-graph.dot'
    if filename == '.dot':
        all_contracts_filename = 'all_contracts.dot'
    info = ''
    results = []
    with open(all_contracts_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {all_contracts_filename}\n'
        all_functionss = [compilation_unit.functions for compilation_unit in self.slither.compilation_units]
        all_functions = [item for sublist in all_functionss for item in sublist]
        all_functions_as_dict = {function.canonical_name: function for function in all_functions}
        content = '\n'.join(['strict digraph {'] + [_process_functions(all_functions_as_dict.values())] + ['}'])
        f.write(content)
        results.append((all_contracts_filename, content))
    for derived_contract in self.slither.contracts_derived:
        derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
        with open(derived_output_filename, 'w', encoding='utf8') as f:
            info += f'Call Graph: {derived_output_filename}\n'
            content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
            f.write(content)
            results.append((derived_output_filename, content))
    self.info(info)
    res = self.generate_output(info)
    for (filename_result, content) in results:
        res.add_file(filename_result, content)
    return res","for derived_contract in self.slither.contracts_derived:
    derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
    with open(derived_output_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {derived_output_filename}\n'
        content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
        f.write(content)
        results.append((derived_output_filename, content))","for i, derived_contract in enumerate(self.slither.contracts_derived):
    derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
    with open(derived_output_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {derived_output_filename}\n'
        content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
        f.write(content)
        results.append((derived_output_filename, content))",1,,,,,,,,,,
slither,https://github.com/crytic/slither/tree/master/slither/printers/call/call_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/slither/slither/printers/call/call_graph.py,PrinterCallGraph,"def output(self, filename):
    """"""
        Output the graph in filename
        Args:
            filename(string)
        """"""
    all_contracts_filename = ''
    if not filename.endswith('.dot'):
        all_contracts_filename = f'{filename}.all_contracts.call-graph.dot'
    if filename == '.dot':
        all_contracts_filename = 'all_contracts.dot'
    info = ''
    results = []
    with open(all_contracts_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {all_contracts_filename}\n'
        all_functionss = [compilation_unit.functions for compilation_unit in self.slither.compilation_units]
        all_functions = [item for sublist in all_functionss for item in sublist]
        all_functions_as_dict = {function.canonical_name: function for function in all_functions}
        content = '\n'.join(['strict digraph {'] + [_process_functions(all_functions_as_dict.values())] + ['}'])
        f.write(content)
        results.append((all_contracts_filename, content))
    for derived_contract in self.slither.contracts_derived:
        derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
        with open(derived_output_filename, 'w', encoding='utf8') as f:
            info += f'Call Graph: {derived_output_filename}\n'
            content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
            f.write(content)
            results.append((derived_output_filename, content))
    self.info(info)
    res = self.generate_output(info)
    for (filename_result, content) in results:
        res.add_file(filename_result, content)
    return res","for (filename_result, content) in results:
    res.add_file(filename_result, content)","for i,(filename_result, content) in enumerate(results):
    res.add_file(filename_result, content)",1,,,,,,,,,,
jittor,https://github.com/Jittor/jittor/tree/master/python/jittor/test/test_transform.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jittor/python/jittor/test/test_transform.py,Tester,"def test_1_channel_ndarray_to_pil_image(self):
    img_data_float = np.random.rand(4, 4, 1).astype(np.float32)
    img_data_byte = np.random.randint(0, 255, (4, 4, 1)).astype(np.uint8)
    img_data_short = np.random.randint(0, 32767, (4, 4, 1)).astype(np.int16)
    img_data_int = np.random.randint(0, 2147483647, (4, 4, 1)).astype(np.int32)
    inputs = [img_data_float, img_data_byte, img_data_short, img_data_int]
    expected_modes = ['F', 'L', 'I;16', 'I']
    for (img_data, mode) in zip(inputs, expected_modes):
        for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
            img = t(img_data)
            self.assertEqual(img.mode, mode)
            self.assertTrue(np.allclose(img_data[:, :, 0], img))","for (img_data, mode) in zip(inputs, expected_modes):
    for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
        img = t(img_data)
        self.assertEqual(img.mode, mode)
        self.assertTrue(np.allclose(img_data[:, :, 0], img))","for i,(img_data, mode) in enumerate(zip(inputs, expected_modes)):
    for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
        img = t(img_data)
        self.assertEqual(img.mode, mode)
        self.assertTrue(np.allclose(img_data[:, :, 0], img))",1,,,,,,,,,,
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/test/training/test_synthetic_reward_training.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/test/training/test_synthetic_reward_training.py,,"def create_sequence_data(state_dim, action_dim, seq_len, batch_size, num_batches):
    SCALE = 2
    weight = SCALE * torch.randn(state_dim + action_dim)
    data = [None for _ in range(num_batches)]
    for i in range(num_batches):
        state = SCALE * torch.randn(seq_len, batch_size, state_dim)
        action = SCALE * torch.randn(seq_len, batch_size, action_dim)
        valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
        feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
        feature_mask = (feature_mask >= seq_len - valid_step).float()
        assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
        feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
        assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
        feature = torch.cat((state, action), dim=2)
        masked_feature = feature * feature_mask
        left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
        right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
        reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
        mask = torch.arange(seq_len).repeat(batch_size, 1)
        mask = (mask >= seq_len - valid_step).float()
        reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
        data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))
    return (weight, data)","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))",1,,,,,,,,,,
zvt,https://github.com/zvtvz/zvt/tree/master/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,ChinaETFDayKdataRecorder,"def on_finish_entity(self, entity):
    kdatas = get_kdata(entity_id=entity.id, level=IntervalLevel.LEVEL_1DAY.value, order=Etf1dKdata.timestamp.asc(), return_type='domain', session=self.session, filters=[Etf1dKdata.cumulative_net_value.is_(None)])
    if kdatas and len(kdatas) > 0:
        start = kdatas[0].timestamp
        end = kdatas[-1].timestamp
        df = self.fetch_cumulative_net_value(entity, start, end)
        if df is not None and (not df.empty):
            for kdata in kdatas:
                if kdata.timestamp in df.index:
                    kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
                    kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']
            self.session.commit()
            self.logger.info(f'{entity.code} - {entity.name}...')","for kdata in kdatas:
    if kdata.timestamp in df.index:
        kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
        kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']","for i,kdata in enumerate(kdatas):
    if kdata.timestamp in df.index:
        kdatas[i].cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
        kdatas[i].change_pct = df.loc[kdata.timestamp, 'JZZZL']",1,,,,,,,,,,
dbt-core,https://github.com/dbt-labs/dbt-core/tree/master/core/dbt/adapters/base/relation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dbt-core/core/dbt/adapters/base/relation.py,BaseRelation,"def matches(self, database: Optional[str]=None, schema: Optional[str]=None, identifier: Optional[str]=None) -> bool:
    search = filter_null_values({ComponentName.Database: database, ComponentName.Schema: schema, ComponentName.Identifier: identifier})
    if not search:
        raise dbt.exceptions.RuntimeException('Tried to match relation, but no search path was passed!')
    exact_match = True
    approximate_match = True
    for (k, v) in search.items():
        if not self._is_exactish_match(k, v):
            exact_match = False
        if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
            approximate_match = False
    if approximate_match and (not exact_match):
        target = self.create(database=database, schema=schema, identifier=identifier)
        dbt.exceptions.approximate_relation_match(target, self)
    return exact_match","for (k, v) in search.items():
    if not self._is_exactish_match(k, v):
        exact_match = False
    if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
        approximate_match = False","for i,(k, v) in enumerate(search.items()):
    if not self._is_exactish_match(k, v):
        exact_match = False
    if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
        approximate_match = False",1,,,,,,,,,,
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines","for check in valid_checks:
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')","for i, check in enumerate(valid_checks):
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')",1,,,,,,,,,,
Kats,https://github.com/facebookresearch/Kats/tree/master/kats/tests/test_consts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kats/kats/tests/test_consts.py,TimeSeriesDataOpsTest,"def test_get_item(self) -> None:
    self.assertEqual(self.ts_date_transform_concat_univ[:len(self.ts_univ_1)], self.ts_univ_1)
    self.assertEqual(self.ts_date_transform_concat_multi[:len(self.ts_multi_1)], self.ts_multi_1)
    for col in self.ts_date_transform_concat_multi.value.columns:
        ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
        self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)
    self.assertEqual(self.ts_date_transform_concat_multi[MULTIVAR_VALUE_DF_COLS], self.ts_date_transform_concat_multi)
    self.assertEqual(self.ts_univ_1[:], self.ts_univ_1)
    self.assertEqual(self.ts_univ_1[0:0], TimeSeriesData(time=pd.Series(name=TIME_COL_NAME), value=pd.Series(name=VALUE_COL_NAME), time_col_name=TIME_COL_NAME))","for col in self.ts_date_transform_concat_multi.value.columns:
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)","for i, col in enumerate(self.ts_date_transform_concat_multi.value.columns):
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)",1,,,,,,,,,,
glance,https://github.com/openstack/glance/tree/master/glance/image_cache/drivers/sqlite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/image_cache/drivers/sqlite.py,Driver,"def delete_stalled_files(self, older_than):
    """"""
        Removes any incomplete cache entries older than a
        supplied modified time.

        :param older_than: Files written to on or before this timestamp
                           will be deleted.
        """"""
    for path in self.get_cache_files(self.incomplete_dir):
        if os.path.getmtime(path) < older_than:
            try:
                fileutils.delete_if_exists(path)
                LOG.info(_LI('Removed stalled cache file %s'), path)
            except Exception as e:
                msg = (_LW('Failed to delete file %(path)s. Got error: %(e)s'), dict(path=path, e=e))
                LOG.warn(msg)","for path in self.get_cache_files(self.incomplete_dir):
    if os.path.getmtime(path) < older_than:
        try:
            fileutils.delete_if_exists(path)
            LOG.info(_LI('Removed stalled cache file %s'), path)
        except Exception as e:
            msg = (_LW('Failed to delete file %(path)s. Got error: %(e)s'), dict(path=path, e=e))
            LOG.warn(msg)","for i,path in enumerate(self.get_cache_files(self.incomplete_dir)):
    if os.path.getmtime(path) < older_than:
        try:
            fileutils.delete_if_exists(path)
            LOG.info(_LI('Removed stalled cache file %s'), path)
        except Exception as e:
            msg = (_LW('Failed to delete file %(path)s. Got error: %(e)s'), dict(path=path, e=e))
            LOG.warn(msg)",1,,,,,,,,,,
leetCode,https://github.com/HuberTRoy/leetCode/tree/master/Array/NumberOfIslands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/leetCode/Array/NumberOfIslands.py,Solution,"def helper(x, y):
    Xy = self.makeXY(x, y)
    for i in Xy:
        try:
            if i[1] < 0 or i[0] < 0:
                continue
            if court[i[1]][i[0]] == '1':
                court[i[1]][i[0]] = '0'
                t = helper(i[0], i[1])
        except IndexError:
            continue
    else:
        return 1","for i in Xy:
    try:
        if i[1] < 0 or i[0] < 0:
            continue
        if court[i[1]][i[0]] == '1':
            court[i[1]][i[0]] = '0'
            t = helper(i[0], i[1])
    except IndexError:
        continue
else:
    return 1","for i, xy in enumerate(Xy):
    try:
        if xy[1] < 0 or xy[0] < 0:
            continue
        if court[xy[1]][xy[0]] == '1':
            court[xy[1]][xy[0]] = '0'
            t = helper(xy[0], xy[1])
    except IndexError:
        continue
else:
    return 1",1,,,,,,,,,,
Project_CodeNet,https://github.com/IBM/Project_CodeNet/tree/master/model-experiments/masked-language-model/infer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Project_CodeNet/model-experiments/masked-language-model/infer.py,,"def encode(text):
    R = [0] * config.MAX_LEN
    text = tokenize(text)
    for i in range(len(text)):
        w = text[i]
        if w in token2id:
            R[i] = token2id[w]
        else:
            R[i] = 1
    return np.array(R)","for i in range(len(text)):
    w = text[i]
    if w in token2id:
        R[i] = token2id[w]
    else:
        R[i] = 1","for i,w in enumerate(text):
    if w in token2id:
        R[i] = token2id[w]
    else:
        R[i] = 1",1,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/hapi/dynamic_flops.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/hapi/dynamic_flops.py,,"def count_parameters(m, x, y):
    total_params = 0
    for p in m.parameters():
        total_params += p.numel()
    m.total_params[0] = abs(int(total_params))","for p in m.parameters():
    total_params += p.numel()","for i,p in enumerate(m.parameters()):
    total_params += p.numel()",1,,,,,,,,,,
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/tests/wikistats_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/tests/wikistats_tests.py,WikiStatsTestCase,"def test_sorting_order(self):
    """"""Test sorting order of languages_by_size.""""""
    family = 'wikipedia'
    ws = WikiStats()
    data = ws.get_dict(family)
    last = sys.maxsize
    last_code = ''
    for code in ws.languages_by_size(family):
        curr = int(data[code]['good'])
        self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
        last = curr
        last_code = code","for code in ws.languages_by_size(family):
    curr = int(data[code]['good'])
    self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
    last = curr
    last_code = code","for i, code in enumerate(ws.languages_by_size(family)):
    curr = int(data[code]['good'])
    self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
    last = curr
    last_code = code",1,,,,,,,,,,
bumblebee-status,https://github.com/tobi-wan-kenobi/bumblebee-status/tree/master/bumblebee_status/modules/contrib/sensors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bumblebee-status/bumblebee_status/modules/contrib/sensors.py,Module,"def _get_temp_from_sensors(self):
    if self._json == True:
        try:
            output = json.loads(util.cli.execute('sensors -j'))
            for key in self.parameter('path').split('/'):
                output = output[key]
            return int(float(output))
        except Exception as e:
            logging.error('unable to read sensors: {}'.format(str(e)))
            return 'unknown'
    else:
        output = util.cli.execute('sensors -u')
        if self._match_pattern:
            temp_pattern = self.parameter('match', 'temp1_input')
            match = re.search('{}.+{}:\\s*([\\d.]+)$'.format(self._match_pattern, temp_pattern), output.replace('\n', ''))
            if match:
                return int(float(match.group(1)))
            else:
                return 'unknown'
        match = self._pattern.findall(output)
        if match:
            return int(float(match[self._match_number]))
    return 'unknown'","for key in self.parameter('path').split('/'):
    output = output[key]","keys = self.parameter('path').split('/')
for i, key in enumerate(keys):
    output = output[key]",1,,,,,,,,,,
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr","for var in self._variables:
    if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
        if var.name is not None:
            continue
        if var.ident.startswith('iarg'):
            var.name = 'arg_%x' % var.offset
        else:
            var.name = 's_%x' % -var.offset
    elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
        if var.name is not None:
            continue
        var.name = var.ident
    elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
        if var.name is not None:
            continue
        if labels is not None and var.addr in labels:
            var.name = labels[var.addr]
            if '@@' in var.name:
                var.name = var.name[:var.name.index('@@')]
        elif isinstance(var.addr, int):
            var.name = 'g_%x' % var.addr
        elif var.ident is not None:
            var.name = var.ident
        else:
            var.name = 'g_%s' % var.addr","for i,var in enumerate(self._variables):
    if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
        if var.name is not None:
            continue
        if var.ident.startswith('iarg'):
            var.name = 'arg_%x' % var.offset
        else:
            var.name = 's_%x' % -var.offset
    elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
        if var.name is not None:
            continue
        var.name = var.ident
    elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
        if var.name is not None:
            continue
        if labels is not None and var.addr in labels:
            var.name = labels[var.addr]
            if '@@' in var.name:
                var.name = var.name[:var.name.index('@@')]
        elif isinstance(var.addr, int):
            var.name = 'g_%x' % var.addr
        elif var.ident is not None:
            var.name = var.ident
        else:
            var.name = 'g_%s' % var.addr",1,,,,,,,,,,
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    runner = self._get_fextractor_instance(fextractor_type)
    runner.run(parallelize=self.parallelize, processes=self.processes)
    results = runner.results
    self.type2results_dict[fextractor_type] = results","for i,fextractor_type in enumerate(self.feature_dict):
    runner = self._get_fextractor_instance(fextractor_type)
    runner.run(parallelize=self.parallelize, processes=self.processes)
    results = runner.results
    self.type2results_dict[fextractor_type] = results",1,,,,,,,,,,
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]","for i,fextractor_type in enumerate(self.feature_dict):
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]",1,,,,,,,,,,
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for atom_feature in self._get_atom_features(fextractor_type):
    scores_key = self._get_scores_key(fextractor_type, atom_feature)
    for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
        try:
            result_dicts[result_index][scores_key] = result[scores_key]
        except KeyError:
            scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
            result_dicts[result_index][scores_key] = result[scores_key_alt]","for (atom_feature_index, atom_feature) in enumerate(self._get_atom_features(fextractor_type)):
    scores_key = self._get_scores_key(fextractor_type, atom_feature)
    for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
        try:
            result_dicts[result_index][scores_key] = result[scores_key]
        except KeyError:
            scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
            result_dicts[result_index][scores_key] = result[scores_key_alt]",1,,,,,,,,,,
qtile,https://github.com/qtile/qtile/tree/master/libqtile/widget/prompt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qtile/libqtile/widget/prompt.py,FileCompleter,"def complete(self, txt: str) -> str:
    """"""Returns the next completion for txt, or None if there is no completion""""""
    if self.lookup is None:
        self.lookup = []
        if txt == '' or txt[0] not in '~/':
            txt = '~/' + txt
        path = os.path.expanduser(txt)
        if os.path.isdir(path):
            files = glob.glob(os.path.join(path, '*'))
            prefix = txt
        else:
            files = glob.glob(path + '*')
            prefix = os.path.dirname(txt)
            prefix = prefix.rstrip('/') or '/'
        for f in files:
            display = os.path.join(prefix, os.path.basename(f))
            if os.path.isdir(f):
                display += '/'
            self.lookup.append((display, f))
            self.lookup.sort()
        self.offset = -1
        self.lookup.append((txt, txt))
    self.offset += 1
    if self.offset >= len(self.lookup):
        self.offset = 0
    ret = self.lookup[self.offset]
    self.thisfinal = ret[1]
    return ret[0]","for f in files:
    display = os.path.join(prefix, os.path.basename(f))
    if os.path.isdir(f):
        display += '/'
    self.lookup.append((display, f))
    self.lookup.sort()","for i,f in enumerate(files):
    display = os.path.join(prefix, os.path.basename(f))
    if os.path.isdir(f):
        display += '/'
    self.lookup.append((display, f))
    self.lookup.sort()",1,,,,,,,,,,
FakeNewsNet,https://github.com/KaiDMML/FakeNewsNet/tree/master/code/tweet_collection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FakeNewsNet/code/tweet_collection.py,TweetCollector,"def collect_data(self, choices):
    for choice in choices:
        news_list = self.load_news_file(choice)
        collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for choice in choices:
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for i, choice in enumerate(choices):
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)",1,,,,,,,,,,
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","for message in messages:
    signal_declarations = []
    for signal in message.signals:
        signal_declaration = ''
        if floating_point_numbers:
            signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declarations.append(signal_declaration)
    declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
    if signal_declarations:
        declaration += '\n' + '\n'.join(signal_declarations)
    declarations.append(declaration)","for i,message in enumerate(messages):
    signal_declarations = []
    for signal in message.signals:
        signal_declaration = ''
        if floating_point_numbers:
            signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declarations.append(signal_declaration)
    declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
    if signal_declarations:
        declaration += '\n' + '\n'.join(signal_declarations)
    declarations.append(declaration)",1,,,,,,,,,,
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","for signal in message.signals:
    signal_declaration = ''
    if floating_point_numbers:
        signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declarations.append(signal_declaration)","for i, signal in enumerate(message.signals):
    signal_declaration = ''
    if floating_point_numbers:
        signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declarations.append(signal_declaration)",1,,,,,,,,,,
toapi,https://github.com/gaojiuli/toapi/tree/master/toapi/item.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toapi/toapi/item.py,ItemType,"def __new__(cls, what, bases=None, attrdict=None):
    __fields__ = OrderedDict()
    for (name, selector) in attrdict.items():
        if isinstance(selector, Selector):
            __fields__[name] = selector
    for name in __fields__.keys():
        del attrdict[name]
    instance = type.__new__(cls, what, bases, attrdict)
    instance._list = None
    instance._site = None
    instance._selector = None
    instance.__fields__ = __fields__
    return instance","for (name, selector) in attrdict.items():
    if isinstance(selector, Selector):
        __fields__[name] = selector","for i,(name, selector) in enumerate(attrdict.items()):
    if isinstance(selector, Selector):
        __fields__[name] = selector",1,,,,,,,,,,
toapi,https://github.com/gaojiuli/toapi/tree/master/toapi/item.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toapi/toapi/item.py,ItemType,"def __new__(cls, what, bases=None, attrdict=None):
    __fields__ = OrderedDict()
    for (name, selector) in attrdict.items():
        if isinstance(selector, Selector):
            __fields__[name] = selector
    for name in __fields__.keys():
        del attrdict[name]
    instance = type.__new__(cls, what, bases, attrdict)
    instance._list = None
    instance._site = None
    instance._selector = None
    instance.__fields__ = __fields__
    return instance","for name in __fields__.keys():
    del attrdict[name]","for i,name in enumerate(__fields__.keys()):
    del attrdict[name]",1,,,,,,,,,,
vid2vid,https://github.com/NVIDIA/vid2vid/tree/master/models/networks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vid2vid/models/networks.py,MultiscaleDiscriminator,"def forward(self, input):
    num_D = self.num_D
    result = []
    input_downsampled = input
    for i in range(num_D):
        if self.getIntermFeat:
            model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
        else:
            model = getattr(self, 'layer' + str(num_D - 1 - i))
        result.append(self.singleD_forward(model, input_downsampled))
        if i != num_D - 1:
            input_downsampled = self.downsample(input_downsampled)
    return result","for i in range(num_D):
    if self.getIntermFeat:
        model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
    else:
        model = getattr(self, 'layer' + str(num_D - 1 - i))
    result.append(self.singleD_forward(model, input_downsampled))
    if i != num_D - 1:
        input_downsampled = self.downsample(input_downsampled)","for i in range(num_D):
    if self.getIntermFeat:
        model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
    else:
        model = getattr(self, 'layer' + str(num_D - 1 - i))
    result.append(self.singleD_forward(model, input_downsampled))
    if i != num_D - 1:
        input_downsampled = self.downsample(input_downsampled)",1,,,,,,,,,,
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/addons/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mitmproxy/mitmproxy/addons/core.py,Core,"def revert(self, flows: typing.Sequence[flow.Flow]) -> None:
    """"""
            Revert flow changes.
        """"""
    updated = []
    for f in flows:
        if f.modified():
            f.revert()
            updated.append(f)
    ctx.log.alert('Reverted %s flows.' % len(updated))
    ctx.master.addons.trigger(hooks.UpdateHook(updated))","for f in flows:
    if f.modified():
        f.revert()
        updated.append(f)","for i,f in enumerate(flows):
    if f.modified():
        f.revert()
        updated.append(f)",1,,,,,,,,,,
configuration,https://github.com/edx/configuration/tree/master/util/jenkins/helm_update_checker/helm_update_checker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/configuration/util/jenkins/helm_update_checker/helm_update_checker.py,,"def get_repo_name(repo_url):
    try:
        get_repo_cmd = 'helm repo list -o json'
        repositories = subprocess.check_output(get_repo_cmd, shell=True).strip()
        repo_list = json.loads(repositories.decode())
        for repo in repo_list:
            if repo['url'] == repo_url:
                return repo['name']
    except subprocess.CalledProcessError as e:
        print(e.output)","for repo in repo_list:
    if repo['url'] == repo_url:
        return repo['name']","for i, repo in enumerate(repo_list):
    if repo['url'] == repo_url:
        return repo['name']",1,,,,,,,,,,
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})","for file_id in ids:
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})","for i,file_id in enumerate(ids):
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})",1,,,,,,,,,,
espresso,https://github.com/freewym/espresso/tree/master/fairseq/data/multilingual/multilingual_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/data/multilingual/multilingual_utils.py,,"def augment_dictionary(dictionary: Dictionary, language_list: List[str], lang_tok_style: str, langtoks_specs: Sequence[str]=(LangTokSpec.main.value,), extra_data: Optional[Dict[str, str]]=None) -> None:
    for spec in langtoks_specs:
        for language in language_list:
            dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))
    if lang_tok_style == LangTokStyle.mbart.value or (extra_data is not None and LangTokSpec.mono_dae.value in extra_data):
        dictionary.add_symbol('<mask>')","for spec in langtoks_specs:
    for language in language_list:
        dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))","for i, spec in enumerate(langtoks_specs):
    for language in language_list:
        dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))",1,,,,,,,,,,
espresso,https://github.com/freewym/espresso/tree/master/fairseq/data/multilingual/multilingual_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/data/multilingual/multilingual_utils.py,,"def augment_dictionary(dictionary: Dictionary, language_list: List[str], lang_tok_style: str, langtoks_specs: Sequence[str]=(LangTokSpec.main.value,), extra_data: Optional[Dict[str, str]]=None) -> None:
    for spec in langtoks_specs:
        for language in language_list:
            dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))
    if lang_tok_style == LangTokStyle.mbart.value or (extra_data is not None and LangTokSpec.mono_dae.value in extra_data):
        dictionary.add_symbol('<mask>')","for language in language_list:
    dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))","for i, language in enumerate(language_list):
    dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))",1,,,,,,,,,,
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/actions_wdgt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fgmk/fgmk/actions_wdgt.py,tinyActionsWdgt,"def removeAction(self):
    if self.ssettings == {}:
        return
    if len(self.ActionList.selectedItems()) < 1:
        return
    previous_actions = self.getValue()
    for item in self.ActionList.selectedItems():
        itemIndex = self.ActionList.row(item)
        self.ActionList.takeItem(itemIndex)
    current_actions = self.getValue()
    self.somethingChanged.emit(previous_actions, current_actions, 'remove', 'remove action')","for item in self.ActionList.selectedItems():
    itemIndex = self.ActionList.row(item)
    self.ActionList.takeItem(itemIndex)","for i,item in enumerate(self.ActionList.selectedItems()):
    itemIndex = self.ActionList.row(item)
    self.ActionList.takeItem(itemIndex)",1,,,,,,,,,,
astropy,https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_groups.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/table/tests/test_groups.py,,"def test_group_mixins():
    """"""
    Test grouping a table with mixin columns
    """"""
    idx = np.arange(4)
    x = np.array([3.0, 1.0, 2.0, 1.0])
    q = x * u.m
    lon = coordinates.Longitude(x * u.deg)
    lat = coordinates.Latitude(x * u.deg)
    tm = time.Time(2000, format='jyear') + time.TimeDelta(x * 1e-10, format='sec')
    sc = coordinates.SkyCoord(ra=lon, dec=lat)
    aw = table_helpers.ArrayWrapper(x)
    nd = np.array([(3, 'c'), (1, 'a'), (2, 'b'), (1, 'a')], dtype='<i4,|S1').view(NdarrayMixin)
    qt = QTable([idx, x, q, lon, lat, tm, sc, aw, nd], names=['idx', 'x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd'])
    mixin_keys = ['x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd']
    for key in mixin_keys:
        qtg = qt.group_by(key)
        assert np.all(qtg['idx'] == [1, 3, 2, 0])
        for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
            assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
            assert np.all(qt[name][[2]] == qtg.groups[1][name])
            assert np.all(qt[name][[0]] == qtg.groups[2][name])
    uqt = unique(qt, keys=mixin_keys)
    assert len(uqt) == 3
    assert np.all(uqt['idx'] == [1, 2, 0])
    assert np.all(uqt['x'] == [1.0, 2.0, 3.0])
    idxg = qt['idx'].group_by(qt[mixin_keys])
    assert np.all(idxg == [1, 3, 2, 0])","for key in mixin_keys:
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])","for i,key in enumerate(mixin_keys):
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])",1,,,,,,,,,,
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()","for stat_set in stat_sets:
    for stat in self.stats[stat_set]:
        self.stats[stat_set][stat].reset()","for i, stat_set in enumerate(stat_sets):
    for stat in self.stats[stat_set]:
        self.stats[stat_set][stat].reset()",1,,,,,,,,,,
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()","for stat in self.stats[stat_set]:
    self.stats[stat_set][stat].reset()","for i, stat in enumerate(self.stats[stat_set]):
    self.stats[stat_set][stat].reset()",1,,,,,,,,,,
mars,https://github.com/mars-project/mars/tree/master/mars/dataframe/arithmetic/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/dataframe/arithmetic/core.py,DataFrameBinOpMixin,"def _tile_dataframe_series(cls, op):
    (left, right) = (op.lhs, op.rhs)
    df = op.outputs[0]
    (nsplits, out_shape, left_chunks, right_chunks) = align_dataframe_series(left, right, axis=op.axis)
    out_chunk_indexes = itertools.product(*(range(s) for s in out_shape))
    out_chunks = []
    for (out_idx, df_chunk) in zip(out_chunk_indexes, left_chunks):
        if op.axis == 'columns' or op.axis == 1:
            series_chunk = right_chunks[out_idx[1]]
            kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
        else:
            series_chunk = right_chunks[out_idx[0]]
            kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
        out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
        out_chunks.append(out_chunk)
    new_op = op.copy()
    return new_op.new_dataframes(op.inputs, df.shape, nsplits=tuple((tuple(ns) for ns in nsplits)), chunks=out_chunks, dtypes=df.dtypes, index_value=df.index_value, columns_value=df.columns_value)","for (out_idx, df_chunk) in zip(out_chunk_indexes, left_chunks):
    if op.axis == 'columns' or op.axis == 1:
        series_chunk = right_chunks[out_idx[1]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
    else:
        series_chunk = right_chunks[out_idx[0]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
    out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
    out_chunks.append(out_chunk)","for i, (out_idx, df_chunk) in enumerate(zip(out_chunk_indexes, left_chunks)):
    if op.axis == 'columns' or op.axis == 1:
        series_chunk = right_chunks[out_idx[1]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
    else:
        series_chunk = right_chunks[out_idx[0]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
    out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
    out_chunks.append(out_chunk)",1,,,,,,,,,,
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/solo/utils/knn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/solo/utils/knn.py,WeightedKNNClassifier,"def compute(self) -> Tuple[float]:
    """"""Computes weighted k-NN accuracy @1 and @5. If cosine distance is selected,
        the weight is computed using the exponential of the temperature scaled cosine
        distance of the samples. If euclidean distance is selected, the weight corresponds
        to the inverse of the euclidean distance.

        Returns:
            Tuple[float]: k-NN accuracy @1 and @5.
        """"""
    train_features = torch.cat(self.train_features)
    train_targets = torch.cat(self.train_targets)
    test_features = torch.cat(self.test_features)
    test_targets = torch.cat(self.test_targets)
    if self.distance_fx == 'cosine':
        train_features = F.normalize(train_features)
        test_features = F.normalize(test_features)
    num_classes = torch.unique(test_targets).numel()
    num_train_images = train_targets.size(0)
    num_test_images = test_targets.size(0)
    num_train_images = train_targets.size(0)
    chunk_size = min(max(1, self.max_distance_matrix_size // num_train_images), num_test_images)
    k = min(self.k, num_train_images)
    (top1, top5, total) = (0.0, 0.0, 0)
    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)
    for idx in range(0, num_test_images, chunk_size):
        features = test_features[idx:min(idx + chunk_size, num_test_images), :]
        targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
        batch_size = targets.size(0)
        if self.distance_fx == 'cosine':
            similarities = torch.mm(features, train_features.t())
        elif self.distance_fx == 'euclidean':
            similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
        else:
            raise NotImplementedError
        (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
        candidates = train_targets.view(1, -1).expand(batch_size, -1)
        retrieved_neighbors = torch.gather(candidates, 1, indices)
        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
        if self.distance_fx == 'cosine':
            similarities = similarities.clone().div_(self.T).exp_()
        probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
        (_, predictions) = probs.sort(1, True)
        correct = predictions.eq(targets.data.view(-1, 1))
        top1 = top1 + correct.narrow(1, 0, 1).sum().item()
        top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
        total += targets.size(0)
    top1 = top1 * 100.0 / total
    top5 = top5 * 100.0 / total
    self.reset()
    return (top1, top5)","for idx in range(0, num_test_images, chunk_size):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)","for i in range(0, num_test_images, chunk_size):
    features = test_features[i:min(i + chunk_size, num_test_images), :]
    targets = test_targets[i:min(i + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)",1,,,,,,,,,,
mayavi,https://github.com/enthought/mayavi/tree/master/tvtk/indenter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/tvtk/indenter.py,VTKDocMassager,"def _rename_methods(self, doc):
    lines = doc.split('\n')
    nl = []
    for line in lines:
        words = line.split(' ')
        nw = []
        for word in words:
            if word[:3] == 'vtk':
                nw.append(word)
            else:
                nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
        nl.append(' '.join(nw))
    return '\n'.join(nl)","for line in lines:
    words = line.split(' ')
    nw = []
    for word in words:
        if word[:3] == 'vtk':
            nw.append(word)
        else:
            nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
    nl.append(' '.join(nw))","for i,line in enumerate(lines):
    words = line.split(' ')
    nw = []
    for word in words:
        if word[:3] == 'vtk':
            nw.append(word)
        else:
            nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
    nl.append(' '.join(nw))",1,,,,,,,,,,
cozmo-python-sdk,https://github.com/anki/cozmo-python-sdk/tree/master/examples/apps/quizmaster_cozmo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cozmo-python-sdk/examples/apps/quizmaster_cozmo.py,CozmoQuizMaster,"def turn_player_lights_on(self):
    for player in self._players:
        player.turn_light_on()","for player in self._players:
    player.turn_light_on()","for i, player in enumerate(self._players):
    player.turn_light_on()",1,,,,,,,,,,
chadtree,https://github.com/ms-jpq/chadtree/tree/master/chadtree/view/render.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chadtree/chadtree/view/render.py,,"def cont() -> Iterator[Any]:
    for sb in sortby:
        if sb is Sortby.is_folder:
            yield (_CompVals.FOLDER if is_dir(node) else _CompVals.FILE)
        elif sb is Sortby.ext:
            yield ('' if is_dir(node) else strxfrm(node.path.suffix))
        elif sb is Sortby.file_name:
            yield strxfrm(node.path.name)
        else:
            never(sb)","for sb in sortby:
    if sb is Sortby.is_folder:
        yield (_CompVals.FOLDER if is_dir(node) else _CompVals.FILE)
    elif sb is Sortby.ext:
        yield ('' if is_dir(node) else strxfrm(node.path.suffix))
    elif sb is Sortby.file_name:
        yield strxfrm(node.path.name)
    else:
        never(sb)","for i,sb in enumerate(sortby):
    if sb is Sortby.is_folder:
        yield (_CompVals.FOLDER if is_dir(node) else _CompVals.FILE)
    elif sb is Sortby.ext:
        yield ('' if is_dir(node) else strxfrm(node.path.suffix))
    elif sb is Sortby.file_name:
        yield strxfrm(node.path.name)
    else:
        never(sb)",1,,,,,,,,,,
xalpha,https://github.com/refraction-ray/xalpha/tree/master/xalpha/indicator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xalpha/xalpha/indicator.py,indicator,"def max_drawdown(self, date=yesterdayobj()):
    """"""
        

        :param date: date obj or string
        :returns: three elements tuple, the first two are the date obj of
            start and end of the time window, the third one is the drawdown amplitude in unit 1.
        """"""
    li = [(row['date'], row['netvalue']) for (i, row) in self.price[self.price['date'] <= date].iterrows()]
    res = []
    for (i, _) in enumerate(li):
        for j in range(i + 1, len(li)):
            res.append((li[i][0], li[j][0], (li[j][1] - li[i][1]) / li[i][1]))
    return min(res, key=lambda x: x[2])","for j in range(i + 1, len(li)):
    res.append((li[i][0], li[j][0], (li[j][1] - li[i][1]) / li[i][1]))","for i, (item_i, value_i) in enumerate(li):
    for j, (item_j, value_j) in enumerate(li[i+1:], i+1):
        res.append((item_i, item_j, (value_j - value_i) / value_i))",1,,,,,,,,,,
DataStructure_Algorithm_ZJU,https://github.com/CYBruce/DataStructure_Algorithm_ZJU/tree/master//04-4 .py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DataStructure_Algorithm_ZJU//04-4 .py,,"def get_input(N, L):
    global same
    LIST = []
    real = input().split()
    for i in range(N):
        real[i] = int(real[i])
    for i in range(L):
        test = input().split()
        for j in range(N):
            test[j] = int(test[j])
        same = 0
        IsBST(real, test)
        if same == 0:
            LIST.append('Yes')
        else:
            LIST.append('No')
    for i in range(len(LIST)):
        print(LIST[i])","for i in range(len(LIST)):
    print(LIST[i])","for i, item in enumerate(LIST):
    print(item)",1,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/modules/status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/status.py,,"def linux_w():
    """"""
        Linux specific implementation for w
        """"""
    user_list = []
    users = __salt__['cmd.run']('w -fh').splitlines()
    for row in users:
        if not row:
            continue
        comps = row.split()
        rec = {'idle': comps[3], 'jcpu': comps[4], 'login': comps[2], 'pcpu': comps[5], 'tty': comps[1], 'user': comps[0], 'what': ' '.join(comps[6:])}
        user_list.append(rec)
    return user_list","for row in users:
    if not row:
        continue
    comps = row.split()
    rec = {'idle': comps[3], 'jcpu': comps[4], 'login': comps[2], 'pcpu': comps[5], 'tty': comps[1], 'user': comps[0], 'what': ' '.join(comps[6:])}
    user_list.append(rec)","for i,row in enumerate(users):
    if not row:
        continue
    comps = row.split()
    rec = {'idle': comps[3], 'jcpu': comps[4], 'login': comps[2], 'pcpu': comps[5], 'tty': comps[1], 'user': comps[0], 'what': ' '.join(comps[6:])}
    user_list.append(rec)",1,,,,,,,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/videa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/videa.py,VideaIE,"def rc4(cipher_text, key):
    res = b''
    key_len = len(key)
    S = list(range(256))
    j = 0
    for i in range(256):
        j = (j + S[i] + ord(key[i % key_len])) % 256
        (S[i], S[j]) = (S[j], S[i])
    i = 0
    j = 0
    for m in range(len(cipher_text)):
        i = (i + 1) % 256
        j = (j + S[i]) % 256
        (S[i], S[j]) = (S[j], S[i])
        k = S[(S[i] + S[j]) % 256]
        res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))
    return res.decode()","for i in range(256):
    j = (j + S[i] + ord(key[i % key_len])) % 256
    (S[i], S[j]) = (S[j], S[i])","for i, _ in enumerate(range(256)):
    j = (j + S[i] + ord(key[i % key_len])) % 256
    (S[i], S[j]) = (S[j], S[i])",1,,,,,,,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/videa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/videa.py,VideaIE,"def rc4(cipher_text, key):
    res = b''
    key_len = len(key)
    S = list(range(256))
    j = 0
    for i in range(256):
        j = (j + S[i] + ord(key[i % key_len])) % 256
        (S[i], S[j]) = (S[j], S[i])
    i = 0
    j = 0
    for m in range(len(cipher_text)):
        i = (i + 1) % 256
        j = (j + S[i]) % 256
        (S[i], S[j]) = (S[j], S[i])
        k = S[(S[i] + S[j]) % 256]
        res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))
    return res.decode()","for m in range(len(cipher_text)):
    i = (i + 1) % 256
    j = (j + S[i]) % 256
    (S[i], S[j]) = (S[j], S[i])
    k = S[(S[i] + S[j]) % 256]
    res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))","for m, cipher_char in enumerate(cipher_text):
    i = (i + 1) % 256
    j = (j + S[i]) % 256
    (S[i], S[j]) = (S[j], S[i])
    k = S[(S[i] + S[j]) % 256]
    res += compat_struct_pack('B', k ^ compat_ord(cipher_char))",1,,,,,,,,,,
GPT2-chitchat,https://github.com/yangjianxin1/GPT2-chitchat/tree/master//train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GPT2-chitchat//train.py,,"def train(model, logger, train_dataset, validate_dataset, args):
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    validate_dataloader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    early_stopping = EarlyStopping(args.patience, verbose=True, save_path=args.save_model_path)
    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs
    optimizer = transformers.AdamW(model.parameters(), lr=args.lr, eps=args.eps)
    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)
    logger.info('starting training')
    (train_losses, validate_losses) = ([], [])
    best_val_loss = 10000
    for epoch in range(args.epochs):
        train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
        train_losses.append(train_loss)
        validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
        validate_losses.append(validate_loss)
        if validate_loss < best_val_loss:
            best_val_loss = validate_loss
            logger.info('saving current best model for epoch {}'.format(epoch + 1))
            model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
            if not os.path.exists(model_path):
                os.mkdir(model_path)
            model_to_save = model.module if hasattr(model, 'module') else model
            model_to_save.save_pretrained(model_path)
        if args.patience == 0:
            continue
        early_stopping(validate_loss, model)
        if early_stopping.early_stop:
            logger.info('Early stopping')
            break
    logger.info('training finished')
    logger.info('train_losses:{}'.format(train_losses))
    logger.info('validate_losses:{}'.format(validate_losses))","for epoch in range(args.epochs):
    train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
    train_losses.append(train_loss)
    validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
    validate_losses.append(validate_loss)
    if validate_loss < best_val_loss:
        best_val_loss = validate_loss
        logger.info('saving current best model for epoch {}'.format(epoch + 1))
        model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
        if not os.path.exists(model_path):
            os.mkdir(model_path)
        model_to_save = model.module if hasattr(model, 'module') else model
        model_to_save.save_pretrained(model_path)
    if args.patience == 0:
        continue
    early_stopping(validate_loss, model)
    if early_stopping.early_stop:
        logger.info('Early stopping')
        break","for epoch, _ in enumerate(range(args.epochs)):
    train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
    train_losses.append(train_loss)
    validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
    validate_losses.append(validate_loss)
    if validate_loss < best_val_loss:
        best_val_loss = validate_loss
        logger.info('saving current best model for epoch {}'.format(epoch + 1))
        model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
        if not os.path.exists(model_path):
            os.mkdir(model_path)
        model_to_save = model.module if hasattr(model, 'module') else model
        model_to_save.save_pretrained(model_path)
    if args.patience == 0:
        continue
    early_stopping(validate_loss, model)
    if early_stopping.early_stop:
        logger.info('Early stopping')
        break",1,,,,,,,,,,
espnet,https://github.com/espnet/espnet/tree/master/test/test_e2e_asr.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/test/test_e2e_asr.py,,"def test_torch_save_and_load():
    args = make_arg()
    model = th_asr.E2E(10, 5, args)
    for p in model.parameters():
        p.data.uniform_()
    if not os.path.exists('.pytest_cache'):
        os.makedirs('.pytest_cache')
    tmppath = tempfile.mktemp()
    asr_utils.torch_save(tmppath, model)
    p_saved = [p.data.numpy() for p in model.parameters()]
    for p in model.parameters():
        p.data.zero_()
    asr_utils.torch_load(tmppath, model)
    for (p1, p2) in zip(p_saved, model.parameters()):
        np.testing.assert_array_equal(p1, p2.data.numpy())
    if os.path.exists(tmppath):
        os.remove(tmppath)","for p in model.parameters():
    p.data.zero_()","for i, p in enumerate(model.parameters()):
    p.data.zero_()",1,,,,,,,,,,
iou-tracker,https://github.com/bochinski/iou-tracker/tree/master//viou_tracker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/iou-tracker//viou_tracker.py,,"def track_viou_matlab_wrapper(frames_path, detections, sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio=1.0):
    """"""
    Matlab wrapper of the v-iou tracker for the detrac evaluation toolkit.

    Args:
         detections (numpy.array): numpy array of detections, usually supplied by run_tracker.m
         sigma_l (float): low detection threshold.
         sigma_h (float): high detection threshold.
         sigma_iou (float): IOU threshold.
         t_min (float): minimum track length in frames.

    Returns:
        float: speed in frames per second.
        list: list of tracks.
    """"""
    detections = detections.reshape((7, -1)).transpose()
    dets = load_mot(detections, with_classes=False)
    start = time()
    tracks = track_viou(frames_path + 'img{:05d}.jpg', dets, sigma_l, sigma_h, sigma_iou, int(t_min), int(ttl), tracker_type, keep_upper_height_ratio)
    end = time()
    id_ = 1
    out = []
    for track in tracks:
        for (i, bbox) in enumerate(track['bboxes']):
            out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
        id_ += 1
    num_frames = len(dets)
    speed = num_frames / (end - start)
    return (speed, out)","for track in tracks:
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
    id_ += 1","for track in tracks:
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
        id_ += 1",1,,,,,,,,,,
poetry,https://github.com/sheepzh/poetry/tree/master/src/poetry/utils/setup_reader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poetry/src/poetry/utils/setup_reader.py,SetupReader,"def _find_install_requires(self, call: ast.Call, body: Iterable[Any]) -> List[str]:
    install_requires = []
    value = self._find_in_call(call, 'install_requires')
    if value is None:
        kwargs = self._find_call_kwargs(call)
        if kwargs is None or not isinstance(kwargs, ast.Name):
            return install_requires
        variable = self._find_variable_in_body(body, kwargs.id)
        if not isinstance(variable, (ast.Dict, ast.Call)):
            return install_requires
        if isinstance(variable, ast.Call):
            if not isinstance(variable.func, ast.Name):
                return install_requires
            if variable.func.id != 'dict':
                return install_requires
            value = self._find_in_call(variable, 'install_requires')
        else:
            value = self._find_in_dict(variable, 'install_requires')
    if value is None:
        return install_requires
    if isinstance(value, ast.List):
        for el in value.elts:
            install_requires.append(el.s)
    elif isinstance(value, ast.Name):
        variable = self._find_variable_in_body(body, value.id)
        if variable is not None and isinstance(variable, ast.List):
            for el in variable.elts:
                install_requires.append(el.s)
    return install_requires","for el in value.elts:
    install_requires.append(el.s)","for i, el in enumerate(value.elts):
    install_requires.append(el.s)",1,,,,,,,,,,
poetry,https://github.com/sheepzh/poetry/tree/master/src/poetry/utils/setup_reader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poetry/src/poetry/utils/setup_reader.py,SetupReader,"def _find_install_requires(self, call: ast.Call, body: Iterable[Any]) -> List[str]:
    install_requires = []
    value = self._find_in_call(call, 'install_requires')
    if value is None:
        kwargs = self._find_call_kwargs(call)
        if kwargs is None or not isinstance(kwargs, ast.Name):
            return install_requires
        variable = self._find_variable_in_body(body, kwargs.id)
        if not isinstance(variable, (ast.Dict, ast.Call)):
            return install_requires
        if isinstance(variable, ast.Call):
            if not isinstance(variable.func, ast.Name):
                return install_requires
            if variable.func.id != 'dict':
                return install_requires
            value = self._find_in_call(variable, 'install_requires')
        else:
            value = self._find_in_dict(variable, 'install_requires')
    if value is None:
        return install_requires
    if isinstance(value, ast.List):
        for el in value.elts:
            install_requires.append(el.s)
    elif isinstance(value, ast.Name):
        variable = self._find_variable_in_body(body, value.id)
        if variable is not None and isinstance(variable, ast.List):
            for el in variable.elts:
                install_requires.append(el.s)
    return install_requires","for el in variable.elts:
    install_requires.append(el.s)","for i, el in enumerate(variable.elts):
    install_requires.append(el.s)",1,,,,,,,,,,
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/visuals.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/visuals.py,,"def get_context_from_uri_vars(only_infos: Optional[List[InfoName]]=None) -> VisualContext:
    context = {}
    for (filter_name, filter_object) in filter_registry.items():
        if only_infos is not None and filter_object.info not in only_infos:
            continue
        this_filter_vars = {}
        for varname in filter_object.htmlvars:
            if not request.has_var(varname):
                continue
            filter_value = request.get_str_input_mandatory(varname)
            if not filter_value:
                continue
            this_filter_vars[varname] = filter_value
        if this_filter_vars:
            context[filter_name] = this_filter_vars
    return context","for (filter_name, filter_object) in filter_registry.items():
    if only_infos is not None and filter_object.info not in only_infos:
        continue
    this_filter_vars = {}
    for varname in filter_object.htmlvars:
        if not request.has_var(varname):
            continue
        filter_value = request.get_str_input_mandatory(varname)
        if not filter_value:
            continue
        this_filter_vars[varname] = filter_value
    if this_filter_vars:
        context[filter_name] = this_filter_vars","for i, (filter_name, filter_object) in enumerate(filter_registry.items()):
    if only_infos is not None and filter_object.info not in only_infos:
        continue
    this_filter_vars = {}
    for varname in filter_object.htmlvars:
        if not request.has_var(varname):
            continue
        filter_value = request.get_str_input_mandatory(varname)
        if not filter_value:
            continue
        this_filter_vars[varname] = filter_value
    if this_filter_vars:
        context[filter_name] = this_filter_vars",1,,,,,,,,,,
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/visuals.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/visuals.py,,"def get_context_from_uri_vars(only_infos: Optional[List[InfoName]]=None) -> VisualContext:
    context = {}
    for (filter_name, filter_object) in filter_registry.items():
        if only_infos is not None and filter_object.info not in only_infos:
            continue
        this_filter_vars = {}
        for varname in filter_object.htmlvars:
            if not request.has_var(varname):
                continue
            filter_value = request.get_str_input_mandatory(varname)
            if not filter_value:
                continue
            this_filter_vars[varname] = filter_value
        if this_filter_vars:
            context[filter_name] = this_filter_vars
    return context","for varname in filter_object.htmlvars:
    if not request.has_var(varname):
        continue
    filter_value = request.get_str_input_mandatory(varname)
    if not filter_value:
        continue
    this_filter_vars[varname] = filter_value","for i,varname in enumerate(filter_object.htmlvars):
    if not request.has_var(varname):
        continue
    filter_value = request.get_str_input_mandatory(varname)
    if not filter_value:
        continue
    this_filter_vars[varname] = filter_value",1,,,,,,,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,FrameToTFE,"def add_point_cloud(self, feature, laser_names, range_image_pose):
    """"""Convert the range images in `feature` to 3D point clouds.

    Adds the point cloud data to the tf.Example feature map.

    Args:
      feature: A tf.Example feature map.
      laser_names: A list of laser names (e.g., 'TOP', 'REAR', 'SIDE_LEFT').
      range_image_pose: A range image pose Tensor for the top laser.
    """"""
    self.laser_info = {}
    for laser_name in laser_names:
        beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
        if beam_inclinations.size == 0:
            beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
            beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
            laser_ri_name = '%s_ri1' % laser_name
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            height = tf.cast(range_image_shape[0], tf.float32)
            beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
            beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
        beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
        for ri_type in ['ri1', 'ri2']:
            laser_ri_name = '%s_%s' % (laser_name, ri_type)
            range_image = np.array(feature[laser_ri_name].float_list.value[:])
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            range_image = range_image.reshape(range_image_shape)
            range_image_mask = range_image[..., 0] >= 0
            range_image_range = range_image[..., 0]
            batched_pixel_pose = None
            batched_frame_pose = None
            if laser_name == 'TOP' and range_image_pose is not None:
                batched_pixel_pose = range_image_pose[tf.newaxis, ...]
                batched_frame_pose = self.frame_pose[tf.newaxis, ...]
            batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
            batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
            range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
            info = py_utils.NestedMap()
            self.laser_info[laser_ri_name] = info
            info.range_image = range_image
            info.range_image_shape = range_image_shape
            ri_indices = tf.where(range_image_mask)
            points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
            info.num_points = tf.shape(points_xyz).numpy()[0]
            points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
            if self._use_range_image_index_as_lidar_feature:
                points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
            else:
                points_data = tf.concat([points_xyz, points_features], axis=-1)
            points_list = list(points_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
            laser_ri_flow_name = '%s_flow' % laser_ri_name
            if laser_ri_flow_name in feature:
                range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
                range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
                range_image_flow = range_image_flow.reshape(range_image_flow_shape)
                flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
                flow_list = list(flow_data.numpy().reshape([-1]))
                feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","for laser_name in laser_names:
    beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
    if beam_inclinations.size == 0:
        beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
        beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
        laser_ri_name = '%s_ri1' % laser_name
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        height = tf.cast(range_image_shape[0], tf.float32)
        beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
        beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
    beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
    for ri_type in ['ri1', 'ri2']:
        laser_ri_name = '%s_%s' % (laser_name, ri_type)
        range_image = np.array(feature[laser_ri_name].float_list.value[:])
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        range_image = range_image.reshape(range_image_shape)
        range_image_mask = range_image[..., 0] >= 0
        range_image_range = range_image[..., 0]
        batched_pixel_pose = None
        batched_frame_pose = None
        if laser_name == 'TOP' and range_image_pose is not None:
            batched_pixel_pose = range_image_pose[tf.newaxis, ...]
            batched_frame_pose = self.frame_pose[tf.newaxis, ...]
        batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
        batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
        range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
        info = py_utils.NestedMap()
        self.laser_info[laser_ri_name] = info
        info.range_image = range_image
        info.range_image_shape = range_image_shape
        ri_indices = tf.where(range_image_mask)
        points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
        info.num_points = tf.shape(points_xyz).numpy()[0]
        points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
        if self._use_range_image_index_as_lidar_feature:
            points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
        else:
            points_data = tf.concat([points_xyz, points_features], axis=-1)
        points_list = list(points_data.numpy().reshape([-1]))
        feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
        laser_ri_flow_name = '%s_flow' % laser_ri_name
        if laser_ri_flow_name in feature:
            range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
            range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
            range_image_flow = range_image_flow.reshape(range_image_flow_shape)
            flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
            flow_list = list(flow_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","for i,laser_name in enumerate(laser_names):
    beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
    if beam_inclinations.size == 0:
        beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
        beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
        laser_ri_name = '%s_ri1' % laser_name
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        height = tf.cast(range_image_shape[0], tf.float32)
        beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
        beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
    beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
    for ri_type in ['ri1', 'ri2']:
        laser_ri_name = '%s_%s' % (laser_name, ri_type)
        range_image = np.array(feature[laser_ri_name].float_list.value[:])
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        range_image = range_image.reshape(range_image_shape)
        range_image_mask = range_image[..., 0] >= 0
        range_image_range = range_image[..., 0]
        batched_pixel_pose = None
        batched_frame_pose = None
        if laser_name == 'TOP' and range_image_pose is not None:
            batched_pixel_pose = range_image_pose[tf.newaxis, ...]
            batched_frame_pose = self.frame_pose[tf.newaxis, ...]
        batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
        batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
        range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
        info = py_utils.NestedMap()
        self.laser_info[laser_ri_name] = info
        info.range_image = range_image
        info.range_image_shape = range_image_shape
        ri_indices = tf.where(range_image_mask)
        points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
        info.num_points = tf.shape(points_xyz).numpy()[0]
        points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
        if self._use_range_image_index_as_lidar_feature:
            points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
        else:
            points_data = tf.concat([points_xyz, points_features], axis=-1)
        points_list = list(points_data.numpy().reshape([-1]))
        feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
        laser_ri_flow_name = '%s_flow' % laser_ri_name
        if laser_ri_flow_name in feature:
            range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
            range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
            range_image_flow = range_image_flow.reshape(range_image_flow_shape)
            flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
            flow_list = list(flow_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list",1,,,,,,,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,FrameToTFE,"def add_point_cloud(self, feature, laser_names, range_image_pose):
    """"""Convert the range images in `feature` to 3D point clouds.

    Adds the point cloud data to the tf.Example feature map.

    Args:
      feature: A tf.Example feature map.
      laser_names: A list of laser names (e.g., 'TOP', 'REAR', 'SIDE_LEFT').
      range_image_pose: A range image pose Tensor for the top laser.
    """"""
    self.laser_info = {}
    for laser_name in laser_names:
        beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
        if beam_inclinations.size == 0:
            beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
            beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
            laser_ri_name = '%s_ri1' % laser_name
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            height = tf.cast(range_image_shape[0], tf.float32)
            beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
            beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
        beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
        for ri_type in ['ri1', 'ri2']:
            laser_ri_name = '%s_%s' % (laser_name, ri_type)
            range_image = np.array(feature[laser_ri_name].float_list.value[:])
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            range_image = range_image.reshape(range_image_shape)
            range_image_mask = range_image[..., 0] >= 0
            range_image_range = range_image[..., 0]
            batched_pixel_pose = None
            batched_frame_pose = None
            if laser_name == 'TOP' and range_image_pose is not None:
                batched_pixel_pose = range_image_pose[tf.newaxis, ...]
                batched_frame_pose = self.frame_pose[tf.newaxis, ...]
            batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
            batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
            range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
            info = py_utils.NestedMap()
            self.laser_info[laser_ri_name] = info
            info.range_image = range_image
            info.range_image_shape = range_image_shape
            ri_indices = tf.where(range_image_mask)
            points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
            info.num_points = tf.shape(points_xyz).numpy()[0]
            points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
            if self._use_range_image_index_as_lidar_feature:
                points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
            else:
                points_data = tf.concat([points_xyz, points_features], axis=-1)
            points_list = list(points_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
            laser_ri_flow_name = '%s_flow' % laser_ri_name
            if laser_ri_flow_name in feature:
                range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
                range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
                range_image_flow = range_image_flow.reshape(range_image_flow_shape)
                flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
                flow_list = list(flow_data.numpy().reshape([-1]))
                feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","for ri_type in ['ri1', 'ri2']:
    laser_ri_name = '%s_%s' % (laser_name, ri_type)
    range_image = np.array(feature[laser_ri_name].float_list.value[:])
    range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
    range_image = range_image.reshape(range_image_shape)
    range_image_mask = range_image[..., 0] >= 0
    range_image_range = range_image[..., 0]
    batched_pixel_pose = None
    batched_frame_pose = None
    if laser_name == 'TOP' and range_image_pose is not None:
        batched_pixel_pose = range_image_pose[tf.newaxis, ...]
        batched_frame_pose = self.frame_pose[tf.newaxis, ...]
    batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
    batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
    batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
    batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
    range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
    info = py_utils.NestedMap()
    self.laser_info[laser_ri_name] = info
    info.range_image = range_image
    info.range_image_shape = range_image_shape
    ri_indices = tf.where(range_image_mask)
    points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
    info.num_points = tf.shape(points_xyz).numpy()[0]
    points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
    if self._use_range_image_index_as_lidar_feature:
        points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
    else:
        points_data = tf.concat([points_xyz, points_features], axis=-1)
    points_list = list(points_data.numpy().reshape([-1]))
    feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
    laser_ri_flow_name = '%s_flow' % laser_ri_name
    if laser_ri_flow_name in feature:
        range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
        range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
        range_image_flow = range_image_flow.reshape(range_image_flow_shape)
        flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
        flow_list = list(flow_data.numpy().reshape([-1]))
        feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","for i, ri_type in enumerate(['ri1', 'ri2']):
    laser_ri_name = '%s_%s' % (laser_name, ri_type)
    range_image = np.array(feature[laser_ri_name].float_list.value[:])
    range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
    range_image = range_image.reshape(range_image_shape)
    range_image_mask = range_image[..., 0] >= 0
    range_image_range = range_image[..., 0]
    batched_pixel_pose = None
    batched_frame_pose = None
    if laser_name == 'TOP' and range_image_pose is not None:
        batched_pixel_pose = range_image_pose[tf.newaxis, ...]
        batched_frame_pose = self.frame_pose[tf.newaxis, ...]
    batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
    batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
    batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
    batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
    range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
    info = py_utils.NestedMap()
    self.laser_info[laser_ri_name] = info
    info.range_image = range_image
    info.range_image_shape = range_image_shape
    ri_indices = tf.where(range_image_mask)
    points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
    info.num_points = tf.shape(points_xyz).numpy()[0]
    points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
    if self._use_range_image_index_as_lidar_feature:
        points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
    else:
        points_data = tf.concat([points_xyz, points_features], axis=-1)
    points_list = list(points_data.numpy().reshape([-1]))
    feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
    laser_ri_flow_name = '%s_flow' % laser_ri_name
    if laser_ri_flow_name in feature:
        range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
        range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
        range_image_flow = range_image_flow.reshape(range_image_flow_shape)
        flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
        flow_list = list(flow_data.numpy().reshape([-1]))
        feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list",1,,,,,,,,,,
Efficient-Segmentation-Networks,https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks/tree/master/dataset/camvid.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Efficient-Segmentation-Networks/dataset/camvid.py,CamVidDataSet,"def __init__(self, root='', list_path='', max_iters=None, crop_size=(360, 360), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=11):
    self.root = root
    self.list_path = list_path
    (self.crop_h, self.crop_w) = crop_size
    self.scale = scale
    self.ignore_label = ignore_label
    self.mean = mean
    self.is_mirror = mirror
    self.img_ids = [i_id.strip() for i_id in open(list_path)]
    if not max_iters == None:
        self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))
    self.files = []
    for name in self.img_ids:
        img_file = osp.join(self.root, name.split()[0])
        label_file = osp.join(self.root, name.split()[1])
        self.files.append({'img': img_file, 'label': label_file, 'name': name})
    print('length of train set: ', len(self.files))","for name in self.img_ids:
    img_file = osp.join(self.root, name.split()[0])
    label_file = osp.join(self.root, name.split()[1])
    self.files.append({'img': img_file, 'label': label_file, 'name': name})","for i, name in enumerate(self.img_ids):
    img_file = osp.join(self.root, name.split()[0])
    label_file = osp.join(self.root, name.split()[1])
    self.files.append({'img': img_file, 'label': label_file, 'name': name})",1,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/core/torch_generator_agent.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/core/torch_generator_agent.py,TopKSampling,"def select_paths(self, logprobs, prior_scores, current_length) -> _PathSelection:
    (values, indices) = logprobs.topk(self.k, dim=-1)
    probs = torch.softmax(values, dim=-1)
    choices = torch.multinomial(probs, 1)[:, 0]
    hyp_ids = torch.arange(logprobs.size(0)).to(logprobs.device)
    tok_ids = indices[hyp_ids, choices]
    scores = values[hyp_ids, choices]
    best_scores = prior_scores.expand_as(scores) + scores
    token_details: Optional[List[_PathSelectionTokenDetails]] = None
    if self.verbose:
        tok_logprobs = probs[hyp_ids, choices].log().view(-1).cpu().numpy()
        tok_ranks = choices.view(-1).cpu().numpy()
        token_details = []
        for (tok_logprob, tok_rank) in zip(tok_logprobs, tok_ranks):
            token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})
    return _PathSelection(hypothesis_ids=hyp_ids, token_ids=tok_ids, scores=best_scores, token_details=token_details)","for (tok_logprob, tok_rank) in zip(tok_logprobs, tok_ranks):
    token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})","for i,(tok_logprob, tok_rank) in enumerate(zip(tok_logprobs, tok_ranks)):
    token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})",1,,,,,,,,,,
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/tools/_paga.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scanpy/scanpy/tools/_paga.py,PAGA,"def _compute_connectivities_v1_2(self):
    import igraph
    ones = self._neighbors.distances.copy()
    ones.data = np.ones(len(ones.data))
    g = _utils.get_igraph_from_adjacency(ones, directed=True)
    vc = igraph.VertexClustering(g, membership=self._adata.obs[self._groups_key].cat.codes.values)
    ns = vc.sizes()
    n = sum(ns)
    es_inner_cluster = [vc.subgraph(i).ecount() for i in range(len(ns))]
    cg = vc.cluster_graph(combine_edges='sum')
    inter_es = _utils.get_sparse_from_igraph(cg, weight_attr='weight')
    es = np.array(es_inner_cluster) + inter_es.sum(axis=1).A1
    inter_es = inter_es + inter_es.T
    connectivities = inter_es.copy()
    expected_n_edges = inter_es.copy()
    inter_es = inter_es.tocoo()
    for (i, j, v) in zip(inter_es.row, inter_es.col, inter_es.data):
        expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
        if expected_random_null != 0:
            scaled_value = v / expected_random_null
        else:
            scaled_value = 1
        if scaled_value > 1:
            scaled_value = 1
        connectivities[i, j] = scaled_value
        expected_n_edges[i, j] = expected_random_null
    self.ns = ns
    self.expected_n_edges_random = expected_n_edges
    self.connectivities = connectivities
    self.connectivities_tree = self._get_connectivities_tree_v1_2()
    return (inter_es.tocsr(), connectivities)","for (i, j, v) in zip(inter_es.row, inter_es.col, inter_es.data):
    expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
    if expected_random_null != 0:
        scaled_value = v / expected_random_null
    else:
        scaled_value = 1
    if scaled_value > 1:
        scaled_value = 1
    connectivities[i, j] = scaled_value
    expected_n_edges[i, j] = expected_random_null","for idx, (i, j, v) in enumerate(zip(inter_es.row, inter_es.col, inter_es.data)):
    expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
    if expected_random_null != 0:
        scaled_value = v / expected_random_null
    else:
        scaled_value = 1
    if scaled_value > 1:
        scaled_value = 1
    connectivities[i, j] = scaled_value
    expected_n_edges[i, j] = expected_random_null",1,,,,,,,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/compatibility/v2/api/_quantum_computer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/compatibility/v2/api/_quantum_computer.py,,"def _symmetrization(program: Program, meas_qubits: List[int], symm_type: int=3) -> Tuple[List[Program], List[Tuple[bool]]]:
    """"""
    For the input program generate new programs which flip the measured qubits with an X gate in
    certain combinations in order to symmetrize readout.

    An expanded list of programs is returned along with a list of bools which indicates which
    qubits are flipped in each program.

    The symmetrization types are specified by an int; the types available are:

    * -1 -- exhaustive symmetrization uses every possible combination of flips
    *  0 -- trivial that is no symmetrization
    *  1 -- symmetrization using an OA with strength 1
    *  2 -- symmetrization using an OA with strength 2
    *  3 -- symmetrization using an OA with strength 3

    In the context of readout symmetrization the strength of the orthogonal array enforces the
    symmetry of the marginal confusion matrices.

    By default a strength 3 OA is used; this ensures expectations of the form <b_k * b_j * b_i>
    for bits any bits i,j,k will have symmetric readout errors. Here expectation of a random
    variable x as is denote <x> = sum_i Pr(i) x_i. It turns out that a strength 3 OA is also a
    strength 2 and strength 1 OA it also ensures <b_j * b_i> and <b_i> have symmetric readout
    errors for any bits b_j and b_i.

    :param programs: a program which will be symmetrized.
    :param meas_qubits: the groups of measurement qubits. Only these qubits will be symmetrized
        over, even if the program acts on other qubits.
    :param sym_type: an int determining the type of symmetrization performed.
    :return: a list of symmetrized programs, the corresponding array of bools indicating which
        qubits were flipped.
    """"""
    if symm_type < -1 or symm_type > 3:
        raise ValueError('symm_type must be one of the following ints [-1, 0, 1, 2, 3].')
    elif symm_type == -1:
        flip_matrix = np.asarray(list(itertools.product([0, 1], repeat=len(meas_qubits))))
    elif symm_type >= 0:
        flip_matrix = _construct_orthogonal_array(len(meas_qubits), symm_type)
    flip_matrix = flip_matrix[:, :len(meas_qubits)]
    symm_programs = []
    flip_arrays = []
    for flip_array in flip_matrix:
        total_prog_symm = program.copy()
        prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
        total_prog_symm += prog_symm
        symm_programs.append(total_prog_symm)
        flip_arrays.append(flip_array)
    return (symm_programs, flip_arrays)","for flip_array in flip_matrix:
    total_prog_symm = program.copy()
    prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
    total_prog_symm += prog_symm
    symm_programs.append(total_prog_symm)
    flip_arrays.append(flip_array)","for i, flip_array in enumerate(flip_matrix):
    total_prog_symm = program.copy()
    prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
    total_prog_symm += prog_symm
    symm_programs.append(total_prog_symm)
    flip_arrays.append(flip_array)",1,,,,,,,,,,
videos,https://github.com/3b1b/videos/tree/master/_2017/waves.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/waves.py,EMWave,"def __init__(self, **kwargs):
    digest_config(self, kwargs)
    if not all(self.propogation_direction == RIGHT):
        self.matrix_transform = np.dot(z_to_vector(self.propogation_direction), np.linalg.inv(z_to_vector(RIGHT)))
    else:
        self.matrix_transform = None
    vector_oscillations = []
    self.E_vects = VGroup()
    self.M_vects = VGroup()
    self.A_vect = np.array(self.A_vect) / get_norm(self.A_vect)
    self.A_vect *= self.amplitude
    for alpha in np.linspace(0, 1, self.n_vectors):
        tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
        phase = -alpha * self.length * self.wave_number
        kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
        E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
        M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
        vector_oscillations += [E_ov, M_ov]
        self.E_vects.add(E_ov.vector)
        self.M_vects.add(M_ov.vector)
    ContinualAnimationGroup.__init__(self, *vector_oscillations)","for alpha in np.linspace(0, 1, self.n_vectors):
    tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
    phase = -alpha * self.length * self.wave_number
    kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
    E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
    M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
    vector_oscillations += [E_ov, M_ov]
    self.E_vects.add(E_ov.vector)
    self.M_vects.add(M_ov.vector)","for i,alpha in enumerate(np.linspace(0, 1, self.n_vectors)):
    tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
    phase = -alpha * self.length * self.wave_number
    kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
    E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
    M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
    vector_oscillations += [E_ov, M_ov]
    self.E_vects.add(E_ov.vector)
    self.M_vects.add(M_ov.vector)",1,,,,,,,,,,
py3status,https://github.com/ultrabug/py3status/tree/master/py3status/modules/net_iplist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/py3status/py3status/modules/net_iplist.py,Py3status,"def _get_data(self):
    txt = self.py3.command_output(['ip', 'address', 'show']).splitlines()
    data = {}
    for line in txt:
        iface = self.iface_re.match(line)
        if iface:
            cur_iface = iface.group('iface')
            if not self.remove_empty:
                data[cur_iface] = {}
            continue
        ip4 = self.ip_re.match(line)
        if ip4:
            data.setdefault(cur_iface, {}).setdefault('ip4', []).append(ip4.group('ip4'))
            continue
        ip6 = self.ip6_re.match(line)
        if ip6:
            data.setdefault(cur_iface, {}).setdefault('ip6', []).append(ip6.group('ip6'))
            continue
    return data","for line in txt:
    iface = self.iface_re.match(line)
    if iface:
        cur_iface = iface.group('iface')
        if not self.remove_empty:
            data[cur_iface] = {}
        continue
    ip4 = self.ip_re.match(line)
    if ip4:
        data.setdefault(cur_iface, {}).setdefault('ip4', []).append(ip4.group('ip4'))
        continue
    ip6 = self.ip6_re.match(line)
    if ip6:
        data.setdefault(cur_iface, {}).setdefault('ip6', []).append(ip6.group('ip6'))
        continue","for i,line in enumerate(txt):
    iface = self.iface_re.match(line)
    if iface:
        cur_iface = iface.group('iface')
        if not self.remove_empty:
            data[cur_iface] = {}
        continue
    ip4 = self.ip_re.match(line)
    if ip4:
        data.setdefault(cur_iface, {}).setdefault('ip4', []).append(ip4.group('ip4'))
        continue
    ip6 = self.ip6_re.match(line)
    if ip6:
        data.setdefault(cur_iface, {}).setdefault('ip6', []).append(ip6.group('ip6'))
        continue",1,,,,,,,,,,
justpy,https://github.com/elimintz/justpy/tree/master/justpy/htmlcomponents.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/justpy/justpy/htmlcomponents.py,Div,"def to_html(self, indent=0, indent_step=0, format=True):
    block_indent = ' ' * indent
    if format:
        ws = '\n'
    else:
        ws = ''
    s = f'{block_indent}<{self.html_tag} '
    d = self.convert_object_to_dict()
    for (attr, value) in d['attrs'].items():
        if value:
            s = f'{s}{attr}=""{value}"" '
    if self.style:
        s = f'{s}style=""{self.style}""'
    if self.classes:
        s = f'{s}class=""{self.classes}"">{ws}'
    else:
        s = f'{s}>{ws}'
    if self.inner_html:
        s = f'{s}{self.inner_html}</{self.html_tag}>{ws}'
        return s
    try:
        s = f'{s}{self.text}{ws}'
    except:
        pass
    for c in self.components:
        s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'
    s = f'{s}{block_indent}</{self.html_tag}>{ws}'
    return s","for c in self.components:
    s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'","for i,c in enumerate(self.components):
    s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'",1,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/thrift/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/thrift/all/conanfile.py,ThriftConan,"def export_sources(self):
    for p in self.conan_data.get('patches', {}).get(self.version, []):
        copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)","for p in self.conan_data.get('patches', {}).get(self.version, []):
    copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)","for i,p in enumerate(self.conan_data.get('patches', {}).get(self.version, [])):
    copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o","for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))","for i, (conf_label, conf) in enumerate((('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf))):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))",1,,,,,,,,,,
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/s3/_read_parquet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-data-wrangler/awswrangler/s3/_read_parquet.py,,"def _read_parquet(path: str, version_id: Optional[str], columns: Optional[List[str]], categories: Optional[List[str]], safe: bool, map_types: bool, boto3_session: Union[boto3.Session, _utils.Boto3PrimitivesType], dataset: bool, validate_schema: Optional[bool], path_root: Optional[str], s3_additional_kwargs: Optional[Dict[str, str]], use_threads: Union[bool, int], pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:
    pyarrow_args = _set_default_pyarrow_additional_kwargs(pyarrow_additional_kwargs)
    boto3_session = _utils.ensure_session(boto3_session)
    df: pd.DataFrame = _arrowtable2df(table=_read_parquet_file(path=path, columns=columns, categories=categories, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, use_threads=use_threads, version_id=version_id, pyarrow_additional_kwargs=pyarrow_args), categories=categories, safe=safe, map_types=map_types, use_threads=use_threads, dataset=dataset, path=path, path_root=path_root, timestamp_as_object=pyarrow_args['timestamp_as_object'])
    if validate_schema and columns:
        for column in columns:
            if column not in df.columns and column not in df.index.names:
                raise exceptions.InvalidArgument(f'column: {column} does not exist')
    return df","for column in columns:
    if column not in df.columns and column not in df.index.names:
        raise exceptions.InvalidArgument(f'column: {column} does not exist')","for i,column in enumerate(columns):
    if column not in df.columns and column not in df.index.names:
        raise exceptions.InvalidArgument(f'column: {column} does not exist')",1,,,,,,,,,,
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for f in filenames:
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))","for i,f in enumerate(filenames):
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))",1,,,,,,,,,,
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for line in input_file:
    filenames.append(line.rstrip())","for i,line in enumerate(input_file):
    filenames.append(line.rstrip())",1,,,,,,,,,,
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for item in json_data:
    print(json.dumps(item))","for i,item in enumerate(json_data):
    print(json.dumps(item))",1,,,,,,,,,,
TSD,https://github.com/Sense-X/TSD/tree/master/mmdet/models/detectors/reppoints_detector.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/detectors/reppoints_detector.py,RepPointsDetector,"def merge_aug_results(self, aug_bboxes, aug_scores, img_metas):
    """"""Merge augmented detection bboxes and scores.

        Args:
            aug_bboxes (list[Tensor]): shape (n, 4*#class)
            aug_scores (list[Tensor] or None): shape (n, #class)
            img_shapes (list[Tensor]): shape (3, ).

        Returns:
            tuple: (bboxes, scores)
        """"""
    recovered_bboxes = []
    for (bboxes, img_info) in zip(aug_bboxes, img_metas):
        img_shape = img_info[0]['img_shape']
        scale_factor = img_info[0]['scale_factor']
        flip = img_info[0]['flip']
        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
        recovered_bboxes.append(bboxes)
    bboxes = torch.cat(recovered_bboxes, dim=0)
    if aug_scores is None:
        return bboxes
    else:
        scores = torch.cat(aug_scores, dim=0)
        return (bboxes, scores)","for (bboxes, img_info) in zip(aug_bboxes, img_metas):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)","for i, (bboxes, img_info) in enumerate(zip(aug_bboxes, img_metas)):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)",1,,,,,,,,,,
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/sr_evaluator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/sr_evaluator.py,ModelOVModel,"def fit_to_input(self, input_data):
    fitted = {}
    for (name, info) in self.inputs.items():
        data = input_data[self._name_to_idx[name]]
        data = np.expand_dims(data, axis=0)
        data = np.transpose(data, [0, 3, 1, 2])
        if not info.get_partial_shape.is_dynamic:
            assert parse_partial_shape(info.input_data.shape) == np.shape(data)
        fitted[name] = data
    return fitted","for (name, info) in self.inputs.items():
    data = input_data[self._name_to_idx[name]]
    data = np.expand_dims(data, axis=0)
    data = np.transpose(data, [0, 3, 1, 2])
    if not info.get_partial_shape.is_dynamic:
        assert parse_partial_shape(info.input_data.shape) == np.shape(data)
    fitted[name] = data","for i, (name, info) in enumerate(self.inputs.items()):
    data = input_data[self._name_to_idx[name]]
    data = np.expand_dims(data, axis=0)
    data = np.transpose(data, [0, 3, 1, 2])
    if not info.get_partial_shape.is_dynamic:
        assert parse_partial_shape(info.input_data.shape) == np.shape(data)
    fitted[name] = data",1,,,,,,,,,,
nnFormer,https://github.com/282857341/nnFormer/tree/master/nnformer/preprocessing/preprocessing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/preprocessing/preprocessing.py,GenericPreprocessor,"def resample_and_normalize(self, data, target_spacing, properties, seg=None, force_separate_z=None):
    """"""
        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values
        (spacing etc)
        :param data:
        :param target_spacing:
        :param properties:
        :param seg:
        :param force_separate_z:
        :return:
        """"""
    original_spacing_transposed = np.array(properties['original_spacing'])[self.transpose_forward]
    before = {'spacing': properties['original_spacing'], 'spacing_transposed': original_spacing_transposed, 'data.shape (data is transposed)': data.shape}
    data[np.isnan(data)] = 0
    (data, seg) = resample_patient(data, seg, np.array(original_spacing_transposed), target_spacing, 3, 1, force_separate_z=force_separate_z, order_z_data=0, order_z_seg=0, separate_z_anisotropy_threshold=self.resample_separate_z_anisotropy_threshold)
    after = {'spacing': target_spacing, 'data.shape (data is resampled)': data.shape}
    print('before:', before, '\nafter: ', after, '\n')
    if seg is not None:
        seg[seg < -1] = 0
    properties['size_after_resampling'] = data[0].shape
    properties['spacing_after_resampling'] = target_spacing
    use_nonzero_mask = self.use_nonzero_mask
    assert len(self.normalization_scheme_per_modality) == len(data), 'self.normalization_scheme_per_modality must have as many entries as data has modalities'
    assert len(self.use_nonzero_mask) == len(data), 'self.use_nonzero_mask must have as many entries as data has modalities'
    for c in range(len(data)):
        scheme = self.normalization_scheme_per_modality[c]
        if scheme == 'CT':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            mean_intensity = self.intensityproperties[c]['mean']
            std_intensity = self.intensityproperties[c]['sd']
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            data[c] = (data[c] - mean_intensity) / std_intensity
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        elif scheme == 'CT2':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            mask = (data[c] > lower_bound) & (data[c] < upper_bound)
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            mn = data[c][mask].mean()
            sd = data[c][mask].std()
            data[c] = (data[c] - mn) / sd
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        else:
            if use_nonzero_mask[c]:
                mask = seg[-1] >= 0
            else:
                mask = np.ones(seg.shape[1:], dtype=bool)
            data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
            data[c][mask == 0] = 0
    return (data, seg, properties)","for c in range(len(data)):
    scheme = self.normalization_scheme_per_modality[c]
    if scheme == 'CT':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        mean_intensity = self.intensityproperties[c]['mean']
        std_intensity = self.intensityproperties[c]['sd']
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        data[c] = (data[c] - mean_intensity) / std_intensity
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    elif scheme == 'CT2':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        mask = (data[c] > lower_bound) & (data[c] < upper_bound)
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        mn = data[c][mask].mean()
        sd = data[c][mask].std()
        data[c] = (data[c] - mn) / sd
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    else:
        if use_nonzero_mask[c]:
            mask = seg[-1] >= 0
        else:
            mask = np.ones(seg.shape[1:], dtype=bool)
        data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
        data[c][mask == 0] = 0","for c,scheme in enumerate(self.normalization_scheme_per_modality):
    if scheme == 'CT':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        mean_intensity = self.intensityproperties[c]['mean']
        std_intensity = self.intensityproperties[c]['sd']
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        data[c] = (data[c] - mean_intensity) / std_intensity
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    elif scheme == 'CT2':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        mask = (data[c] > lower_bound) & (data[c] < upper_bound)
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        mn = data[c][mask].mean()
        sd = data[c][mask].std()
        data[c] = (data[c] - mn) / sd
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    else:
        if use_nonzero_mask[c]:
            mask = seg[-1] >= 0
        else:
            mask = np.ones(seg.shape[1:], dtype=bool)
        data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
        data[c][mask == 0] = 0",1,,,,,,,,,,
indico,https://github.com/indico/indico/tree/master/indico/vendor/django_mail/backends/smtp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/indico/indico/vendor/django_mail/backends/smtp.py,EmailBackend,"def send_messages(self, email_messages):
    """"""
        Send one or more EmailMessage objects and return the number of email
        messages sent.
        """"""
    if not email_messages:
        return 0
    with self._lock:
        new_conn_created = self.open()
        if not self.connection or new_conn_created is None:
            return 0
        num_sent = 0
        for message in email_messages:
            sent = self._send(message)
            if sent:
                num_sent += 1
        if new_conn_created:
            self.close()
    return num_sent","for message in email_messages:
    sent = self._send(message)
    if sent:
        num_sent += 1","for i,message in enumerate(email_messages):
    sent = self._send(message)
    if sent:
        num_sent += 1",1,,,,,,,,,,
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)","for cell_block in mesh.cells:
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))","for i, cell_block in enumerate(mesh.cells):
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))",1,,,,,,,,,,
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)","for cell in cell_block.data:
    ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))","for i, cell in enumerate(cell_block.data):
    ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))",1,,,,,,,,,,
hyperpose,https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pifpaf/eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyperpose/hyperpose/Model/pifpaf/eval.py,,"def visualize(img, img_id, processed_img, pd_pif_maps, pd_paf_maps, humans, stride=8, save_dir='./save_dir'):
    print(f'{len(humans)} human found!')
    print('visualizing...')
    os.makedirs(save_dir, exist_ok=True)
    ori_img = np.clip(img * 255.0, 0.0, 255.0).astype(np.uint8)
    processed_img = np.clip(processed_img * 255.0, 0.0, 255.0).astype(np.uint8)
    vis_img = ori_img.copy()
    for human in humans:
        vis_img = human.draw_human(vis_img)
    (pd_pif_conf, pd_pif_vec, _, pd_pif_scale) = pd_pif_maps
    (pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, _, _, _, _) = pd_paf_maps
    pd_pif_conf_show = np.amax(pd_pif_conf, axis=0)
    pd_pif_hr_conf_show = np.amax(get_hr_conf(pd_pif_conf, pd_pif_vec, pd_pif_scale, stride=stride, thresh=0.1), axis=0)
    pd_paf_conf_show = np.amax(pd_paf_conf, axis=0)
    pd_paf_vec_show = np.zeros(shape=(pd_pif_hr_conf_show.shape[0], pd_pif_hr_conf_show.shape[1], 3)).astype(np.int8)
    pd_paf_vec_show = get_arrow_map(pd_paf_vec_show, pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, thresh=0.1)
    fig = plt.figure(figsize=(12, 12))
    a = fig.add_subplot(3, 3, 1)
    a.set_title('input image')
    plt.imshow(ori_img)
    a = fig.add_subplot(3, 3, 3)
    a.set_title('output result')
    plt.imshow(vis_img)
    a = fig.add_subplot(3, 3, 4)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 5)
    a.set_title('pif_conf_map')
    plt.imshow(pd_pif_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 6)
    a.set_title('pif_hr_conf_map')
    plt.imshow(pd_pif_hr_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 7)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 8)
    a.set_title('paf_conf_map')
    plt.imshow(pd_paf_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 9)
    a.set_title('paf_vec_map')
    plt.imshow(pd_paf_vec_show, alpha=0.8)
    plt.colorbar()
    plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))
    plt.close()","for human in humans:
    vis_img = human.draw_human(vis_img)","for i,human in enumerate(humans):
    vis_img = human.draw_human(vis_img)",1,,,,,,,,,,
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for marker in self.tracker.markers:
    marker_type = marker.marker_type
    if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
        continue
    centroid = marker.centroid()
    if marker.uid in surface.registered_markers_dist.keys():
        active_markers = active_markers_by_type.get(marker_type, [])
        active_markers.append(centroid)
        active_markers_by_type[marker_type] = active_markers
    else:
        inactive_markers = inactive_markers_by_type.get(marker_type, [])
        inactive_markers.append(centroid)
        inactive_markers_by_type[marker_type] = inactive_markers","for i, marker in enumerate(self.tracker.markers):
    marker_type = marker.marker_type
    if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
        continue
    centroid = marker.centroid()
    if marker.uid in surface.registered_markers_dist.keys():
        active_markers = active_markers_by_type.get(marker_type, [])
        active_markers.append(centroid)
        active_markers_by_type[marker_type] = active_markers
    else:
        inactive_markers = inactive_markers_by_type.get(marker_type, [])
        inactive_markers.append(centroid)
        inactive_markers_by_type[marker_type] = inactive_markers",1,,,,,,,,,,
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for (marker_type, inactive_markers) in inactive_markers_by_type.items():
    color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in inactive_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, (marker_type, inactive_markers) in enumerate(inactive_markers_by_type.items()):
    color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in inactive_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))",1,,,,,,,,,,
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for (marker_type, active_markers) in active_markers_by_type.items():
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, (marker_type, active_markers) in enumerate(active_markers_by_type.items()):
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))",1,,,,,,,,,,
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for pt in inactive_markers:
    self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, pt in enumerate(inactive_markers):
    self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))",1,,,,,,,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0","for (counter, count) in capa.perf.counters.most_common():
    logger.debug('perf: counter: {:}: {:,}'.format(counter, count))","for i, (counter, count) in enumerate(capa.perf.counters.most_common()):
    logger.debug('perf: counter: {:}: {:,}'.format(counter, count))",1,,,,,,,,,,
MultiQC,https://github.com/ewels/MultiQC/tree/master/multiqc/utils/log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/utils/log.py,,"def get_log_stream(logger):
    """"""
    Returns a stream to the root log file.
    If there is no logfile return the stderr log stream

    Returns:
        A stream to the root log file or stderr stream.
    """"""
    file_stream = None
    log_stream = None
    for handler in logger.handlers:
        if isinstance(handler, logging.FileHandler):
            file_stream = handler.stream
        else:
            log_stream = handler.stream
    if file_stream:
        return file_stream
    return log_stream","for handler in logger.handlers:
    if isinstance(handler, logging.FileHandler):
        file_stream = handler.stream
    else:
        log_stream = handler.stream","for i,handler in enumerate(logger.handlers):
    if isinstance(handler, logging.FileHandler):
        file_stream = handler.stream
    else:
        log_stream = handler.stream",1,,,,,,,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/experiment/_main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/experiment/_main.py,Experiment,"def build_symmetrization_memory_maps(self, qubits: Sequence[int], label: str='symmetrization') -> List[Dict[str, List[float]]]:
    """"""
        Build a list of memory maps to be used in a program that is trying to perform readout
        symmetrization via parametric compilation. For example, if we have the following program:

            RX(symmetrization[0]) 0
            RX(symmetrization[1]) 1
            MEASURE 0 ro[0]
            MEASURE 1 ro[1]

        We can perform exhaustive readout symmetrization on our two qubits by providing the four
        following memory maps, and then appropriately flipping the resultant bitstrings:

            {'symmetrization': [0.0, 0.0]} -> XOR results with [0,0]
            {'symmetrization': [0.0, pi]}  -> XOR results with [0,1]
            {'symmetrization': [pi, 0.0]}  -> XOR results with [1,0]
            {'symmetrization': [pi, pi]}   -> XOR results with [1,1]

        :param qubits: List of qubits to symmetrize readout for.
        :param label: Name of the declared memory region. Defaults to ""symmetrization"".
        :return: List of memory maps that performs the desired level of symmetrization.
        """"""
    num_meas_registers = len(self.get_meas_qubits())
    symm_registers = self.get_meas_registers(qubits)
    if self.symmetrization == SymmetrizationLevel.NONE:
        return [{}]
    if self.symmetrization != SymmetrizationLevel.EXHAUSTIVE:
        raise ValueError('We only support exhaustive symmetrization for now.')
    import numpy as np
    import itertools
    assignments = itertools.product(np.array([0, np.pi]), repeat=len(symm_registers))
    memory_maps = []
    for a in assignments:
        zeros = np.zeros(num_meas_registers)
        for (idx, r) in enumerate(symm_registers):
            zeros[r] = a[idx]
        memory_maps.append({f'{label}': list(zeros)})
    return memory_maps","for a in assignments:
    zeros = np.zeros(num_meas_registers)
    for (idx, r) in enumerate(symm_registers):
        zeros[r] = a[idx]
    memory_maps.append({f'{label}': list(zeros)})","for i,a in enumerate(assignments):
    zeros = np.zeros(num_meas_registers)
    for (idx, r) in enumerate(symm_registers):
        zeros[r] = a[idx]
    memory_maps.append({f'{label}': list(zeros)})",1,,,,,,,,,,
orchest,https://github.com/orchest/orchest/tree/master/services/orchest-api/app/app/core/tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orchest/services/orchest-api/app/app/core/tasks.py,,"def delete_job_pipeline_run_directories(self, project_uuid: str, pipeline_uuid: str, job_uuid: str, pipeline_run_uuids: List[str]) -> str:
    """"""Deletes a list of job pipeline run directories given uuids.""""""
    job_dir = os.path.join('/userdir', 'jobs', project_uuid, pipeline_uuid, job_uuid)
    for uuid in pipeline_run_uuids:
        shutil.rmtree(os.path.join(job_dir, uuid), ignore_errors=True)
    return 'SUCCESS'","for uuid in pipeline_run_uuids:
    shutil.rmtree(os.path.join(job_dir, uuid), ignore_errors=True)","for i, uuid in enumerate(pipeline_run_uuids):
    shutil.rmtree(os.path.join(job_dir, uuid), ignore_errors=True)",1,,,,,,,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","for i in range(bbox.shape[0]):
    if easy_view and label[i] not in g_easy_view_labels:
        continue
    if exclude_table and label[i] == g_classes.index('table'):
        continue
    length = bbox[i, 3:6] - bbox[i, 0:3]
    a = length[0]
    b = length[1]
    c = length[2]
    x = bbox[i, 0]
    y = bbox[i, 1]
    z = bbox[i, 2]
    color = np.array(g_label2color[label[i]], dtype=float) / 255.0
    material = 'material%d' % ins_cnt
    fout_obj.write('usemtl %s\n' % material)
    fout_obj.write('v %f %f %f\n' % (x, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y, z))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z))
    fout_obj.write('g default\n')
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
    fout_obj.write('\n')
    fout_mtl.write('newmtl %s\n' % material)
    fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
    fout_mtl.write('\n')
    v_cnt += 8
    ins_cnt += 1","for i in range(bbox.shape[0]):
    if easy_view and label[i] not in g_easy_view_labels:
        continue
    if exclude_table and label[i] == g_classes.index('table'):
        continue
    length = bbox[i, 3:6] - bbox[i, 0:3]
    a = length[0]
    b = length[1]
    c = length[2]
    x = bbox[i, 0]
    y = bbox[i, 1]
    z = bbox[i, 2]
    color = np.array(g_label2color[label[i]], dtype=float) / 255.0
    material = 'material%d' % ins_cnt
    fout_obj.write('usemtl %s\n' % material)
    fout_obj.write('v %f %f %f\n' % (x, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y, z))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z))
    fout_obj.write('g default\n')
    fout_obj.write('f %d %d %d %d\n' % (4 + i*8, 3 + i*8, 2 + i*8, 1 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (1 + i*8, 2 + i*8, 6 + i*8, 5 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (7 + i*8, 6 + i*8, 2 + i*8, 3 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (4 + i*8, 8 + i*8, 7 + i*8, 3 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (5 + i*8, 8 + i*8, 4 + i*8, 1 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (5 + i*8, 6 + i*8, 7 + i*8, 8 + i*8))
    fout_obj.write('\n')
    fout_mtl.write('newmtl %s\n' % material)
    fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
    fout_mtl.write('\n')
    ins_cnt += 1",1,,,,,,,,,,
zentral,https://github.com/zentralopensource/zentral/tree/master/tests/inventory/test_metrics_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zentral/tests/inventory/test_metrics_views.py,PrometheusViewsTestCase,"def test_prometheus_metrics_osx_apps_bundle_names(self):
    old_config = settings._collection['apps']['zentral.contrib.inventory'].pop('metrics_options', None)
    settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = ConfigDict({'osx_apps': {'sources': ['zentral tests'], 'bundle_names': ['Baller']}})
    response = self.client.get(reverse('inventory_metrics:all'), HTTP_AUTHORIZATION='Bearer CHANGE ME!!!')
    self.assertEqual(response.status_code, 200)
    seen = False
    for family in text_string_to_metric_families(response.content.decode('utf-8')):
        if family.name == 'zentral_inventory_active_machines_bucket':
            continue
        self.assertEqual(len(family.samples), 7)
        for sample in family.samples:
            self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
            le = sample.labels['le']
            self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
            if le == '1':
                self.assertEqual(sample.value, 0)
            else:
                self.assertEqual(sample.value, 1)
        self.assertFalse(seen)
        seen = True
    self.assertTrue(seen)
    if old_config:
        settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = old_config","for family in text_string_to_metric_families(response.content.decode('utf-8')):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True","for i,family in enumerate(text_string_to_metric_families(response.content.decode('utf-8'))):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True",1,,,,,,,,,,
pretrained-models.pytorch,https://github.com/Cadene/pretrained-models.pytorch/tree/master/pretrainedmodels/datasets/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretrained-models.pytorch/pretrainedmodels/datasets/utils.py,AveragePrecisionMeter,"def value(self):
    """"""Returns the model's average precision for each class
        Return:
            ap (FloatTensor): 1xK tensor, with avg precision for each class k
        """"""
    if self.scores.numel() == 0:
        return 0
    ap = torch.zeros(self.scores.size(1))
    rg = torch.arange(1, self.scores.size(0)).float()
    for k in range(self.scores.size(1)):
        scores = self.scores[:, k]
        targets = self.targets[:, k]
        ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)
    return ap","for k in range(self.scores.size(1)):
    scores = self.scores[:, k]
    targets = self.targets[:, k]
    ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)","for k, _ in enumerate(range(self.scores.size(1))):
    scores = self.scores[:, k]
    targets = self.targets[:, k]
    ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)",1,,,,,,,,,,
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for size in self.photo_sizes.all():
    size.clear_cache()","for i,size in enumerate(self.photo_sizes.all()):
    size.clear_cache()",1,,,,,,,,,,
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
    for obj in getattr(self, prop).all():
        obj.clear_cache()
        obj.pre_cache()","for i, prop in enumerate([prop for prop in dir(self) if prop[-8:] == '_related']):
    for obj in getattr(self, prop).all():
        obj.clear_cache()
        obj.pre_cache()",1,,,,,,,,,,
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for obj in getattr(self, prop).all():
    obj.clear_cache()
    obj.pre_cache()","for i,obj in enumerate(getattr(self, prop).all()):
    obj.clear_cache()
    obj.pre_cache()",1,,,,,,,,,,
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for i in range(len(olist) // 2):
    (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
    (FB, FC, FH, FW) = ocls.size()
    stride = 2 ** (i + 2)
    anchor = stride * 4
    poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
    for (Iindex, hindex, windex) in poss:
        (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
        score = ocls[0, 1, hindex, windex]
        loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
        priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
        variances = [0.1, 0.2]
        box = decode(loc, priors, variances)
        (x1, y1, x2, y2) = box[0]
        bboxlist.append([x1, y1, x2, y2, score])","for i, _ in enumerate(range(len(olist) // 2)):
    (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
    (FB, FC, FH, FW) = ocls.size()
    stride = 2 ** (i + 2)
    anchor = stride * 4
    poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
    for (Iindex, hindex, windex) in poss:
        (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
        score = ocls[0, 1, hindex, windex]
        loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
        priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
        variances = [0.1, 0.2]
        box = decode(loc, priors, variances)
        (x1, y1, x2, y2) = box[0]
        bboxlist.append([x1, y1, x2, y2, score])",1,,,,,,,,,,
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
    (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
    self.writer.add_summary(summary, i)
    (accuracy, _) = self.accuracy(predictions, labels)
    if i % 50 == 0:
        print('Minibatch loss at step %d: %f' % (i, l))
        print('Minibatch accuracy: %.1f%%' % accuracy)","for i, (samples, labels) in enumerate(data_iterator(train_samples, train_labels, self.train_batch_size)):
    (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
    self.writer.add_summary(summary, i)
    (accuracy, _) = self.accuracy(predictions, labels)
    if i % 50 == 0:
        print('Minibatch loss at step %d: %f' % (i, l))
        print('Minibatch accuracy: %.1f%%' % accuracy)",1,,,,,,,,,,
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
    print('samples shape', samples.shape)
    (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
    self.writer.add_summary(summary, i)
    (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
    accuracies.append(accuracy)
    confusionMatrices.append(cm)
    print('Test Accuracy: %.1f%%' % accuracy)","for i, (samples, labels) in enumerate(data_iterator(test_samples, test_labels, self.test_batch_size)):
    print('samples shape', samples.shape)
    (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
    self.writer.add_summary(summary, i)
    (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
    accuracies.append(accuracy)
    confusionMatrices.append(cm)
    print('Test Accuracy: %.1f%%' % accuracy)",1,,,,,,,,,,
mypy,https://github.com/python/mypy/tree/master/mypy/join.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/join.py,,"def combine_arg_names(t: CallableType, s: CallableType) -> List[Optional[str]]:
    """"""Produces a list of argument names compatible with both callables.

    For example, suppose 't' and 's' have the following signatures:

    - t: (a: int, b: str, X: str) -> None
    - s: (a: int, b: str, Y: str) -> None

    This function would return [""a"", ""b"", None]. This information
    is then used above to compute the join of t and s, which results
    in a signature of (a: int, b: str, str) -> None.

    Note that the third argument's name is omitted and 't' and 's'
    are both valid subtypes of this inferred signature.

    Precondition: is_similar_types(t, s) is true.
    """"""
    num_args = len(t.arg_types)
    new_names = []
    for i in range(num_args):
        t_name = t.arg_names[i]
        s_name = s.arg_names[i]
        if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
            new_names.append(t_name)
        else:
            new_names.append(None)
    return new_names","for i in range(num_args):
    t_name = t.arg_names[i]
    s_name = s.arg_names[i]
    if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
        new_names.append(t_name)
    else:
        new_names.append(None)","for i, (t_name, s_name) in enumerate(zip(t.arg_names, s.arg_names)):
    if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
        new_names.append(t_name)
    else:
        new_names.append(None)",1,,,,,,,,,,
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch12/tx.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/programmingbitcoin/code-ch12/tx.py,Tx,"def fee(self):
    """"""Returns the fee of this transaction in satoshi""""""
    (input_sum, output_sum) = (0, 0)
    for tx_in in self.tx_ins:
        input_sum += tx_in.value(self.testnet)
    for tx_out in self.tx_outs:
        output_sum += tx_out.amount
    return input_sum - output_sum","for tx_out in self.tx_outs:
    output_sum += tx_out.amount","for i, tx_out in enumerate(self.tx_outs):
    output_sum += tx_out.amount",1,,,,,,,,,,
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str","for index in range(service_count):
    service = services[index]
    index += 1
    commandData = ''
    if service.upper() == 'ZOOKEEPER':
        commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
    else:
        commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
    payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})","for index, service in enumerate(services):
    index += 1
    commandData = ''
    if service.upper() == 'ZOOKEEPER':
        commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
    else:
        commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
    payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})",1,,,,,,,,,,
nltk,https://github.com/nltk/nltk/tree/master/nltk/tokenize/punkt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tokenize/punkt.py,PunktBaseClass,"def _tokenize_words(self, plaintext):
    """"""
        Divide the given text into tokens, using the punkt word
        segmentation regular expression, and generate the resulting list
        of tokens augmented as three-tuples with two boolean values for whether
        the given token occurs at the start of a paragraph or a new line,
        respectively.
        """"""
    parastart = False
    for line in plaintext.split('\n'):
        if line.strip():
            line_toks = iter(self._lang_vars.word_tokenize(line))
            try:
                tok = next(line_toks)
            except StopIteration:
                continue
            yield self._Token(tok, parastart=parastart, linestart=True)
            parastart = False
            for tok in line_toks:
                yield self._Token(tok)
        else:
            parastart = True","for line in plaintext.split('\n'):
    if line.strip():
        line_toks = iter(self._lang_vars.word_tokenize(line))
        try:
            tok = next(line_toks)
        except StopIteration:
            continue
        yield self._Token(tok, parastart=parastart, linestart=True)
        parastart = False
        for tok in line_toks:
            yield self._Token(tok)
    else:
        parastart = True","for i,line in enumerate(plaintext.split('\n')):
    if line.strip():
        line_toks = iter(self._lang_vars.word_tokenize(line))
        try:
            tok = next(line_toks)
        except StopIteration:
            continue
        yield self._Token(tok, parastart=parastart, linestart=True)
        parastart = False
        for tok in line_toks:
            yield self._Token(tok)
    else:
        parastart = True",1,,,,,,,,,,
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for nonterminal in nonterminals:
    if nonterminal not in first_plans:
        _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)","for i, nonterminal in enumerate(nonterminals):
    if nonterminal not in first_plans:
        _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)",1,,,,,,,,,,
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for dfas in nonterminal_to_dfas.values():
    for dfa_state in dfas:
        transitions = dfa_state.transitions
        for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
            for (transition, pushes) in first_plans[nonterminal].items():
                if transition in transitions:
                    prev_plan = transitions[transition]
                    choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                    raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                transitions[transition] = DFAPlan(next_dfa, pushes)","for i, dfas in enumerate(nonterminal_to_dfas.values()):
    for dfa_state in dfas:
        transitions = dfa_state.transitions
        for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
            for (transition, pushes) in first_plans[nonterminal].items():
                if transition in transitions:
                    prev_plan = transitions[transition]
                    choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                    raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                transitions[transition] = DFAPlan(next_dfa, pushes)",1,,,,,,,,,,
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
    for (transition, pushes) in first_plans[nonterminal].items():
        if transition in transitions:
            prev_plan = transitions[transition]
            choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
            raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
        transitions[transition] = DFAPlan(next_dfa, pushes)","for i, (nonterminal, next_dfa) in enumerate(dfa_state.nonterminal_arcs.items()):
    for (transition, pushes) in first_plans[nonterminal].items():
        if transition in transitions:
            prev_plan = transitions[transition]
            choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
            raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
        transitions[transition] = DFAPlan(next_dfa, pushes)",1,,,,,,,,,,
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for (transition, pushes) in first_plans[nonterminal].items():
    if transition in transitions:
        prev_plan = transitions[transition]
        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
    transitions[transition] = DFAPlan(next_dfa, pushes)","for i, (transition, pushes) in enumerate(first_plans[nonterminal].items()):
    if transition in transitions:
        prev_plan = transitions[transition]
        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
    transitions[transition] = DFAPlan(next_dfa, pushes)",1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/dependencies/repository/relation_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/dependencies/repository/relation_builder.py,RelationBuilder,"def get_updated_changeset_revisions_for_repository_dependencies(self, key_rd_dicts):
    updated_key_rd_dicts = []
    for key_rd_dict in key_rd_dicts:
        key = next(iter(key_rd_dict))
        repository_dependency = key_rd_dict[key]
        (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
        if suc.tool_shed_is_this_tool_shed(rd_toolshed):
            repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
            if repository:
                repository_id = self.app.security.encode_id(repository.id)
                repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = repository_dependency
                    updated_key_rd_dicts.append(key_rd_dict)
                else:
                    changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                    if changeset_revision != rd_changeset_revision:
                        repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                    if repository_metadata:
                        new_key_rd_dict = {}
                        new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                        updated_key_rd_dicts.append(new_key_rd_dict)
                    else:
                        repository_components_tuple = container_util.get_components_from_key(key)
                        components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                        (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                        if len(components_list) in (4, 5):
                            rd_only_if_compiling_contained_td = 'False'
                        message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                        message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                        log.debug(message)
            else:
                repository_components_tuple = container_util.get_components_from_key(key)
                components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                log.debug(message)
    return updated_key_rd_dicts","for key_rd_dict in key_rd_dicts:
    key = next(iter(key_rd_dict))
    repository_dependency = key_rd_dict[key]
    (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
    if suc.tool_shed_is_this_tool_shed(rd_toolshed):
        repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
        if repository:
            repository_id = self.app.security.encode_id(repository.id)
            repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
            if repository_metadata:
                new_key_rd_dict = {}
                new_key_rd_dict[key] = repository_dependency
                updated_key_rd_dicts.append(key_rd_dict)
            else:
                changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                if changeset_revision != rd_changeset_revision:
                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                    updated_key_rd_dicts.append(new_key_rd_dict)
                else:
                    repository_components_tuple = container_util.get_components_from_key(key)
                    components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                    (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                    if len(components_list) in (4, 5):
                        rd_only_if_compiling_contained_td = 'False'
                    message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                    message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                    log.debug(message)
        else:
            repository_components_tuple = container_util.get_components_from_key(key)
            components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
            (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
            message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
            message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
            log.debug(message)","for i,key_rd_dict in enumerate(key_rd_dicts):
    key = next(iter(key_rd_dict))
    repository_dependency = key_rd_dict[key]
    (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
    if suc.tool_shed_is_this_tool_shed(rd_toolshed):
        repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
        if repository:
            repository_id = self.app.security.encode_id(repository.id)
            repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
            if repository_metadata:
                new_key_rd_dict = {}
                new_key_rd_dict[key] = repository_dependency
                updated_key_rd_dicts.append(key_rd_dict)
            else:
                changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                if changeset_revision != rd_changeset_revision:
                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                    updated_key_rd_dicts.append(new_key_rd_dict)
                else:
                    repository_components_tuple = container_util.get_components_from_key(key)
                    components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                    (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                    if len(components_list) in (4, 5):
                        rd_only_if_compiling_contained_td = 'False'
                    message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                    message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                    log.debug(message)
        else:
            repository_components_tuple = container_util.get_components_from_key(key)
            components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
            (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
            message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
            message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
            log.debug(message)",1,,,,,,,,,,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for person in peoplelist:
    if args.vv == True or args.debug == True:
        print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
    else:
        sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
        sys.stdout.flush()
    count = count + 1
    if person.person_image:
        try:
            target_image = face_recognition.load_image_file(person.person_image)
            target_encoding = face_recognition.face_encodings(target_image)[0]
            profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
            if args.debug == True:
                print(profilelist)
        except:
            continue
    else:
        continue
    early_break = False
    updatedlist = []
    for (profilelink, profilepic, distance) in profilelist:
        try:
            os.remove('potential_target_image.jpg')
        except:
            pass
        if early_break:
            break
        image_link = profilepic
        if image_link:
            try:
                urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                try:
                    potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                except:
                    continue
                results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                for result in results:
                    if args.mode == 'fast':
                        if result < threshold:
                            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                            if args.vv == True:
                                print('\tMatch found: ' + person.full_name)
                                print('\tLinkedIn: ' + person.linkedin)
                            early_break = True
                            break
                    elif args.mode == 'accurate':
                        if result < threshold:
                            updatedlist.append([profilelink, image_link, result])
            except Exception as e:
                print(e)
    if args.mode == 'accurate':
        highestdistance = 1.0
        bestprofilelink = ''
        bestimagelink = ''
        for (profilelink, image_link, distance) in updatedlist:
            if distance < highestdistance:
                highestdistance = distance
                bestprofilelink = profilelink
                bestimagelink = image_link
        if highestdistance < threshold:
            person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)","for count,person in enumerate(peoplelist):
    if args.vv == True or args.debug == True:
        print('LinkedIn Check %i/%i : %s' % (count+1, len(peoplelist), person.full_name))
    else:
        sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count+1, len(peoplelist), person.full_name))
        sys.stdout.flush()
    if person.person_image:
        try:
            target_image = face_recognition.load_image_file(person.person_image)
            target_encoding = face_recognition.face_encodings(target_image)[0]
            profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
            if args.debug == True:
                print(profilelist)
        except:
            continue
    else:
        continue
    early_break = False
    updatedlist = []
    for (profilelink, profilepic, distance) in profilelist:
        try:
            os.remove('potential_target_image.jpg')
        except:
            pass
        if early_break:
            break
        image_link = profilepic
        if image_link:
            try:
                urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                try:
                    potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                except:
                    continue
                results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                for result in results:
                    if args.mode == 'fast':
                        if result < threshold:
                            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                            if args.vv == True:
                                print('\tMatch found: ' + person.full_name)
                                print('\tLinkedIn: ' + person.linkedin)
                            early_break = True
                            break
                    elif args.mode == 'accurate':
                        if result < threshold:
                            updatedlist.append([profilelink, image_link, result])
            except Exception as e:
                print(e)
    if args.mode == 'accurate':
        highestdistance = 1.0
        bestprofilelink = ''
        bestimagelink = ''
        for (profilelink, image_link, distance) in updatedlist:
            if distance < highestdistance:
                highestdistance = distance
                bestprofilelink = profilelink
                bestimagelink = image_link
        if highestdistance < threshold:
            person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)",1,,,,,,,,,,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for (profilelink, profilepic, distance) in profilelist:
    try:
        os.remove('potential_target_image.jpg')
    except:
        pass
    if early_break:
        break
    image_link = profilepic
    if image_link:
        try:
            urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
            potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
            try:
                potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
            except:
                continue
            results = face_recognition.face_distance([target_encoding], potential_target_encoding)
            for result in results:
                if args.mode == 'fast':
                    if result < threshold:
                        person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                        person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                        if args.vv == True:
                            print('\tMatch found: ' + person.full_name)
                            print('\tLinkedIn: ' + person.linkedin)
                        early_break = True
                        break
                elif args.mode == 'accurate':
                    if result < threshold:
                        updatedlist.append([profilelink, image_link, result])
        except Exception as e:
            print(e)","for i, (profilelink, profilepic, distance) in enumerate(profilelist):
    try:
        os.remove('potential_target_image.jpg')
    except:
        pass
    if early_break:
        break
    image_link = profilepic
    if image_link:
        try:
            urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
            potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
            try:
                potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
            except:
                continue
            results = face_recognition.face_distance([target_encoding], potential_target_encoding)
            for result in results:
                if args.mode == 'fast':
                    if result < threshold:
                        person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                        person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                        if args.vv == True:
                            print('\tMatch found: ' + person.full_name)
                            print('\tLinkedIn: ' + person.linkedin)
                        early_break = True
                        break
                elif args.mode == 'accurate':
                    if result < threshold:
                        updatedlist.append([profilelink, image_link, result])
        except Exception as e:
            print(e)",1,,,,,,,,,,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for (profilelink, image_link, distance) in updatedlist:
    if distance < highestdistance:
        highestdistance = distance
        bestprofilelink = profilelink
        bestimagelink = image_link","for i,(profilelink, image_link, distance) in enumerate(updatedlist):
    if distance < highestdistance:
        highestdistance = distance
        bestprofilelink = profilelink
        bestimagelink = image_link",1,,,,,,,,,,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for result in results:
    if args.mode == 'fast':
        if result < threshold:
            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)
            early_break = True
            break
    elif args.mode == 'accurate':
        if result < threshold:
            updatedlist.append([profilelink, image_link, result])","for i,result in enumerate(results):
    if args.mode == 'fast':
        if result < threshold:
            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)
            early_break = True
            break
    elif args.mode == 'accurate':
        if result < threshold:
            updatedlist.append([profilelink, image_link, result])",1,,,,,,,,,,
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code","for x in term:
    if x != str2[-1]:
        str2 = str2 + x","for i,x in enumerate(term):
    if x != str2[-1]:
        str2 = str2 + x",1,,,,,,,,,,
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code","for x in str2[1:]:
    if re.search('[^aeiou]', x):
        str3 = str3 + x","for i,x in enumerate(str2[1:]):
    if re.search('[^aeiou]', x):
        str3 = str3 + x",1,,,,,,,,,,
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for res_type in result_types:
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}","for i,res_type in enumerate(result_types):
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}",1,,,,,,,,,,
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for id_i in tgt_ids:
    cocoEval = COCOeval(coco, coco_dets, iou_type)
    if res_type == 'proposal':
        cocoEval.params.useCats = 0
        cocoEval.params.maxDets = list(max_dets)
    cocoEval.params.imgIds = [id_i]
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
    res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})","for i,id_i in enumerate(tgt_ids):
    cocoEval = COCOeval(coco, coco_dets, iou_type)
    if res_type == 'proposal':
        cocoEval.params.useCats = 0
        cocoEval.params.maxDets = list(max_dets)
    cocoEval.params.imgIds = [id_i]
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
    res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})",1,,,,,,,,,,
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/dygraph/prune/filter_pruner.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/paddleslim/dygraph/prune/filter_pruner.py,FilterPruner,"def prune_var(self, var_name, pruned_axis, pruned_ratio, apply='impretive'):
    """"""
        Pruning a variable.
        Parameters:
            var_name(str): The name of variable.
            pruned_axis(int): The axis to be pruned. For convolution with format [out_c, in_c, k, k],
                             'axis=0' means pruning filters.
            pruned_ratio(float): The ratio of pruned values in one variable.
            apply(str): How to apply pruning plan to graph. It can be 'impretive', 'lazy' or None. None
                        means just returning an instance of 'PruningPlan' but not applying it to graph.

        Returns:
            plan: An instance of PruningPlan that can be applied on model by calling 'plan.apply(model)'.

        """"""
    pruned_axis = pruned_axis[0] if isinstance(pruned_axis, list) else pruned_axis
    assert isinstance(pruned_axis, int)
    if var_name in self.skip_vars:
        _logger.warn(f'{var_name} is skiped beacause it is not supported for pruning directly.')
        return
    collection = self.collections.find_collection_by_master(var_name, pruned_axis)
    plan = PruningPlan(self.model.full_name)
    if collection is None:
        _logger.debug(f""Can not find collection with master ['name': {var_name}, 'axis': {pruned_axis}]"")
        return plan
    _logger.info(f'Pruning variable [{var_name}] and its relatives {list(collection.variables())}')
    mask = self.cal_mask(pruned_ratio, collection, num_head=self.num_head)
    for _detail in collection.all_pruning_details():
        src_mask = copy.deepcopy(mask)
        var_shape = _detail.var.shape()
        for tran in _detail.transform:
            src_mask = self._transform_mask(src_mask, tran)
        current_mask = src_mask
        groups = _detail.op.attr('groups')
        if groups is None or groups == 1:
            assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
        plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))
    if apply == 'lazy':
        plan.apply(self.model, lazy=True)
    elif apply == 'impretive':
        plan.apply(self.model, lazy=False, opt=self.opt, prune_type=self.prune_type)
    return plan","for _detail in collection.all_pruning_details():
    src_mask = copy.deepcopy(mask)
    var_shape = _detail.var.shape()
    for tran in _detail.transform:
        src_mask = self._transform_mask(src_mask, tran)
    current_mask = src_mask
    groups = _detail.op.attr('groups')
    if groups is None or groups == 1:
        assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
    plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))","for i,_detail in enumerate(collection.all_pruning_details()):
    src_mask = copy.deepcopy(mask)
    var_shape = _detail.var.shape()
    for tran in _detail.transform:
        src_mask = self._transform_mask(src_mask, tran)
    current_mask = src_mask
    groups = _detail.op.attr('groups')
    if groups is None or groups == 1:
        assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
    plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))",1,,,,,,,,,,
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for sel in view.sel():
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))","for i, sel in enumerate(view.sel()):
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))",1,,,,,,,,,,
nltk,https://github.com/nltk/nltk/tree/master/nltk/tag/senna.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tag/senna.py,SennaNERTagger,"def tag_sents(self, sentences):
    """"""
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        """"""
    tagged_sents = super().tag_sents(sentences)
    for i in range(len(tagged_sents)):
        for j in range(len(tagged_sents[i])):
            annotations = tagged_sents[i][j]
            tagged_sents[i][j] = (annotations['word'], annotations['ner'])
    return tagged_sents","for i in range(len(tagged_sents)):
    for j in range(len(tagged_sents[i])):
        annotations = tagged_sents[i][j]
        tagged_sents[i][j] = (annotations['word'], annotations['ner'])","for i, sent in enumerate(tagged_sents):
    for j, annotations in enumerate(sent):
        tagged_sents[i][j] = (annotations['word'], annotations['ner'])",1,,,,,,,,,,
nltk,https://github.com/nltk/nltk/tree/master/nltk/tag/senna.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tag/senna.py,SennaNERTagger,"def tag_sents(self, sentences):
    """"""
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        """"""
    tagged_sents = super().tag_sents(sentences)
    for i in range(len(tagged_sents)):
        for j in range(len(tagged_sents[i])):
            annotations = tagged_sents[i][j]
            tagged_sents[i][j] = (annotations['word'], annotations['ner'])
    return tagged_sents","for j in range(len(tagged_sents[i])):
    annotations = tagged_sents[i][j]
    tagged_sents[i][j] = (annotations['word'], annotations['ner'])","for j, annotations in enumerate(tagged_sents[i]):
    tagged_sents[i][j] = (annotations['word'], annotations['ner'])",1,,,,,,,,,,
pdpipe,https://github.com/pdpipe/pdpipe/tree/master/pdpipe/sklearn_stages.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pdpipe/pdpipe/sklearn_stages.py,Encode,"def _transform(self, df, verbose):
    inter_df = df
    for colname in self.encoders:
        lbl_enc = self.encoders[colname]
        source_col = df[colname]
        loc = df.columns.get_loc(colname) + 1
        new_name = colname + '_enc'
        if self._drop:
            inter_df = inter_df.drop(colname, axis=1)
            new_name = colname
            loc -= 1
        inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)
    return inter_df","for colname in self.encoders:
    lbl_enc = self.encoders[colname]
    source_col = df[colname]
    loc = df.columns.get_loc(colname) + 1
    new_name = colname + '_enc'
    if self._drop:
        inter_df = inter_df.drop(colname, axis=1)
        new_name = colname
        loc -= 1
    inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)","for i,colname in enumerate(self.encoders):
    lbl_enc = self.encoders[colname]
    source_col = df[colname]
    loc = df.columns.get_loc(colname) + 1
    new_name = colname + '_enc'
    if self._drop:
        inter_df = inter_df.drop(colname, axis=1)
        new_name = colname
        loc -= 1
    inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)",1,,,,,,,,,,
nova,https://github.com/openstack/nova/tree/master/nova/virt/libvirt/driver.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/libvirt/driver.py,LibvirtDriver,"def _reattach_instance_vifs(self, context, instance, network_info):
    guest = self._host.get_guest(instance)
    guest_interfaces = guest.get_interfaces()
    if len(guest_interfaces) < len(network_info):
        direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
        for vif in network_info:
            if vif['vnic_type'] in direct_vnics:
                LOG.info('Attaching vif %s to instance %s', vif['id'], instance.id)
                self.attach_interface(context, instance, instance.image_meta, vif)","for vif in network_info:
    if vif['vnic_type'] in direct_vnics:
        LOG.info('Attaching vif %s to instance %s', vif['id'], instance.id)
        self.attach_interface(context, instance, instance.image_meta, vif)","for i,vif in enumerate(network_info):
    if vif['vnic_type'] in direct_vnics:
        LOG.info('Attaching vif %s to instance %s', vif['id'], instance.id)
        self.attach_interface(context, instance, instance.image_meta, vif)",1,,,,,,,,,,
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/tests/synthesis/test_deep_feature_synthesis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/featuretools/featuretools/tests/synthesis/test_deep_feature_synthesis.py,,"def test_abides_by_max_depth_param(es):
    for i in [0, 1, 2, 3]:
        dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
        features = dfs_obj.build_features()
        for f in features:
            assert f.get_depth() <= i","for i in [0, 1, 2, 3]:
    dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
    features = dfs_obj.build_features()
    for f in features:
        assert f.get_depth() <= i","for i in range(4):
    dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
    features = dfs_obj.build_features()
    for f in features:
        assert f.get_depth() <= i",1,,,,,,,,,,
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/tests/synthesis/test_deep_feature_synthesis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/featuretools/featuretools/tests/synthesis/test_deep_feature_synthesis.py,,"def test_abides_by_max_depth_param(es):
    for i in [0, 1, 2, 3]:
        dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
        features = dfs_obj.build_features()
        for f in features:
            assert f.get_depth() <= i","for f in features:
    assert f.get_depth() <= i","for i,f in enumerate(features):
    assert f.get_depth() <= i",1,,,,,,,,,,
synapse,https://github.com/matrix-org/synapse/tree/master/tests/replication/test_federation_sender_shard.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/synapse/tests/replication/test_federation_sender_shard.py,FederationSenderTestCase,"def test_send_event_sharded(self):
    """"""Test that using two federation sender workers correctly sends
        new events.
        """"""
    mock_client1 = Mock(spec=['put_json'])
    mock_client1.put_json.return_value = make_awaitable({})
    self.make_worker_hs('synapse.app.generic_worker', {'worker_name': 'federation_sender1', 'federation_sender_instances': ['federation_sender1', 'federation_sender2']}, federation_http_client=mock_client1)
    mock_client2 = Mock(spec=['put_json'])
    mock_client2.put_json.return_value = make_awaitable({})
    self.make_worker_hs('synapse.app.generic_worker', {'worker_name': 'federation_sender2', 'federation_sender_instances': ['federation_sender1', 'federation_sender2']}, federation_http_client=mock_client2)
    user = self.register_user('user2', 'pass')
    token = self.login('user2', 'pass')
    sent_on_1 = False
    sent_on_2 = False
    for i in range(20):
        server_name = 'other_server_%d' % (i,)
        room = self.create_room_with_remote_server(user, token, server_name)
        mock_client1.reset_mock()
        mock_client2.reset_mock()
        self.create_and_send_event(room, UserID.from_string(user))
        self.replicate()
        if mock_client1.put_json.called:
            sent_on_1 = True
            mock_client2.put_json.assert_not_called()
            self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)
            self.assertTrue(mock_client1.put_json.call_args[1]['data'].get('pdus'))
        elif mock_client2.put_json.called:
            sent_on_2 = True
            mock_client1.put_json.assert_not_called()
            self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)
            self.assertTrue(mock_client2.put_json.call_args[1]['data'].get('pdus'))
        else:
            raise AssertionError('Expected send transaction from one or the other sender')
        if sent_on_1 and sent_on_2:
            break
    self.assertTrue(sent_on_1)
    self.assertTrue(sent_on_2)","for i in range(20):
    server_name = 'other_server_%d' % (i,)
    room = self.create_room_with_remote_server(user, token, server_name)
    mock_client1.reset_mock()
    mock_client2.reset_mock()
    self.create_and_send_event(room, UserID.from_string(user))
    self.replicate()
    if mock_client1.put_json.called:
        sent_on_1 = True
        mock_client2.put_json.assert_not_called()
        self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)
        self.assertTrue(mock_client1.put_json.call_args[1]['data'].get('pdus'))
    elif mock_client2.put_json.called:
        sent_on_2 = True
        mock_client1.put_json.assert_not_called()
        self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)
        self.assertTrue(mock_client2.put_json.call_args[1]['data'].get('pdus'))
    else:
        raise AssertionError('Expected send transaction from one or the other sender')
    if sent_on_1 and sent_on_2:
        break","for i in range(20):
    server_name = 'other_server_%d' % (i,)
    room = self.create_room_with_remote_server(user, token, server_name)
    mock_client1.reset_mock()
    mock_client2.reset_mock()
    self.create_and_send_event(room, UserID.from_string(user))
    self.replicate()
    sent_on_1 = False
    sent_on_2 = False
    for j, mock_client in enumerate([mock_client1, mock_client2]):
        if mock_client.put_json.called:
            sent_on_1 = True if j == 0 else False
            sent_on_2 = True if j == 1 else False
            self.assertEqual(mock_client.put_json.call_args[0][0], server_name)
            self.assertTrue(mock_client.put_json.call_args[1]['data'].get('pdus'))
            break
    else:
        raise AssertionError('Expected send transaction from one or the other sender')
    if sent_on_1 and sent_on_2:
        break",1,,,,,,,,,,
chia-rosechain,https://github.com/snight1983/chia-rosechain/tree/master/chia/daemon/server.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chia-rosechain/chia/daemon/server.py,WebSocketServer,"def remove_connection(self, websocket: WebSocketServerProtocol):
    service_name = None
    if websocket in self.remote_address_map:
        service_name = self.remote_address_map[websocket]
        self.remote_address_map.pop(websocket)
    if service_name in self.connections:
        after_removal = []
        for connection in self.connections[service_name]:
            if connection == websocket:
                continue
            else:
                after_removal.append(connection)
        self.connections[service_name] = after_removal","for connection in self.connections[service_name]:
    if connection == websocket:
        continue
    else:
        after_removal.append(connection)","for i, connection in enumerate(self.connections[service_name]):
    if connection == websocket:
        continue
    else:
        after_removal.append(connection)",1,,,,,,,,,,
angr,https://github.com/angr/angr/tree/master/angr/procedures/msvcr/_initterm.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/procedures/msvcr/_initterm.py,_initterm,"def get_callbacks(self, fp_a, fp_z):
    callbacks = []
    table_size = fp_z - fp_a + self.state.arch.bytes
    for addr in reversed(self.state.memory.load(fp_a, table_size, endness=self.state.arch.memory_endness).chop(self.state.arch.bits)):
        addr = self.state.solver.eval(addr)
        if addr != 0:
            callbacks.append(addr)
    return callbacks","for addr in reversed(self.state.memory.load(fp_a, table_size, endness=self.state.arch.memory_endness).chop(self.state.arch.bits)):
    addr = self.state.solver.eval(addr)
    if addr != 0:
        callbacks.append(addr)","for i, addr in enumerate(reversed(self.state.memory.load(fp_a, table_size, endness=self.state.arch.memory_endness).chop(self.state.arch.bits))):
    addr = self.state.solver.eval(addr)
    if addr != 0:
        callbacks.append(addr)",1,,,,,,,,,,
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for rname in recorder_names:
    if not self.recorders.get(rname):
        raise NoSuchConfigurationRecorderException(rname)
    recorders.append(self.recorders[rname].to_dict())","for i,rname in enumerate(recorder_names):
    if not self.recorders.get(rname):
        raise NoSuchConfigurationRecorderException(rname)
    recorders.append(self.recorders[rname].to_dict())",1,,,,,,,,,,
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for recorder in self.recorders.values():
    recorders.append(recorder.to_dict())","for i, recorder in enumerate(self.recorders.values()):
    recorders.append(recorder.to_dict())",1,,,,,,,,,,
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/democracynow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/democracynow.py,DemocracynowIE,"def _real_extract(self, url):
    display_id = self._match_id(url)
    webpage = self._download_webpage(url, display_id)
    json_data = self._parse_json(self._search_regex('<script[^>]+type=""text/json""[^>]*>\\s*({[^>]+})', webpage, 'json'), display_id)
    title = json_data['title']
    formats = []
    video_id = None
    for key in ('file', 'audio', 'video', 'high_res_video'):
        media_url = json_data.get(key, '')
        if not media_url:
            continue
        media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
        video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
        formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})
    self._sort_formats(formats)
    default_lang = 'en'
    subtitles = {}

    def add_subtitle_item(lang, info_dict):
        if lang not in subtitles:
            subtitles[lang] = []
        subtitles[lang].append(info_dict)
    if 'caption_file' in json_data:
        add_subtitle_item(default_lang, {'url': compat_urlparse.urljoin(url, json_data['caption_file'])})
    for subtitle_item in json_data.get('captions', []):
        lang = subtitle_item.get('language', '').lower() or default_lang
        add_subtitle_item(lang, {'url': compat_urlparse.urljoin(url, subtitle_item['url'])})
    description = self._og_search_description(webpage, default=None)
    return {'id': video_id or display_id, 'title': title, 'description': description, 'thumbnail': json_data.get('image'), 'subtitles': subtitles, 'formats': formats}","for key in ('file', 'audio', 'video', 'high_res_video'):
    media_url = json_data.get(key, '')
    if not media_url:
        continue
    media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
    video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
    formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})","for i,key in enumerate(('file', 'audio', 'video', 'high_res_video')):
    media_url = json_data.get(key, '')
    if not media_url:
        continue
    media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
    video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
    formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})",1,,,,,,,,,,
cmake_format,https://github.com/cheshirekow/cmake_format/tree/master/cmakelang/tools/bump_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cmake_format/cmakelang/tools/bump_version.py,,"def main():
    fields = ['major', 'minor', 'patch', 'dev', 'drop-dev']
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('field', nargs='?', choices=fields, default='dev')
    args = parser.parse_args()
    thisdir = os.path.dirname(os.path.realpath(__file__))
    rootdir = os.path.dirname(os.path.dirname(thisdir))
    init_path = os.path.join(rootdir, 'cmakelang/__init__.py')
    current_version = get_current_version(init_path)
    if args.field == 'drop-dev':
        new_version = current_version[:3]
    else:
        field_idx = fields.index(args.field)
        new_version = list(current_version)
        new_version[field_idx] += 1
        for idx in range(field_idx + 1, len(new_version)):
            new_version[idx] = 0
    process_init(init_path, new_version)
    process_installation_rst(os.path.join(rootdir, 'cmakelang/doc/installation.rst'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package.json'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package-lock.json'), new_version)
    return 0","for idx in range(field_idx + 1, len(new_version)):
    new_version[idx] = 0","for i in range(field_idx + 1, len(new_version)):
    new_version[i] = 0",1,,,,,,,,,,
WatchAD,https://github.com/Qianlitp/WatchAD/tree/master/models/Log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WatchAD/models/Log.py,TargetInfo,"def __init__(self, event_data):
    self.domain_name = None
    self.user_name = None
    self.user_sid = None
    self.logon_id = None
    self.info = None
    self.server_name = None
    self.sid = None
    self.full_user_name = None
    self._field_map = {'TargetDomainName': 'domain_name', 'TargetUserName': 'user_name', 'TargetUserSid': 'user_sid', 'TargetSid': 'sid', 'TargetLogonId': 'logon_id', 'TargetInfo': 'info', 'TargetServerName': 'server_name'}
    for (key, value) in self._field_map.items():
        if key not in event_data:
            continue
        if key == 'TargetUserName':
            if '@' in event_data[key]:
                user_name = event_data[key].split('@')[0]
                self.__dict__.update({value: user_name})
            else:
                self.__dict__.update({value: event_data[key]})
            self.__dict__.update({'full_user_name': event_data[key]})
        elif key in event_data:
            self.__dict__.update({value: event_data[key]})","for (key, value) in self._field_map.items():
    if key not in event_data:
        continue
    if key == 'TargetUserName':
        if '@' in event_data[key]:
            user_name = event_data[key].split('@')[0]
            self.__dict__.update({value: user_name})
        else:
            self.__dict__.update({value: event_data[key]})
        self.__dict__.update({'full_user_name': event_data[key]})
    elif key in event_data:
        self.__dict__.update({value: event_data[key]})","for i,(key, value) in enumerate(self._field_map.items()):
    if key not in event_data:
        continue
    if key == 'TargetUserName':
        if '@' in event_data[key]:
            user_name = event_data[key].split('@')[0]
            self.__dict__.update({value: user_name})
        else:
            self.__dict__.update({value: event_data[key]})
        self.__dict__.update({'full_user_name': event_data[key]})
    elif key in event_data:
        self.__dict__.update({value: event_data[key]})",1,,,,,,,,,,
model-optimization,https://github.com/tensorflow/model-optimization/tree/master/tensorflow_model_optimization/python/core/clustering/keras/cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-optimization/tensorflow_model_optimization/python/core/clustering/keras/cluster.py,,"def _wrap_list(layers):
    output = []
    for layer in layers:
        output.append(_add_clustering_wrapper(layer))
    return output","for layer in layers:
    output.append(_add_clustering_wrapper(layer))","for i, layer in enumerate(layers):
    output.append(_add_clustering_wrapper(layer))",1,,,,,,,,,,
docassemble,https://github.com/jhpyle/docassemble/tree/master/tests/features/steps/docassemble.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/tests/features/steps/docassemble.py,,"def select_nth_option(step, value, ordinal, label):
    try:
        elem = world.browser.find_element_by_id(world.browser.find_element_by_xpath('(//label[text()=""' + label + '""])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']').get_attribute('for'))
    except:
        label += '\xa0'
        elem = world.browser.find_element_by_id(world.browser.find_element_by_xpath('(//label[text()=""' + label + '""])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']').get_attribute('for'))
    found = False
    for option in elem.find_elements_by_tag_name('option'):
        if option.text == value:
            found = True
            option.click()
            break
    assert found","for option in elem.find_elements_by_tag_name('option'):
    if option.text == value:
        found = True
        option.click()
        break","for i, option in enumerate(elem.find_elements_by_tag_name('option')):
    if option.text == value:
        found = True
        option.click()
        break",1,,,,,,,,,,
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","for role_id in roles:
    role = get_role(role_id)
    if role is not None:
        base.value |= role._permissions","for i, role_id in enumerate(roles):
    role = get_role(role_id)
    if role is not None:
        base.value |= role._permissions",1,,,,,,,,,,
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","for overwrite in remaining_overwrites:
    if overwrite.is_role() and roles.has(overwrite.id):
        denies |= overwrite.deny
        allows |= overwrite.allow","for i,overwrite in enumerate(remaining_overwrites):
    if overwrite.is_role() and roles.has(overwrite.id):
        denies |= overwrite.deny
        allows |= overwrite.allow",1,,,,,,,,,,
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","for overwrite in remaining_overwrites:
    if overwrite.is_member() and overwrite.id == obj.id:
        base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
        break","for i, overwrite in enumerate(remaining_overwrites):
    if overwrite.is_member() and overwrite.id == obj.id:
        base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
        break",1,,,,,,,,,,
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/neuron_view.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/neuron_view.py,,"def format_delimiters(tokens, tokenizer):
    formatted_tokens = []
    for t in tokens:
        if tokenizer.sep_token:
            t = t.replace(tokenizer.sep_token, '[SEP]')
        if tokenizer.cls_token:
            t = t.replace(tokenizer.cls_token, '[CLS]')
        formatted_tokens.append(t)
    return formatted_tokens","for t in tokens:
    if tokenizer.sep_token:
        t = t.replace(tokenizer.sep_token, '[SEP]')
    if tokenizer.cls_token:
        t = t.replace(tokenizer.cls_token, '[CLS]')
    formatted_tokens.append(t)","for i,t in enumerate(tokens):
    if tokenizer.sep_token:
        t = t.replace(tokenizer.sep_token, '[SEP]')
    if tokenizer.cls_token:
        t = t.replace(tokenizer.cls_token, '[CLS]')
    formatted_tokens.append(t)",1,,,,,,,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for account_id in accounts_d.keys():
    if config.get('environment') in ['prod', 'dev']:
        tasks.append(cache_iam_resources_for_account.s(account_id))
    else:
        log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
        if account_id in config.get('celery.test_account_ids', []):
            tasks.append(cache_iam_resources_for_account.s(account_id))","for i, account_id in enumerate(accounts_d.keys()):
    if config.get('environment') in ['prod', 'dev']:
        tasks.append(cache_iam_resources_for_account.s(account_id))
    else:
        log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
        if account_id in config.get('celery.test_account_ids', []):
            tasks.append(cache_iam_resources_for_account.s(account_id))",1,,,,,,,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for arn in roles_to_delete_from_cache:
    all_roles.pop(arn, None)","for i, arn in enumerate(roles_to_delete_from_cache):
    all_roles.pop(arn, None)",1,,,,,,,,,,
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/cv/models/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/cv/models/base.py,BaseAPI,"def save_model(self, save_dir):
    if not osp.isdir(save_dir):
        if osp.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)
    if self.train_prog is not None:
        fluid.save(self.train_prog, osp.join(save_dir, 'model'))
    else:
        fluid.save(self.test_prog, osp.join(save_dir, 'model'))
    model_info = self.get_model_info()
    model_info['status'] = self.status
    with open(osp.join(save_dir, 'model.yml'), encoding='utf-8', mode='w') as f:
        yaml.dump(model_info, f)
    if hasattr(self, 'eval_details'):
        with open(osp.join(save_dir, 'eval_details.json'), 'w') as f:
            json.dump(self.eval_details, f)
    if self.status == 'Prune':
        shapes = {}
        for block in self.train_prog.blocks:
            for param in block.all_parameters():
                pd_var = fluid.global_scope().find_var(param.name)
                pd_param = pd_var.get_tensor()
                shapes[param.name] = np.array(pd_param).shape
        with open(osp.join(save_dir, 'prune.yml'), encoding='utf-8', mode='w') as f:
            yaml.dump(shapes, f)
    open(osp.join(save_dir, '.success'), 'w').close()
    logging.info('Model saved in {}.'.format(save_dir))","for block in self.train_prog.blocks:
    for param in block.all_parameters():
        pd_var = fluid.global_scope().find_var(param.name)
        pd_param = pd_var.get_tensor()
        shapes[param.name] = np.array(pd_param).shape","for i, block in enumerate(self.train_prog.blocks):
    for param in block.all_parameters():
        pd_var = fluid.global_scope().find_var(param.name)
        pd_param = pd_var.get_tensor()
        shapes[param.name] = np.array(pd_param).shape",1,,,,,,,,,,
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/cv/models/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/cv/models/base.py,BaseAPI,"def save_model(self, save_dir):
    if not osp.isdir(save_dir):
        if osp.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)
    if self.train_prog is not None:
        fluid.save(self.train_prog, osp.join(save_dir, 'model'))
    else:
        fluid.save(self.test_prog, osp.join(save_dir, 'model'))
    model_info = self.get_model_info()
    model_info['status'] = self.status
    with open(osp.join(save_dir, 'model.yml'), encoding='utf-8', mode='w') as f:
        yaml.dump(model_info, f)
    if hasattr(self, 'eval_details'):
        with open(osp.join(save_dir, 'eval_details.json'), 'w') as f:
            json.dump(self.eval_details, f)
    if self.status == 'Prune':
        shapes = {}
        for block in self.train_prog.blocks:
            for param in block.all_parameters():
                pd_var = fluid.global_scope().find_var(param.name)
                pd_param = pd_var.get_tensor()
                shapes[param.name] = np.array(pd_param).shape
        with open(osp.join(save_dir, 'prune.yml'), encoding='utf-8', mode='w') as f:
            yaml.dump(shapes, f)
    open(osp.join(save_dir, '.success'), 'w').close()
    logging.info('Model saved in {}.'.format(save_dir))","for param in block.all_parameters():
    pd_var = fluid.global_scope().find_var(param.name)
    pd_param = pd_var.get_tensor()
    shapes[param.name] = np.array(pd_param).shape","for i,param in enumerate(block.all_parameters()):
    pd_var = fluid.global_scope().find_var(param.name)
    pd_param = pd_var.get_tensor()
    shapes[param.name] = np.array(pd_param).shape",1,,,,,,,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/map_detection/grid_predictor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/map_detection/grid_predictor.py,GridPredictor,"def predict_enemy_genre(self):
    image_dic = {}
    scaling_dic = self.config.MAP_ENEMY_GENRE_DETECTION_SCALING
    for (name, template) in self.template_enemy_genre.items():
        if template is None:
            logger.warning(f'Enemy detection template not found: {name}')
            logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
            raise ScriptError(f'Enemy detection template not found: {name}')
        short_name = name[6:] if name.startswith('Siren_') else name
        scaling = scaling_dic.get(short_name, 1)
        scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
        for scale in scaling:
            if scale not in image_dic:
                shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
                image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
            if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
                return name
    return None","for (name, template) in self.template_enemy_genre.items():
    if template is None:
        logger.warning(f'Enemy detection template not found: {name}')
        logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
        raise ScriptError(f'Enemy detection template not found: {name}')
    short_name = name[6:] if name.startswith('Siren_') else name
    scaling = scaling_dic.get(short_name, 1)
    scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
    for scale in scaling:
        if scale not in image_dic:
            shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
            image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
        if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
            return name","for i, (name, template) in enumerate(self.template_enemy_genre.items()):
    if template is None:
        logger.warning(f'Enemy detection template not found: {name}')
        logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
        raise ScriptError(f'Enemy detection template not found: {name}')
    short_name = name[6:] if name.startswith('Siren_') else name
    scaling = scaling_dic.get(short_name, 1)
    scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
    for scale in scaling:
        if scale not in image_dic:
            shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
            image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
        if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
            return name",1,,,,,,,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/map_detection/grid_predictor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/map_detection/grid_predictor.py,GridPredictor,"def predict_enemy_genre(self):
    image_dic = {}
    scaling_dic = self.config.MAP_ENEMY_GENRE_DETECTION_SCALING
    for (name, template) in self.template_enemy_genre.items():
        if template is None:
            logger.warning(f'Enemy detection template not found: {name}')
            logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
            raise ScriptError(f'Enemy detection template not found: {name}')
        short_name = name[6:] if name.startswith('Siren_') else name
        scaling = scaling_dic.get(short_name, 1)
        scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
        for scale in scaling:
            if scale not in image_dic:
                shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
                image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
            if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
                return name
    return None","for scale in scaling:
    if scale not in image_dic:
        shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
        image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
    if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
        return name","for i,scale in enumerate(scaling):
    if scale not in image_dic:
        shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
        image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
    if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
        return name",1,,,,,,,,,,
PathPicker,https://github.com/facebook/PathPicker/tree/master/src/tests/test_parsing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPicker/src/tests/test_parsing.py,TestParseFunction,"def test_all_input_matches(self) -> None:
    for test_case in ALL_INPUT_TEST_CASES:
        result = parse.match_line(test_case.test_input, False, True)
        if not result:
            self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
            continue
        (match, _, _) = result
        self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')
    print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.')","for test_case in ALL_INPUT_TEST_CASES:
    result = parse.match_line(test_case.test_input, False, True)
    if not result:
        self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
        continue
    (match, _, _) = result
    self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')","for i,test_case in enumerate(ALL_INPUT_TEST_CASES):
    result = parse.match_line(test_case.test_input, False, True)
    if not result:
        self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
        continue
    (match, _, _) = result
    self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')",1,,,,,,,,,,
python-telegram-bot,https://github.com/python-telegram-bot/python-telegram-bot/tree/master/tests/test_inlinequeryresultcacheddocument.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-telegram-bot/tests/test_inlinequeryresultcacheddocument.py,TestInlineQueryResultCachedDocument,"def test_slot_behaviour(self, inline_query_result_cached_document, mro_slots):
    inst = inline_query_result_cached_document
    for attr in inst.__slots__:
        assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""
    assert len(mro_slots(inst)) == len(set(mro_slots(inst))), 'duplicate slot'","for attr in inst.__slots__:
    assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""","for i, attr in enumerate(inst.__slots__):
    assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""",1,,,,,,,,,,
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_saving_in_empty_folder(self):
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs:
            if isinstance(temp_fs, TempFS):
                self.eopatch.save(temp_fs.root_path)
            else:
                self.eopatch.save('/', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
            subfolder = 'new-subfolder'
            self.eopatch.save('new-subfolder', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for fs_loader in self.filesystem_loaders:
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for i, fs_loader in enumerate(self.filesystem_loaders):
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))",1,,,,,,,,,,
binderhub,https://github.com/jupyterhub/binderhub/tree/master/helm-chart/binderhub/files/binderhub_config.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/binderhub/helm-chart/binderhub/files/binderhub_config.py,,"def get_value(key, default=None):
    """"""
    Find an item in values.yaml of a given name & return it

    get_value(""a.b.c"") returns values['a']['b']['c']
    """"""
    value = _load_values()
    for level in key.split('.'):
        if not isinstance(value, dict):
            return default
        if level not in value:
            return default
        else:
            value = value[level]
    return value","for level in key.split('.'):
    if not isinstance(value, dict):
        return default
    if level not in value:
        return default
    else:
        value = value[level]","for i, level in enumerate(key.split('.')):
    if not isinstance(value, dict):
        return default
    if level not in value:
        return default
    else:
        value = value[level]",1,,,,,,,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/os/operation_siren.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/os/operation_siren.py,OperationSiren,"def _os_explore(self):
    """"""
        Explore all dangerous zones at the beginning of month.
        """"""

    def end():
        logger.info('OS explore finished, delay to next reset')
        next_reset = get_os_next_reset()
        logger.attr('OpsiNextReset', next_reset)
        logger.info('To run again, clear OpsiExplore.Scheduler.NextRun and set OpsiExplore.OpsiExplore.LastZone=0')
        with self.config.multi_set():
            self.config.OpsiExplore_LastZone = 0
            self.config.task_delay(target=next_reset)
            self.config.task_call('OpsiDaily', force_call=False)
            self.config.task_call('OpsiShop', force_call=False)
        self.config.task_stop()
    logger.hr('OS explore', level=1)
    order = [int(f.strip(' \t\r\n')) for f in self.config.OS_EXPLORE_FILTER.split('>')]
    try:
        last_zone = self.name_to_zone(self.config.OpsiExplore_LastZone).zone_id
    except ScriptError:
        logger.warning(f'Invalid OpsiExplore_LastZone={self.config.OpsiExplore_LastZone}, re-explore')
        last_zone = 0
    if last_zone in order:
        order = order[order.index(last_zone) + 1:]
        logger.info(f'Last zone: {self.name_to_zone(last_zone)}, next zone: {order[:1]}')
    elif last_zone == 0:
        logger.info(f'First run, next zone: {order[:1]}')
    else:
        raise ScriptError(f'Invalid last_zone: {last_zone}')
    if not len(order):
        end()
    for zone in order:
        if not self.globe_goto(zone, stop_if_safe=True):
            logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
            self.config.OpsiExplore_LastZone = zone
            continue
        logger.hr(f'OS explore {zone}', level=1)
        if not self.config.OpsiExplore_SpecialRadar:
            self.tuning_sample_use()
        self.fleet_set(self.config.OpsiFleet_Fleet)
        self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
        self._os_explore_task_delay()
        self.run_auto_search()
        self.config.OpsiExplore_LastZone = zone
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.handle_after_auto_search()
        self.config.check_task_switch()
        if zone == order[-1]:
            end()","for zone in order:
    if not self.globe_goto(zone, stop_if_safe=True):
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.config.OpsiExplore_LastZone = zone
        continue
    logger.hr(f'OS explore {zone}', level=1)
    if not self.config.OpsiExplore_SpecialRadar:
        self.tuning_sample_use()
    self.fleet_set(self.config.OpsiFleet_Fleet)
    self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
    self._os_explore_task_delay()
    self.run_auto_search()
    self.config.OpsiExplore_LastZone = zone
    logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
    self.handle_after_auto_search()
    self.config.check_task_switch()
    if zone == order[-1]:
        end()","for i, zone in enumerate(order):
    if not self.globe_goto(zone, stop_if_safe=True):
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.config.OpsiExplore_LastZone = zone
        continue
    logger.hr(f'OS explore {zone}', level=1)
    if not self.config.OpsiExplore_SpecialRadar:
        self.tuning_sample_use()
    self.fleet_set(self.config.OpsiFleet_Fleet)
    self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
    self._os_explore_task_delay()
    self.run_auto_search()
    self.config.OpsiExplore_LastZone = zone
    logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
    self.handle_after_auto_search()
    self.config.check_task_switch()
    if zone == order[-1]:
        end()",1,,,,,,,,,,
swift,https://github.com/openstack/swift/tree/master/swift/obj/diskfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/obj/diskfile.py,ECDiskFile,"def validate_metadata(self):
    required_metadata = ['Content-Length', 'X-Object-Sysmeta-Ec-Frag-Index', 'X-Object-Sysmeta-Ec-Etag']
    for header in required_metadata:
        if not self._datafile_metadata.get(header):
            return False
    return True","for header in required_metadata:
    if not self._datafile_metadata.get(header):
        return False","for i, header in enumerate(required_metadata):
    if not self._datafile_metadata.get(header):
        return False",1,,,,,,,,,,
volatility3,https://github.com/volatilityfoundation/volatility3/tree/master/volatility3/framework/plugins/windows/hashdump.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/volatility3/volatility3/framework/plugins/windows/hashdump.py,Hashdump,"def run(self):
    offset = self.config.get('offset', None)
    syshive = None
    samhive = None
    kernel = self.context.modules[self.config['kernel']]
    for hive in hivelist.HiveList.list_hives(self.context, self.config_path, kernel.layer_name, kernel.symbol_table_name, hive_offsets=None if offset is None else [offset]):
        if hive.get_name().split('\\')[-1].upper() == 'SYSTEM':
            syshive = hive
        if hive.get_name().split('\\')[-1].upper() == 'SAM':
            samhive = hive
    return renderers.TreeGrid([('User', str), ('rid', int), ('lmhash', str), ('nthash', str)], self._generator(syshive, samhive))","for hive in hivelist.HiveList.list_hives(self.context, self.config_path, kernel.layer_name, kernel.symbol_table_name, hive_offsets=None if offset is None else [offset]):
    if hive.get_name().split('\\')[-1].upper() == 'SYSTEM':
        syshive = hive
    if hive.get_name().split('\\')[-1].upper() == 'SAM':
        samhive = hive","hives = hivelist.HiveList.list_hives(self.context, self.config_path, kernel.layer_name, kernel.symbol_table_name, hive_offsets=None if offset is None else [offset])
for i,hive in enumerate(hives):
    if hive.get_name().split('\\')[-1].upper() == 'SYSTEM':
        syshive = hive
    if hive.get_name().split('\\')[-1].upper() == 'SAM':
        samhive = hive",1,,,,,,,,,,
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for el in edge_lists:
    for e in el:
        self.delete(e)","for i, el in enumerate(edge_lists):
    for e in el:
        self.delete(e)",1,,,,,,,,,,
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for e in el:
    self.delete(e)","for i,e in enumerate(el):
    self.delete(e)",1,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","for data in ori_data:
    vertexSet = data['vertexSet']
    for label in data['labels']:
        rel = label['r']
        for n1 in vertexSet[label['h']]:
            for n2 in vertexSet[label['t']]:
                fact_in_train.add((n1['name'], n2['name'], rel))","for i,data in enumerate(ori_data):
    vertexSet = data['vertexSet']
    for label in data['labels']:
        rel = label['r']
        for j,n1 in enumerate(vertexSet[label['h']]):
            for k,n2 in enumerate(vertexSet[label['t']]):
                fact_in_train.add((n1['name'], n2['name'], rel))",1,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","for x in triples:
    fact_in_train.add(tuple(x))","for i,x in enumerate(triples):
    fact_in_train.add(tuple(x))",1,,,,,,,,,,
rasa,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/core/domain.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/rasa/shared/core/domain.py,Domain,"def _add_categorical_slot_default_value(self) -> None:
    """"""Add a default value to all categorical slots.

        All unseen values found for the slot will be mapped to this default value
        for featurization.
        """"""
    for slot in [s for s in self.slots if isinstance(s, CategoricalSlot)]:
        slot.add_default_value()","for slot in [s for s in self.slots if isinstance(s, CategoricalSlot)]:
    slot.add_default_value()","for i, slot in enumerate([s for s in self.slots if isinstance(s, CategoricalSlot)]):
    slot.add_default_value()",1,,,,,,,,,,
pyrsistent,https://github.com/tobgu/pyrsistent/tree/master/pyrsistent/_pbag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyrsistent/pyrsistent/_pbag.py,PBag,"def __iter__(self):
    """"""
        Return an iterator of all elements, including duplicates.

        >>> list(pbag([1, 1, 2]))
        [1, 1, 2]
        >>> list(pbag([1, 2]))
        [1, 2]
        """"""
    for (elt, count) in self._counts.iteritems():
        for i in range(count):
            yield elt","for (elt, count) in self._counts.iteritems():
    for i in range(count):
        yield elt","for elt, count in self._counts.items():
    for i in range(count):
        yield elt

Note: In Python 3, we should use `items()` instead of `iteritems()`.",1,,,,,,,,,,
kube-janitor,https://github.com/hjacobs/kube-janitor/tree/master/kube_janitor/resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kube-janitor/kube_janitor/resources.py,,"def get_namespaced_resource_types(api):
    for (api_version, resource) in discover_namespaced_api_resources(api):
        clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
        yield clazz","for (api_version, resource) in discover_namespaced_api_resources(api):
    clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
    yield clazz","for i,(api_version, resource) in enumerate(discover_namespaced_api_resources(api)):
    clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
    yield clazz",1,,,,,,,,,,
SDV,https://github.com/sdv-dev/SDV/tree/master//tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV//tasks.py,,"def install_minimum(c):
    with open('setup.py', 'r') as setup_py:
        lines = setup_py.read().splitlines()
    versions = []
    started = False
    for line in lines:
        if started:
            if line == ']':
                started = False
                continue
            line = line.strip()
            if _validate_python_version(line):
                requirement = re.match('[^>]*', line).group(0)
                requirement = re.sub('[\'"",]', '', requirement)
                version = re.search('>=?[^(,|#)]*', line).group(0)
                if version:
                    version = re.sub('>=?', '==', version)
                    version = re.sub('[\'"",]', '', version)
                    requirement += version
                versions.append(requirement)
        elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
            started = True
    c.run(f""python -m pip install {' '.join(versions)}"")","for line in lines:
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True","for i,line in enumerate(lines):
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True",1,,,,,,,,,,
enumerate-iam,https://github.com/andresriancho/enumerate-iam/tree/master/enumerate_iam/generate_bruteforce_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enumerate-iam/enumerate_iam/generate_bruteforce_tests.py,,"def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)","for filename in os.listdir(API_DEFINITIONS):
    if not filename.endswith('.min.json'):
        continue
    api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
    api_json = json.loads(api_json_data)
    service_name = extract_service_name(filename, api_json)
    if service_name is None:
        print('%s does not define a service name' % filename)
        continue
    operations = extract_operations(api_json)
    if not operations:
        continue
    if service_name in bruteforce_tests:
        bruteforce_tests[service_name].extend(operations)
    else:
        bruteforce_tests[service_name] = operations","for i,filename in enumerate(os.listdir(API_DEFINITIONS)):
    if not filename.endswith('.min.json'):
        continue
    api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
    api_json = json.loads(api_json_data)
    service_name = extract_service_name(filename, api_json)
    if service_name is None:
        print('%s does not define a service name' % filename)
        continue
    operations = extract_operations(api_json)
    if not operations:
        continue
    if service_name in bruteforce_tests:
        bruteforce_tests[service_name].extend(operations)
    else:
        bruteforce_tests[service_name] = operations",1,,,,,,,,,,
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)","for (img_path, annotation_path) in tqdm(filename_pairs):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)","for i, (img_path, annotation_path) in enumerate(tqdm(filename_pairs)):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)",1,,,,,,,,,,
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/test_client_regress/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v22/tests/test_client_regress/tests.py,AssertFormsetErrorTests,"def test_no_error_field(self):
    """"""An assertion is raised if the field doesn't have any errors""""""
    for (prefix, kwargs) in self.msg_prefixes:
        msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
        with self.assertRaisesMessage(AssertionError, msg):
            self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)","for (prefix, kwargs) in self.msg_prefixes:
    msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
    with self.assertRaisesMessage(AssertionError, msg):
        self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)","for i, (prefix, kwargs) in enumerate(self.msg_prefixes):
    msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
    with self.assertRaisesMessage(AssertionError, msg):
        self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)",1,,,,,,,,,,
transformers,https://github.com/huggingface/transformers/tree/master/examples/flax/question-answering/run_qa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/examples/flax/question-answering/run_qa.py,,"def eval_data_collator(dataset: Dataset, batch_size: int):
    """"""Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.""""""
    batch_idx = np.arange(len(dataset))
    steps_per_epoch = math.ceil(len(dataset) / batch_size)
    batch_idx = np.array_split(batch_idx, steps_per_epoch)
    for idx in batch_idx:
        batch = dataset[idx]
        batch = {k: np.array(v) for (k, v) in batch.items()}
        yield batch","for idx in batch_idx:
    batch = dataset[idx]
    batch = {k: np.array(v) for (k, v) in batch.items()}
    yield batch","for i,idx in enumerate(batch_idx):
    batch = dataset[idx]
    batch = {k: np.array(v) for (k, v) in batch.items()}
    yield batch",1,,,,,,,,,,
keras,https://github.com/keras-team/keras/tree/master/keras/layers/preprocessing/image_preprocessing_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras/keras/layers/preprocessing/image_preprocessing_test.py,RandomZoomTest,"def test_random_zoom_in_numeric(self):
    for dtype in (np.int64, np.float32):
        with testing_utils.use_gpu():
            input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
            layer = image_preprocessing.RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation='nearest')
            output_image = layer(np.expand_dims(input_image, axis=0))
            expected_output = np.asarray([[6, 7, 7, 8, 8], [11, 12, 12, 13, 13], [11, 12, 12, 13, 13], [16, 17, 17, 18, 18], [16, 17, 17, 18, 18]]).astype(dtype)
            expected_output = np.reshape(expected_output, (1, 5, 5, 1))
            self.assertAllEqual(expected_output, output_image)","for dtype in (np.int64, np.float32):
    with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([[6, 7, 7, 8, 8], [11, 12, 12, 13, 13], [11, 12, 12, 13, 13], [16, 17, 17, 18, 18], [16, 17, 17, 18, 18]]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)","for i,dtype in enumerate((np.int64, np.float32)):
    with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([[6, 7, 7, 8, 8], [11, 12, 12, 13, 13], [11, 12, 12, 13, 13], [16, 17, 17, 18, 18], [16, 17, 17, 18, 18]]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)",1,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for fn in possibleFiles:
    mass_store.append([False, fn[len(path):]])","for i, fn in enumerate(possibleFiles):
    mass_store.append([False, fn[len(path):]])",1,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for entry in mass_store:
    if entry[0]:
        newengine = discoverer.getReferencedEngine(path + entry[1])
        if newengine is not None:
            discoverer.addEngineFromReference(newengine)
            found = True","for i, entry in enumerate(mass_store):
    if entry[0]:
        newengine = discoverer.getReferencedEngine(path + entry[1])
        if newengine is not None:
            discoverer.addEngineFromReference(newengine)
            found = True",1,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for iso in ISO3166_LIST:
    if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
        widget.set_active(idx)
        break
    idx += 1","for idx, iso in enumerate(ISO3166_LIST):
    if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
        widget.set_active(idx)
        break",1,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for iso in ISO3166_LIST:
    if iso.iso2 == country:
        self.widgets['engine_country_combo'].set_active(idx)
        break
    idx += 1","for idx, iso in enumerate(ISO3166_LIST):
    if iso.iso2 == country:
        self.widgets['engine_country_combo'].set_active(idx)
        break",1,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for checker in checkers:
    check_ok = checker(engine_command)
    if check_ok:
        uci = checker is is_uci
        break","for i, checker in enumerate(checkers):
    check_ok = checker(engine_command)
    if check_ok:
        uci = checker is is_uci
        break",1,,,,,,,,,,
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for node in nodes:
    walk.append([node])","for i,node in enumerate(nodes):
    walk.append([node])",1,,,,,,,,,,
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for l in range(max_depth):
    outdegree = self.outdegree(cur_nodes)
    mask = outdegree != 0
    if np.any(mask):
        cur_walk_ids = cur_walk_ids[mask]
        cur_nodes = cur_nodes[mask]
        outdegree = outdegree[mask]
    else:
        break
    succ = self.successor(cur_nodes)
    sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
    nxt_cur_nodes = []
    for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
        walk[walk_id].append(s[ind])
        nxt_cur_nodes.append(s[ind])
    cur_nodes = np.array(nxt_cur_nodes)","for l in range(max_depth):
    outdegree = self.outdegree(cur_nodes)
    mask = outdegree != 0
    if np.any(mask):
        cur_walk_ids = cur_walk_ids[mask]
        cur_nodes = cur_nodes[mask]
        outdegree = outdegree[mask]
    else:
        break
    succ = self.successor(cur_nodes)
    sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
    nxt_cur_nodes = []
    for (s, ind, walk_id), i in zip(zip(succ, sample_index, cur_walk_ids), range(len(succ))):
        walk[walk_id].append(s[ind])
        nxt_cur_nodes.append(s[ind])
    cur_nodes = np.array(nxt_cur_nodes)",1,,,,,,,,,,
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])","for i, (s, ind, walk_id) in enumerate(zip(succ, sample_index, cur_walk_ids)):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])",1,,,,,,,,,,
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/lstm.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/fairseq/models/lstm.py,LSTMDecoder,"def extract_features(self, prev_output_tokens, encoder_out: Optional[Tuple[Tensor, Tensor, Tensor, Tensor]]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):
    """"""
        Similar to *forward* but only return features.
        """"""
    if encoder_out is not None:
        encoder_outs = encoder_out[0]
        encoder_hiddens = encoder_out[1]
        encoder_cells = encoder_out[2]
        encoder_padding_mask = encoder_out[3]
    else:
        encoder_outs = torch.empty(0)
        encoder_hiddens = torch.empty(0)
        encoder_cells = torch.empty(0)
        encoder_padding_mask = torch.empty(0)
    srclen = encoder_outs.size(0)
    if incremental_state is not None and len(incremental_state) > 0:
        prev_output_tokens = prev_output_tokens[:, -1:]
    (bsz, seqlen) = prev_output_tokens.size()
    x = self.embed_tokens(prev_output_tokens)
    x = self.dropout_in_module(x)
    x = x.transpose(0, 1)
    if incremental_state is not None and len(incremental_state) > 0:
        (prev_hiddens, prev_cells, input_feed) = self.get_cached_state(incremental_state)
    elif encoder_out is not None:
        prev_hiddens = [encoder_hiddens[i] for i in range(self.num_layers)]
        prev_cells = [encoder_cells[i] for i in range(self.num_layers)]
        if self.encoder_hidden_proj is not None:
            prev_hiddens = [self.encoder_hidden_proj(y) for y in prev_hiddens]
            prev_cells = [self.encoder_cell_proj(y) for y in prev_cells]
        input_feed = x.new_zeros(bsz, self.hidden_size)
    else:
        zero_state = x.new_zeros(bsz, self.hidden_size)
        prev_hiddens = [zero_state for i in range(self.num_layers)]
        prev_cells = [zero_state for i in range(self.num_layers)]
        input_feed = None
    assert srclen > 0 or self.attention is None, 'attention is not supported if there are no encoder outputs'
    attn_scores: Optional[Tensor] = x.new_zeros(srclen, seqlen, bsz) if self.attention is not None else None
    outs = []
    for j in range(seqlen):
        if input_feed is not None:
            input = torch.cat((x[j, :, :], input_feed), dim=1)
        else:
            input = x[j]
        for (i, rnn) in enumerate(self.layers):
            (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
            input = self.dropout_out_module(hidden)
            if self.residuals:
                input = input + prev_hiddens[i]
            prev_hiddens[i] = hidden
            prev_cells[i] = cell
        if self.attention is not None:
            assert attn_scores is not None
            (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
        else:
            out = hidden
        out = self.dropout_out_module(out)
        if input_feed is not None:
            input_feed = out
        outs.append(out)
    prev_hiddens_tensor = torch.stack(prev_hiddens)
    prev_cells_tensor = torch.stack(prev_cells)
    cache_state = torch.jit.annotate(Dict[str, Optional[Tensor]], {'prev_hiddens': prev_hiddens_tensor, 'prev_cells': prev_cells_tensor, 'input_feed': input_feed})
    self.set_incremental_state(incremental_state, 'cached_state', cache_state)
    x = torch.cat(outs, dim=0).view(seqlen, bsz, self.hidden_size)
    x = x.transpose(1, 0)
    if hasattr(self, 'additional_fc') and self.adaptive_softmax is None:
        x = self.additional_fc(x)
        x = self.dropout_out_module(x)
    if not self.training and self.need_attn and (self.attention is not None):
        assert attn_scores is not None
        attn_scores = attn_scores.transpose(0, 2)
    else:
        attn_scores = None
    return (x, attn_scores)","for j in range(seqlen):
    if input_feed is not None:
        input = torch.cat((x[j, :, :], input_feed), dim=1)
    else:
        input = x[j]
    for (i, rnn) in enumerate(self.layers):
        (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
        input = self.dropout_out_module(hidden)
        if self.residuals:
            input = input + prev_hiddens[i]
        prev_hiddens[i] = hidden
        prev_cells[i] = cell
    if self.attention is not None:
        assert attn_scores is not None
        (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
    else:
        out = hidden
    out = self.dropout_out_module(out)
    if input_feed is not None:
        input_feed = out
    outs.append(out)","for j in range(seqlen):
    if input_feed is not None:
        input = torch.cat((x[j, :, :], input_feed), dim=1)
    else:
        input = x[j]
    for (i, rnn) in enumerate(self.layers):
        (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
        input = self.dropout_out_module(hidden)
        if self.residuals:
            input = input + prev_hiddens[i]
        prev_hiddens[i] = hidden
        prev_cells[i] = cell
    if self.attention is not None:
        assert attn_scores is not None
        (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
    else:
        out = hidden
    out = self.dropout_out_module(out)
    if input_feed is not None:
        input_feed = out
    outs.append(out)",1,,,,,,,,,,
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/modeling/data_handler/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/haystack/haystack/modeling/data_handler/processor.py,TextClassificationProcessor,"def convert_labels(self, dictionary: Dict):
    ret: Dict = {}
    for (task_name, task) in self.tasks.items():
        label_name = task['label_name']
        label_raw = dictionary[label_name]
        label_list = task['label_list']
        if task['task_type'] == 'classification':
            label_ids = [label_list.index(label_raw)]
        elif task['task_type'] == 'multilabel_classification':
            label_ids = [0] * len(label_list)
            for l in label_raw.split(','):
                if l != '':
                    label_ids[label_list.index(l)] = 1
        ret[task['label_tensor_name']] = label_ids
    return ret","for (task_name, task) in self.tasks.items():
    label_name = task['label_name']
    label_raw = dictionary[label_name]
    label_list = task['label_list']
    if task['task_type'] == 'classification':
        label_ids = [label_list.index(label_raw)]
    elif task['task_type'] == 'multilabel_classification':
        label_ids = [0] * len(label_list)
        for l in label_raw.split(','):
            if l != '':
                label_ids[label_list.index(l)] = 1
    ret[task['label_tensor_name']] = label_ids","for i, (task_name, task) in enumerate(self.tasks.items()):
    label_name = task['label_name']
    label_raw = dictionary[label_name]
    label_list = task['label_list']
    if task['task_type'] == 'classification':
        label_ids = [label_list.index(label_raw)]
    elif task['task_type'] == 'multilabel_classification':
        label_ids = [0] * len(label_list)
        for l in label_raw.split(','):
            if l != '':
                label_ids[label_list.index(l)] = 1
    ret[task['label_tensor_name']] = label_ids",1,,,,,,,,,,
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/modeling/data_handler/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/haystack/haystack/modeling/data_handler/processor.py,TextClassificationProcessor,"def convert_labels(self, dictionary: Dict):
    ret: Dict = {}
    for (task_name, task) in self.tasks.items():
        label_name = task['label_name']
        label_raw = dictionary[label_name]
        label_list = task['label_list']
        if task['task_type'] == 'classification':
            label_ids = [label_list.index(label_raw)]
        elif task['task_type'] == 'multilabel_classification':
            label_ids = [0] * len(label_list)
            for l in label_raw.split(','):
                if l != '':
                    label_ids[label_list.index(l)] = 1
        ret[task['label_tensor_name']] = label_ids
    return ret","for l in label_raw.split(','):
    if l != '':
        label_ids[label_list.index(l)] = 1","for i,l in enumerate(label_raw.split(',')):
    if l != '':
        label_ids[label_list.index(l)] = 1",1,,,,,,,,,,
pyhanlp,https://github.com/hankcs/pyhanlp/tree/master/tests/book/ch10/demo_clustering_f.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyhanlp/tests/book/ch10/demo_clustering_f.py,,"if __name__ == '__main__':
    for algorithm in ('kmeans', 'repeated bisection'):
        print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","for algorithm in ('kmeans', 'repeated bisection'):
    print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","for i, algorithm in enumerate(('kmeans', 'repeated bisection')):
    print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))",1,,,,,,,,,,
heatmap,https://github.com/sethoscope/heatmap/tree/master//heatmap.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/heatmap//heatmap.py,AppendingMatrix,"def reduce(decay, values):
    """"""
        Returns a weighted sum of the values, where weight N is
        pow(decay,N).  This means the largest value counts fully, but
        additional values have diminishing contributions. decay=0 makes
        the reduction equivalent to max(), which makes each data point
        visible, but says nothing about their relative magnitude.
        decay=1 makes this like sum(), which makes the relative
        magnitude of the points more visible, but could make smaller
        values hard to see.  Experiment with values between 0 and 1.
        Values outside that range will give weird results.
        """"""
    weight = 1.0
    total = 0.0
    values.sort(reverse=True)
    for value in values:
        total += value * weight
        weight *= decay
    return total","for value in values:
    total += value * weight
    weight *= decay","for i,value in enumerate(values):
    total += value * weight
    weight *= decay",1,,,,,,,,,,
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/cmdline_tmpl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/cmdline_tmpl.py,CmdlineTmpl,"def cleanup(self, output_file='result.json', script_name='cmdline_test.py'):
    if os.path.exists(script_name):
        os.remove(script_name)
    if output_file:
        if type(output_file) is list:
            for f in output_file:
                os.remove(f)
        elif type(output_file) is str:
            if os.path.exists(output_file):
                if os.path.isdir(output_file):
                    shutil.rmtree(output_file)
                elif os.path.isfile(output_file):
                    os.remove(output_file)
        else:
            raise Exception('Unexpected output file argument')","for f in output_file:
    os.remove(f)","for i,f in enumerate(output_file):
    os.remove(output_file[i])",1,,,,,,,,,,
Tuxemon,https://github.com/Tuxemon/Tuxemon/tree/master/tuxemon/cli/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tuxemon/tuxemon/cli/processor.py,CommandProcessor,"def collect_commands(self, folder: str) -> Iterable[CLICommand]:
    """"""
        Use plugins to load CLICommand classes for commands.

        Parameters:
            folder: Folder to search.

        """"""
    pm = PluginManager()
    pm.setPluginPlaces([folder])
    pm.include_patterns = ['commands']
    pm.exclude_classes = ['CLICommand']
    pm.collectPlugins()
    for cmd_class in get_available_classes(pm, interface=CLICommand):
        if cmd_class.usable_from_root:
            yield cmd_class()","for cmd_class in get_available_classes(pm, interface=CLICommand):
    if cmd_class.usable_from_root:
        yield cmd_class()","for i, cmd_class in enumerate(get_available_classes(pm, interface=CLICommand)):
    if cmd_class.usable_from_root:
        yield cmd_class()",1,,,,,,,,,,
swift,https://github.com/openstack/swift/tree/master/test/unit/common/middleware/s3api/test_service.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/common/middleware/s3api/test_service.py,TestS3ApiService,"def test_service_GET_with_blind_resource(self):
    buckets = (('apple', 1, 200), ('orange', 3, 430), ('apple+segment', 1, 200))
    expected = buckets[:-1]
    bucket_list = create_bucket_list_json(buckets)
    self.swift.register('GET', '/v1/AUTH_test', swob.HTTPOk, {}, bucket_list)
    req = Request.blank('/', environ={'REQUEST_METHOD': 'GET'}, headers={'Authorization': 'AWS test:tester:hmac', 'Date': self.get_date_header()})
    (status, headers, body) = self.call_s3api(req)
    self.assertEqual(status.split()[0], '200')
    elem = fromstring(body, 'ListAllMyBucketsResult')
    all_buckets = elem.find('./Buckets')
    buckets = all_buckets.iterchildren('Bucket')
    listing = list(list(buckets)[0])
    self.assertEqual(len(listing), 2)
    names = []
    for b in all_buckets.iterchildren('Bucket'):
        names.append(b.find('./Name').text)
    self.assertEqual(len(names), len(expected))
    for i in expected:
        self.assertIn(i[0], names)","for b in all_buckets.iterchildren('Bucket'):
    names.append(b.find('./Name').text)","for i,b in enumerate(all_buckets.iterchildren('Bucket')):
    names.append(b.find('./Name').text)",1,,,,,,,,,,
swift,https://github.com/openstack/swift/tree/master/test/unit/common/middleware/s3api/test_service.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/common/middleware/s3api/test_service.py,TestS3ApiService,"def test_service_GET_with_blind_resource(self):
    buckets = (('apple', 1, 200), ('orange', 3, 430), ('apple+segment', 1, 200))
    expected = buckets[:-1]
    bucket_list = create_bucket_list_json(buckets)
    self.swift.register('GET', '/v1/AUTH_test', swob.HTTPOk, {}, bucket_list)
    req = Request.blank('/', environ={'REQUEST_METHOD': 'GET'}, headers={'Authorization': 'AWS test:tester:hmac', 'Date': self.get_date_header()})
    (status, headers, body) = self.call_s3api(req)
    self.assertEqual(status.split()[0], '200')
    elem = fromstring(body, 'ListAllMyBucketsResult')
    all_buckets = elem.find('./Buckets')
    buckets = all_buckets.iterchildren('Bucket')
    listing = list(list(buckets)[0])
    self.assertEqual(len(listing), 2)
    names = []
    for b in all_buckets.iterchildren('Bucket'):
        names.append(b.find('./Name').text)
    self.assertEqual(len(names), len(expected))
    for i in expected:
        self.assertIn(i[0], names)","for i in expected:
    self.assertIn(i[0], names)","for i, val in enumerate(expected):
    self.assertIn(val[0], names)",1,,,,,,,,,,
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/kfserving/transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kale/backend/kale/kfserving/transformer.py,KaleTransformer,"def _load_transformer_assets(self):
    marshal.set_data_dir(serveutils.TRANSFORMER_ASSETS_DIR)
    log.info('Loading transformer function...')
    _fn = marshal.load(serveutils.TRANSFORMER_FN_ASSET_NAME)
    self.fn = types.FunctionType(_fn.__code__, globals(), _fn.__name__, _fn.__defaults__, _fn.__closure__)
    log.info('Processing source notebook for imports and functions...')
    processor = NotebookProcessor(nb_path=os.path.join(serveutils.TRANSFORMER_ASSETS_DIR, serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME), skip_validation=True)
    self.init_code = processor.get_imports_and_functions()
    log.info('Initialization code:\n%s' % self.init_code)
    log.info('Running initialization code...')
    exec(self.init_code, globals())
    log.info(""Loading transformer's assets..."")
    for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
        if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
            continue
        basename = os.path.splitext(file)[0]
        self.assets[basename] = marshal.load(basename)
    log.info('Assets successfully loaded: %s' % self.assets.keys())
    log.info('Initializing assets...')
    for (asset_name, asset_value) in self.assets.items():
        globals()[asset_name] = asset_value","for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
    if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
        continue
    basename = os.path.splitext(file)[0]
    self.assets[basename] = marshal.load(basename)","for i,file in enumerate(os.listdir(serveutils.TRANSFORMER_ASSETS_DIR)):
    if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
        continue
    basename = os.path.splitext(file)[0]
    self.assets[basename] = marshal.load(basename)",1,,,,,,,,,,
zao-,https://github.com/qiucheng025/zao-/tree/master/lib/gui/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zao-/lib/gui/stats.py,SessionsSummary,"def format_stats(compiled_stats):
    """""" Format for display """"""
    logger.debug('Formatting stats')
    for summary in compiled_stats:
        (hrs, mins, secs) = convert_time(summary['elapsed'])
        summary['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
        summary['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
        summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
        summary['rate'] = '{0:.1f}'.format(summary['rate'])
    return compiled_stats","for summary in compiled_stats:
    (hrs, mins, secs) = convert_time(summary['elapsed'])
    summary['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
    summary['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
    summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
    summary['rate'] = '{0:.1f}'.format(summary['rate'])","for i, summary in enumerate(compiled_stats):
    (hrs, mins, secs) = convert_time(summary['elapsed'])
    compiled_stats[i]['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
    compiled_stats[i]['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
    compiled_stats[i]['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
    compiled_stats[i]['rate'] = '{0:.1f}'.format(summary['rate'])",1,,,,,,,,,,
sunpy,https://github.com/sunpy/sunpy/tree/master/examples/time_series/timeseries_peak_finding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/examples/time_series/timeseries_peak_finding.py,,"def findpeaks(series, DELTA):
    """"""
    Finds extrema in a pandas series data.

    Parameters
    ----------
    series : `pandas.Series`
        The data series from which we need to find extrema.

    DELTA : `float`
        The minimum difference between data values that defines a peak.

    Returns
    -------
    minpeaks, maxpeaks : `list`
        Lists consisting of pos, val pairs for both local minima points and
        local maxima points.
    """"""
    (mn, mx) = (np.Inf, -np.Inf)
    minpeaks = []
    maxpeaks = []
    lookformax = True
    start = True
    for (time_pos, value) in series.iteritems():
        if value > mx:
            mx = value
            mxpos = time_pos
        if value < mn:
            mn = value
            mnpos = time_pos
        if lookformax:
            if value < mx - DELTA:
                maxpeaks.append((mxpos, mx))
                mn = value
                mnpos = time_pos
                lookformax = False
            elif start:
                minpeaks.append((mnpos, mn))
                mx = value
                mxpos = time_pos
                start = False
        elif value > mn + DELTA:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            lookformax = True
    if value > mn + DELTA:
        maxpeaks.append((mxpos, mx))
    elif value < mx - DELTA:
        minpeaks.append((mnpos, mn))
    return (minpeaks, maxpeaks)","for (time_pos, value) in series.iteritems():
    if value > mx:
        mx = value
        mxpos = time_pos
    if value < mn:
        mn = value
        mnpos = time_pos
    if lookformax:
        if value < mx - DELTA:
            maxpeaks.append((mxpos, mx))
            mn = value
            mnpos = time_pos
            lookformax = False
        elif start:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            start = False
    elif value > mn + DELTA:
        minpeaks.append((mnpos, mn))
        mx = value
        mxpos = time_pos
        lookformax = True","for time_pos, value in enumerate(series):
    if value > mx:
        mx = value
        mxpos = time_pos
    if value < mn:
        mn = value
        mnpos = time_pos
    if lookformax:
        if value < mx - DELTA:
            maxpeaks.append((mxpos, mx))
            mn = value
            mnpos = time_pos
            lookformax = False
        elif start:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            start = False
    elif value > mn + DELTA:
        minpeaks.append((mnpos, mn))
        mx = value
        mxpos = time_pos
        lookformax = True",1,,,,,,,,,,
cubes,https://github.com/DataBrewery/cubes/tree/master/cubes/query/cells.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cubes/cubes/query/cells.py,,"def cuts_from_string(cube, string, member_converters=None, role_member_converters=None):
    """"""Return list of cuts specified in `string`. You can use this function to
    parse cuts encoded in a URL.

    Arguments:

    * `string`  string containing the cut descritption (see below)
    * `cube`  cube for which the cuts are being created
    * `member_converters`  callables converting single-item values into paths.
      Keys are dimension names.
    * `role_member_converters`  callables converting single-item values into
      paths. Keys are dimension role names (`Dimension.role`).

    Examples::

        date:2004
        date:2004,1
        date:2004,1|class=5
        date:2004,1,1|category:5,10,12|class:5

    Ranges are in form ``from-to`` with possibility of open range::

        date:2004-2010
        date:2004,5-2010,3
        date:2004,5-2010
        date:2004,5-
        date:-2010

    Sets are in form ``path1;path2;path3`` (none of the paths should be
    empty)::

        date:2004;2010
        date:2004;2005,1;2010,10

    Grammar::

        <list> ::= <cut> | <cut> '|' <list>
        <cut> ::= <dimension> ':' <path>
        <dimension> ::= <identifier>
        <path> ::= <value> | <value> ',' <path>

    The characters '|', ':' and ',' are configured in `CUT_STRING_SEPARATOR`,
    `DIMENSION_STRING_SEPARATOR`, `PATH_STRING_SEPARATOR` respectively.
    """"""
    if not string:
        return []
    cuts = []
    dim_cuts = CUT_STRING_SEPARATOR.split(string)
    for dim_cut in dim_cuts:
        cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
        cuts.append(cut)
    return cuts","for dim_cut in dim_cuts:
    cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
    cuts.append(cut)","for i,dim_cut in enumerate(dim_cuts):
    cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
    cuts.append(cut)",1,,,,,,,,,,
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/examples/pointnet2_segmentation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_geometric/examples/pointnet2_segmentation.py,,"def test(loader):
    model.eval()
    y_mask = loader.dataset.y_mask
    ious = [[] for _ in range(len(loader.dataset.categories))]
    for data in loader:
        data = data.to(device)
        pred = model(data).argmax(dim=1)
        (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
        iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
        iou[torch.isnan(iou)] = 1
        for (iou, category) in zip(iou.unbind(), data.category.unbind()):
            ious[category.item()].append(iou[y_mask[category]])
    ious = [torch.stack(iou).mean(0).mean(0) for iou in ious]
    return torch.tensor(ious).mean().item()","for data in loader:
    data = data.to(device)
    pred = model(data).argmax(dim=1)
    (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
    iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
    iou[torch.isnan(iou)] = 1
    for (iou, category) in zip(iou.unbind(), data.category.unbind()):
        ious[category.item()].append(iou[y_mask[category]])","for i, data in enumerate(loader):
    data = data.to(device)
    pred = model(data).argmax(dim=1)
    (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
    iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
    iou[torch.isnan(iou)] = 1
    for (iou, category) in zip(iou.unbind(), data.category.unbind()):
        ious[category.item()].append(iou[y_mask[category]])",1,,,,,,,,,,
numpy,https://github.com/numpy/numpy/tree/master/tools/refguide_check.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/tools/refguide_check.py,,"def check_documentation(base_path, results, args, dots):
    """"""
    Check examples in any *.rst located inside `base_path`.
    Add the output to `results`.

    See Also
    --------
    check_doctests_testfile
    """"""
    for filename in iter_included_files(base_path, args.verbose):
        if dots:
            sys.stderr.write(filename + ' ')
            sys.stderr.flush()
        tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

        def scratch():
            pass
        scratch.__name__ = filename
        results.append((scratch, tut_results))
        if dots:
            sys.stderr.write('\n')
            sys.stderr.flush()","for filename in iter_included_files(base_path, args.verbose):
    if dots:
        sys.stderr.write(filename + ' ')
        sys.stderr.flush()
    tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

    def scratch():
        pass
    scratch.__name__ = filename
    results.append((scratch, tut_results))
    if dots:
        sys.stderr.write('\n')
        sys.stderr.flush()","for i,filename in enumerate(iter_included_files(base_path, args.verbose)):
    if dots:
        sys.stderr.write(filename + ' ')
        sys.stderr.flush()
    tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

    def scratch():
        pass
    scratch.__name__ = filename
    results.append((scratch, tut_results))
    if dots:
        sys.stderr.write('\n')
        sys.stderr.flush()",1,,,,,,,,,,
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for glyphName in glyphNames:
    try:
        gid = ttFont.getGlyphID(glyphName)
    except:
        assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
    vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])","for i,glyphName in enumerate(glyphNames):
    try:
        gid = ttFont.getGlyphID(glyphName)
    except:
        assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
    vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])",1,,,,,,,,,,
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for entry in vOriginTable:
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)","for i, entry in enumerate(vOriginTable):
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)",1,,,,,,,,,,
MetPy,https://github.com/Unidata/MetPy/tree/master/src/metpy/plots/station_plot.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MetPy/src/metpy/plots/station_plot.py,StationPlotLayout,"def names(self):
    """"""Get the list of names used by the layout.

        Returns
        -------
        list[str]
            the list of names of variables used by the layout

        """"""
    ret = []
    for item in self.values():
        if item[0] == self.PlotTypes.barb:
            ret.extend(item[1])
        else:
            ret.append(item[1])
    return ret","for item in self.values():
    if item[0] == self.PlotTypes.barb:
        ret.extend(item[1])
    else:
        ret.append(item[1])","for i,item in enumerate(self.values()):
    if item[0] == self.PlotTypes.barb:
        ret.extend(item[1])
    else:
        ret.append(item[1])",1,,,,,,,,,,
speechbrain,https://github.com/speechbrain/speechbrain/tree/master/recipes/KsponSpeech/ASR/transformer/train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/speechbrain/recipes/KsponSpeech/ASR/transformer/train.py,,"if __name__ == '__main__':
    (hparams_file, run_opts, overrides) = sb.parse_arguments(sys.argv[1:])
    with open(hparams_file) as fin:
        hparams = load_hyperpyyaml(fin, overrides)
    sb.utils.distributed.ddp_init_group(run_opts)
    from ksponspeech_prepare import prepare_ksponspeech
    sb.create_experiment_directory(experiment_directory=hparams['output_folder'], hyperparams_to_save=hparams_file, overrides=overrides)
    run_on_main(prepare_ksponspeech, kwargs={'data_folder': hparams['data_folder'], 'tr_splits': hparams['train_splits'], 'dev_splits': hparams['dev_splits'], 'te_splits': hparams['test_splits'], 'save_folder': hparams['data_folder'], 'merge_lst': hparams['train_splits'], 'merge_name': hparams['train_csv'], 'skip_prep': hparams['skip_prep']})
    (train_data, valid_data, test_datasets, tokenizer) = dataio_prepare(hparams)
    run_on_main(hparams['pretrainer'].collect_files)
    hparams['pretrainer'].load_collected(device=run_opts['device'])
    asr_brain = ASR(modules=hparams['modules'], opt_class=hparams['Adam'], hparams=hparams, run_opts=run_opts, checkpointer=hparams['checkpointer'])
    asr_brain.tokenizer = hparams['tokenizer']
    asr_brain.fit(asr_brain.hparams.epoch_counter, train_data, valid_data, train_loader_kwargs=hparams['train_dataloader_opts'], valid_loader_kwargs=hparams['valid_dataloader_opts'])
    for k in test_datasets.keys():
        asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
        asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])","for k in test_datasets.keys():
    asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
    asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])","for i,k in enumerate(test_datasets.keys()):
    asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
    asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for item in items:
    if not isinstance(item, dict):
        continue
    year = int_or_none(item.get('year'))
    if year is None:
        continue
    months = item.get('months')
    if not isinstance(months, list):
        continue
    for month_dict in months:
        if not isinstance(month_dict, dict) or not month_dict:
            continue
        month_number = int_or_none(list(month_dict.keys())[0])
        if month_number is None:
            continue
        entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))","for i,item in enumerate(items):
    if not isinstance(item, dict):
        continue
    year = int_or_none(item.get('year'))
    if year is None:
        continue
    months = item.get('months')
    if not isinstance(months, list):
        continue
    for j,month_dict in enumerate(months):
        if not isinstance(month_dict, dict) or not month_dict:
            continue
        month_number = int_or_none(list(month_dict.keys())[0])
        if month_number is None:
            continue
        entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for month_dict in months:
    if not isinstance(month_dict, dict) or not month_dict:
        continue
    month_number = int_or_none(list(month_dict.keys())[0])
    if month_number is None:
        continue
    entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))","for i, month_dict in enumerate(months):
    if not isinstance(month_dict, dict) or not month_dict:
        continue
    month_number = int_or_none(list(month_dict.keys())[0])
    if month_number is None:
        continue
    entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for item in items:
    if not isinstance(item, dict):
        continue
    season_number = int_or_none(item.get('season'))
    if season_number is None:
        continue
    entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))","for i,item in enumerate(items):
    if not isinstance(item, dict):
        continue
    season_number = int_or_none(item.get('season'))
    if season_number is None:
        continue
    entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))",1,,,,,,,,,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,,"def make_accrual_interest_entry_for_term_loans(posting_date, process_loan_interest, term_loan=None, loan_type=None, accrual_type='Regular'):
    curr_date = posting_date or add_days(nowdate(), 1)
    term_loans = get_term_loans(curr_date, term_loan, loan_type)
    accrued_entries = []
    for loan in term_loans:
        accrued_entries.append(loan.payment_entry)
        args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
        make_loan_interest_accrual_entry(args)
    if accrued_entries:
        frappe.db.sql('UPDATE `tabRepayment Schedule`\n\t\t\tSET is_accrued = 1 where name in (%s)' % ', '.join(['%s'] * len(accrued_entries)), tuple(accrued_entries))","for loan in term_loans:
    accrued_entries.append(loan.payment_entry)
    args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
    make_loan_interest_accrual_entry(args)","for i, loan in enumerate(term_loans):
    accrued_entries.append(loan.payment_entry)
    args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
    make_loan_interest_accrual_entry(args)",1,,,,,,,,,,
openpilot,https://github.com/commaai/openpilot/tree/master/tools/sim/lib/manual_ctrl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/sim/lib/manual_ctrl.py,,"def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')","for _axis in buf[:num_axes]:
    axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
    axis_map.append(axis_name)
    axis_states[axis_name] = 0.0","for i,_axis in enumerate(buf[:num_axes]):
    axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
    axis_map.append(axis_name)
    axis_states[axis_name] = 0.0",1,,,,,,,,,,
openpilot,https://github.com/commaai/openpilot/tree/master/tools/sim/lib/manual_ctrl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/sim/lib/manual_ctrl.py,,"def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')","for btn in buf[:num_buttons]:
    btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
    button_map.append(btn_name)
    button_states[btn_name] = 0","for i, btn in enumerate(buf[:num_buttons]):
    btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
    button_map.append(btn_name)
    button_states[btn_name] = 0",1,,,,,,,,,,
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/transpiler/passes/basis/basis_translator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/transpiler/passes/basis/basis_translator.py,BasisTranslator,"def _extract_basis_target(self, dag, qarg_indices, source_basis=None, qargs_local_source_basis=None):
    if source_basis is None:
        source_basis = set()
    if qargs_local_source_basis is None:
        qargs_local_source_basis = defaultdict(set)
    for node in dag.op_nodes():
        qargs = tuple((qarg_indices[bit] for bit in node.qargs))
        if dag.has_calibration_for(node):
            continue
        if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
            qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
        else:
            source_basis.add((node.name, node.op.num_qubits))
        if isinstance(node.op, ControlFlowOp):
            for block in node.op.blocks:
                block_dag = circuit_to_dag(block)
                (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)
    return (source_basis, qargs_local_source_basis)","for node in dag.op_nodes():
    qargs = tuple((qarg_indices[bit] for bit in node.qargs))
    if dag.has_calibration_for(node):
        continue
    if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
        qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
    else:
        source_basis.add((node.name, node.op.num_qubits))
    if isinstance(node.op, ControlFlowOp):
        for block in node.op.blocks:
            block_dag = circuit_to_dag(block)
            (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)","for i,node in enumerate(dag.op_nodes()):
    qargs = tuple((qarg_indices[bit] for bit in node.qargs))
    if dag.has_calibration_for(node):
        continue
    if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
        qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
    else:
        source_basis.add((node.name, node.op.num_qubits))
    if isinstance(node.op, ControlFlowOp):
        for block in node.op.blocks:
            block_dag = circuit_to_dag(block)
            (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)",1,,,,,,,,,,
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/transpiler/passes/basis/basis_translator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/transpiler/passes/basis/basis_translator.py,BasisTranslator,"def _extract_basis_target(self, dag, qarg_indices, source_basis=None, qargs_local_source_basis=None):
    if source_basis is None:
        source_basis = set()
    if qargs_local_source_basis is None:
        qargs_local_source_basis = defaultdict(set)
    for node in dag.op_nodes():
        qargs = tuple((qarg_indices[bit] for bit in node.qargs))
        if dag.has_calibration_for(node):
            continue
        if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
            qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
        else:
            source_basis.add((node.name, node.op.num_qubits))
        if isinstance(node.op, ControlFlowOp):
            for block in node.op.blocks:
                block_dag = circuit_to_dag(block)
                (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)
    return (source_basis, qargs_local_source_basis)","for block in node.op.blocks:
    block_dag = circuit_to_dag(block)
    (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)","for i, block in enumerate(node.op.blocks):
    block_dag = circuit_to_dag(block)
    (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)",1,,,,,,,,,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/host.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/host.py,host,"def get_managed_hosts(self, dn):
    host_filter = 'managedBy=%s' % dn
    host_attrs = ['fqdn']
    ldap = self.api.Backend.ldap2
    managed_hosts = []
    try:
        (hosts, _truncated) = ldap.find_entries(base_dn=DN(self.container_dn, api.env.basedn), filter=host_filter, attrs_list=host_attrs)
        for host in hosts:
            managed_hosts.append(host.dn)
    except errors.NotFound:
        return []
    return managed_hosts","for host in hosts:
    managed_hosts.append(host.dn)","for i, host in enumerate(hosts):
    managed_hosts.append(host.dn)",1,,,,,,,,,,
RigNet,https://github.com/zhan-xu/RigNet/tree/master/utils/rig_parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RigNet/utils/rig_parser.py,Skel,"def save(self, filename):
    fout = open(filename, 'w')
    this_level = [self.root]
    hier_level = 1
    while this_level:
        next_level = []
        for p_node in this_level:
            pos = p_node.pos
            parent = p_node.parent.name if p_node.parent is not None else 'None'
            line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
            fout.write(line)
            for c_node in p_node.children:
                next_level.append(c_node)
        this_level = next_level
        hier_level += 1
    fout.close()","for p_node in this_level:
    pos = p_node.pos
    parent = p_node.parent.name if p_node.parent is not None else 'None'
    line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
    fout.write(line)
    for c_node in p_node.children:
        next_level.append(c_node)","for i,p_node in enumerate(this_level):
    pos = p_node.pos
    parent = p_node.parent.name if p_node.parent is not None else 'None'
    line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
    fout.write(line)
    for j,c_node in enumerate(p_node.children):
        next_level.append(c_node)",1,,,,,,,,,,
RigNet,https://github.com/zhan-xu/RigNet/tree/master/utils/rig_parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RigNet/utils/rig_parser.py,Skel,"def save(self, filename):
    fout = open(filename, 'w')
    this_level = [self.root]
    hier_level = 1
    while this_level:
        next_level = []
        for p_node in this_level:
            pos = p_node.pos
            parent = p_node.parent.name if p_node.parent is not None else 'None'
            line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
            fout.write(line)
            for c_node in p_node.children:
                next_level.append(c_node)
        this_level = next_level
        hier_level += 1
    fout.close()","for c_node in p_node.children:
    next_level.append(c_node)","for i,c_node in enumerate(p_node.children):
    next_level.append(c_node)",1,,,,,,,,,,
espresso,https://github.com/freewym/espresso/tree/master/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,KLDivergenceRerankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    sample_size = sample['id'].numel()
    assert sample_size % self.task.cfg.mt_beam == 0, f'sample_size ({sample_size}) cannot be divided by beam size ({self.task.cfg.mt_beam}).Please set --required-batch-size-multiple={self.task.cfg.mt_beam}.'
    batch_out = []
    for i in range(0, sample_size, self.forward_batch_size):
        j = min(i + self.forward_batch_size, sample_size)
        out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
        batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))
    batch_out = torch.cat(batch_out, dim=0).view(self.task.cfg.mt_beam, sample_size // self.task.cfg.mt_beam, -1)
    if model.joint_classification == 'sent':
        batch_out = model.joint_forward(batch_out)
    scores = model.classification_forward(batch_out.view(sample_size, 1, -1)).view(-1, self.task.cfg.mt_beam)
    loss = self.compute_kl_loss(scores, sample['target'][:, 0].view(-1, self.task.cfg.mt_beam))
    sample_size = sample_size // self.task.cfg.mt_beam
    logging_output = {'loss': loss.detach(), 'ntokens': sample['ntokens'], 'nsentences': sample_size * self.task.cfg.mt_beam, 'sample_size': sample_size, 'scores': scores.detach()}
    return (loss, sample_size, logging_output)","for i in range(0, sample_size, self.forward_batch_size):
    j = min(i + self.forward_batch_size, sample_size)
    out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
    batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))","for i in range(0, sample_size, self.forward_batch_size):
    j = min(i + self.forward_batch_size, sample_size)
    out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
    batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))",1,,,,,,,,,,
FARM,https://github.com/deepset-ai/FARM/tree/master/farm/modeling/biadaptive_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FARM/farm/modeling/biadaptive_model.py,BaseBiAdaptiveModel,"def connect_heads_with_processor(self, tasks, require_labels=True):
    """"""
        Populates prediction head with information coming from tasks.

        :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)
        :param require_labels: If True, an error will be thrown when a task is not supplied with labels)
        :return:
        """"""
    for head in self.prediction_heads:
        head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
        label_list = tasks[head.task_name]['label_list']
        if not label_list and require_labels:
            raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
        label_list = tasks[head.task_name]['label_list']
        head.label_list = label_list
        num_labels = len(label_list)
        head.metric = tasks[head.task_name]['metric']","for head in self.prediction_heads:
    head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
    label_list = tasks[head.task_name]['label_list']
    if not label_list and require_labels:
        raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
    label_list = tasks[head.task_name]['label_list']
    head.label_list = label_list
    num_labels = len(label_list)
    head.metric = tasks[head.task_name]['metric']","for i, head in enumerate(self.prediction_heads):
    head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
    label_list = tasks[head.task_name]['label_list']
    if not label_list and require_labels:
        raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
    label_list = tasks[head.task_name]['label_list']
    head.label_list = label_list
    num_labels = len(label_list)
    head.metric = tasks[head.task_name]['metric']",1,,,,,,,,,,
BLINK,https://github.com/facebookresearch/BLINK/tree/master/blink/biencoder/zeshel_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BLINK/blink/biencoder/zeshel_utils.py,Stats,"def __init__(self, top_k=1000):
    self.cnt = 0
    self.hits = []
    self.top_k = top_k
    self.rank = [1, 4, 8, 16, 32, 64, 100, 128, 256, 512]
    self.LEN = len(self.rank)
    for i in range(self.LEN):
        self.hits.append(0)","for i in range(self.LEN):
    self.hits.append(0)","for i,_ in enumerate(range(self.LEN)):
    self.hits.append(0)",1,,,,,,,,,,
data-driven-web-apps-with-flask,https://github.com/talkpython/data-driven-web-apps-with-flask/tree/master/app/ch12-forms/starter/pypi_org/bin/load_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-driven-web-apps-with-flask/app/ch12-forms/starter/pypi_org/bin/load_data.py,,"def do_import_languages(file_data: List[dict]):
    imported = set()
    print('Importing languages ... ', flush=True)
    with progressbar.ProgressBar(max_value=len(file_data)) as bar:
        for (idx, p) in enumerate(file_data):
            info = p.get('info')
            classifiers = info.get('classifiers')
            for c in classifiers:
                if 'Programming Language' not in c:
                    continue
                original = c
                c = c.replace('Implementation ::', '').replace('::', ':')
                text = c
                parts = c.split(':')
                if len(parts) > 1:
                    text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
                if text not in imported:
                    imported.add(text)
                    session = db_session.create_session()
                    lang = ProgrammingLanguage()
                    lang.description = original
                    lang.id = text
                    session.add(lang)
                    session.commit()
            bar.update(idx)
    sys.stderr.flush()
    sys.stdout.flush()","for c in classifiers:
    if 'Programming Language' not in c:
        continue
    original = c
    c = c.replace('Implementation ::', '').replace('::', ':')
    text = c
    parts = c.split(':')
    if len(parts) > 1:
        text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
    if text not in imported:
        imported.add(text)
        session = db_session.create_session()
        lang = ProgrammingLanguage()
        lang.description = original
        lang.id = text
        session.add(lang)
        session.commit()","for i,c in enumerate(classifiers):
    if 'Programming Language' not in c:
        continue
    original = c
    c = c.replace('Implementation ::', '').replace('::', ':')
    text = c
    parts = c.split(':')
    if len(parts) > 1:
        text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
    if text not in imported:
        imported.add(text)
        session = db_session.create_session()
        lang = ProgrammingLanguage()
        lang.description = original
        lang.id = text
        session.add(lang)
        session.commit()",1,,,,,,,,,,
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)","for item in builder.widgets.items():
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)","for i, item in enumerate(builder.widgets.items()):
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)",1,,,,,,,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","for i in range(batch_size):
    width_begin = map_width * i
    width_end = map_width * (i + 1)
    image = dataset.reverse_transform(batch_images[i])
    grid_image[:, width_begin:width_end, :] = image
    if 'semantic' in target_keys:
        gt_sem = batch_targets['semantic'][i].cpu().numpy()
        gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
        grid_target[:map_height, width_begin:width_end, :] = gt_sem
    if 'center' in target_keys:
        gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
        gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        gt_ctr = gt_ctr.clip(0, 255)
        grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
    if 'offset' in target_keys:
        gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
        gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
        grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
    if 'semantic_weights' in target_keys:
        gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
    if 'center_weights' in target_keys:
        gt_ign = batch_targets['center_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
    if 'offset_weights' in target_keys:
        gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
    if 'foreground' in target_keys:
        gt_fg = batch_targets['foreground'][i].cpu().numpy()
        gt_fg = gt_fg[:, :, None] * 255
        grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
    if 'semantic' in output_keys:
        pred_sem = semantic_pred[i].cpu().numpy()
        pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
        grid_output[:map_height, width_begin:width_end, :] = pred_sem
    if 'center' in output_keys:
        pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
        pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        pred_ctr = pred_ctr.clip(0, 255)
        grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
    if 'offset' in output_keys:
        pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
        pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
        grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
    if 'foreground' in output_keys:
        if foreground_pred is not None:
            pred_fg = foreground_pred[i].cpu().numpy()
            pred_fg = pred_fg[:, :, None] * 255
            grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg","for i in range(batch_size):
    width_begin = map_width * i
    width_end = map_width * (i + 1)
    image = dataset.reverse_transform(batch_images[i])
    grid_image[:, width_begin:width_end, :] = image
    for target_key in target_keys:
        if target_key == 'semantic':
            gt_sem = batch_targets[target_key][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        elif target_key == 'center':
            gt_ctr = batch_targets[target_key][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        elif target_key == 'offset':
            gt_off = batch_targets[target_key][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        elif target_key == 'semantic_weights':
            gt_ign = batch_targets[target_key][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        elif target_key == 'center_weights':
            gt_ign = batch_targets[target_key][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        elif target_key == 'offset_weights':
            gt_ign = batch_targets[target_key][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        elif target_key == 'foreground':
            gt_fg = batch_targets[target_key][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
    for output_key in output_keys:
        if output_key == 'semantic':
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        elif output_key == 'center':
            pred_ctr = batch_outputs[output_key][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        elif output_key == 'offset':
            pred_ctr = batch_outputs[output_key][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        elif output_key == 'foreground':
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg",1,,,,,,,,,,
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug","for (param, expection) in expections.items():
    url = reverse('milestones-list') + '?' + param
    response = client.get(url)
    number_of_milestones = len(response.data)
    assert response.status_code == 200
    assert number_of_milestones == expection, param
    if number_of_milestones > 0:
        assert response.data[0]['slug'] == milestone.slug","for i, (param, expection) in enumerate(expections.items()):
    url = reverse('milestones-list') + '?' + param
    response = client.get(url)
    number_of_milestones = len(response.data)
    assert response.status_code == 200
    assert number_of_milestones == expection, param
    if number_of_milestones > 0:
        assert response.data[0]['slug'] == milestone.slug",1,,,,,,,,,,
django-rest-framework,https://github.com/encode/django-rest-framework/tree/master/rest_framework/viewsets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rest-framework/rest_framework/viewsets.py,ViewSetMixin,"def get_extra_action_url_map(self):
    """"""
        Build a map of {names: urls} for the extra actions.

        This method will noop if `detail` was not provided as a view initkwarg.
        """"""
    action_urls = OrderedDict()
    if self.detail is None:
        return action_urls
    actions = [action for action in self.get_extra_actions() if action.detail == self.detail]
    for action in actions:
        try:
            url_name = '%s-%s' % (self.basename, action.url_name)
            url = reverse(url_name, self.args, self.kwargs, request=self.request)
            view = self.__class__(**action.kwargs)
            action_urls[view.get_view_name()] = url
        except NoReverseMatch:
            pass
    return action_urls","for action in actions:
    try:
        url_name = '%s-%s' % (self.basename, action.url_name)
        url = reverse(url_name, self.args, self.kwargs, request=self.request)
        view = self.__class__(**action.kwargs)
        action_urls[view.get_view_name()] = url
    except NoReverseMatch:
        pass","for i, action in enumerate(actions):
    try:
        url_name = '%s-%s' % (self.basename, action.url_name)
        url = reverse(url_name, self.args, self.kwargs, request=self.request)
        view = self.__class__(**action.kwargs)
        action_urls[view.get_view_name()] = url
    except NoReverseMatch:
        pass",1,,,,,,,,,,
kaggle_ndsb2017,https://github.com/juliandewit/kaggle_ndsb2017/tree/master//helpers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kaggle_ndsb2017//helpers.py,,"def get_segmented_lungs(im, plot=False):
    binary = im < -400
    cleared = clear_border(binary)
    label_image = label(cleared)
    areas = [r.area for r in regionprops(label_image)]
    areas.sort()
    if len(areas) > 2:
        for region in regionprops(label_image):
            if region.area < areas[-2]:
                for coordinates in region.coords:
                    label_image[coordinates[0], coordinates[1]] = 0
    binary = label_image > 0
    selem = disk(2)
    binary = binary_erosion(binary, selem)
    selem = disk(10)
    binary = binary_closing(binary, selem)
    edges = roberts(binary)
    binary = ndi.binary_fill_holes(edges)
    get_high_vals = binary == 0
    im[get_high_vals] = -2000
    return (im, binary)","for coordinates in region.coords:
    label_image[coordinates[0], coordinates[1]] = 0","for i, coordinates in enumerate(region.coords):
    label_image[coordinates[0], coordinates[1]] = 0",1,,,,,,,,,,
doit,https://github.com/pydoit/doit/tree/master/doit/task.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/doit/doit/task.py,Task,"def clean(self, outstream, dryrun):
    """"""Execute task's clean
        @ivar outstream: 'write' output into this stream
        @ivar dryrun (bool): if True clean tasks are not executed
                             (just print out what would be executed)
        """"""
    self.init_options()
    if self._remove_targets is True:
        clean_targets(self, dryrun)
    else:
        for action in self.clean_actions:
            msg = ""%s - executing '%s'\n""
            outstream.write(msg % (self.name, action))
            execute_on_dryrun = False
            if isinstance(action, PythonAction):
                action_sig = inspect.signature(action.py_callable)
                if 'dryrun' in action_sig.parameters:
                    execute_on_dryrun = True
                    action.kwargs['dryrun'] = dryrun
            if not dryrun or execute_on_dryrun:
                result = action.execute(out=outstream)
                if isinstance(result, CatchedException):
                    sys.stderr.write(str(result))","for action in self.clean_actions:
    msg = ""%s - executing '%s'\n""
    outstream.write(msg % (self.name, action))
    execute_on_dryrun = False
    if isinstance(action, PythonAction):
        action_sig = inspect.signature(action.py_callable)
        if 'dryrun' in action_sig.parameters:
            execute_on_dryrun = True
            action.kwargs['dryrun'] = dryrun
    if not dryrun or execute_on_dryrun:
        result = action.execute(out=outstream)
        if isinstance(result, CatchedException):
            sys.stderr.write(str(result))","for i, action in enumerate(self.clean_actions):
    msg = ""%s - executing '%s'\n""
    outstream.write(msg % (self.name, action))
    execute_on_dryrun = False
    if isinstance(action, PythonAction):
        action_sig = inspect.signature(action.py_callable)
        if 'dryrun' in action_sig.parameters:
            execute_on_dryrun = True
            action.kwargs['dryrun'] = dryrun
    if not dryrun or execute_on_dryrun:
        result = action.execute(out=outstream)
        if isinstance(result, CatchedException):
            sys.stderr.write(str(result))",1,,,,,,,,,,
MeshCNN,https://github.com/ranahanocka/MeshCNN/tree/master/util/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MeshCNN/util/util.py,,"def print_network(net):
    """"""Print the total number of parameters in the network
    Parameters:
        network
    """"""
    print('---------- Network initialized -------------')
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print('[Network] Total number of parameters : %.3f M' % (num_params / 1000000.0))
    print('-----------------------------------------------')","for param in net.parameters():
    num_params += param.numel()","for i,param in enumerate(net.parameters()):
    num_params += param.numel()",1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/reports/controllers/system.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/reports/controllers/system.py,System,"def deleted_histories(self, trans, **kwd):
    """"""
        The number of histories that were deleted more than the specified number of days ago, but have not yet been purged.
        Also included is the number of datasets associated with the histories.
        """"""
    params = util.Params(kwd)
    message = ''
    if params.deleted_histories_days:
        deleted_histories_days = int(params.deleted_histories_days)
        cutoff_time = datetime.utcnow() - timedelta(days=deleted_histories_days)
        history_count = 0
        dataset_count = 0
        disk_space = 0
        histories = trans.sa_session.query(model.History).filter(and_(model.History.table.c.deleted == true(), model.History.table.c.purged == false(), model.History.update_time < cutoff_time)).options(eagerload('datasets'))
        for history in histories:
            for hda in history.datasets:
                if not hda.dataset.purged:
                    dataset_count += 1
                    try:
                        disk_space += hda.dataset.file_size
                    except Exception:
                        pass
            history_count += 1
        message = '%d histories ( including a total of %d datasets ) were deleted more than %d days ago, but have not yet been purged, disk space: %s.' % (history_count, dataset_count, deleted_histories_days, nice_size(disk_space, True))
    else:
        message = 'Enter the number of days.'
    return (str(deleted_histories_days), message)","for hda in history.datasets:
    if not hda.dataset.purged:
        dataset_count += 1
        try:
            disk_space += hda.dataset.file_size
        except Exception:
            pass","for i,hda in enumerate(history.datasets):
    if not hda.dataset.purged:
        dataset_count += 1
        try:
            disk_space += hda.dataset.file_size
        except Exception:
            pass",1,,,,,,,,,,
sentinelsat,https://github.com/sentinelsat/sentinelsat/tree/master/tests/test_docs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentinelsat/tests/test_docs.py,,"def test_rst(rst_file):
    with open(rst_file) as input_file:
        contents = input_file.read()
    all_errors = []
    errors = rstcheck.check(contents, report_level=2, ignore={'languages': ['python', 'bash']})
    for (line_number, error) in errors:
        if 'Title underline too short' in error:
            continue
        m = re.search('Unknown interpreted text role ""([^""]+)""', error)
        if m and m.group(1) in ['program', 'paramref']:
            continue
        m = re.search('Unknown directive type ""([^""]+)""', error)
        if m and m.group(1) in ['automodule']:
            continue
        all_errors.append((line_number, error))
    assert len(all_errors) == 0","for (line_number, error) in errors:
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))","for i,(line_number, error) in enumerate(errors):
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))",1,,,,,,,,,,
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for b in range(value.shape[0]):
    b_kernel = kernel[min(b, kernel.shape[0] - 1)]
    for o in range(kernel.shape[1]):
        for i in range(value.shape[1]):
            result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for b, b_value in enumerate(value):
    b_kernel = kernel[min(b, kernel.shape[0] - 1)]
    for o in range(kernel.shape[1]):
        for i, i_value in enumerate(b_value):
            result[b, o, ...] += scipy.signal.correlate(i_value, b_kernel[o, i, ...], mode=mode)",1,,,,,,,,,,
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for o in range(kernel.shape[1]):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for o, _ in enumerate(range(kernel.shape[1])):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)",1,,,,,,,,,,
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for i in range(value.shape[1]):
    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for i in enumerate(range(value.shape[1])):
    result[b, o, ...] += scipy.signal.correlate(value[b, i[1], ...], b_kernel[o, i[1], ...], mode=mode)",1,,,,,,,,,,
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in embed_layer.collect_params().items():
    if '_embed' in k:
        arr_id = int(k[-len('_weight') - 1])
        embed_weights[arr_id] = v.data()[0].asnumpy()
        embed_grads[arr_id] = v.grad()[0].asnumpy()
    elif '_inter_proj' in k:
        arr_id = int(k[-len('_weight') - 1])
        proj_weights[arr_id] = v.data()[0].asnumpy()
        proj_grads[arr_id] = v.grad()[0].asnumpy()","for i, (k, v) in enumerate(embed_layer.collect_params().items()):
    if '_embed' in k:
        arr_id = int(k[-len('_weight') - 1])
        embed_weights[arr_id] = v.data()[0].asnumpy()
        embed_grads[arr_id] = v.grad()[0].asnumpy()
    elif '_inter_proj' in k:
        arr_id = int(k[-len('_weight') - 1])
        proj_weights[arr_id] = v.data()[0].asnumpy()
        proj_grads[arr_id] = v.grad()[0].asnumpy()",1,,,,,,,,,,
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_proj.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for i,(k, v) in enumerate(layer_with_shared_proj.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])",1,,,,,,,,,,
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_embed.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])","for i, (k, v) in enumerate(layer_with_shared_embed.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])",1,,,,,,,,,,
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_proj_embed.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for i, (k, v) in enumerate(layer_with_shared_proj_embed.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])",1,,,,,,,,,,
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for building in self.production_buildings:
    producer = building.get_component(Producer)
    for production in producer.get_productions():
        if not production.get_produced_resources():
            continue
        all_full = True
        for (resource_id, min_amount) in production.get_produced_resources().items():
            if production.inventory.get_free_space_for(resource_id) >= min_amount:
                all_full = False
                break
        if all_full and (not isinstance(building, Mine)):
            for resource_id in production.get_consumed_resources():
                if production.inventory.get_free_space_for(resource_id) > 0:
                    all_full = False
                    break
        if all_full:
            if not production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
        elif production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for i, building in enumerate(self.production_buildings):
    producer = building.get_component(Producer)
    for production in producer.get_productions():
        if not production.get_produced_resources():
            continue
        all_full = True
        for (resource_id, min_amount) in production.get_produced_resources().items():
            if production.inventory.get_free_space_for(resource_id) >= min_amount:
                all_full = False
                break
        if all_full and (not isinstance(building, Mine)):
            for resource_id in production.get_consumed_resources():
                if production.inventory.get_free_space_for(resource_id) > 0:
                    all_full = False
                    break
        if all_full:
            if not production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
        elif production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)",1,,,,,,,,,,
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for production in producer.get_productions():
    if not production.get_produced_resources():
        continue
    all_full = True
    for (resource_id, min_amount) in production.get_produced_resources().items():
        if production.inventory.get_free_space_for(resource_id) >= min_amount:
            all_full = False
            break
    if all_full and (not isinstance(building, Mine)):
        for resource_id in production.get_consumed_resources():
            if production.inventory.get_free_space_for(resource_id) > 0:
                all_full = False
                break
    if all_full:
        if not production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
    elif production.is_paused():
        ToggleActive(producer, production).execute(self.land_manager.session)
        self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for i, production in enumerate(producer.get_productions()):
    if not production.get_produced_resources():
        continue
    all_full = True
    for (resource_id, min_amount) in production.get_produced_resources().items():
        if production.inventory.get_free_space_for(resource_id) >= min_amount:
            all_full = False
            break
    if all_full and (not isinstance(building, Mine)):
        for resource_id in production.get_consumed_resources():
            if production.inventory.get_free_space_for(resource_id) > 0:
                all_full = False
                break
    if all_full:
        if not production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
    elif production.is_paused():
        ToggleActive(producer, production).execute(self.land_manager.session)
        self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)",1,,,,,,,,,,
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for (resource_id, min_amount) in production.get_produced_resources().items():
    if production.inventory.get_free_space_for(resource_id) >= min_amount:
        all_full = False
        break","for i, (resource_id, min_amount) in enumerate(production.get_produced_resources().items()):
    if production.inventory.get_free_space_for(resource_id) >= min_amount:
        all_full = False
        break",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/once.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/once.py,OnceIE,"def _extract_once_formats(self, url, http_formats_preference=None):
    (domain_id, application_id, media_item_id) = re.match(OnceIE._VALID_URL, url).groups()
    formats = self._extract_m3u8_formats(self.ADAPTIVE_URL_TEMPLATE % (domain_id, application_id, media_item_id), media_item_id, 'mp4', m3u8_id='hls', fatal=False)
    progressive_formats = []
    for adaptive_format in formats:
        adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
        rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
        if rendition_id:
            progressive_format = adaptive_format.copy()
            progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
            progressive_formats.append(progressive_format)
    self._check_formats(progressive_formats, media_item_id)
    formats.extend(progressive_formats)
    return formats","for adaptive_format in formats:
    adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
    rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
    if rendition_id:
        progressive_format = adaptive_format.copy()
        progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
        progressive_formats.append(progressive_format)","for i, adaptive_format in enumerate(formats):
    adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
    rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
    if rendition_id:
        progressive_format = adaptive_format.copy()
        progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
        progressive_formats.append(progressive_format)",1,,,,,,,,,,
DFDNet,https://github.com/csxmli2016/DFDNet/tree/master/models/base_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DFDNet/models/base_model.py,BaseModel,"def print_networks(self, verbose):
    for name in self.model_names:
        if isinstance(name, str):
            net = getattr(self, 'net' + name)
            num_params = 0
            for param in net.parameters():
                num_params += param.numel()","for name in self.model_names:
    if isinstance(name, str):
        net = getattr(self, 'net' + name)
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()","for i, name in enumerate(self.model_names):
    if isinstance(name, str):
        net = getattr(self, 'net' + name)
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()",1,,,,,,,,,,
DFDNet,https://github.com/csxmli2016/DFDNet/tree/master/models/base_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DFDNet/models/base_model.py,BaseModel,"def print_networks(self, verbose):
    for name in self.model_names:
        if isinstance(name, str):
            net = getattr(self, 'net' + name)
            num_params = 0
            for param in net.parameters():
                num_params += param.numel()","for param in net.parameters():
    num_params += param.numel()","for i,param in enumerate(net.parameters()):
    num_params += param.numel()",1,,,,,,,,,,
dm_control,https://github.com/deepmind/dm_control/tree/master/dm_control/suite/suite_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm_control/dm_control/suite/suite_test.py,SuiteTest,"def assertCorrectColors(self, physics, reward):
    colors = physics.named.model.mat_rgba
    for material_name in ('self', 'effector', 'target'):
        highlight = colors[material_name + '_highlight']
        default = colors[material_name + '_default']
        blend_coef = reward ** 4
        expected = blend_coef * highlight + (1.0 - blend_coef) * default
        actual = colors[material_name]
        err_msg = 'Material {!r} has unexpected color.\nExpected: {!r}\nActual: {!r}'.format(material_name, expected, actual)
        np.testing.assert_array_almost_equal(expected, actual, err_msg=err_msg)","for material_name in ('self', 'effector', 'target'):
    highlight = colors[material_name + '_highlight']
    default = colors[material_name + '_default']
    blend_coef = reward ** 4
    expected = blend_coef * highlight + (1.0 - blend_coef) * default
    actual = colors[material_name]
    err_msg = 'Material {!r} has unexpected color.\nExpected: {!r}\nActual: {!r}'.format(material_name, expected, actual)
    np.testing.assert_array_almost_equal(expected, actual, err_msg=err_msg)","for i, material_name in enumerate(('self', 'effector', 'target')):
    highlight = colors[material_name + '_highlight']
    default = colors[material_name + '_default']
    blend_coef = reward ** 4
    expected = blend_coef * highlight + (1.0 - blend_coef) * default
    actual = colors[material_name]
    err_msg = 'Material {!r} has unexpected color.\nExpected: {!r}\nActual: {!r}'.format(material_name, expected, actual)
    np.testing.assert_array_almost_equal(expected, actual, err_msg=err_msg)",1,,,,,,,,,,
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/query_labels.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ALiPy/alipy/query_strategy/query_labels.py,QueryInstanceQBC,"def calc_avg_KL_divergence(cls, predict_matrices):
    """"""Calculate the average Kullback-Leibler (KL) divergence for measuring the
        level of disagreement in QBC.

        Parameters
        ----------
        predict_matrices: list
            The prediction matrix for each committee.
            Each committee predict matrix should have the shape [n_samples, n_classes] for probabilistic output
            or [n_samples] for class output.

        Returns
        -------
        score: list
            Score for each instance. Shape [n_samples]

        References
        ----------
        [1] A. McCallum and K. Nigam. Employing EM in pool-based active learning for
            text classification. In Proceedings of the International Conference on Machine
            Learning (ICML), pages 359-367. Morgan Kaufmann, 1998.
        """"""
    score = []
    (input_shape, committee_size) = cls()._check_committee_results(predict_matrices)
    if len(input_shape) == 2:
        label_num = input_shape[1]
        for i in range(input_shape[0]):
            instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
            tmp = 0
            for lab in range(label_num):
                committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
                for committee in range(committee_size):
                    tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
            score.append(tmp)
    else:
        raise Exception('A 2D probabilistic prediction matrix must be provided, with the shape like [n_samples, n_class]')
    return score","for i in range(input_shape[0]):
    instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
    tmp = 0
    for lab in range(label_num):
        committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
        for committee in range(committee_size):
            tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
    score.append(tmp)","for i in range(input_shape[0]):
    instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
    tmp = 0
    for lab in range(label_num):
        committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
        for j,committee in enumerate(range(committee_size)):
            tmp += instance_mat[j, lab] * np.log((instance_mat[j, lab] + 1e-09) / committee_consensus)
    score.append(tmp)",1,,,,,,,,,,
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/query_labels.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ALiPy/alipy/query_strategy/query_labels.py,QueryInstanceQBC,"def calc_avg_KL_divergence(cls, predict_matrices):
    """"""Calculate the average Kullback-Leibler (KL) divergence for measuring the
        level of disagreement in QBC.

        Parameters
        ----------
        predict_matrices: list
            The prediction matrix for each committee.
            Each committee predict matrix should have the shape [n_samples, n_classes] for probabilistic output
            or [n_samples] for class output.

        Returns
        -------
        score: list
            Score for each instance. Shape [n_samples]

        References
        ----------
        [1] A. McCallum and K. Nigam. Employing EM in pool-based active learning for
            text classification. In Proceedings of the International Conference on Machine
            Learning (ICML), pages 359-367. Morgan Kaufmann, 1998.
        """"""
    score = []
    (input_shape, committee_size) = cls()._check_committee_results(predict_matrices)
    if len(input_shape) == 2:
        label_num = input_shape[1]
        for i in range(input_shape[0]):
            instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
            tmp = 0
            for lab in range(label_num):
                committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
                for committee in range(committee_size):
                    tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
            score.append(tmp)
    else:
        raise Exception('A 2D probabilistic prediction matrix must be provided, with the shape like [n_samples, n_class]')
    return score","for lab in range(label_num):
    committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
    for committee in range(committee_size):
        tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)","for lab in range(label_num):
    committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
    for i,committee in enumerate(range(committee_size)):
        tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)",1,,,,,,,,,,
PyQt5-Apps,https://github.com/taseikyo/PyQt5-Apps/tree/master/hust-lib/src/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/hust-lib/src/main.py,MWin,"def resolveDataDone(self, data):
    self.statusBar.showMessage('...', 1000)
    if self.first_kw:
        self.total_pages = int(data[0])
        self.jump_page.setMaximum(self.total_pages)
        self.page_count.setText(f' {self.total_pages} ')
        self.first_kw = False
    self.current_page_index.setText(f' {self.cur_page} ')
    self.jump_page.setValue(self.cur_page)
    self.pre = data[1]
    self.suf = data[2]
    self.request.pre = self.pre
    self.request.suf = self.suf
    books = data[-1]
    if not self.table_has_header:
        self.table.setColumnCount(2)
        self.table.setHorizontalHeaderLabels(['', ''])
        self.table_has_header = True
    else:
        self.table.clearContents()
        self.table.setRowCount(0)
    lists = []
    for x in books:
        tl = x.find('span', {'class', 'briefcitTitle'}).a
        title = tl.text
        link = tl['href']
        detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
        lists.append([title, detail, link])
        row = self.table.rowCount()
        self.table.insertRow(row)
        self.table.setItem(row, 0, QTableWidgetItem(title))
        self.table.setItem(row, 1, QTableWidgetItem(detail))
    self.book_lists = lists","for x in books:
    tl = x.find('span', {'class', 'briefcitTitle'}).a
    title = tl.text
    link = tl['href']
    detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
    lists.append([title, detail, link])
    row = self.table.rowCount()
    self.table.insertRow(row)
    self.table.setItem(row, 0, QTableWidgetItem(title))
    self.table.setItem(row, 1, QTableWidgetItem(detail))","for i,x in enumerate(books):
    tl = x.find('span', {'class', 'briefcitTitle'}).a
    title = tl.text
    link = tl['href']
    detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
    lists.append([title, detail, link])
    row = self.table.rowCount()
    self.table.insertRow(row)
    self.table.setItem(row, 0, QTableWidgetItem(title))
    self.table.setItem(row, 1, QTableWidgetItem(detail))",1,,,,,,,,,,
deepdrive,https://github.com/deepdrive/deepdrive/tree/master/vendor/tensorflow/models/research/slim/datasets/download_and_convert_mnist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deepdrive/vendor/tensorflow/models/research/slim/datasets/download_and_convert_mnist.py,,"def _clean_up_temporary_files(dataset_dir):
    """"""Removes temporary files used to create the dataset.

  Args:
    dataset_dir: The directory where the temporary files are stored.
  """"""
    for filename in [_TRAIN_DATA_FILENAME, _TRAIN_LABELS_FILENAME, _TEST_DATA_FILENAME, _TEST_LABELS_FILENAME]:
        filepath = os.path.join(dataset_dir, filename)
        tf.gfile.Remove(filepath)","for filename in [_TRAIN_DATA_FILENAME, _TRAIN_LABELS_FILENAME, _TEST_DATA_FILENAME, _TEST_LABELS_FILENAME]:
    filepath = os.path.join(dataset_dir, filename)
    tf.gfile.Remove(filepath)","for i,filename in enumerate([_TRAIN_DATA_FILENAME, _TRAIN_LABELS_FILENAME, _TEST_DATA_FILENAME, _TEST_LABELS_FILENAME]):
    filepath = os.path.join(dataset_dir, filename)
    tf.gfile.Remove(filepath)",1,,,,,,,,,,
grover,https://github.com/rowanz/grover/tree/master/realnews/prepare_lm_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grover/realnews/prepare_lm_data.py,,"def _stream_from_buffer(buffer, current_desired_size, pad_token=0, add_articles_to_end=False):
    """""" Combines short articles that are in a buffer """"""
    random.shuffle(buffer)
    i = 0
    while i < len(buffer):
        article = buffer[i]
        if add_articles_to_end:
            for article2add in buffer[i + 1:]:
                i += 1
                article['input_ids'].append(encoder.padding)
                article['input_ids'].append(encoder.reset_context)
                article['input_ids'].extend(article2add['input_ids'])
                if len(article['input_ids']) >= current_desired_size:
                    article['input_ids'] = article['input_ids'][:current_desired_size]
                    break
        amount_to_pad = current_desired_size - len(article['input_ids'])
        article['input_ids'].extend([pad_token] * amount_to_pad)
        article['sub_index'] = 0
        yield article
        i += 1","for article2add in buffer[i + 1:]:
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break","for j, article2add in enumerate(buffer[i + 1:]):
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break",1,,,,,,,,,,
heamy,https://github.com/rushter/heamy/tree/master/heamy/pipeline.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/heamy/heamy/pipeline.py,ModelsPipeline,"def stack(self, k=5, stratify=False, shuffle=True, seed=100, full_test=True, add_diff=False):
    """"""Stacks sequence of models.

        Parameters
        ----------

        k : int, default 5
            Number of folds.
        stratify : bool, default False
        shuffle : bool, default True
        seed : int, default 100
        full_test : bool, default True
            If True then evaluate test dataset on the full data otherwise take the mean of every fold.
        add_diff : bool, default False

        Returns
        -------
        `DataFrame`

        Examples
        --------
        >>> pipeline = ModelsPipeline(model_rf,model_lr)
        >>> stack_ds = pipeline.stack(k=10, seed=111)
        """"""
    result_train = []
    result_test = []
    y = None
    for model in self.models:
        result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
        train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
        test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
        result_train.append(train_df)
        result_test.append(test_df)
        if y is None:
            y = result.y_train
    result_train = pd.concat(result_train, axis=1)
    result_test = pd.concat(result_test, axis=1)
    if add_diff:
        result_train = feature_combiner(result_train)
        result_test = feature_combiner(result_test)
    ds = Dataset(X_train=result_train, y_train=y, X_test=result_test)
    return ds","for model in self.models:
    result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
    train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
    test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
    result_train.append(train_df)
    result_test.append(test_df)
    if y is None:
        y = result.y_train","for i,model in enumerate(self.models):
    result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
    train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
    test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
    result_train.append(train_df)
    result_test.append(test_df)
    if y is None:
        y = result.y_train",1,,,,,,,,,,
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/database/mssql/mssql_db.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/database/mssql/mssql_db.py,,"def db_import(conn, cursor, module, db, target):
    if os.path.isfile(target):
        backup = open(target, 'r')
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
        return (0, 'import successful', '')
    else:
        return (1, 'cannot find target file', 'cannot find target file')","for line in backup:
    if line is None:
        break
    elif line.startswith('GO'):
        cursor.execute(sqlQuery)
        sqlQuery = 'USE [%s]\n' % db
    else:
        sqlQuery += line","for i,line in enumerate(backup):
    if line is None:
        break
    elif line.startswith('GO'):
        cursor.execute(sqlQuery)
        sqlQuery = 'USE [%s]\n' % db
    else:
        sqlQuery += line",1,,,,,,,,,,
binaryalert,https://github.com/airbnb/binaryalert/tree/master/rules/clone_rules.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/binaryalert/rules/clone_rules.py,,"def _clone_repo(url: str, include: Optional[List[str]], exclude: Optional[List[str]]) -> int:
    """"""Clone the given repo and copy only the YARA files from the specified paths.

    Returns:
        Number of files copied.
    """"""
    cloned_repo_root = os.path.join(tempfile.gettempdir(), os.path.basename(url))
    if os.path.exists(cloned_repo_root):
        shutil.rmtree(cloned_repo_root)
    subprocess.check_call(['git', 'clone', '--quiet', '--depth', '1', url, cloned_repo_root])
    if '//' in url:
        target_repo_root = os.path.join(RULES_DIR, url.split('//')[1])
    else:
        target_repo_root = os.path.join(RULES_DIR, url.split('@')[1].replace(':', '/', 1))
    if os.path.exists(target_repo_root):
        shutil.rmtree(target_repo_root)
    files_copied = 0
    for relative_path in _files_to_copy(cloned_repo_root, include, exclude):
        os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
        src = os.path.join(cloned_repo_root, relative_path)
        dst = os.path.join(target_repo_root, relative_path)
        shutil.copy(src, dst)
        files_copied += 1
    shutil.rmtree(cloned_repo_root)
    return files_copied","for relative_path in _files_to_copy(cloned_repo_root, include, exclude):
    os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
    src = os.path.join(cloned_repo_root, relative_path)
    dst = os.path.join(target_repo_root, relative_path)
    shutil.copy(src, dst)
    files_copied += 1","for i, relative_path in enumerate(_files_to_copy(cloned_repo_root, include, exclude)):
    os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
    src = os.path.join(cloned_repo_root, relative_path)
    dst = os.path.join(target_repo_root, relative_path)
    shutil.copy(src, dst)
    files_copied += 1",1,,,,,,,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","for ci in qs.iterator():
    if ci.position:
        try:
            ia = ci.position.order.invoice_address
        except InvoiceAddress.DoesNotExist:
            ia = InvoiceAddress()
    yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","for i, ci in enumerate(qs.iterator()):
    if ci.position:
        try:
            ia = ci.position.order.invoice_address
        except InvoiceAddress.DoesNotExist:
            ia = InvoiceAddress()
    yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",1,,,,,,,,,,
agents,https://github.com/tensorflow/agents/tree/master/tf_agents/eval/metric_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/agents/tf_agents/eval/metric_utils.py,,"def compute(metrics, environment, policy, num_episodes=1):
    """"""Compute metrics using `policy` on the `environment`.

  Args:
    metrics: List of metrics to compute.
    environment: py_environment instance.
    policy: py_policy instance used to step the environment. A tf_policy can be
      used in_eager_mode.
    num_episodes: Number of episodes to compute the metrics over.

  Returns:
    A dictionary of results {metric_name: metric_value}
  """"""
    for metric in metrics:
        metric.reset()
    time_step = environment.reset()
    policy_state = policy.get_initial_state(environment.batch_size)
    driver = py_driver.PyDriver(environment, policy, observers=metrics, max_steps=None, max_episodes=num_episodes)
    driver.run(time_step, policy_state)
    results = [(metric.name, metric.result()) for metric in metrics]
    return collections.OrderedDict(results)","for metric in metrics:
    metric.reset()","for i, metric in enumerate(metrics):
    metric.reset()",1,,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/utils/addrwidth.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/utils/addrwidth.py,,"def gen_addrs():
    for (block_type, top_bottom, cfg_row, cfg_col, frame_count) in bitstream.gen_part_base_addrs():
        yield (bitstream.addr_bits2word(block_type, top_bottom, cfg_row, cfg_col, 0), frame_count)","for (block_type, top_bottom, cfg_row, cfg_col, frame_count) in bitstream.gen_part_base_addrs():
    yield (bitstream.addr_bits2word(block_type, top_bottom, cfg_row, cfg_col, 0), frame_count)","for i,(block_type, top_bottom, cfg_row, cfg_col, frame_count) in enumerate(bitstream.gen_part_base_addrs()):
    yield (bitstream.addr_bits2word(block_type, top_bottom, cfg_row, cfg_col, 0), frame_count)",1,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])","for i in range(RUN_STEP):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)","for i in enumerate(range(RUN_STEP)):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)",1,,,,,,,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (name, cloud) in clouds.items():
    for (resource_name, resource) in cloud.resources.items():
        issues = issues.union(check_deprecations(resource))
        for (fname, f) in resource.filter_registry.items():
            if fname in ('and', 'or', 'not'):
                continue
            issues = issues.union(check_deprecations(f))
        for (aname, a) in resource.action_registry.items():
            issues = issues.union(check_deprecations(a))","for i, (name, cloud) in enumerate(clouds.items()):
    for (resource_name, resource) in cloud.resources.items():
        issues = issues.union(check_deprecations(resource))
        for (fname, f) in resource.filter_registry.items():
            if fname in ('and', 'or', 'not'):
                continue
            issues = issues.union(check_deprecations(f))
        for (aname, a) in resource.action_registry.items():
            issues = issues.union(check_deprecations(a))",1,,,,,,,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for dep in getattr(source, 'deprecations', ()):
    when = dep.removed_after
    if when is not None:
        name = f'{source.__module__}.{source.__name__}'
        if not isinstance(when, str):
            issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
            continue
        try:
            datetime.strptime(when, '%Y-%m-%d')
        except ValueError:
            issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")","for i, dep in enumerate(getattr(source, 'deprecations', ())):
    when = dep.removed_after
    if when is not None:
        name = f'{source.__module__}.{source.__name__}'
        if not isinstance(when, str):
            issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
            continue
        try:
            datetime.strptime(when, '%Y-%m-%d')
        except ValueError:
            issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")",1,,,,,,,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (resource_name, resource) in cloud.resources.items():
    issues = issues.union(check_deprecations(resource))
    for (fname, f) in resource.filter_registry.items():
        if fname in ('and', 'or', 'not'):
            continue
        issues = issues.union(check_deprecations(f))
    for (aname, a) in resource.action_registry.items():
        issues = issues.union(check_deprecations(a))","for i, (resource_name, resource) in enumerate(cloud.resources.items()):
    issues = issues.union(check_deprecations(resource))
    for j, (fname, f) in enumerate(resource.filter_registry.items()):
        if fname in ('and', 'or', 'not'):
            continue
        issues = issues.union(check_deprecations(f))
    for k, (aname, a) in enumerate(resource.action_registry.items()):
        issues = issues.union(check_deprecations(a))",1,,,,,,,,,,
2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement,https://github.com/Mingtzge/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/tree/master/recognize_process/tools/test_crnn_jmz.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/recognize_process/tools/test_crnn_jmz.py,,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict_path=None):
    """"""
    listhttps://github.com/bai-shang/crnn_ctc_ocr.Tensorflow
    :param sparse_matrix:
    :param char_map_dict_path:
    :return:
    """"""
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    char_map_dict = json.load(open(char_map_dict_path, 'r', encoding='utf-8'))
    if char_map_dict is None:
        print('error')
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","for row in dense_matrix:
    string = []
    for val in row:
        string.append(_int_to_string(val, char_map_dict))
    string_list.append(''.join((s for s in string if s != '*')))","for i,row in enumerate(dense_matrix):
    string = []
    for val in row:
        string.append(_int_to_string(val, char_map_dict))
    string_list.append(''.join((s for s in string if s != '*')))",1,,,,,,,,,,
transformers,https://github.com/huggingface/transformers/tree/master/tests/test_modeling_tf_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/tests/test_modeling_tf_common.py,TFModelTesterMixin,"def test_numpy_arrays_inputs(self):
    (config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()

    def prepare_numpy_arrays(inputs_dict):
        inputs_np_dict = {}
        for (k, v) in inputs_dict.items():
            if tf.is_tensor(v):
                inputs_np_dict[k] = v.numpy()
            else:
                inputs_np_dict[k] = np.array(k)
        return inputs_np_dict
    for model_class in self.all_model_classes:
        model = model_class(config)
        inputs = self._prepare_for_class(inputs_dict, model_class)
        inputs_np = prepare_numpy_arrays(inputs)
        output_for_dict_input = model(inputs_np)
        output_for_kw_input = model(**inputs_np)
        self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for model_class in self.all_model_classes:
    model = model_class(config)
    inputs = self._prepare_for_class(inputs_dict, model_class)
    inputs_np = prepare_numpy_arrays(inputs)
    output_for_dict_input = model(inputs_np)
    output_for_kw_input = model(**inputs_np)
    self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for i,model_class in enumerate(self.all_model_classes):
    model = model_class(config)
    inputs = self._prepare_for_class(inputs_dict, model_class)
    inputs_np = prepare_numpy_arrays(inputs)
    output_for_dict_input = model(inputs_np)
    output_for_kw_input = model(**inputs_np)
    self.assert_outputs_same(output_for_dict_input, output_for_kw_input)",1,,,,,,,,,,
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return","for row in fdf.itertuples():
    if block_start <= row.timestamp < block_end:
        readblock.append(row.file)
        continue
    if readblock or (schema.type == 'record' and block_start in state.poller_periods):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
    if wr_polling_period and readblock:
        state.poller_periods.add(block_start)
    if readblock:
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    block_start = block_end
    block_end = block_start + state.period
    readblock = []
    if schema.type != 'record':
        block_start = compute_block_start(row.timestamp)
        block_end = block_start + state.period
        if row.timestamp > block_end or block_end > utcnow:
            break
        readblock = [row.file]
        continue
    while row.timestamp > block_end:
        if block_start in state.poller_periods:
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        block_start = block_end
        block_end = block_start + state.period
    if block_end > utcnow:
        break
    readblock = [row.file]","for i,row in enumerate(fdf.itertuples()):
    if block_start <= row.timestamp < block_end:
        readblock.append(row.file)
        continue
    if readblock or (schema.type == 'record' and block_start in state.poller_periods):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
    if wr_polling_period and readblock:
        state.poller_periods.add(block_start)
    if readblock:
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    block_start = block_end
    block_end = block_start + state.period
    readblock = []
    if schema.type != 'record':
        block_start = compute_block_start(row.timestamp)
        block_end = block_start + state.period
        if row.timestamp > block_end or block_end > utcnow:
            break
        readblock = [row.file]
        continue
    while row.timestamp > block_end:
        if block_start in state.poller_periods:
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        block_start = block_end
        block_end = block_start + state.period
    if block_end > utcnow:
        break
    readblock = [row.file]",1,,,,,,,,,,
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return","for interval in polled_periods:
    if not fdf.empty and block_end < interval:
        break
    pre_block_start = compute_block_start(interval)
    pre_block_end = pre_block_start + state.period
    write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)","for i,interval in enumerate(polled_periods):
    if not fdf.empty and block_end < interval:
        break
    pre_block_start = compute_block_start(interval)
    pre_block_end = pre_block_start + state.period
    write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)",1,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/tests/components/recorder/test_init.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/recorder/test_init.py,,"def test_entity_id_filter(hass_recorder):
    """"""Test that entity ID filtering filters string and list.""""""
    hass = hass_recorder({'include': {'domains': 'hello'}, 'exclude': {'domains': 'hidden_domain'}})
    for (idx, data) in enumerate(({}, {'entity_id': 'hello.world'}, {'entity_id': ['hello.world']}, {'entity_id': ['hello.world', 'hidden_domain.person']}, {'entity_id': {'unexpected': 'data'}})):
        hass.bus.fire('hello', data)
        wait_recording_done(hass)
        with session_scope(hass=hass) as session:
            db_events = list(session.query(Events).filter_by(event_type='hello'))
            assert len(db_events) == idx + 1, data
    for data in ({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']}):
        hass.bus.fire('hello', data)
        wait_recording_done(hass)
        with session_scope(hass=hass) as session:
            db_events = list(session.query(Events).filter_by(event_type='hello'))
            assert len(db_events) == idx + 1, data","for data in ({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']}):
    hass.bus.fire('hello', data)
    wait_recording_done(hass)
    with session_scope(hass=hass) as session:
        db_events = list(session.query(Events).filter_by(event_type='hello'))
        assert len(db_events) == idx + 1, data","for idx, data in enumerate(({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']})):
    hass.bus.fire('hello', data)
    wait_recording_done(hass)
    with session_scope(hass=hass) as session:
        db_events = list(session.query(Events).filter_by(event_type='hello'))
        assert len(db_events) == idx + 1, data",1,,,,,,,,,,
Invertible-Image-Rescaling,https://github.com/pkuxmq/Invertible-Image-Rescaling/tree/master/codes/models/modules/discriminator_vgg_arch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Invertible-Image-Rescaling/codes/models/modules/discriminator_vgg_arch.py,VGGFeatureExtractor,"def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, device=torch.device('cpu')):
    super(VGGFeatureExtractor, self).__init__()
    self.use_input_norm = use_input_norm
    if use_bn:
        model = torchvision.models.vgg19_bn(pretrained=True)
    else:
        model = torchvision.models.vgg19(pretrained=True)
    if self.use_input_norm:
        mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
        self.register_buffer('mean', mean)
        self.register_buffer('std', std)
    self.features = nn.Sequential(*list(model.features.children())[:feature_layer + 1])
    for (k, v) in self.features.named_parameters():
        v.requires_grad = False","for (k, v) in self.features.named_parameters():
    v.requires_grad = False","for i, (k, v) in enumerate(self.features.named_parameters()):
    v.requires_grad = False",1,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for doc_span_index in range(len(spans)):
    for j in range(spans[doc_span_index]['paragraph_len']):
        is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
        index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
        spans[doc_span_index]['token_is_max_context'][index] = is_max_context","for doc_span_index, span in enumerate(spans):
    for j in range(span['paragraph_len']):
        is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
        index = j if tokenizer.padding_side == 'left' else span['truncated_query_with_special_tokens_length'] + j
        span['token_is_max_context'][index] = is_max_context",1,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for span in spans:
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))","for i, span in enumerate(spans):
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))",1,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for sub_token in sub_tokens:
    tok_to_orig_index.append(i)
    all_doc_tokens.append(sub_token)","for i, sub_token in enumerate(sub_tokens):
    tok_to_orig_index.append(i)
    all_doc_tokens.append(sub_token)",1,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for i in range(paragraph_len):
    index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
    token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]","for i in range(paragraph_len):
    index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
    j = len(spans) * doc_stride + i
    token_to_orig_map[index] = tok_to_orig_index[j]",1,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for j in range(spans[doc_span_index]['paragraph_len']):
    is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
    index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
    spans[doc_span_index]['token_is_max_context'][index] = is_max_context","for j, _ in enumerate(range(spans[doc_span_index]['paragraph_len'])):
    is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
    index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
    spans[doc_span_index]['token_is_max_context'][index] = is_max_context",1,,,,,,,,,,
routersploit,https://github.com/threat9/routersploit/tree/master/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/routersploit/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,Exploit,"def run(self):
    creds = []
    print_status('Requesting {}'.format(self.get_target_url()))
    response = self.http_request(method='GET', path='/password.cgi')
    if response is None:
        print_error('Exploit failed - empty response')
        return
    tokens = [('admin', ""pwdAdmin = '(.+?)'""), ('support', ""pwdSupport = '(.+?)'""), ('user', ""pwdUser = '(.+?)'"")]
    print_status('Trying to extract credentials')
    for token in tokens:
        res = re.findall(token[1], response.text)
        if res:
            creds.append((token[0], res[0]))
    if creds:
        print_success('Credentials found')
        print_table(('Login', 'Password'), *creds)
    else:
        print_error('Exploit failed - credentials could not be found')","for token in tokens:
    res = re.findall(token[1], response.text)
    if res:
        creds.append((token[0], res[0]))","for i, token in enumerate(tokens):
    res = re.findall(token[1], response.text)
    if res:
        creds.append((token[0], res[0]))",1,,,,,,,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for epoch in range(hp.epochs):
    dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
    pbar = tqdm(dataloader)
    for (i, data) in enumerate(pbar):
        pbar.set_description('Processing at epoch %d' % epoch)
        global_step += 1
        if global_step < 400000:
            adjust_learning_rate(optimizer, global_step)
        (character, mel, mel_input, pos_text, pos_mel, _) = data
        stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
        character = character.cuda()
        mel = mel.cuda()
        mel_input = mel_input.cuda()
        pos_text = pos_text.cuda()
        pos_mel = pos_mel.cuda()
        (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
        mel_loss = nn.L1Loss()(mel_pred, mel)
        post_mel_loss = nn.L1Loss()(postnet_pred, mel)
        loss = mel_loss + post_mel_loss
        writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
        writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
        if global_step % hp.image_step == 1:
            for (i, prob) in enumerate(attn_probs):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_enc):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_dec):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        optimizer.step()
        if global_step % hp.save_step == 0:
            t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for epoch in range(hp.epochs):
    dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
    pbar = tqdm(dataloader)
    for (i, data) in enumerate(pbar):
        pbar.set_description('Processing at epoch %d' % epoch)
        global_step += 1
        if global_step < 400000:
            adjust_learning_rate(optimizer, global_step)
        (character, mel, mel_input, pos_text, pos_mel, _) = data
        stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
        character = character.cuda()
        mel = mel.cuda()
        mel_input = mel_input.cuda()
        pos_text = pos_text.cuda()
        pos_mel = pos_mel.cuda()
        (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
        mel_loss = nn.L1Loss()(mel_pred, mel)
        post_mel_loss = nn.L1Loss()(postnet_pred, mel)
        loss = mel_loss + post_mel_loss
        writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
        writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
        if global_step % hp.image_step == 1:
            for (i, prob) in enumerate(attn_probs):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_enc):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_dec):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        optimizer.step()
        if global_step % hp.save_step == 0:
            t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",1,,,,,,,,,,
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/modeling_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/modeling_utils.py,,"def find_pruneable_heads_and_indices(heads: List[int], n_heads: int, head_size: int, already_pruned_heads: Set[int]) -> Tuple[Set[int], torch.LongTensor]:
    """"""
    Finds the heads and their indices taking :obj:`already_pruned_heads` into account.

    Args:
        heads (:obj:`List[int]`): List of the indices of heads to prune.
        n_heads (:obj:`int`): The number of heads in the model.
        head_size (:obj:`int`): The size of each head.
        already_pruned_heads (:obj:`Set[int]`): A set of already pruned heads.

    Returns:
        :obj:`Tuple[Set[int], torch.LongTensor]`: A tuple with the remaining heads and their corresponding indices.
    """"""
    mask = torch.ones(n_heads, head_size)
    heads = set(heads) - already_pruned_heads
    for head in heads:
        head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
        mask[head] = 0
    mask = mask.view(-1).contiguous().eq(1)
    index: torch.LongTensor = torch.arange(len(mask))[mask].long()
    return (heads, index)","for head in heads:
    head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
    mask[head] = 0","for i, head in enumerate(heads):
    head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
    mask[head] = 0",1,,,,,,,,,,
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/tests/utils/test_auto_resumer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/tests/utils/test_auto_resumer.py,,"def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    loaded_cfg = json.load(open(args_path))
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)","for k in cfg_dict:
    assert cfg_dict[k] == loaded_cfg[k]","for i,k in enumerate(cfg_dict):
    assert cfg_dict[k] == loaded_cfg[k]",1,,,,,,,,,,
plaso,https://github.com/log2timeline/plaso/tree/master/plaso/parsers/winreg_plugins/usb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plaso/plaso/parsers/winreg_plugins/usb.py,USBPlugin,"def ExtractEvents(self, parser_mediator, registry_key, **kwargs):
    """"""Extracts events from a Windows Registry key.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfVFS.
      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.
    """"""
    event_data = WindowsUSBDeviceEventData()
    event_data.key_path = registry_key.path
    for subkey in registry_key.GetSubkeys():
        event_data.subkey_name = subkey.name
        vendor_identification = None
        product_identification = None
        try:
            subkey_name_parts = subkey.name.split('&')
            if len(subkey_name_parts) >= 2:
                vendor_identification = subkey_name_parts[0]
                product_identification = subkey_name_parts[1]
        except ValueError as exception:
            logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
        event_data.vendor = vendor_identification
        event_data.product = product_identification
        for devicekey in subkey.GetSubkeys():
            event_data.last_written_time = devicekey.last_written_time
            event_data.serial = devicekey.name
            parser_mediator.ProduceEventData(event_data)","for subkey in registry_key.GetSubkeys():
    event_data.subkey_name = subkey.name
    vendor_identification = None
    product_identification = None
    try:
        subkey_name_parts = subkey.name.split('&')
        if len(subkey_name_parts) >= 2:
            vendor_identification = subkey_name_parts[0]
            product_identification = subkey_name_parts[1]
    except ValueError as exception:
        logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
    event_data.vendor = vendor_identification
    event_data.product = product_identification
    for devicekey in subkey.GetSubkeys():
        event_data.last_written_time = devicekey.last_written_time
        event_data.serial = devicekey.name
        parser_mediator.ProduceEventData(event_data)","for i, subkey in enumerate(registry_key.GetSubkeys()):
    event_data.subkey_name = subkey.name
    vendor_identification = None
    product_identification = None
    try:
        subkey_name_parts = subkey.name.split('&')
        if len(subkey_name_parts) >= 2:
            vendor_identification = subkey_name_parts[0]
            product_identification = subkey_name_parts[1]
    except ValueError as exception:
        logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
    event_data.vendor = vendor_identification
    event_data.product = product_identification
    for j, devicekey in enumerate(subkey.GetSubkeys()):
        event_data.last_written_time = devicekey.last_written_time
        event_data.serial = devicekey.name
        parser_mediator.ProduceEventData(event_data)",1,,,,,,,,,,
plaso,https://github.com/log2timeline/plaso/tree/master/plaso/parsers/winreg_plugins/usb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plaso/plaso/parsers/winreg_plugins/usb.py,USBPlugin,"def ExtractEvents(self, parser_mediator, registry_key, **kwargs):
    """"""Extracts events from a Windows Registry key.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfVFS.
      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.
    """"""
    event_data = WindowsUSBDeviceEventData()
    event_data.key_path = registry_key.path
    for subkey in registry_key.GetSubkeys():
        event_data.subkey_name = subkey.name
        vendor_identification = None
        product_identification = None
        try:
            subkey_name_parts = subkey.name.split('&')
            if len(subkey_name_parts) >= 2:
                vendor_identification = subkey_name_parts[0]
                product_identification = subkey_name_parts[1]
        except ValueError as exception:
            logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
        event_data.vendor = vendor_identification
        event_data.product = product_identification
        for devicekey in subkey.GetSubkeys():
            event_data.last_written_time = devicekey.last_written_time
            event_data.serial = devicekey.name
            parser_mediator.ProduceEventData(event_data)","for devicekey in subkey.GetSubkeys():
    event_data.last_written_time = devicekey.last_written_time
    event_data.serial = devicekey.name
    parser_mediator.ProduceEventData(event_data)","for i, devicekey in enumerate(subkey.GetSubkeys()):
    event_data.last_written_time = devicekey.last_written_time
    event_data.serial = devicekey.name
    parser_mediator.ProduceEventData(event_data)",1,,,,,,,,,,
SDV,https://github.com/sdv-dev/SDV/tree/master/sdv/timeseries/deepecho.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV/sdv/timeseries/deepecho.py,DeepEchoModel,"def _fit(self, timeseries_data):
    self._model = self._build_model()
    if self._sequence_index:
        timeseries_data = timeseries_data.rename(columns={self._sequence_index + '.value': self._sequence_index})
    self._output_columns = list(timeseries_data.columns)
    self._data_columns = [column for column in timeseries_data.columns if column not in self._entity_columns + self._context_columns]
    sequences = assemble_sequences(timeseries_data, self._entity_columns, self._context_columns, self._segment_size, self._sequence_index, drop_sequence_index=False)
    data_types = list()
    context_types = list()
    for field in self._output_columns:
        dtype = timeseries_data[field].dtype
        kind = dtype.kind
        if kind in ('i', 'f'):
            data_type = 'continuous'
        elif kind in ('O', 'b'):
            data_type = 'categorical'
        else:
            raise ValueError(f'Unsupported dtype {dtype}')
        if field in self._data_columns:
            data_types.append(data_type)
        elif field in self._context_columns:
            context_types.append(data_type)
    if self._sequence_index:
        self._transform_sequence_index(sequences)
        data_types.append('continuous')
    self._model.fit_sequences(sequences, context_types, data_types)","for field in self._output_columns:
    dtype = timeseries_data[field].dtype
    kind = dtype.kind
    if kind in ('i', 'f'):
        data_type = 'continuous'
    elif kind in ('O', 'b'):
        data_type = 'categorical'
    else:
        raise ValueError(f'Unsupported dtype {dtype}')
    if field in self._data_columns:
        data_types.append(data_type)
    elif field in self._context_columns:
        context_types.append(data_type)","for i, field in enumerate(self._output_columns):
    dtype = timeseries_data[field].dtype
    kind = dtype.kind
    if kind in ('i', 'f'):
        data_type = 'continuous'
    elif kind in ('O', 'b'):
        data_type = 'categorical'
    else:
        raise ValueError(f'Unsupported dtype {dtype}')
    if field in self._data_columns:
        data_types.append(data_type)
    elif field in self._context_columns:
        context_types.append(data_type)",1,,,,,,,,,,
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","for token in tokens:
    if len(token.strip()) == 0:
        return f'Invalid market. Ticker {token} has an invalid length.'
    if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
        return f'Invalid market. Ticker {token} contains invalid characters.'
    if token in tokens_list:
        return f'Duplicate market {token}.'
    tokens_list.append(token)","for i, token in enumerate(tokens):
    if len(token.strip()) == 0:
        return f'Invalid market. Ticker {token} has an invalid length.'
    if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
        return f'Invalid market. Ticker {token} contains invalid characters.'
    if token in tokens_list:
        return f'Duplicate market {token}.'
    tokens_list.append(token)",1,,,,,,,,,,
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)","for collider in collrates:
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table","for i,collider in enumerate(collrates):
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table",1,,,,,,,,,,
DeepRobust,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/defense/gcn_preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRobust/deeprobust/graph/defense/gcn_preprocess.py,,"def dropedge_both(A, iA, jA, features, threshold1=2.5, threshold2=0.01):
    removed_cnt = 0
    for row in range(len(iA) - 1):
        for i in range(iA[row], iA[row + 1]):
            n1 = row
            n2 = jA[i]
            C1 = np.linalg.norm(features[n1] - features[n2])
            (a, b) = (features[n1], features[n2])
            inner_product = (a * b).sum()
            C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
            if C1 > threshold1 or threshold2 < 0:
                A[i] = 0
                removed_cnt += 1
    return removed_cnt","for row in range(len(iA) - 1):
    for i in range(iA[row], iA[row + 1]):
        n1 = row
        n2 = jA[i]
        C1 = np.linalg.norm(features[n1] - features[n2])
        (a, b) = (features[n1], features[n2])
        inner_product = (a * b).sum()
        C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
        if C1 > threshold1 or threshold2 < 0:
            A[i] = 0
            removed_cnt += 1","for row, val in enumerate(iA[:-1]):
    for i in range(val, iA[row + 1]):
        n1 = row
        n2 = jA[i]
        C1 = np.linalg.norm(features[n1] - features[n2])
        (a, b) = (features[n1], features[n2])
        inner_product = (a * b).sum()
        C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
        if C1 > threshold1 or threshold2 < 0:
            A[i] = 0
            removed_cnt += 1",1,,,,,,,,,,
DeepRobust,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/defense/gcn_preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRobust/deeprobust/graph/defense/gcn_preprocess.py,,"def dropedge_both(A, iA, jA, features, threshold1=2.5, threshold2=0.01):
    removed_cnt = 0
    for row in range(len(iA) - 1):
        for i in range(iA[row], iA[row + 1]):
            n1 = row
            n2 = jA[i]
            C1 = np.linalg.norm(features[n1] - features[n2])
            (a, b) = (features[n1], features[n2])
            inner_product = (a * b).sum()
            C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
            if C1 > threshold1 or threshold2 < 0:
                A[i] = 0
                removed_cnt += 1
    return removed_cnt","for i in range(iA[row], iA[row + 1]):
    n1 = row
    n2 = jA[i]
    C1 = np.linalg.norm(features[n1] - features[n2])
    (a, b) = (features[n1], features[n2])
    inner_product = (a * b).sum()
    C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
    if C1 > threshold1 or threshold2 < 0:
        A[i] = 0
        removed_cnt += 1","for i in range(iA[row], iA[row + 1]):
    n1 = row
    n2 = jA[i]
    C1 = np.linalg.norm(features[n1] - features[n2])
    (a, b) = (features[n1], features[n2])
    inner_product = (a * b).sum()
    C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
    if C1 > threshold1 or threshold2 < 0:
        A[i] = 0
        removed_cnt += 1",1,,,,,,,,,,
projects,https://github.com/explosion/projects/tree/master/tutorials/ner_fashion_brands/scripts/visualize_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/projects/tutorials/ner_fashion_brands/scripts/visualize_data.py,,"def load_data(filepath):
    examples = list(srsly.read_jsonl(filepath))
    rows = []
    n_total_ents = 0
    n_no_ents = 0
    labels = set()
    for eg in examples:
        row = {'text': eg['text'], 'ents': eg.get('spans', [])}
        n_total_ents += len(row['ents'])
        if not row['ents']:
            n_no_ents += 1
        labels.update([span['label'] for span in row['ents']])
        rows.append(row)
    return (rows, labels, n_total_ents, n_no_ents)","for eg in examples:
    row = {'text': eg['text'], 'ents': eg.get('spans', [])}
    n_total_ents += len(row['ents'])
    if not row['ents']:
        n_no_ents += 1
    labels.update([span['label'] for span in row['ents']])
    rows.append(row)","for i,eg in enumerate(examples):
    row = {'text': eg['text'], 'ents': eg.get('spans', [])}
    n_total_ents += len(row['ents'])
    if not row['ents']:
        n_no_ents += 1
    labels.update([span['label'] for span in row['ents']])
    rows.append(row)",1,,,,,,,,,,
tfc,https://github.com/maqp/tfc/tree/master/tests/common/test_crypto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tfc/tests/common/test_crypto.py,TestArgon2KDF,"def test_argon2_cffi_using_the_official_command_line_utility(self) -> None:
    min_password_length = 1
    max_password_length = 127
    min_salt_length = 8
    min_parallelism = 1
    max_parallelism = multiprocessing.cpu_count()
    min_time_cost = 1
    min_memory_cost = 7
    min_key_length = 4
    max_salt_length = 128
    max_time_cost = 3
    max_memory_cost = 15
    max_key_length = 64
    sys_rand = random.SystemRandom()
    for _ in range(self.number_of_tests):
        len_password = sys_rand.randint(min_password_length, max_password_length)
        len_salt = sys_rand.randint(min_salt_length, max_salt_length)
        parallelism = sys_rand.randint(min_parallelism, max_parallelism)
        time_cost = sys_rand.randint(min_time_cost, max_time_cost)
        memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
        key_length = sys_rand.randint(min_key_length, max_key_length)
        password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
        salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
        output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
        key_test_vector = output.split('\n')[4].split('\t')[-1]
        purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
        self.assertEqual(purported_key, key_test_vector)","for _ in range(self.number_of_tests):
    len_password = sys_rand.randint(min_password_length, max_password_length)
    len_salt = sys_rand.randint(min_salt_length, max_salt_length)
    parallelism = sys_rand.randint(min_parallelism, max_parallelism)
    time_cost = sys_rand.randint(min_time_cost, max_time_cost)
    memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
    key_length = sys_rand.randint(min_key_length, max_key_length)
    password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
    salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
    output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
    key_test_vector = output.split('\n')[4].split('\t')[-1]
    purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
    self.assertEqual(purported_key, key_test_vector)","for i in range(self.number_of_tests):
    len_password = sys_rand.randint(min_password_length, max_password_length)
    len_salt = sys_rand.randint(min_salt_length, max_salt_length)
    parallelism = sys_rand.randint(min_parallelism, max_parallelism)
    time_cost = sys_rand.randint(min_time_cost, max_time_cost)
    memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
    key_length = sys_rand.randint(min_key_length, max_key_length)
    password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
    salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
    output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
    key_test_vector = output.split('\n')[4].split('\t')[-1]
    purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
    self.assertEqual(purported_key, key_test_vector)",1,,,,,,,,,,
