scipy,https://github.com/scipy/scipy/tree/master/scipy/optimize/_trustregion_constr/tests/test_projections.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/optimize/_trustregion_constr/tests/test_projections.py,TestProjections,"def test_compare_dense_and_sparse2(self):
    D1 = np.diag([-1.7, 1, 0.5])
    D2 = np.diag([1, -0.6, -0.3])
    D3 = np.diag([-0.3, -1.5, 2])
    A = np.hstack([D1, D2, D3])
    A_sparse = csc_matrix(A)
    np.random.seed(0)
    (Z, LS, Y) = projections(A)
    (Z_sparse, LS_sparse, Y_sparse) = projections(A_sparse)
    for k in range(1):
        z = np.random.normal(size=(9,))
        assert_array_almost_equal(Z.dot(z), Z_sparse.dot(z))
        assert_array_almost_equal(LS.dot(z), LS_sparse.dot(z))
        x = np.random.normal(size=(3,))
        assert_array_almost_equal(Y.dot(x), Y_sparse.dot(x))","for k in range(1):
    z = np.random.normal(size=(9,))
    assert_array_almost_equal(Z.dot(z), Z_sparse.dot(z))
    assert_array_almost_equal(LS.dot(z), LS_sparse.dot(z))
    x = np.random.normal(size=(3,))
    assert_array_almost_equal(Y.dot(x), Y_sparse.dot(x))",Answer: No,,,,,,,,,,,
scikit-kge,https://github.com/mnick/scikit-kge/tree/master/skge/sample.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-kge/skge/sample.py,RandomSampler,"def _sample(self, x, mode):
    res = None
    for _ in range(self.ntries):
        nex = (randint(self.sz[0]), randint(self.sz[0]), randint(self.sz[1]))
        if nex not in self.xs:
            res = (nex, -1.0)
            break
    return res","for _ in range(self.ntries):
    nex = (randint(self.sz[0]), randint(self.sz[0]), randint(self.sz[1]))
    if nex not in self.xs:
        res = (nex, -1.0)
        break",Answer: No,,,,,,,,,,,
2s-AGCN,https://github.com/lshiwjx/2s-AGCN/tree/master/model/aagcn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/2s-AGCN/model/aagcn.py,unit_gcn,"def __init__(self, in_channels, out_channels, A, coff_embedding=4, num_subset=3, adaptive=True, attention=True):
    super(unit_gcn, self).__init__()
    inter_channels = out_channels // coff_embedding
    self.inter_c = inter_channels
    self.out_c = out_channels
    self.in_c = in_channels
    self.num_subset = num_subset
    num_jpts = A.shape[-1]
    self.conv_d = nn.ModuleList()
    for i in range(self.num_subset):
        self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))
    if adaptive:
        self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)))
        self.alpha = nn.Parameter(torch.zeros(1))
        self.conv_a = nn.ModuleList()
        self.conv_b = nn.ModuleList()
        for i in range(self.num_subset):
            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))
            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))
    else:
        self.A = Variable(torch.from_numpy(A.astype(np.float32)), requires_grad=False)
    self.adaptive = adaptive
    if attention:
        self.conv_ta = nn.Conv1d(out_channels, 1, 9, padding=4)
        nn.init.constant_(self.conv_ta.weight, 0)
        nn.init.constant_(self.conv_ta.bias, 0)
        ker_jpt = num_jpts - 1 if not num_jpts % 2 else num_jpts
        pad = (ker_jpt - 1) // 2
        self.conv_sa = nn.Conv1d(out_channels, 1, ker_jpt, padding=pad)
        nn.init.xavier_normal_(self.conv_sa.weight)
        nn.init.constant_(self.conv_sa.bias, 0)
        rr = 2
        self.fc1c = nn.Linear(out_channels, out_channels // rr)
        self.fc2c = nn.Linear(out_channels // rr, out_channels)
        nn.init.kaiming_normal_(self.fc1c.weight)
        nn.init.constant_(self.fc1c.bias, 0)
        nn.init.constant_(self.fc2c.weight, 0)
        nn.init.constant_(self.fc2c.bias, 0)
    self.attention = attention
    if in_channels != out_channels:
        self.down = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1), nn.BatchNorm2d(out_channels))
    else:
        self.down = lambda x: x
    self.bn = nn.BatchNorm2d(out_channels)
    self.soft = nn.Softmax(-2)
    self.tan = nn.Tanh()
    self.sigmoid = nn.Sigmoid()
    self.relu = nn.ReLU(inplace=True)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            conv_init(m)
        elif isinstance(m, nn.BatchNorm2d):
            bn_init(m, 1)
    bn_init(self.bn, 1e-06)
    for i in range(self.num_subset):
        conv_branch_init(self.conv_d[i], self.num_subset)","for i in range(self.num_subset):
    self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))
    self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))","Answer: No. 

Explanation: The code cannot be refactored with `enumerate` because it is using the `range` function to iterate over a range of numbers, not a collection of items. The `enumerate` function is used to iterate over a collection of items while also keeping track of the index of each item.",,,,,,,,,,,
PythonLinearNonlinearControl,https://github.com/Shunichi09/PythonLinearNonlinearControl/tree/master/tests/configs/test_cartpole.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonLinearNonlinearControl/tests/configs/test_cartpole.py,TestGradient,"def test_input_gradient(self):
    """"""
        """"""
    us = np.ones((1, 1))
    cost_grad = CartPoleConfigModule.gradient_cost_fn_input(None, us)
    eps = 0.0001
    expected_grad = np.zeros((1, 1))
    for i in range(1):
        tmp_u = us.copy()
        tmp_u[0, i] = us[0, i] + eps
        forward = CartPoleConfigModule.input_cost_fn(tmp_u)
        tmp_u = us.copy()
        tmp_u[0, i] = us[0, i] - eps
        backward = CartPoleConfigModule.input_cost_fn(tmp_u)
        expected_grad[0, i] = (forward - backward) / (2.0 * eps)
    assert cost_grad == pytest.approx(expected_grad)","for i in range(1):
    tmp_u = us.copy()
    tmp_u[0, i] = us[0, i] + eps
    forward = CartPoleConfigModule.input_cost_fn(tmp_u)
    tmp_u = us.copy()
    tmp_u[0, i] = us[0, i] - eps
    backward = CartPoleConfigModule.input_cost_fn(tmp_u)
    expected_grad[0, i] = (forward - backward) / (2.0 * eps)","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is already using a for loop with range to iterate over a single value (0). Using enumerate would not add any value to the code.",,,,,,,,,,,
athena,https://github.com/athena-team/athena/tree/master/athena/solver.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/athena/athena/solver.py,DecoderSolver,"def inference_saved_model(self, dataset_builder, rank_size=1, conf=None):
    """""" decode the model """"""
    if dataset_builder is None:
        return
    dataset = dataset_builder.as_dataset(batch_size=conf.batch_size)
    metric = CharactorAccuracy(rank_size=rank_size)
    st = time.time()
    if self.hparams.predict_path is None or self.hparams.label_path is None:
        logging.warning('Please set inference_config.predict_path and inference_config.label_path!')
        self.hparams.predict_path = 'inference.result'
        self.hparams.label_path = 'inference.label'
    if self.lm_model is not None:
        self.lm_model.text_featurizer = dataset_builder.text_featurizer
    if not os.path.exists(os.path.dirname(self.hparams.predict_path)):
        os.mkdir(os.path.dirname(self.hparams.predict_path))
    with open(self.hparams.predict_path, 'w') as predict_file_out:
        with open(self.hparams.label_path, 'w') as label_file_out:
            for (_, samples) in enumerate(dataset):
                begin = time.time()
                predictions = self.model.deploy_function(samples['input'])
                predict_txt_list = dataset_builder.text_featurizer.decode_to_list(predictions.numpy().tolist(), [dataset_builder.num_class, dataset_builder.num_class])
                for (predict, utt_id) in zip(predict_txt_list, samples['utt_id']):
                    predict_file_out.write(' '.join(predict) + '({utt_id})\n'.format(utt_id=utt_id.numpy().decode()))
                label_txt_list = dataset_builder.text_featurizer.decode_to_list(samples['output'].numpy().tolist(), [dataset_builder.num_class, dataset_builder.num_class])
                for (label, utt_id) in zip(label_txt_list, samples['utt_id']):
                    label_file_out.write(' '.join(label) + '({utt_id})\n'.format(utt_id=utt_id.numpy().decode()))
                (predict_file_out.flush(), label_file_out.flush())
                predictions = tf.cast(predictions, tf.int64)
                (validated_preds, _) = validate_seqs(predictions, dataset_builder.num_class)
                validated_preds = tf.cast(validated_preds, tf.int64)
                (num_errs, _) = metric.update_state(validated_preds, samples)
                reports = 'predictions: %s\tlabels: %s\terrs: %d\tavg_acc: %.4f\tsec/iter: %.4f' % (predictions, samples['output'].numpy(), num_errs, metric.result(), time.time() - begin)
                logging.info(reports)
            ed = time.time()
            logging.info('decoding finished, cost %.4f s' % (ed - st))","for (label, utt_id) in zip(label_txt_list, samples['utt_id']):
    label_file_out.write(' '.join(label) + '({utt_id})\n'.format(utt_id=utt_id.numpy().decode()))","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is already using the zip function to iterate over two lists in parallel. Using enumerate would not be appropriate in this case.",,,,,,,,,,,
conan,https://github.com/conan-io/conan/tree/master/conans/model/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan/conans/model/options.py,Options,"def propagate_upstream(self, down_package_values, down_ref, own_ref):
    """""" used to propagate from downstream the options to the upper requirements
        :param: down_package_values => {""*"": PackageOptionValues({""shared"": ""True""})}
        :param: down_ref
        :param: own_ref: Reference of the current package => ConanFileReference
        """"""
    if not down_package_values:
        return
    assert isinstance(down_package_values, dict)
    option_values = PackageOptionValues()
    for (package_pattern, package_option_values) in sorted(down_package_values.items()):
        if own_ref.name != package_pattern and fnmatch.fnmatch(own_ref.name, package_pattern):
            option_values.update(package_option_values)
    pattern_options = list(option_values.keys())
    down_options = down_package_values.get(own_ref.name)
    if down_options is not None:
        option_values.update(down_options)
    self._package_options.propagate_upstream(option_values, down_ref, own_ref, pattern_options=pattern_options)
    for (name, option_values) in sorted(list(down_package_values.items())):
        if name != own_ref.name:
            pkg_values = self._deps_package_values.setdefault(name, PackageOptionValues())
            pkg_values.propagate_upstream(option_values, down_ref, own_ref, name)","for (package_pattern, package_option_values) in sorted(down_package_values.items()):
    if own_ref.name != package_pattern and fnmatch.fnmatch(own_ref.name, package_pattern):
        option_values.update(package_option_values)","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is already using tuple unpacking to iterate over the items of the dictionary. Using enumerate would not make the code more readable or efficient in this case.",,,,,,,,,,,
django,https://github.com/django/django/tree/master/django/contrib/postgres/forms/hstore.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/django/contrib/postgres/forms/hstore.py,HStoreField,"def to_python(self, value):
    if not value:
        return {}
    if not isinstance(value, dict):
        try:
            value = json.loads(value)
        except json.JSONDecodeError:
            raise ValidationError(self.error_messages['invalid_json'], code='invalid_json')
    if not isinstance(value, dict):
        raise ValidationError(self.error_messages['invalid_format'], code='invalid_format')
    for (key, val) in value.items():
        if val is not None:
            val = str(val)
        value[key] = val
    return value","for (key, val) in value.items():
    if val is not None:
        val = str(val)
    value[key] = val","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is iterating over the key-value pairs of a dictionary using the `.items()` method. Using enumerate would not be appropriate in this case.",,,,,,,,,,,
pony,https://github.com/ponyorm/pony/tree/master/pony/orm/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pony/pony/orm/core.py,,"def has_perm(user, perm, x):
    if isinstance(x, EntityMeta):
        entity = x
    elif isinstance(x, Entity):
        entity = x.__class__
    elif isinstance(x, Attribute):
        if x.hidden:
            return False
        entity = x.entity
    else:
        throw(TypeError, ""The third parameter of 'has_perm' function should be entity class, entity instance or attribute. Got: %r"" % x)
    access_rules = entity._access_rules_.get(perm)
    if not access_rules:
        return False
    cache = entity._database_._get_cache()
    perm_cache = cache.perm_cache[user][perm]
    result = perm_cache.get(x)
    if result is not None:
        return result
    user_groups = get_user_groups(user)
    result = False
    if isinstance(x, EntityMeta):
        for rule in access_rules:
            if user_groups.issuperset(rule.groups) and entity not in rule.entities_to_exclude:
                result = True
                break
    elif isinstance(x, Attribute):
        attr = x
        for rule in access_rules:
            if user_groups.issuperset(rule.groups) and entity not in rule.entities_to_exclude and (attr not in rule.attrs_to_exclude):
                result = True
                break
            reverse = attr.reverse
            if reverse:
                reverse_rules = reverse.entity._access_rules_.get(perm)
                if not reverse_rules:
                    return False
                for reverse_rule in access_rules:
                    if user_groups.issuperset(reverse_rule.groups) and reverse.entity not in reverse_rule.entities_to_exclude and (reverse not in reverse_rule.attrs_to_exclude):
                        result = True
                        break
                if result:
                    break
    else:
        obj = x
        user_roles = get_user_roles(user, obj)
        obj_labels = get_object_labels(obj)
        for rule in access_rules:
            if x in rule.entities_to_exclude:
                continue
            elif not user_groups.issuperset(rule.groups):
                pass
            elif not user_roles.issuperset(rule.roles):
                pass
            elif not obj_labels.issuperset(rule.labels):
                pass
            else:
                result = True
                break
    perm_cache[perm] = result
    return result","for reverse_rule in access_rules:
    if user_groups.issuperset(reverse_rule.groups) and reverse.entity not in reverse_rule.entities_to_exclude and (reverse not in reverse_rule.attrs_to_exclude):
        result = True
        break","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is using a for loop to iterate over a set of access rules and checking conditions for each rule.",,,,,,,,,,,
docassemble,https://github.com/jhpyle/docassemble/tree/master/tests/features/steps/docassemble.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/tests/features/steps/docassemble.py,,"def click_button(step, button_name):
    try:
        world.browser.find_element_by_id('daMainQuestion').click()
    except:
        pass
    do_wait()
    success = False
    try:
        world.browser.find_element_by_xpath('//button[text()=""' + button_name + '""]').click()
        success = True
    except:
        pass
    if not success:
        for elem in world.browser.find_elements_by_xpath('//a[text()=""' + button_name + '""]'):
            try:
                elem.click()
                success = True
            except:
                pass
            if success:
                break
    if not success:
        try:
            world.browser.find_element_by_xpath('//button/span[text()=""' + button_name + '""]').click()
            success = True
        except:
            pass
    assert success
    world.browser.wait_for_it()","for elem in world.browser.find_elements_by_xpath('//a[text()=""' + button_name + '""]'):
    try:
        elem.click()
        success = True
    except:
        pass
    if success:
        break","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is using the `find_elements_by_xpath` method to find a specific element on a webpage and then performing an action on it.",,,,,,,,,,,
nucypher,https://github.com/nucypher/nucypher/tree/master/tests/integration/conftest.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nucypher/tests/integration/conftest.py,,"def successful_mock_keyfile_reader(_keystore, path):
    assert MOCK_KEYSTORE_PATH in path
    full_path = path
    del path
    for (filename, account) in mock_accounts.items():
        if filename in full_path:
            break
    else:
        raise FileNotFoundError(f'No such file {full_path}')
    return (account.address, dict(version=3, address=account.address))","for (filename, account) in mock_accounts.items():
    if filename in full_path:
        break
else:
    raise FileNotFoundError(f'No such file {full_path}')","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is not iterating over a sequence of elements. It is iterating over a dictionary using the `.items()` method which returns a sequence of tuples containing the key-value pairs of the dictionary. Using enumerate in this case would not make sense.",,,,,,,,,,,
dynaconf,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/ruamel/yaml/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dynaconf/dynaconf/vendor_src/ruamel/yaml/main.py,,"def dump_all(documents, stream=None, Dumper=Dumper, default_style=None, default_flow_style=None, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding=enc, explicit_start=None, explicit_end=None, version=None, tags=None, block_seq_indent=None, top_level_colon_align=None, prefix_colon=None):
    """"""
    Serialize a sequence of Python objects into a YAML stream.
    If stream is None, return the produced string instead.
    """"""
    getvalue = None
    if top_level_colon_align is True:
        top_level_colon_align = max([len(str(x)) for x in documents[0]])
    if stream is None:
        if encoding is None:
            stream = StringIO()
        else:
            stream = BytesIO()
        getvalue = stream.getvalue
    dumper = Dumper(stream, default_style=default_style, default_flow_style=default_flow_style, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break, encoding=encoding, explicit_start=explicit_start, explicit_end=explicit_end, version=version, tags=tags, block_seq_indent=block_seq_indent, top_level_colon_align=top_level_colon_align, prefix_colon=prefix_colon)
    try:
        dumper._serializer.open()
        for data in documents:
            try:
                dumper._representer.represent(data)
            except AttributeError:
                raise
        dumper._serializer.close()
    finally:
        try:
            dumper._emitter.dispose()
        except AttributeError:
            raise
            dumper.dispose()
    if getvalue is not None:
        return getvalue()
    return None","for data in documents:
    try:
        dumper._representer.represent(data)
    except AttributeError:
        raise","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is not iterating over a sequence of elements. It is simply iterating over the elements of the `documents` list.",,,,,,,,,,,
LibCST,https://github.com/Instagram/LibCST/tree/master/libcst/codemod/visitors/_gather_exports.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LibCST/libcst/codemod/visitors/_gather_exports.py,GatherExportsVisitor,"def visit_Assign(self, node: cst.Assign) -> bool:
    for target_node in node.targets:
        if self._handle_assign_target(target_node.target, node.value):
            return True
    return False","for target_node in node.targets:
    if self._handle_assign_target(target_node.target, node.value):
        return True","Answer: No. 

Explanation: The code cannot be refactored with enumerate as it is not iterating over a sequence or collection. It is simply iterating over the targets attribute of the node object.",,,,,,,,,,,
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ctci-solutions/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,,"def show(placement):
    parts = ['\n+-----------------+\n']
    for row in xrange(8):
        parts.append('| ')
        for col in xrange(8):
            bit = 1 << row * 8 + col
            if bit & placement:
                parts.append('Q ')
            else:
                parts.append('  ')
        parts.append('|\n')
    parts.append('+-----------------+\n')
    return ''.join(parts)","for col in xrange(8):
    bit = 1 << row * 8 + col
    if bit & placement:
        parts.append('Q ')
    else:
        parts.append('  ')","Answer: No. 

Explanation: The code is already using the `xrange` function to iterate over a range of integers, which is the most efficient way to do so in Python 2. Using `enumerate` would not be appropriate in this case, as there is no need to keep track of the index of each iteration.",,,,,,,,,,,
subfinder,https://github.com/ausaki/subfinder/tree/master/subfinder/subfinder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/subfinder/subfinder/subfinder.py,SubFinder,"def _download(self, videofile):
    """"""调用 SubSearcher 搜索并下载字幕""""""
    basename = os.path.basename(videofile)
    subinfos = []
    for subsearcher_cls in self.subsearcher:
        subsearcher = subsearcher_cls(self, api_urls=self.api_urls)
        self.logger.info('{0}：开始使用 {1} 搜索字幕'.format(basename, subsearcher))
        try:
            subinfos = subsearcher.search_subs(videofile, self.languages, self.exts, self.keyword)
        except Exception as e:
            err = str(e)
            if self.debug:
                err = traceback.format_exc()
            self.logger.error('{}：搜索字幕发生错误： {}'.format(basename, err))
            continue
        if subinfos:
            break
    self.logger.info('{1}：找到 {0} 个字幕, 准备下载'.format(len(subinfos), basename))
    for subinfo in subinfos:
        if isinstance(subinfo['subname'], (list, tuple)):
            self._history[videofile].extend(subinfo['subname'])
        else:
            self._history[videofile].append(subinfo['subname'])","for subinfo in subinfos:
    if isinstance(subinfo['subname'], (list, tuple)):
        self._history[videofile].extend(subinfo['subname'])
    else:
        self._history[videofile].append(subinfo['subname'])","Answer: No. 

Explanation: The code is iterating over a list of dictionaries and checking if the value of the key 'subname' is a list or tuple. If it is, it extends the list of subtitles for a given video file in the history dictionary. If it is not, it appends the subtitle to the list of subtitles for the given video file in the history dictionary. Using enumerate would not be appropriate in this case as we are not iterating over a list of indices, but rather over a list of dictionaries.",,,,,,,,,,,
PyWebIO,https://github.com/pywebio/PyWebIO/tree/master/pywebio/platform/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyWebIO/pywebio/platform/utils.py,,"def config(*, title=None, description=None, theme=None, js_code=None, js_file=[], css_style=None, css_file=[]):
    '''PyWebIO application configuration

    :param str title: Application title
    :param str description: Application description
    :param str theme: Application theme. Available themes are: ``dark``, ``sketchy``, ``minty``, ``yeti``.
        You can also use environment variable ``PYWEBIO_THEME`` to specify the theme (with high priority).

        :demo_host:`Theme preview demo </theme>`

        .. collapse:: Open Source Credits

            The dark theme is modified from ForEvolve's `bootstrap-dark <https://github.com/ForEvolve/bootstrap-dark>`_.
            The sketchy, minty and yeti theme are from `bootswatch <https://bootswatch.com/4/>`_.

    :param str js_code: The javascript code that you want to inject to page.
    :param str/list js_file: The javascript files that inject to page, can be a URL in str or a list of it.
    :param str css_style: The CSS style that you want to inject to page.
    :param str/list css_file: The CSS files that inject to page, can be a URL in str or a list of it.

    ``config()`` can be used in 2 ways: direct call and decorator.
    If you call ``config()`` directly, the configuration will be global.
    If you use ``config()`` as decorator, the configuration will only work on single PyWebIO application function.
    ::

        config(title=""My application"")

        @config(css_style=""* { color:red }"")
        def app():
            put_text(""hello PyWebIO"")

    ``title`` and ``description`` are used for SEO, which are provided when indexed by search engines.
    If no ``title`` and ``description`` set for a PyWebIO application function,
    the `docstring <https://www.python.org/dev/peps/pep-0257/>`_ of the function will be used as title and description by default::

        def app():
            """"""Application title

            Application description...
            (A empty line is used to separate the description and title)
            """"""
            pass

    The above code is equal to::

        @config(title=""Application title"", description=""Application description..."")
        def app():
            pass

    .. versionadded:: 1.4

    .. versionchanged:: 1.5
       add ``theme`` parameter
    '''
    if isinstance(js_file, str):
        js_file = [js_file]
    if isinstance(css_file, str):
        css_file = [css_file]
    configs = locals()

    class Decorator:

        def __init__(self):
            self.called = False

        def __call__(self, func):
            self.called = True
            try:
                func = partial(func)
                for (key, val) in configs.items():
                    if val:
                        setattr(func, '_pywebio_%s' % key, val)
            except Exception:
                pass
            return func

        def __del__(self):
            if self.called:
                return
            global _global_config
            _global_config = configs
    return Decorator()","for (key, val) in configs.items():
    if val:
        setattr(func, '_pywebio_%s' % key, val)","Answer: No. 

Explanation: The given code is already using the `.items()` method to iterate over the dictionary `configs` and unpacking the key-value pairs into `(key, val)`. Using `enumerate` is not necessary here as we are not iterating over a sequence of values.",,,,,,,,,,,
vispy,https://github.com/vispy/vispy/tree/master/examples/demo/visuals/wiggly_bar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vispy/examples/demo/visuals/wiggly_bar.py,,"def make_spring(num_points=300, num_turns=4, height=12, radius=2.0, xnot=None, ynot=None, znot=None):
    """"""
        Generate a list of points corresponding to a spring.

        Parameters
        ----------
        num_points : int
            Number of points to map spring over. More points means a rounder 
            spring.
        num_turns : int
            Number of coils in the spring
        height : float
            The height of the spring. Keep it in whatever units the rest of the 
            spring is in.
        radius : float
            The radius of the coils. The spring will end up being
            2*radius wide.
        xnot : float
            Initial x-coordinate for the spring coordinates to start at.
        ynot : float
            Initial y-coordinate for the spring coordinates to start at.
        znot : float
            Initial z-coordinate for the spring coordinates to start at.

        Returns
        -------
        coord_list: list of tuples
            Coordinate list of (x, y, z) positions for the spring

        Notes
        -----
        Right now, this assumes the center is at x=0, y=0. Later, it might be 
        good to add in stuff to change that.

        Right now, the length of the ""ends"" is 10% of the overall length, as 
        well as a small ""turn"" that is length radius / 2. In the future, maybe 
        there could be a kwarg to set the length of the sides of the spring. 
        For now, 10% looks good.

        """"""
    coords_list = []
    init_pts = num_points // 10
    znot = 0 if znot is None else znot
    xnot = 0 if xnot is None else xnot
    ynot = 0 if ynot is None else ynot
    coords_list.append((xnot, ynot, znot))
    for _ in range(init_pts):
        znot += height / num_points
        coords_list.append((xnot, ynot, znot))
    hold_z = znot
    for i in range(init_pts // 2):
        small_theta = (i + 1) * np.pi / init_pts
        xnot = radius / 2 * (1 - np.cos(small_theta))
        znot = hold_z + radius / 2 * np.sin(small_theta)
        coords_list.append((xnot, ynot, znot))
    coords_list += make_spiral(num_points=num_points - 3 * init_pts, num_turns=num_turns, height=height - 91 * height / num_points - radius / 2, radius=radius, xnot=xnot, ynot=ynot, znot=znot)
    hold_z = coords_list[-1][-1]
    for i in range(init_pts // 2):
        small_theta = np.pi / 2 - (i + 1) * np.pi / init_pts
        xnot = radius / 2 * (1 - np.cos(small_theta))
        znot = hold_z + radius / 2 * np.cos(small_theta)
        coords_list.append((xnot, ynot, znot))
    xnot = 0.0
    znot += height / num_points
    for _ in range(init_pts):
        znot += height / num_points
        coords_list.append((xnot, ynot, znot))
    coords_list.append((0, 0, height))
    return coords_list","for _ in range(init_pts):
    znot += height / num_points
    coords_list.append((xnot, ynot, znot))",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a collection of items.,,,,,,,,,,,
napari,https://github.com/napari/napari/tree/master/napari/_qt/_tests/test_threading_progress.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napari/napari/_qt/_tests/test_threading_progress.py,,"def test_unstarted_worker_no_widget(make_napari_viewer):
    viewer = make_napari_viewer()

    def func():
        for _ in range(5):
            yield
    thread_func = qthreading.thread_worker(func, progress={'total': 5}, start_thread=False)
    thread_func()
    assert not bool(viewer.window._qt_viewer.window()._activity_dialog.findChildren(QtLabeledProgressBar))","for _ in range(5):
    yield",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a collection or sequence.,,,,,,,,,,,
MuGo,https://github.com/brilee/MuGo/tree/master//go.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MuGo//go.py,LibertyTracker,"def _handle_captures(self, captured_stones):
    for s in captured_stones:
        for n in NEIGHBORS[s]:
            group_id = self.group_index[n]
            if group_id != MISSING_GROUP_ID:
                self._update_liberties(group_id, add={s})","for s in captured_stones:
    for n in NEIGHBORS[s]:
        group_id = self.group_index[n]
        if group_id != MISSING_GROUP_ID:
            self._update_liberties(group_id, add={s})",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a list or sequence.,,,,,,,,,,,
st2,https://github.com/StackStorm/st2/tree/master/st2common/tests/unit/test_crypto_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/st2/st2common/tests/unit/test_crypto_utils.py,CryptoUtilsKeyczarCompatibilityTestCase,"def test_symmetric_encrypt_decrypt_cryptography(self):
    key = AESKey.generate()
    plaintexts = ['a b c', 'ab', 'hello foo', 'hell', 'bar5hello hello bar bar hello', 'a', '', 'c']
    for plaintext in plaintexts:
        encrypted = cryptography_symmetric_encrypt(key, plaintext)
        decrypted = cryptography_symmetric_decrypt(key, encrypted)
        self.assertEqual(decrypted, plaintext)","for plaintext in plaintexts:
    encrypted = cryptography_symmetric_encrypt(key, plaintext)
    decrypted = cryptography_symmetric_decrypt(key, encrypted)
    self.assertEqual(decrypted, plaintext)",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a list or sequence.,,,,,,,,,,,
keyring,https://github.com/jaraco/keyring/tree/master/keyring/testing/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keyring/keyring/testing/util.py,ImportKiller,"def __enter__(self):
    self.original = {}
    for name in self.names:
        self.original[name] = sys.modules.pop(name, None)
    sys.meta_path.insert(0, self)","for name in self.names:
    self.original[name] = sys.modules.pop(name, None)",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements.,,,,,,,,,,,
maskrcnn-benchmark,https://github.com/facebookresearch/maskrcnn-benchmark/tree/master/maskrcnn_benchmark/modeling/backbone/resnet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maskrcnn-benchmark/maskrcnn_benchmark/modeling/backbone/resnet.py,ResNet,"def _freeze_backbone(self, freeze_at):
    if freeze_at < 0:
        return
    for stage_index in range(freeze_at):
        if stage_index == 0:
            m = self.stem
        else:
            m = getattr(self, 'layer' + str(stage_index))
        for p in m.parameters():
            p.requires_grad = False","for p in m.parameters():
    p.requires_grad = False",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements.,,,,,,,,,,,
kaggle_ndsb2017,https://github.com/juliandewit/kaggle_ndsb2017/tree/master//helpers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kaggle_ndsb2017//helpers.py,,"def get_segmented_lungs(im, plot=False):
    binary = im < -400
    cleared = clear_border(binary)
    label_image = label(cleared)
    areas = [r.area for r in regionprops(label_image)]
    areas.sort()
    if len(areas) > 2:
        for region in regionprops(label_image):
            if region.area < areas[-2]:
                for coordinates in region.coords:
                    label_image[coordinates[0], coordinates[1]] = 0
    binary = label_image > 0
    selem = disk(2)
    binary = binary_erosion(binary, selem)
    selem = disk(10)
    binary = binary_closing(binary, selem)
    edges = roberts(binary)
    binary = ndi.binary_fill_holes(edges)
    get_high_vals = binary == 0
    im[get_high_vals] = -2000
    return (im, binary)","for region in regionprops(label_image):
    if region.area < areas[-2]:
        for coordinates in region.coords:
            label_image[coordinates[0], coordinates[1]] = 0",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements.,,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/utils/win_system.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/win_system.py,,"def get_pending_component_servicing():
    """"""
    Determine whether there are pending Component Based Servicing tasks that
    require a reboot.

    If any the following registry keys exist then a reboot is pending:

    ``HKLM:\\\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing\\RebootPending``
    ``HKLM:\\\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing\\RebootInProgress``
    ``HKLM:\\\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing\\PackagesPending``

    .. versionadded:: 3001

    Returns:
        bool: ``True`` if there are pending Component Based Servicing tasks,
        otherwise ``False``

    CLI Example:

    .. code-block:: bash

        salt '*' system.get_pending_component_servicing
    """"""
    base_key = 'SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing'
    sub_keys = ('RebootPending', 'RebootInProgress', 'PackagesPending')
    for sub_key in sub_keys:
        key = '\\'.join((base_key, sub_key))
        if salt.utils.win_reg.key_exists(hive='HKLM', key=key):
            return True
    return False","for sub_key in sub_keys:
    key = '\\'.join((base_key, sub_key))
    if salt.utils.win_reg.key_exists(hive='HKLM', key=key):
        return True",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements.,,,,,,,,,,,
goatools,https://github.com/tanghaibao/goatools/tree/master/goatools/grouper/grprobj.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/goatools/goatools/grouper/grprobj.py,Grouper,"def get_usrgos_g_hdrgos(self, hdrgos):
    """"""Return usrgos under provided hdrgos.""""""
    usrgos_all = set()
    if isinstance(hdrgos, str):
        hdrgos = [hdrgos]
    for hdrgo in hdrgos:
        usrgos_cur = self.hdrgo2usrgos.get(hdrgo, None)
        if usrgos_cur is not None:
            usrgos_all |= usrgos_cur
        if hdrgo in self.hdrgo_is_usrgo:
            usrgos_all.add(hdrgo)
    return usrgos_all","for hdrgo in hdrgos:
    usrgos_cur = self.hdrgo2usrgos.get(hdrgo, None)
    if usrgos_cur is not None:
        usrgos_all |= usrgos_cur
    if hdrgo in self.hdrgo_is_usrgo:
        usrgos_all.add(hdrgo)",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements.,,,,,,,,,,,
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for pt in active_markers:
    self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements.,,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/modules/virt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/virt.py,,"def _diff_lists(old, new, comparator):
    """"""
    Compare lists to extract the changes

    :param old: old list
    :param new: new list
    :return: a dictionary with ``unchanged``, ``new``, ``deleted`` and ``sorted`` keys

    The sorted list is the union of unchanged and new lists, but keeping the original
    order from the new list.
    """"""

    def _remove_indent(node):
        """"""
        Remove the XML indentation to compare XML trees more easily
        """"""
        node_copy = copy.deepcopy(node)
        node_copy.text = None
        for item in node_copy.iter():
            item.tail = None
        return node_copy
    diff = {'unchanged': [], 'new': [], 'deleted': [], 'sorted': []}
    old_devices = copy.deepcopy(old)
    for new_item in new:
        found = [item for item in old_devices if comparator(_remove_indent(item), _remove_indent(new_item))]
        if found:
            old_devices.remove(found[0])
            diff['unchanged'].append(found[0])
            diff['sorted'].append(found[0])
        else:
            diff['new'].append(new_item)
            diff['sorted'].append(new_item)
    diff['deleted'] = old_devices
    return diff","for item in node_copy.iter():
    item.tail = None",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements. It is iterating over the elements of an XML tree using the `iter()` method.,,,,,,,,,,,
conan,https://github.com/conan-io/conan/tree/master/conans/client/conf/compiler_id.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan/conans/client/conf/compiler_id.py,,"def detect_compiler_id(executable, runner=None):
    runner = runner or ConanRunner()
    tmpdir = tempfile.mkdtemp()
    tmpname = os.path.join(tmpdir, 'temp.c')
    with open(tmpname, 'wb') as f:
        f.write(b'\n')
    cmd = os.path.join(tmpdir, 'file.cmd')
    with open(cmd, 'wb') as f:
        f.write(b'echo off\nset MSC_CMD_FLAGS\n')
    detectors = ['-dM -E -x c', '--driver-mode=g++ -dM -E -x c', '-c -xdumpmacros', '/nologo /E /B1 ""%s"" /c /TC' % cmd, '/QdM /E /TC-Wp,-dM -E -x c']
    try:
        for detector in detectors:
            command = '%s %s ""%s""' % (executable, detector, tmpname)
            result = StringIO()
            if 0 == runner(command, output=result):
                output = result.getvalue()
                defines = dict()
                for line in output.splitlines():
                    tokens = line.split(' ', 3)
                    if len(tokens) == 3 and tokens[0] == '#define':
                        defines[tokens[1]] = tokens[2]
                    elif line.startswith('MSC_CMD_FLAGS='):
                        line = line[len('MSC_CMD_FLAGS='):].rstrip()
                        defines = dict()
                        tokens = line.split()
                        for token in tokens:
                            if token.startswith('-D') or token.startswith('/D'):
                                token = token[2:]
                                if '=' in token:
                                    (name, value) = token.split('=', 2)
                                else:
                                    (name, value) = (token, '1')
                                defines[name] = value
                        break
                compiler = _parse_compiler_version(defines)
                if compiler == UNKNOWN_COMPILER:
                    continue
                return compiler
        return UNKNOWN_COMPILER
    finally:
        try:
            rmdir(tmpdir)
        except OSError:
            pass","for token in tokens:
    if token.startswith('-D') or token.startswith('/D'):
        token = token[2:]
        if '=' in token:
            (name, value) = token.split('=', 2)
        else:
            (name, value) = (token, '1')
        defines[name] = value",Answer: No. The code cannot be refactored with enumerate as it does not involve iterating over a sequence.,,,,,,,,,,,
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/mako/codegen.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/mako/codegen.py,_GenerateRenderMethod,"def visitExpression(self, node):
    if hasattr(node.code, 'names_used'):
        for x in node.code.names_used:
            self.compiler.names_used.add(x)
    if hasattr(node.code, 'names_set'):
        for x in node.code.names_set:
            self.compiler.names_set.add(x)
    self.printer.start_source(node.lineno)
    if len(node.escapes) or (self.compiler.pagetag is not None and len(self.compiler.pagetag.filter_args.args)) or len(self.compiler.default_filters):
        s = self.create_filter_callable(node.escapes_code.args, '%s' % node.text, True)
        self.printer.writeline('__M_writer(%s)' % s)
    else:
        self.printer.writeline('__M_writer(%s)' % node.text)","for x in node.code.names_set:
    self.compiler.names_set.add(x)",Answer: No. The code cannot be refactored with enumerate as it is already iterating over a set of names.,,,,,,,,,,,
awesome-semantic-segmentation-pytorch,https://github.com/Tramac/awesome-semantic-segmentation-pytorch/tree/master/core/utils/download.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/awesome-semantic-segmentation-pytorch/core/utils/download.py,,"def download(url, path=None, overwrite=False, sha1_hash=None):
    """"""Download an given URL
    Parameters
    ----------
    url : str
        URL to download
    path : str, optional
        Destination path to store downloaded file. By default stores to the
        current directory with same name as in url.
    overwrite : bool, optional
        Whether to overwrite destination file if already exists.
    sha1_hash : str, optional
        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified
        but doesn't match.
    Returns
    -------
    str
        The file path of the downloaded file.
    """"""
    if path is None:
        fname = url.split('/')[-1]
    else:
        path = os.path.expanduser(path)
        if os.path.isdir(path):
            fname = os.path.join(path, url.split('/')[-1])
        else:
            fname = path
    if overwrite or not os.path.exists(fname) or (sha1_hash and (not check_sha1(fname, sha1_hash))):
        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        print('Downloading %s from %s...' % (fname, url))
        r = requests.get(url, stream=True)
        if r.status_code != 200:
            raise RuntimeError('Failed downloading url %s' % url)
        total_length = r.headers.get('content-length')
        with open(fname, 'wb') as f:
            if total_length is None:
                for chunk in r.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            else:
                total_length = int(total_length)
                for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024.0 + 0.5), unit='KB', unit_scale=False, dynamic_ncols=True):
                    f.write(chunk)
        if sha1_hash and (not check_sha1(fname, sha1_hash)):
            raise UserWarning('File {} is downloaded but the content hash does not match. The repo may be outdated or download may be incomplete. If the ""repo_url"" is overridden, consider switching to the default repo.'.format(fname))
    return fname","for chunk in r.iter_content(chunk_size=1024):
    if chunk:
        f.write(chunk)",Answer: No. The code cannot be refactored with enumerate as it is already iterating over the content of the response in chunks of 1024 bytes.,,,,,,,,,,,
i3pystatus,https://github.com/enkore/i3pystatus/tree/master/i3pystatus/deluge.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/i3pystatus/i3pystatus/deluge.py,Deluge,"def parse_values(self, values):
    for (k, v) in values.items():
        if v:
            if k in ['total_upload', 'total_download', 'download_rate', 'upload_rate'] or k.endswith('_bytes'):
                values[k] = '{value:.{round}f}{unit}'.format(round=self.rounding, **bytes_info_dict(v))","for (k, v) in values.items():
    if v:
        if k in ['total_upload', 'total_download', 'download_rate', 'upload_rate'] or k.endswith('_bytes'):
            values[k] = '{value:.{round}f}{unit}'.format(round=self.rounding, **bytes_info_dict(v))",Answer: No. The code cannot be refactored with enumerate as it is already iterating over the key-value pairs of a dictionary using the `.items()` method.,,,,,,,,,,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tagstore/snuba/backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/tagstore/snuba/backend.py,SnubaTagStorage,"def __get_tag_keys_for_projects(self, projects, group, environments, start, end, limit=1000, keys=None, include_values_seen=True, use_cache=False, include_transactions=False, denylist=None, **kwargs):
    """"""Query snuba for tag keys based on projects

        When use_cache is passed, we'll attempt to use the cache. There's an exception if group_id was passed
        which refines the query enough caching isn't required.
        The cache key is based on the filters being passed so that different queries don't hit the same cache, with
        exceptions for start and end dates. Since even a microsecond passing would result in a different caching
        key, which means always missing the cache.
        Instead, to keep the cache key the same for a short period we append the duration, and the end time rounded
        with a certain jitter to the cache key.
        This jitter is based on the hash of the key before duration/end time is added for consistency per query.
        The jitter's intent is to avoid a dogpile effect of many queries being invalidated at the same time.
        This is done by changing the rounding of the end key to a random offset. See snuba.quantize_time for
        further explanation of how that is done.
        """"""
    (default_start, default_end) = default_start_end_dates()
    if start is None:
        start = default_start
    if end is None:
        end = default_end
    dataset = Dataset.Events
    if include_transactions:
        dataset = Dataset.Discover
    conditions = []
    aggregations = [['count()', '', 'count']]
    filters = {'project_id': sorted(projects)}
    if environments:
        filters['environment'] = sorted(environments)
    if group is not None:
        (dataset, conditions, filters) = self.apply_group_filters_conditions(group, conditions, filters)
    if keys is not None:
        filters['tags_key'] = sorted(keys)
    if include_values_seen:
        aggregations.append(['uniq', 'tags_value', 'values_seen'])
    should_cache = use_cache and group is None
    result = None
    cache_key = None
    if should_cache:
        filtering_strings = [f'{key}={value}' for (key, value) in filters.items()]
        filtering_strings.append(f'dataset={dataset.name}')
        cache_key = 'tagstore.__get_tag_keys:{}'.format(md5_text(*filtering_strings).hexdigest())
        key_hash = hash(cache_key)
        duration = (end - start).total_seconds()
        end = snuba.quantize_time(end, key_hash)
        cache_key += f':{duration}@{end.isoformat()}'
        result = cache.get(cache_key, None)
        if result is not None:
            metrics.incr('testing.tagstore.cache_tag_key.hit')
        else:
            metrics.incr('testing.tagstore.cache_tag_key.miss')
    if result is None:
        result = snuba.query(dataset=dataset, start=start, end=end, groupby=['tags_key'], conditions=conditions, filter_keys=filters, aggregations=aggregations, limit=limit, orderby='-count', referrer='tagstore.__get_tag_keys', **kwargs)
        if should_cache:
            cache.set(cache_key, result, 300)
            metrics.incr('testing.tagstore.cache_tag_key.len', amount=len(result))
    if group is None:
        ctor = TagKey
    else:
        ctor = functools.partial(GroupTagKey, group_id=group.id)
    results = set()
    for (key, data) in result.items():
        if denylist is not None and key in denylist:
            continue
        params = {'key': key}
        if include_values_seen:
            params['values_seen'] = data['values_seen']
            params['count'] = data['count']
        else:
            params['count'] = data
        results.add(ctor(**params))
    return results","for (key, data) in result.items():
    if denylist is not None and key in denylist:
        continue
    params = {'key': key}
    if include_values_seen:
        params['values_seen'] = data['values_seen']
        params['count'] = data['count']
    else:
        params['count'] = data
    results.add(ctor(**params))",Answer: No. The code cannot be refactored with enumerate as it is already iterating over the key-value pairs of a dictionary using the items() method.,,,,,,,,,,,
kedro,https://github.com/quantumblacklabs/kedro/tree/master/tests/extras/datasets/email/test_message_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kedro/tests/extras/datasets/email/test_message_dataset.py,TestEmailMessageDataSet,"def test_load_extra_params(self, message_data_set, load_args):
    """"""Test overriding the default load arguments.""""""
    for (key, value) in load_args.items():
        assert message_data_set._load_args[key] == value","for (key, value) in load_args.items():
    assert message_data_set._load_args[key] == value",Answer: No. The code cannot be refactored with enumerate as it is already iterating over the key-value pairs of the dictionary using the items() method.,,,,,,,,,,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tasks/collect_project_platforms.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/tasks/collect_project_platforms.py,,"def collect_project_platforms(paginate=1000, **kwargs):
    now = timezone.now()
    for page_of_project_ids in paginate_project_ids(paginate):
        queryset = Group.objects.using_replica().filter(last_seen__gte=now - timedelta(days=1), project_id__in=page_of_project_ids, platform__isnull=False).values_list('platform', 'project_id').distinct()
        for (platform, project_id) in queryset:
            platform = platform.lower()
            if platform not in VALID_PLATFORMS:
                continue
            ProjectPlatform.objects.create_or_update(project_id=project_id, platform=platform, values={'last_seen': now})
    ProjectPlatform.objects.filter(last_seen__lte=now - timedelta(days=90)).delete()","for (platform, project_id) in queryset:
    platform = platform.lower()
    if platform not in VALID_PLATFORMS:
        continue
    ProjectPlatform.objects.create_or_update(project_id=project_id, platform=platform, values={'last_seen': now})",Answer: No. The code cannot be refactored with enumerate as it is already unpacking the tuple into two variables (platform and project_id) in the for loop.,,,,,,,,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(1, c.shape[1]):
    f = f + c[:, k] * Pg ** k",Answer: No. The code cannot be refactored with enumerate as it is already using a range function to iterate over a sequence of numbers.,,,,,,,,,,,
django,https://github.com/django/django/tree/master/tests/test_client/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/tests/test_client/tests.py,ClientTest,"def test_follow_307_and_308_preserves_post_data(self):
    for code in (307, 308):
        with self.subTest(code=code):
            response = self.client.post('/redirect_view_%s/' % code, data={'value': 'test'}, follow=True)
            self.assertContains(response, 'test is the value')","for code in (307, 308):
    with self.subTest(code=code):
        response = self.client.post('/redirect_view_%s/' % code, data={'value': 'test'}, follow=True)
        self.assertContains(response, 'test is the value')",Answer: No. The code cannot be refactored with enumerate as it is already using a tuple to iterate over the values of 307 and 308.,,,,,,,,,,,
byte-of-python,https://github.com/swaroopch/byte-of-python/tree/master/programs/function_varargs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/byte-of-python/programs/function_varargs.py,,"def total(a=5, *numbers, **phonebook):
    print('a', a)
    for single_item in numbers:
        print('single_item', single_item)
    for (first_part, second_part) in phonebook.items():
        print(first_part, second_part)","for (first_part, second_part) in phonebook.items():
    print(first_part, second_part)",Answer: No. The code cannot be refactored with enumerate as it is already using the .items() method to iterate over the dictionary and unpacking the key-value pairs into the variables first_part and second_part.,,,,,,,,,,,
pgadmin4,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/dashboard/tests/test_dashboard_graphs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/dashboard/tests/test_dashboard_graphs.py,DashboardGraphsTestCase,"def runTest(self):
    self.server_id = parent_node_dict['server'][-1]['server_id']
    server_response = server_utils.connect_server(self, self.server_id)
    if server_response['info'] == 'Server connected.':
        url = self.getStatsUrl(self.server_id, self.did, ','.join(self.chart_data.keys()))
        response = self.tester.get(url)
        self.assertEqual(response.status_code, 200)
        resp_data = json.loads(response.data)
        self.assertEqual(len(resp_data.keys()), len(self.chart_data.keys()))
        for (chart_name, chart_vals) in self.chart_data.items():
            self.assertEqual(set(resp_data[chart_name].keys()), set(chart_vals))
    else:
        raise Exception('Error while connecting server to add the database.')","for (chart_name, chart_vals) in self.chart_data.items():
    self.assertEqual(set(resp_data[chart_name].keys()), set(chart_vals))",Answer: No. The code cannot be refactored with enumerate as it is already using the .items() method to iterate over the dictionary keys and values.,,,,,,,,,,,
videos,https://github.com/3b1b/videos/tree/master/_2017/nn/part1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/nn/part1.py,LayOutPlan,"def show_learning(self):
    word = self.words[0][1].copy()
    rect = SurroundingRectangle(word, color=YELLOW)
    self.network_mob.neuron_fill_color = YELLOW
    layer = self.network_mob.layers[-1]
    activation = np.zeros(len(layer.neurons))
    activation[1] = 1.0
    active_layer = self.network_mob.get_active_layer(-1, activation)
    word_group = VGroup(word, rect)
    word_group.generate_target()
    word_group.target.move_to(self.equation, LEFT)
    word_group.target[0].set_color(YELLOW)
    word_group.target[1].set_stroke(width=0)
    self.play(ShowCreation(rect))
    self.play(Transform(layer, active_layer), FadeOut(self.equation), MoveToTarget(word_group))
    for edge_group in reversed(self.network_mob.edge_groups):
        edge_group.generate_target()
        for edge in edge_group.target:
            edge.set_stroke(YELLOW, width=4 * np.random.random() ** 2)
        self.play(MoveToTarget(edge_group))
    self.wait()
    self.learning_word = word","for edge_group in reversed(self.network_mob.edge_groups):
    edge_group.generate_target()
    for edge in edge_group.target:
        edge.set_stroke(YELLOW, width=4 * np.random.random() ** 2)
    self.play(MoveToTarget(edge_group))",Answer: No. The code cannot be refactored with enumerate as it is already using the `reversed` function to iterate over the `self.network_mob.edge_groups` list in reverse order.,,,,,,,,,,,
sfepy,https://github.com/sfepy/sfepy/tree/master/sfepy/discrete/fem/mesh.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/discrete/fem/mesh.py,Mesh,"def from_region(region, mesh_in, localize=False, is_surface=False):
    """"""
        Create a mesh corresponding to cells, or, if `is_surface` is True, to
        facets, of a given region.
        """"""
    cmesh_in = mesh_in.cmesh
    if not is_surface:
        if not region.shape.n_cell:
            raise ValueError('region %s has no cells!' % region.name)
        cmesh = cmesh_in.create_new(region.cells, region.tdim, localize=localize)
    else:
        if not region.shape.n_facet:
            raise ValueError('region %s has no facets!' % region.name)
        cmesh = cmesh_in.create_new(region.facets, region.tdim - 1, localize=localize)
    mesh = Mesh(mesh_in.name + '_reg', cmesh=cmesh)
    if localize:
        remap = nm.empty(mesh_in.cmesh.n_coor, dtype=nm.int32)
        remap[:] = -1
        remap[region.vertices] = nm.arange(region.vertices.shape[0])
        mesh.nodal_bcs = {}
        for (key, val) in six.iteritems(mesh_in.nodal_bcs):
            new_val = remap[val]
            mesh.nodal_bcs[key] = new_val[new_val >= 0]
    else:
        mesh.nodal_bcs = mesh_in.nodal_bcs.copy()
    return mesh","for (key, val) in six.iteritems(mesh_in.nodal_bcs):
    new_val = remap[val]
    mesh.nodal_bcs[key] = new_val[new_val >= 0]",Answer: No. The code cannot be refactored with enumerate as it is already using the appropriate syntax for iterating over key-value pairs in a dictionary.,,,,,,,,,,,
causalnex,https://github.com/quantumblacklabs/causalnex/tree/master/causalnex/inference/inference.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/causalnex/causalnex/inference/inference.py,InferenceEngine,"def reset_do(self, observation: str):
    """"""
        Resets any do_interventions that have been applied to the observation.

        Args:
            observation: observation that will be reset.
        """"""
    self._cpds[observation] = self._cpds_original[observation]
    for (node, cpd) in self._detached_cpds.items():
        self._cpds[node] = cpd
    self._detached_cpds = {}
    self._generate_bbn()","for (node, cpd) in self._detached_cpds.items():
    self._cpds[node] = cpd","Answer: No. The code cannot be refactored with enumerate as it is already using the items() method to iterate over the dictionary and unpacking the key-value pairs into (node, cpd) variables.",,,,,,,,,,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/notifications/notifications/digest.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/notifications/notifications/digest.py,DigestNotification,"def send(self) -> None:
    shared_context = self.get_context()
    if should_send_as_alert_notification(shared_context):
        return send_as_alert_notification(shared_context, self.target_type, self.target_identifier)
    participants_by_provider_by_event = get_participants_by_event(self.digest, self.project, self.target_type, self.target_identifier)
    actor_ids = set()
    combined_participants_by_provider = defaultdict(set)
    for participants_by_provider in participants_by_provider_by_event.values():
        for (provider, participants) in participants_by_provider.items():
            for participant in participants:
                actor_ids.add(participant.actor_id)
                combined_participants_by_provider[provider].add(participant)
    if not actor_ids:
        return
    logger.info('mail.adapter.notify_digest', extra={'project_id': self.project.id, 'target_type': self.target_type.value, 'target_identifier': self.target_identifier, 'actor_ids': actor_ids})
    extra_context: Mapping[int, Mapping[str, Any]] = {}
    if should_get_personalized_digests(self.target_type, self.project.id):
        extra_context = self.get_extra_context(participants_by_provider_by_event)
    for (provider, participants) in combined_participants_by_provider.items():
        notify(provider, self, participants, shared_context, extra_context)","for (provider, participants) in combined_participants_by_provider.items():
    notify(provider, self, participants, shared_context, extra_context)",Answer: No. The code cannot be refactored with enumerate as it is already using the items() method to iterate over the dictionary and unpacking the key-value pairs into the variables provider and participants.,,,,,,,,,,,
transformers,https://github.com/huggingface/transformers/tree/master/tests/test_modeling_tf_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/tests/test_modeling_tf_common.py,TFModelTesterMixin,"def test_numpy_arrays_inputs(self):
    (config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()

    def prepare_numpy_arrays(inputs_dict):
        inputs_np_dict = {}
        for (k, v) in inputs_dict.items():
            if tf.is_tensor(v):
                inputs_np_dict[k] = v.numpy()
            else:
                inputs_np_dict[k] = np.array(k)
        return inputs_np_dict
    for model_class in self.all_model_classes:
        model = model_class(config)
        inputs = self._prepare_for_class(inputs_dict, model_class)
        inputs_np = prepare_numpy_arrays(inputs)
        output_for_dict_input = model(inputs_np)
        output_for_kw_input = model(**inputs_np)
        self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for (k, v) in inputs_dict.items():
    if tf.is_tensor(v):
        inputs_np_dict[k] = v.numpy()
    else:
        inputs_np_dict[k] = np.array(k)",Answer: No. The code cannot be refactored with enumerate as it is already using the items() method to iterate over the dictionary keys and values.,,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/vta/tests/python/unittest/test_vta_insn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/vta/tests/python/unittest/test_vta_insn.py,,"def _run(env, remote):

    def check_alu(tvm_op, np_op=None, use_imm=False, test_name=None):
        """"""Test ALU""""""
        m = 8
        n = 8
        imm = np.random.randint(1, 5)
        a = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='a', dtype=env.acc_dtype)
        a_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: a(*i), 'a_buf')
        if use_imm:
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), imm), 'res_buf')
        else:
            b = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='b', dtype=env.acc_dtype)
            b_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: b(*i), 'b_buf')
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), b_buf(*i)), 'res_buf')
        res = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: res_buf(*i).astype(env.inp_dtype), 'res')
        s = te.create_schedule(res.op)
        s[a_buf].set_scope(env.acc_scope)
        s[a_buf].pragma(a_buf.op.axis[0], env.dma_copy)
        s[res_buf].set_scope(env.acc_scope)
        s[res_buf].pragma(res_buf.op.axis[0], env.alu)
        s[res].pragma(res.op.axis[0], env.dma_copy)
        if not use_imm:
            s[b_buf].set_scope(env.acc_scope)
            s[b_buf].pragma(b_buf.op.axis[0], env.dma_copy)
        if not remote:
            return
        with vta.build_config():
            if use_imm:
                mod = vta.build(s, [a, res], tvm.target.Target('ext_dev', host=env.target_host))
            else:
                mod = vta.build(s, [a, b, res], tvm.target.Target('ext_dev', host=env.target_host))
        temp = utils.tempdir()
        mod.save(temp.relpath('load_act.o'))
        remote.upload(temp.relpath('load_act.o'))
        f = remote.load_module('load_act.o')
        dev = remote.ext_dev(0)
        a_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(a.dtype)
        if use_imm:
            res_np = np_op(a_np, imm) if np_op else tvm_op(a_np, imm)
        else:
            b_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(b.dtype)
            res_np = np_op(a_np, b_np) if np_op else tvm_op(a_np, b_np)
        res_np = res_np.astype(res.dtype)
        a_nd = tvm.nd.array(a_np, dev)
        res_nd = tvm.nd.array(np.zeros((m, n, env.BATCH, env.BLOCK_OUT)).astype(res.dtype), dev)
        if env.TARGET in ['sim', 'tsim']:
            simulator.clear_stats()
        if use_imm:
            f(a_nd, res_nd)
        else:
            b_nd = tvm.nd.array(b_np, dev)
            f(a_nd, b_nd, res_nd)
        np.testing.assert_equal(res_np, res_nd.numpy())
        if env.TARGET in ['sim', 'tsim']:
            sim_stats = simulator.stats()
            print('ALU {} execution statistics:'.format(test_name))
            for (k, v) in sim_stats.items():
                print('\t{:<16}: {:>16}'.format(k, v))
    check_alu(lambda x, y: x << y, np.left_shift, use_imm=True, test_name='SHL')
    check_alu(tvm.te.max, np.maximum, use_imm=True, test_name='MAX')
    check_alu(tvm.te.max, np.maximum, test_name='MAX')
    check_alu(lambda x, y: x + y, use_imm=True, test_name='ADD')
    check_alu(lambda x, y: x + y, test_name='ADD')
    check_alu(lambda x, y: x >> y, np.right_shift, use_imm=True, test_name='SHR')","for (k, v) in sim_stats.items():
    print('\t{:<16}: {:>16}'.format(k, v))",Answer: No. The code cannot be refactored with enumerate as it is already using the items() method to iterate over the dictionary keys and values.,,,,,,,,,,,
scikit-bio,https://github.com/biocore/scikit-bio/tree/master/skbio/io/format/tests/test_newick.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-bio/skbio/io/format/tests/test_newick.py,TestNewick,"def _assert_equal(self, n1, n2):

    def name(x):
        return (str(x.name), float(x.length) if x.length is not None else 0, len(x.children))
    self._assert_node_equal(n1, n2)
    for (c1, c2) in zip(sorted(n1.children, key=name), sorted(n2.children, key=name)):
        self.assertTrue(c1.parent is n1)
        self.assertTrue(c2.parent is n2)
        self._assert_equal(c1, c2)","for (c1, c2) in zip(sorted(n1.children, key=name), sorted(n2.children, key=name)):
    self.assertTrue(c1.parent is n1)
    self.assertTrue(c2.parent is n2)
    self._assert_equal(c1, c2)",Answer: No. The code cannot be refactored with enumerate as it is already using the zip function to iterate over two lists in parallel.,,,,,,,,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/core/layers_with_gpipe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/core/layers_with_gpipe.py,GPipeBatchMajorTransformerStack,"def __init__(self, params):
    p = params.Copy()
    num_layers = p.num_encoder_layers + p.num_decoder_layers
    if isinstance(p.splits, (list, tuple)):
        assert p.splits[-1] == num_layers
        for (i, j) in zip(p.splits[:-1], p.splits[1:]):
            assert i <= j, 'Splits must be in increasing order.'
    else:
        num_splits = p.splits
        layers_per_split = (num_layers - 1) // num_splits + 1
        p.splits = []
        for i in range(num_splits):
            p.splits.append((i + 1) * layers_per_split)
        p.splits[-1] = num_layers
    p.state_dtype = p.dtype
    if p.fprop_dtype:
        p.state_dtype = p.fprop_dtype
    transformers = []
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        p.emb_tpl.dropout_tpl = layers.DeterministicDropoutLayer.Params()
    p.emb_tpl.packed_input = p.packed_input
    p.emb_tpl.add_tgt_embedding_layer = p.num_decoder_layers > 0
    p.emb_tpl.name = 'emb'
    transformers.append(p.emb_tpl)
    if p.softmax_tpl:
        p.softmax_tpl.name = 'softmax'
        p.softmax_tpl.inputs_from_decoder = p.num_decoder_layers > 0
    for i in range(p.num_encoder_layers):
        params = p.encoder_tpl.Copy()
        params.name = 'encoder_%d' % i
        if i == p.num_encoder_layers - 1:
            params.output_layer_norm = True
        params.packed_input = p.packed_input
        if len(p.splits) > 1 or p.num_micro_batches > 1:
            params = params.cls.SetupDeterministicDropout(params)
        assert not params.has_aux_atten
        transformers.append(params)
    for i in range(p.num_decoder_layers):
        params = p.decoder_tpl.Copy()
        params.name = 'decoder_%d' % i
        params.mask_self_atten = True
        if i == p.num_decoder_layers - 1:
            params.output_layer_norm = True
        params.packed_input = p.packed_input
        if len(p.splits) > 1 or p.num_micro_batches > 1:
            params = params.cls.SetupDeterministicDropout(params)
        assert params.has_aux_atten
        transformers.append(params)
    cells = []
    cell_start = 0
    offset = 1
    for (split, cell_end) in enumerate(p.splits):
        sub = transformers[cell_start:cell_end + offset]
        if split == len(p.splits) - 1 and p.softmax_tpl:
            sub.append(p.softmax_tpl)
        cell = FeatureExtractionLayer.Params().Set(name='cell_{}'.format(split), sub=sub)
        cells.append(cell)
        cell_start = cell_end + offset
    p.cell_tpl = cells
    super().__init__(p)
    if p.label_smoothing:
        self.CreateChild('smoother', p.label_smoothing)","for (i, j) in zip(p.splits[:-1], p.splits[1:]):
    assert i <= j, 'Splits must be in increasing order.'",Answer: No. The code cannot be refactored with enumerate as it is already using the zip function to iterate over two lists in parallel.,,,,,,,,,,,
ssr-command-client,https://github.com/TyrantLucifer/ssr-command-client/tree/master/shadowsocksr_cli/shadowsocks/crypto/openssl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ssr-command-client/shadowsocksr_cli/shadowsocks/crypto/openssl.py,,"def test_all():
    for (k, v) in ciphers.items():
        print(k)
        try:
            run_method(k)
        except AssertionError as e:
            eprint('AssertionError===========' + k)
            eprint(e)","for (k, v) in ciphers.items():
    print(k)
    try:
        run_method(k)
    except AssertionError as e:
        eprint('AssertionError===========' + k)
        eprint(e)",Answer: No. The code cannot be refactored with enumerate as it is already using tuple unpacking to iterate over the key-value pairs in the dictionary.,,,,,,,,,,,
weblate,https://github.com/WeblateOrg/weblate/tree/master/weblate/utils/classloader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/weblate/weblate/utils/classloader.py,ClassLoader,"def load_data(self):
    result = {}
    value = self.get_settings()
    for path in value:
        obj = load_class(path, self.name)
        if self.construct:
            obj = obj()
        result[obj.get_identifier()] = obj
    return result","for path in value:
    obj = load_class(path, self.name)
    if self.construct:
        obj = obj()
    result[obj.get_identifier()] = obj",Answer: No. The code cannot be refactored with enumerate as it is iterating over a list of paths and not a sequence of values with an index.,,,,,,,,,,,
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py,,"def recursive_print(name, val, spaces=0):
    if name is None:
        msg = None
    else:
        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'
        msg = fmt.format(name)
    if isinstance(val, dict):
        if msg is not None:
            print(msg)
        for k in val.keys():
            recursive_print(k, val[k], spaces + 2)
    elif isinstance(val, torch.Tensor):
        print(msg, ':', val.size())
    else:
        print(msg, ':', val)","for k in val.keys():
    recursive_print(k, val[k], spaces + 2)",Answer: No. The code cannot be refactored with enumerate as it is iterating over the keys of a dictionary and not a sequence.,,,,,,,,,,,
cogdl,https://github.com/THUDM/cogdl/tree/master/cogdl/wrappers/model_wrapper/base_model_wrapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/wrappers/model_wrapper/base_model_wrapper.py,ModelWrapper,"def collect_notes(self):
    if len(self.__record__) == 0:
        return None
    out = dict()
    for (key, val) in self.__record__.items():
        if key.endswith('_metric'):
            _val = self._evaluator.evaluate()
            if _val == 0:
                _val = val[0]
        else:
            _val = merge_batch_indexes(val)
        out[key] = _val
    self.__record__ = dict()
    return out","for (key, val) in self.__record__.items():
    if key.endswith('_metric'):
        _val = self._evaluator.evaluate()
        if _val == 0:
            _val = val[0]
    else:
        _val = merge_batch_indexes(val)
    out[key] = _val",Answer: No. The code cannot be refactored with enumerate as it is iterating over the key-value pairs of a dictionary and not a list.,,,,,,,,,,,
fake2db,https://github.com/emirozer/fake2db/tree/master/fake2db/couchdb_handler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fake2db/fake2db/couchdb_handler.py,Fake2dbCouchdbHandler,"def data_filler_simple_registration(self, number_of_rows, db):
    """"""creates and fills the table with simple regis. information
        """"""
    try:
        simple_registration = db
        for i in range(0, number_of_rows):
            post_simple_reg = {'id': rnd_id_generator(self), 'email': self.faker.safe_email(), 'password': self.faker.md5(raw_output=False)}
            simple_registration.save(post_simple_reg)
        logger.warning('simple_registration Commits are successful after write job!', extra=d)
    except Exception as e:
        logger.error(e, extra=d)","for i in range(0, number_of_rows):
    post_simple_reg = {'id': rnd_id_generator(self), 'email': self.faker.safe_email(), 'password': self.faker.md5(raw_output=False)}
    simple_registration.save(post_simple_reg)",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or iterable. It is simply using a range to generate a sequence of numbers to be used in the loop.,,,,,,,,,,,
SoftTeacher,https://github.com/microsoft/SoftTeacher/tree/master/tools/test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SoftTeacher/tools/test.py,,"def main():
    args = parse_args()
    assert args.out or args.eval or args.format_only or args.show or args.show_dir, 'Please specify at least one operation (save/eval/format/show the results / save the results) with the argument ""--out"", ""--eval"", ""--format-only"", ""--show"" or ""--show-dir""'
    if args.eval and args.format_only:
        raise ValueError('--eval and --format_only cannot be both specified')
    if args.out is not None and (not args.out.endswith(('.pkl', '.pickle'))):
        raise ValueError('The output file must be a pkl file.')
    cfg = Config.fromfile(args.config)
    if args.cfg_options is not None:
        cfg.merge_from_dict(args.cfg_options)
    if cfg.get('custom_imports', None):
        from mmcv.utils import import_modules_from_strings
        import_modules_from_strings(**cfg['custom_imports'])
    if cfg.get('cudnn_benchmark', False):
        torch.backends.cudnn.benchmark = True
    if 'pretrained' in cfg.model:
        cfg.model.pretrained = None
    if cfg.model.get('neck'):
        if isinstance(cfg.model.neck, list):
            for neck_cfg in cfg.model.neck:
                if neck_cfg.get('rfp_backbone'):
                    if neck_cfg.rfp_backbone.get('pretrained'):
                        neck_cfg.rfp_backbone.pretrained = None
        elif cfg.model.neck.get('rfp_backbone'):
            if cfg.model.neck.rfp_backbone.get('pretrained'):
                cfg.model.neck.rfp_backbone.pretrained = None
    samples_per_gpu = 1
    if isinstance(cfg.data.test, dict):
        cfg.data.test.test_mode = True
        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
        if samples_per_gpu > 1:
            cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)
    elif isinstance(cfg.data.test, list):
        for ds_cfg in cfg.data.test:
            ds_cfg.test_mode = True
        samples_per_gpu = max([ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
        if samples_per_gpu > 1:
            for ds_cfg in cfg.data.test:
                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
    if args.launcher == 'none':
        distributed = False
    else:
        distributed = True
        init_dist(args.launcher, **cfg.dist_params)
    (rank, _) = get_dist_info()
    if args.work_dir is not None and rank == 0:
        cfg.work_dir = args.work_dir
        mmcv.mkdir_or_exist(osp.abspath(args.work_dir))
        timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
        json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(args.config))[0])
    cfg = patch_config(cfg)
    dataset = build_dataset(cfg.data.test)
    data_loader = build_dataloader(dataset, samples_per_gpu=samples_per_gpu, workers_per_gpu=cfg.data.workers_per_gpu, dist=distributed, shuffle=False)
    cfg.model.train_cfg = None
    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
    fp16_cfg = cfg.get('fp16', None)
    if fp16_cfg is not None:
        wrap_fp16_model(model)
    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
    if args.fuse_conv_bn:
        model = fuse_conv_bn(model)
    if 'CLASSES' in checkpoint.get('meta', {}):
        model.CLASSES = checkpoint['meta']['CLASSES']
    else:
        model.CLASSES = dataset.CLASSES
    if not distributed:
        model = MMDataParallel(model, device_ids=[0])
        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir, args.show_score_thr)
    else:
        model = MMDistributedDataParallel(model.cuda(), device_ids=[torch.cuda.current_device()], broadcast_buffers=False)
        outputs = multi_gpu_test(model, data_loader, args.tmpdir, args.gpu_collect)
    (rank, _) = get_dist_info()
    if rank == 0:
        if args.out:
            print(f'\nwriting results to {args.out}')
            mmcv.dump(outputs, args.out)
        kwargs = {} if args.eval_options is None else args.eval_options
        if args.format_only:
            dataset.format_results(outputs, **kwargs)
        if args.eval:
            eval_kwargs = cfg.get('evaluation', {}).copy()
            for key in ['type', 'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best', 'rule']:
                eval_kwargs.pop(key, None)
            eval_kwargs.update(dict(metric=args.eval, **kwargs))
            metric = dataset.evaluate(outputs, **eval_kwargs)
            print(metric)
            metric_dict = dict(config=args.config, metric=metric)
            if args.work_dir is not None and rank == 0:
                mmcv.dump(metric_dict, json_file)","for neck_cfg in cfg.model.neck:
    if neck_cfg.get('rfp_backbone'):
        if neck_cfg.rfp_backbone.get('pretrained'):
            neck_cfg.rfp_backbone.pretrained = None",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence.,,,,,,,,,,,
swift,https://github.com/openstack/swift/tree/master/test/unit/obj/test_ssync_sender.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/obj/test_ssync_sender.py,TestSender,"def test_connect_bad_status(self):
    self.daemon.node_timeout = 0.02
    node = dict(replication_ip='1.2.3.4', replication_port=5678, device='sda1', index=0)
    job = dict(partition='9', policy=POLICIES.legacy)

    class FakeBufferedHTTPConnection(NullBufferedHTTPConnection):

        def getresponse(*args, **kwargs):
            response = FakeResponse()
            response.status = 503
            response.read = lambda : 'an error message'
            return response
    missing_check_fn = 'swift.obj.ssync_sender.Sender.missing_check'
    with mock.patch(missing_check_fn) as mock_missing_check:
        with mock.patch.object(ssync_sender, 'SsyncBufferedHTTPConnection', FakeBufferedHTTPConnection):
            self.sender = ssync_sender.Sender(self.daemon, node, job, ['abc'])
            (success, candidates) = self.sender()
            self.assertFalse(success)
            self.assertEqual(candidates, {})
    error_lines = self.daemon_logger.get_lines_for_level('error')
    for line in error_lines:
        self.assertTrue(line.startswith('1.2.3.4:5678/sda1/9 Expected status 200; got 503'))
        self.assertIn('an error message', line)
    self.assertFalse(mock_missing_check.called)","for line in error_lines:
    self.assertTrue(line.startswith('1.2.3.4:5678/sda1/9 Expected status 200; got 503'))
    self.assertIn('an error message', line)",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence.,,,,,,,,,,,
Unsupervised-Classification,https://github.com/wvangansbeke/Unsupervised-Classification/tree/master/data/stl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Unsupervised-Classification/data/stl.py,STL10,"def _check_integrity(self):
    root = self.root
    for fentry in self.train_list + self.test_list:
        (filename, md5) = (fentry[0], fentry[1])
        fpath = os.path.join(root, self.base_folder, filename)
        if not check_integrity(fpath, md5):
            return False
    return True","for fentry in self.train_list + self.test_list:
    (filename, md5) = (fentry[0], fentry[1])
    fpath = os.path.join(root, self.base_folder, filename)
    if not check_integrity(fpath, md5):
        return False",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is concatenating two lists and then iterating over the resulting list.,,,,,,,,,,,
nuscenes-devkit,https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/nuscenes/scripts/export_instance_videos.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nuscenes-devkit/python-sdk/nuscenes/scripts/export_instance_videos.py,,"def get_most_visible_camera_annotation(camera_data_dict: dict) -> dict:
    """"""
    Get the most visibile camera's annotation.
    :param camera_data_dict: Dictionary of form:
      {
        'CAM_BACK': {'attribute_tokens': ['cb5118da1ab342aa947717dc53544259'],
          'bbox_corners': [600.8315617945755,
          426.38901275036744,
          643.6756536789582,
          476.66593163100237],
          'category_name': 'vehicle.bus.rigid',
          'filename': 'samples/CAM_BACK/n015-2018-10-02-10-50-40+0800__CAM_BACK__1538448750037525.jpg',
          'instance_token': '9cba9cd8af85487fb010652c90d845b5',
          'next': 'ef90c2e525244b7d9eeb759837cf2277',
          'num_lidar_pts': 0,
          'num_radar_pts': 0,
          'prev': '6628e81912584a72bd448a44931afb42',
          'sample_annotation_token': '06b4886e79d2435c80bd23e7ac60c618',
          'sample_data_token': '0008443755a14b3ca483f1489c767040',
          'visibility_token': '4'},
        'CAM_FRONT': ...
        ...
      }
    :return: The camera annotation with highest visibility.
    """"""
    best_visibility = ''
    largest_area = -1
    best_camera_token = None
    for camera_token in camera_data_dict:
        visibility = camera_data_dict[camera_token]['visibility_token']
        bbox_area = camera_data_dict[camera_token]['bbox_area']
        if visibility > best_visibility or (visibility == best_visibility and bbox_area > largest_area):
            best_camera_token = camera_token
            largest_area = bbox_area
            best_visibility = visibility
    if not best_camera_token:
        print('Unable to find any good views for camera data dict: {}'.format(camera_data_dict))
    best_instance_data = camera_data_dict[best_camera_token]
    return best_instance_data","for camera_token in camera_data_dict:
    visibility = camera_data_dict[camera_token]['visibility_token']
    bbox_area = camera_data_dict[camera_token]['bbox_area']
    if visibility > best_visibility or (visibility == best_visibility and bbox_area > largest_area):
        best_camera_token = camera_token
        largest_area = bbox_area
        best_visibility = visibility",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is iterating over a dictionary and accessing its values using keys.,,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","for x in triples:
    fact_in_train.add(tuple(x))",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is iterating over a variable named "triples" which could be any iterable object.,,,,,,,,,,,
ansible,https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/basic.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/lib/ansible/module_utils/basic.py,AnsibleModule,"def is_special_selinux_path(self, path):
    """"""
        Returns a tuple containing (True, selinux_context) if the given path is on a
        NFS or other 'special' fs  mount point, otherwise the return will be (False, None).
        """"""
    try:
        f = open('/proc/mounts', 'r')
        mount_data = f.readlines()
        f.close()
    except Exception:
        return (False, None)
    path_mount_point = self.find_mount_point(path)
    for line in mount_data:
        (device, mount_point, fstype, options, rest) = line.split(' ', 4)
        if to_bytes(path_mount_point) == to_bytes(mount_point):
            for fs in self._selinux_special_fs:
                if fs in fstype:
                    special_context = self.selinux_context(path_mount_point)
                    return (True, special_context)
    return (False, None)","for line in mount_data:
    (device, mount_point, fstype, options, rest) = line.split(' ', 4)
    if to_bytes(path_mount_point) == to_bytes(mount_point):
        for fs in self._selinux_special_fs:
            if fs in fstype:
                special_context = self.selinux_context(path_mount_point)
                return (True, special_context)",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is iterating over the lines of a file or string.,,,,,,,,,,,
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/youtube.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/youtube.py,YoutubeIE,"def _get_automatic_captions(self, video_id, webpage):
    """"""We need the webpage for getting the captions url, pass it as an
           argument to speed up the process.""""""
    self.to_screen('%s: Looking for automatic captions' % video_id)
    player_config = self._get_ytplayer_config(video_id, webpage)
    err_msg = ""Couldn't find automatic captions for %s"" % video_id
    if not player_config:
        self._downloader.report_warning(err_msg)
        return {}
    try:
        args = player_config['args']
        caption_url = args.get('ttsurl')
        if caption_url:
            timestamp = args['timestamp']
            list_params = compat_urllib_parse_urlencode({'type': 'list', 'tlangs': 1, 'asrs': 1})
            list_url = caption_url + '&' + list_params
            caption_list = self._download_xml(list_url, video_id)
            original_lang_node = caption_list.find('track')
            if original_lang_node is None:
                self._downloader.report_warning(""Video doesn't have automatic captions"")
                return {}
            original_lang = original_lang_node.attrib['lang_code']
            caption_kind = original_lang_node.attrib.get('kind', '')
            sub_lang_list = {}
            for lang_node in caption_list.findall('target'):
                sub_lang = lang_node.attrib['lang_code']
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
                    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})
                sub_lang_list[sub_lang] = sub_formats
            return sub_lang_list

        def make_captions(sub_url, sub_langs):
            parsed_sub_url = compat_urllib_parse_urlparse(sub_url)
            caption_qs = compat_parse_qs(parsed_sub_url.query)
            captions = {}
            for sub_lang in sub_langs:
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    caption_qs.update({'tlang': [sub_lang], 'fmt': [ext]})
                    sub_url = compat_urlparse.urlunparse(parsed_sub_url._replace(query=compat_urllib_parse_urlencode(caption_qs, True)))
                    sub_formats.append({'url': sub_url, 'ext': ext})
                captions[sub_lang] = sub_formats
            return captions
        player_response = args.get('player_response')
        if player_response and isinstance(player_response, compat_str):
            player_response = self._parse_json(player_response, video_id, fatal=False)
            if player_response:
                renderer = player_response['captions']['playerCaptionsTracklistRenderer']
                base_url = renderer['captionTracks'][0]['baseUrl']
                sub_lang_list = []
                for lang in renderer['translationLanguages']:
                    lang_code = lang.get('languageCode')
                    if lang_code:
                        sub_lang_list.append(lang_code)
                return make_captions(base_url, sub_lang_list)
        caption_tracks = args['caption_tracks']
        caption_translation_languages = args['caption_translation_languages']
        caption_url = compat_parse_qs(caption_tracks.split(',')[0])['u'][0]
        sub_lang_list = []
        for lang in caption_translation_languages.split(','):
            lang_qs = compat_parse_qs(compat_urllib_parse_unquote_plus(lang))
            sub_lang = lang_qs.get('lc', [None])[0]
            if sub_lang:
                sub_lang_list.append(sub_lang)
        return make_captions(caption_url, sub_lang_list)
    except (KeyError, IndexError, ExtractorError):
        self._downloader.report_warning(err_msg)
        return {}","for ext in self._SUBTITLE_FORMATS:
    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is simply assigning values from a dictionary to a variable.,,,,,,,,,,,
Hypernets,https://github.com/DataCanvasIO/Hypernets/tree/master/hypernets/tabular/dask_ex/_toolbox.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Hypernets/hypernets/tabular/dask_ex/_toolbox.py,DaskToolBox,"def exist_dask_dataframe(*args):
    for a in args:
        if isinstance(a, dd.DataFrame):
            return True
        if isinstance(a, (tuple, list, set)):
            return DaskToolBox.exist_dask_dataframe(*a)
    return False","for a in args:
    if isinstance(a, dd.DataFrame):
        return True
    if isinstance(a, (tuple, list, set)):
        return DaskToolBox.exist_dask_dataframe(*a)",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a sequence of elements.,,,,,,,,,,,
nn_vis,https://github.com/julrog/nn_vis/tree/master/utility/file.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nn_vis/utility/file.py,DictFile,"def read_data(self, data: Dict):
    read_data = dict()
    try:
        with open(self.file_path, 'r') as stats_file:
            file_data = stats_file.read()
            if file_data:
                read_data = convert_values(json.loads(file_data), str_to_nnvis)
    except FileNotFoundError:
        with open(self.file_path, 'w+'):
            pass
    for key in read_data.keys():
        data[key] = read_data[key]
    return data","for key in read_data.keys():
    data[key] = read_data[key]",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a sequence of elements. It is iterating over the keys of a dictionary.,,,,,,,,,,,
SoftTeacher,https://github.com/microsoft/SoftTeacher/tree/master/tools/test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SoftTeacher/tools/test.py,,"def main():
    args = parse_args()
    assert args.out or args.eval or args.format_only or args.show or args.show_dir, 'Please specify at least one operation (save/eval/format/show the results / save the results) with the argument ""--out"", ""--eval"", ""--format-only"", ""--show"" or ""--show-dir""'
    if args.eval and args.format_only:
        raise ValueError('--eval and --format_only cannot be both specified')
    if args.out is not None and (not args.out.endswith(('.pkl', '.pickle'))):
        raise ValueError('The output file must be a pkl file.')
    cfg = Config.fromfile(args.config)
    if args.cfg_options is not None:
        cfg.merge_from_dict(args.cfg_options)
    if cfg.get('custom_imports', None):
        from mmcv.utils import import_modules_from_strings
        import_modules_from_strings(**cfg['custom_imports'])
    if cfg.get('cudnn_benchmark', False):
        torch.backends.cudnn.benchmark = True
    if 'pretrained' in cfg.model:
        cfg.model.pretrained = None
    if cfg.model.get('neck'):
        if isinstance(cfg.model.neck, list):
            for neck_cfg in cfg.model.neck:
                if neck_cfg.get('rfp_backbone'):
                    if neck_cfg.rfp_backbone.get('pretrained'):
                        neck_cfg.rfp_backbone.pretrained = None
        elif cfg.model.neck.get('rfp_backbone'):
            if cfg.model.neck.rfp_backbone.get('pretrained'):
                cfg.model.neck.rfp_backbone.pretrained = None
    samples_per_gpu = 1
    if isinstance(cfg.data.test, dict):
        cfg.data.test.test_mode = True
        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
        if samples_per_gpu > 1:
            cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)
    elif isinstance(cfg.data.test, list):
        for ds_cfg in cfg.data.test:
            ds_cfg.test_mode = True
        samples_per_gpu = max([ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
        if samples_per_gpu > 1:
            for ds_cfg in cfg.data.test:
                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
    if args.launcher == 'none':
        distributed = False
    else:
        distributed = True
        init_dist(args.launcher, **cfg.dist_params)
    (rank, _) = get_dist_info()
    if args.work_dir is not None and rank == 0:
        cfg.work_dir = args.work_dir
        mmcv.mkdir_or_exist(osp.abspath(args.work_dir))
        timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
        json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(args.config))[0])
    cfg = patch_config(cfg)
    dataset = build_dataset(cfg.data.test)
    data_loader = build_dataloader(dataset, samples_per_gpu=samples_per_gpu, workers_per_gpu=cfg.data.workers_per_gpu, dist=distributed, shuffle=False)
    cfg.model.train_cfg = None
    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
    fp16_cfg = cfg.get('fp16', None)
    if fp16_cfg is not None:
        wrap_fp16_model(model)
    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
    if args.fuse_conv_bn:
        model = fuse_conv_bn(model)
    if 'CLASSES' in checkpoint.get('meta', {}):
        model.CLASSES = checkpoint['meta']['CLASSES']
    else:
        model.CLASSES = dataset.CLASSES
    if not distributed:
        model = MMDataParallel(model, device_ids=[0])
        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir, args.show_score_thr)
    else:
        model = MMDistributedDataParallel(model.cuda(), device_ids=[torch.cuda.current_device()], broadcast_buffers=False)
        outputs = multi_gpu_test(model, data_loader, args.tmpdir, args.gpu_collect)
    (rank, _) = get_dist_info()
    if rank == 0:
        if args.out:
            print(f'\nwriting results to {args.out}')
            mmcv.dump(outputs, args.out)
        kwargs = {} if args.eval_options is None else args.eval_options
        if args.format_only:
            dataset.format_results(outputs, **kwargs)
        if args.eval:
            eval_kwargs = cfg.get('evaluation', {}).copy()
            for key in ['type', 'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best', 'rule']:
                eval_kwargs.pop(key, None)
            eval_kwargs.update(dict(metric=args.eval, **kwargs))
            metric = dataset.evaluate(outputs, **eval_kwargs)
            print(metric)
            metric_dict = dict(config=args.config, metric=metric)
            if args.work_dir is not None and rank == 0:
                mmcv.dump(metric_dict, json_file)","for ds_cfg in cfg.data.test:
    ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a sequence of elements. It is simply iterating over the elements of a list.,,,,,,,,,,,
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_lookup.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hypothesis/hypothesis-python/tests/cover/test_lookup.py,,"def test_bytestring_not_treated_as_generic_sequence(val):
    assert not isinstance(val, typing.ByteString)
    for x in val:
        assert isinstance(x, set)","for x in val:
    assert isinstance(x, set)",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a sequence of values that can be indexed.,,,,,,,,,,,
WatchAD,https://github.com/Qianlitp/WatchAD/tree/master/libs/pyasn1/type/univ.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WatchAD/libs/pyasn1/type/univ.py,BitString,"def prettyIn(self, value):
    r = []
    if not value:
        return ()
    elif isinstance(value, str):
        if value[0] == ""'"":
            if value[-2:] == ""'B"":
                for v in value[1:-2]:
                    if v == '0':
                        r.append(0)
                    elif v == '1':
                        r.append(1)
                    else:
                        raise error.PyAsn1Error('Non-binary BIT STRING initializer %s' % (v,))
                return tuple(r)
            elif value[-2:] == ""'H"":
                for v in value[1:-2]:
                    i = 4
                    v = int(v, 16)
                    while i:
                        i = i - 1
                        r.append(v >> i & 1)
                return tuple(r)
            else:
                raise error.PyAsn1Error('Bad BIT STRING value notation %s' % (value,))
        else:
            for i in value.split(','):
                j = self.__namedValues.getValue(i)
                if j is None:
                    raise error.PyAsn1Error(""Unknown bit identifier '%s'"" % (i,))
                if j >= len(r):
                    r.extend([0] * (j - len(r) + 1))
                r[j] = 1
            return tuple(r)
    elif isinstance(value, (tuple, list)):
        r = tuple(value)
        for b in r:
            if b and b != 1:
                raise error.PyAsn1Error(""Non-binary BitString initializer '%s'"" % (r,))
        return r
    elif isinstance(value, BitString):
        return tuple(value)
    else:
        raise error.PyAsn1Error(""Bad BitString initializer type '%s'"" % (value,))","for b in r:
    if b and b != 1:
        raise error.PyAsn1Error(""Non-binary BitString initializer '%s'"" % (r,))",Answer: No. The code cannot be refactored with enumerate as it is not iterating over a sequence.,,,,,,,,,,,
hachoir,https://github.com/vstinner/hachoir/tree/master/hachoir/parser/program/nds.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hachoir/hachoir/parser/program/nds.py,FATContent,"def createFields(self):
    num_entries = self.parent['header']['fat_size'].value // 8
    for i in range(0, num_entries):
        yield FATFileEntry(self, 'entry[]')","for i in range(0, num_entries):
    yield FATFileEntry(self, 'entry[]')",Answer: No. The code cannot be refactored with enumerate as it is using the range function to generate a sequence of numbers to iterate over.,,,,,,,,,,,
animation_nodes,https://github.com/JacquesLucke/animation_nodes/tree/master/animation_nodes/nodes/list/create_list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/animation_nodes/animation_nodes/nodes/list/create_list.py,CreateListNode,"def recreateSockets(self, inputAmount=2):
    self.clearSockets()
    self.newInput('Node Control', '...')
    for i in range(inputAmount):
        self.newInputSocket()
    self.newOutput(toListDataType(self.assignedType), 'List', 'outList')","for i in range(inputAmount):
    self.newInputSocket()","Answer: No. The code cannot be refactored with enumerate as it is using the range function to iterate a specific number of times, which is not related to the index of any iterable.",,,,,,,,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for j in range(4):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)",Answer: No. The code cannot be refactored with enumerate as it is using the range function to iterate over a fixed number of iterations.,,,,,,,,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/os/map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/os/map.py,OSMap,"def handle_fleet_resolve(self, revert=False):
    """"""
        Check each fleet if afflicted with the low
        resolve debuff
        If so, handle by completing an easy zone

        Args:
            revert (bool): If go back to previous zone.

        Returns:
            bool:
        """"""
    if self.is_in_special_zone():
        logger.info('OS is in a special zone type, skip fleet resolve')
        return False
    for index in [1, 2, 3, 4]:
        if not self.fleet_set(index):
            self.device.screenshot()
        if self.fleet_low_resolve_appear():
            logger.info('At least one fleet is afflicted with the low resolve debuff')
            self.fleet_resolve(revert)
            return True
    logger.info('None of the fleets are afflicted with the low resolve debuff')
    return False","for index in [1, 2, 3, 4]:
    if not self.fleet_set(index):
        self.device.screenshot()
    if self.fleet_low_resolve_appear():
        logger.info('At least one fleet is afflicted with the low resolve debuff')
        self.fleet_resolve(revert)
        return True",Answer: No. The code cannot be refactored with enumerate as the loop is not iterating over a list or any iterable object.,,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/md_gender/yelp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/md_gender/yelp.py,YelpTeacher,"def _check_data_downloaded(self, opt):
    RESET = '\x1b[0m'
    RED = '\x1b[1;91m'
    YELLOW = '\x1b[1;93m'
    GREEN = '\x1b[1;92m'
    BLUE = '\x1b[1;96m'
    CYAN = '\x1b[1;94m'
    MAGENTA = '\x1b[1;95m'
    USE_COLORS = _sys.stdout.isatty()
    if not USE_COLORS:
        RESET = RED = YELLOW = GREEN = BLUE = CYAN = MAGENTA = ''
    rainbow = [RED, YELLOW, GREEN, CYAN, BLUE, MAGENTA]
    size = 78 // len(rainbow)
    stars = ''.join([color + '*' * size for color in rainbow])
    stars += RESET
    self.data_path = os.path.join(opt['datapath'], 'md_gender', 'yelp')
    if not os.path.exists(self.data_path):
        PathManager.mkdirs(self.data_path)
    if not PathManager.exists(os.path.join(self.data_path, 'valid.fader.with_cat.40000')):
        raise RuntimeError(f'\n\n{stars}\nThis data must be downloaded following instructions in the README here:<https://github.com/facebookresearch/MultipleAttributeTextRewriting/blob/main/data/README.md>. \nIt cannot be automatically downloaded, as one must agree to the terms outlined on the website before gaining access to the data.\n\nOnce downloaded, please put the data in the following directory: \n{self.data_path}\n{stars}')
    elif not PathManager.exists(os.path.join(self.data_path, 'classtrain.txt')):
        logging.info('[ Building data ... ]')
        with open(os.path.join(self.data_path, 'classtrain.txt'), 'w') as f:
            for fle_num in [4000, 6000, 8000]:
                train_fle = f'train.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, train_fle)) as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            f.write(f'male\t{text}\n')
                        elif gend == '1':
                            f.write(f'female\t{text}\n')
        for pair in [('dev', 'valid'), ('test', 'test')]:
            with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
                with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
                    for fle_num in [4000, 6000, 8000]:
                        valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                        with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                            lines = g.readlines()
                            for line in lines:
                                tabs = line.split('\t')
                                text = tabs[0]
                                gend = tabs[1]
                                if gend == '0':
                                    masc_val.write(f'{text}\n')
                                elif gend == '1':
                                    fem_val.write(f'{text}\n')","for fle_num in [4000, 6000, 8000]:
    train_fle = f'train.fader.with_cat.{fle_num}'
    with open(os.path.join(self.data_path, train_fle)) as g:
        lines = g.readlines()
        for line in lines:
            tabs = line.split('\t')
            text = tabs[0]
            gend = tabs[1]
            if gend == '0':
                f.write(f'male\t{text}\n')
            elif gend == '1':
                f.write(f'female\t{text}\n')",Answer: No. The code cannot be refactored with enumerate as the loop is not iterating over a list or iterable.,,,,,,,,,,,
dtshare,https://github.com/DTShare/dtshare/tree/master/dtshare/obor/get_countries_from_invest_web.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dtshare/dtshare/obor/get_countries_from_invest_web.py,,"def get_countries_url():
    url = 'https://cn.investing.com/indices/world-indices'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36'}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'lxml')
    soup_data = soup.find_all(attrs={'class': 'wide selectBox inlineblock js-indice-country-filter'})
    soup_list = list()
    for country in soup_data:
        soup_list.append(country.get_text().strip())
    soup_en = soup.find_all('option')[1:96]
    english_url = []
    for item in soup_en:
        english_url.append(item['value'])
    chinese_name = []
    for item in soup_en:
        chinese_name.append(item.get_text())
    en_ch_data = pd.DataFrame([english_url, chinese_name]).T
    useful_data = cons.countries_dict.get('report_map_invest').values()
    web_site = pd.DataFrame()
    for item in useful_data:
        web_site = web_site.append(en_ch_data[en_ch_data.iloc[:, 1] == item])
    web_site = web_site.reset_index().iloc[:, 1:]
    return web_site","for item in soup_en:
    english_url.append(item['value'])",Answer: No. The code cannot be refactored with enumerate as the variable `item` is not being used as an index in the loop.,,,,,,,,,,,
AttGAN-Tensorflow,https://github.com/LynnHo/AttGAN-Tensorflow/tree/master//test_multi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AttGAN-Tensorflow//test_multi.py,,"def sample_graph():
    test_next = test_iter.get_next()
    if not os.path.exists(py.join(output_dir, 'generator.pb')):
        (Genc, Gdec, _) = module.get_model(args.model, n_atts, weight_decay=args.weight_decay)
        xa = tf.placeholder(tf.float32, shape=[None, args.crop_size, args.crop_size, 3])
        b_ = tf.placeholder(tf.float32, shape=[None, n_atts])
        x = Gdec(Genc(xa, training=False), b_, training=False)
    else:
        with tf.gfile.GFile(py.join(output_dir, 'generator.pb'), 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
            tf.import_graph_def(graph_def, name='generator')
        xa = sess.graph.get_tensor_by_name('generator/xa:0')
        b_ = sess.graph.get_tensor_by_name('generator/b_:0')
        x = sess.graph.get_tensor_by_name('generator/xb:0')
    save_dir = './output/%s/samples_testing_multi' % args.experiment_name
    tmp = ''
    for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
        tmp += '_%s_%s' % (test_att_name, '{:g}'.format(test_int))
    save_dir = py.join(save_dir, tmp[1:])
    py.mkdir(save_dir)

    def run():
        cnt = 0
        for _ in tqdm.trange(len_test_dataset):
            (xa_ipt, a_ipt) = sess.run(test_next)
            b_ipt = np.copy(a_ipt)
            for test_att_name in args.test_att_names:
                i = args.att_names.index(test_att_name)
                b_ipt[..., i] = 1 - b_ipt[..., i]
                b_ipt = data.check_attribute_conflict(b_ipt, test_att_name, args.att_names)
            b__ipt = (b_ipt * 2 - 1).astype(np.float32)
            for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
                i = args.att_names.index(test_att_name)
                b__ipt[..., i] = b__ipt[..., i] * test_int
            x_opt_list = [xa_ipt]
            x_opt = sess.run(x, feed_dict={xa: xa_ipt, b_: b__ipt})
            x_opt_list.append(x_opt)
            sample = np.transpose(x_opt_list, (1, 2, 0, 3, 4))
            sample = np.reshape(sample, (sample.shape[0], -1, sample.shape[2] * sample.shape[3], sample.shape[4]))
            for s in sample:
                cnt += 1
                im.imwrite(s, '%s/%d.jpg' % (save_dir, cnt))
    return run","for _ in tqdm.trange(len_test_dataset):
    (xa_ipt, a_ipt) = sess.run(test_next)
    b_ipt = np.copy(a_ipt)
    for test_att_name in args.test_att_names:
        i = args.att_names.index(test_att_name)
        b_ipt[..., i] = 1 - b_ipt[..., i]
        b_ipt = data.check_attribute_conflict(b_ipt, test_att_name, args.att_names)
    b__ipt = (b_ipt * 2 - 1).astype(np.float32)
    for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
        i = args.att_names.index(test_att_name)
        b__ipt[..., i] = b__ipt[..., i] * test_int
    x_opt_list = [xa_ipt]
    x_opt = sess.run(x, feed_dict={xa: xa_ipt, b_: b__ipt})
    x_opt_list.append(x_opt)
    sample = np.transpose(x_opt_list, (1, 2, 0, 3, 4))
    sample = np.reshape(sample, (sample.shape[0], -1, sample.shape[2] * sample.shape[3], sample.shape[4]))
    for s in sample:
        cnt += 1
        im.imwrite(s, '%s/%d.jpg' % (save_dir, cnt))",Answer: No. The code cannot be refactored with enumerate as there is no need to access the index of the loop in this case.,,,,,,,,,,,
trankit,https://github.com/nlp-uoregon/trankit/tree/master/trankit/layers/seq2seq.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trankit/trankit/layers/seq2seq.py,Seq2SeqModel,"def update_state(states, idx, positions, beam_size):
    """""" Select the states according to back pointers. """"""
    for e in states:
        (br, d) = e.size()
        s = e.contiguous().view(beam_size, br // beam_size, d)[:, idx]
        s.data.copy_(s.data.index_select(0, positions))","for e in states:
    (br, d) = e.size()
    s = e.contiguous().view(beam_size, br // beam_size, d)[:, idx]
    s.data.copy_(s.data.index_select(0, positions))","Answer: No. The given code is not related to iterating over a list or sequence, so it cannot be refactored with `enumerate()`.",,,,,,,,,,,
advertools,https://github.com/eliasdabbas/advertools/tree/master/advertools/serp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/advertools/advertools/serp.py,,"def youtube_channel_details(key, channel_ids):
    """"""Return details of channels for which the ids are given.
    Assumes ``ids`` is a comma-separated list of channel ids with
    no spaces.""""""
    base_url = 'https://www.googleapis.com/youtube/v3/channels?part=snippet,contentDetails,statistics'
    channel_ids = _split_by_comma(channel_ids, length=50)
    final_df = pd.DataFrame()
    for channel_id in channel_ids:
        params = {'id': channel_id, 'key': key}
        logging.info(msg='Requesting: ' + 'channel details')
        channel_resp = requests.get(base_url, params=params)
        if channel_resp.status_code >= 400:
            raise Exception(channel_resp.json())
        items_df = pd.DataFrame(channel_resp.json()['items'])
        details = ['snippet', 'statistics', 'contentDetails']
        detail_df = pd.DataFrame()
        for detail in details:
            try:
                detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
            except KeyError:
                continue
        temp_df = pd.concat([items_df, detail_df], axis=1)
        final_df = final_df.append(temp_df, sort=False, ignore_index=True)
    return final_df","for channel_id in channel_ids:
    params = {'id': channel_id, 'key': key}
    logging.info(msg='Requesting: ' + 'channel details')
    channel_resp = requests.get(base_url, params=params)
    if channel_resp.status_code >= 400:
        raise Exception(channel_resp.json())
    items_df = pd.DataFrame(channel_resp.json()['items'])
    details = ['snippet', 'statistics', 'contentDetails']
    detail_df = pd.DataFrame()
    for detail in details:
        try:
            detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
        except KeyError:
            continue
    temp_df = pd.concat([items_df, detail_df], axis=1)
    final_df = final_df.append(temp_df, sort=False, ignore_index=True)","final_df = pd.DataFrame()
for i, channel_id in enumerate(channel_ids):
    params = {'id': channel_id, 'key': key}
    logging.info(msg='Requesting: ' + 'channel details')
    channel_resp = requests.get(base_url, params=params)
    if channel_resp.status_code >= 400:
        raise Exception(channel_resp.json())
    items_df = pd.DataFrame(channel_resp.json()['items'])
    details = ['snippet', 'statistics', 'contentDetails']
    detail_df = pd.DataFrame()
    for detail in details:
        try:
            detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
        except KeyError:
            continue
    temp_df = pd.concat([items_df, detail_df], axis=1)
    final_df = final_df.append(temp_df, sort=False, ignore_index=True)",0,多了一个语句,,,,,,,,,
iou-tracker,https://github.com/bochinski/iou-tracker/tree/master//viou_tracker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/iou-tracker//viou_tracker.py,,"def track_viou_matlab_wrapper(frames_path, detections, sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio=1.0):
    """"""
    Matlab wrapper of the v-iou tracker for the detrac evaluation toolkit.

    Args:
         detections (numpy.array): numpy array of detections, usually supplied by run_tracker.m
         sigma_l (float): low detection threshold.
         sigma_h (float): high detection threshold.
         sigma_iou (float): IOU threshold.
         t_min (float): minimum track length in frames.

    Returns:
        float: speed in frames per second.
        list: list of tracks.
    """"""
    detections = detections.reshape((7, -1)).transpose()
    dets = load_mot(detections, with_classes=False)
    start = time()
    tracks = track_viou(frames_path + 'img{:05d}.jpg', dets, sigma_l, sigma_h, sigma_iou, int(t_min), int(ttl), tracker_type, keep_upper_height_ratio)
    end = time()
    id_ = 1
    out = []
    for track in tracks:
        for (i, bbox) in enumerate(track['bboxes']):
            out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
        id_ += 1
    num_frames = len(dets)
    speed = num_frames / (end - start)
    return (speed, out)","for track in tracks:
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
    id_ += 1","for (id_, track) in enumerate(tracks):
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]",1,,,,,,,,,,
imbalanced-semi-self,https://github.com/YyzHarry/imbalanced-semi-self/tree/master/dataset/imbalance_svhn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imbalanced-semi-self/dataset/imbalance_svhn.py,ImbalanceSVHN,"def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):
    img_max = 1000
    img_num_per_cls = []
    if imb_type == 'exp':
        for cls_idx in range(cls_num):
            num = img_max * imb_factor ** (cls_idx / (cls_num - 1.0))
            img_num_per_cls.append(int(num))
    elif imb_type == 'step':
        for cls_idx in range(cls_num // 2):
            img_num_per_cls.append(int(img_max))
        for cls_idx in range(cls_num // 2):
            img_num_per_cls.append(int(img_max * imb_factor))
    else:
        img_num_per_cls.extend([int(img_max)] * cls_num)
    return img_num_per_cls","for cls_idx in range(cls_num // 2):
    img_num_per_cls.append(int(img_max))","for cls_idx, _ in enumerate(range(cls_num // 2)):
    img_num_per_cls.append(int(img_max))",1,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/tests/convert_gl2tf_gconv2d.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/tests/convert_gl2tf_gconv2d.py,,"def conv2d(x, in_channels, out_channels, kernel_size, strides=1, padding=0, groups=1, use_bias=True, name='conv2d'):
    """"""
    Convolution 2D layer wrapper.

    Parameters:
    ----------
    x : Tensor
        Input tensor.
    in_channels : int
        Number of input channels.
    out_channels : int
        Number of output channels.
    kernel_size : int or tuple/list of 2 int
        Convolution window size.
    strides : int or tuple/list of 2 int
        Strides of the convolution.
    padding : int or tuple/list of 2 int
        Padding value for convolution layer.
    groups : int, default 1
        Number of groups.
    use_bias : bool, default False
        Whether the layer uses a bias vector.
    name : str, default 'conv2d'
        Layer name.

    Returns:
    -------
    Tensor
        Resulted tensor.
    """"""
    if isinstance(kernel_size, int):
        kernel_size = (kernel_size, kernel_size)
    if isinstance(strides, int):
        strides = (strides, strides)
    if isinstance(padding, int):
        padding = (padding, padding)
    if padding[0] > 0 or padding[1] > 0:
        x = tf.pad(x, [[0, 0], [0, 0], list(padding), list(padding)])
    if groups == 1:
        x = tf.layers.conv2d(inputs=x, filters=out_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name)
    elif groups == out_channels and out_channels == in_channels:
        kernel = tf.get_variable(name=name + '/dw_kernel', shape=kernel_size + (in_channels, 1), initializer=tf.variance_scaling_initializer(2.0))
        x = tf.nn.depthwise_conv2d(input=x, filter=kernel, strides=(1, 1) + strides, padding='VALID', rate=(1, 1), name=name, data_format='NCHW')
        if use_bias:
            raise NotImplementedError
    else:
        assert in_channels % groups == 0
        assert out_channels % groups == 0
        in_group_channels = in_channels // groups
        out_group_channels = out_channels // groups
        group_list = []
        for gi in range(groups):
            xi = x[:, gi * in_group_channels:(gi + 1) * in_group_channels, :, :]
            xi = tf.layers.conv2d(inputs=xi, filters=out_group_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name + '/convgroup{}'.format(gi + 1))
            group_list.append(xi)
        x = tf.concat(group_list, axis=1, name=name + '/concat')
    return x","for gi in range(groups):
    xi = x[:, gi * in_group_channels:(gi + 1) * in_group_channels, :, :]
    xi = tf.layers.conv2d(inputs=xi, filters=out_group_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name + '/convgroup{}'.format(gi + 1))
    group_list.append(xi)","for gi, _ in enumerate(range(groups)):
    xi = x[:, gi * in_group_channels:(gi + 1) * in_group_channels, :, :]
    xi = tf.layers.conv2d(inputs=xi, filters=out_group_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name + '/convgroup{}'.format(gi + 1))
    group_list.append(xi)",1,,,,,,,,,,
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/test/training/test_synthetic_reward_training.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/test/training/test_synthetic_reward_training.py,,"def create_sequence_data(state_dim, action_dim, seq_len, batch_size, num_batches):
    SCALE = 2
    weight = SCALE * torch.randn(state_dim + action_dim)
    data = [None for _ in range(num_batches)]
    for i in range(num_batches):
        state = SCALE * torch.randn(seq_len, batch_size, state_dim)
        action = SCALE * torch.randn(seq_len, batch_size, action_dim)
        valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
        feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
        feature_mask = (feature_mask >= seq_len - valid_step).float()
        assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
        feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
        assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
        feature = torch.cat((state, action), dim=2)
        masked_feature = feature * feature_mask
        left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
        right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
        reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
        mask = torch.arange(seq_len).repeat(batch_size, 1)
        mask = (mask >= seq_len - valid_step).float()
        reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
        data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))
    return (weight, data)","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))",2,code no change,,,,,,,,,
UnityPack,https://github.com/HearthSim/UnityPack/tree/master/unitypack/type.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UnityPack/unitypack/type.py,TypeMetadata,"def load(self, buf, format=None):
    if format is None:
        format = self.asset.format
    self.generator_version = buf.read_string()
    self.target_platform = RuntimePlatform(buf.read_uint())
    if format >= 13:
        has_type_trees = buf.read_boolean()
        num_types = buf.read_int()
        for i in range(num_types):
            class_id = buf.read_int()
            if format >= 17:
                unk0 = buf.read_byte()
                script_id = buf.read_int16()
                if class_id == 114:
                    if script_id >= 0:
                        class_id = -2 - script_id
                    else:
                        class_id = -1
            self.class_ids.append(class_id)
            if class_id < 0:
                hash = buf.read(32)
            else:
                hash = buf.read(16)
            self.hashes[class_id] = hash
            if has_type_trees:
                tree = TypeTree(format)
                tree.load(buf)
                self.type_trees[class_id] = tree
            if format >= 21:
                unk1 = buf.read(4)
    else:
        num_fields = buf.read_int()
        for i in range(num_fields):
            class_id = buf.read_int()
            tree = TypeTree(format)
            tree.load(buf)
            self.type_trees[class_id] = tree","for i in range(num_types):
    class_id = buf.read_int()
    if format >= 17:
        unk0 = buf.read_byte()
        script_id = buf.read_int16()
        if class_id == 114:
            if script_id >= 0:
                class_id = -2 - script_id
            else:
                class_id = -1
    self.class_ids.append(class_id)
    if class_id < 0:
        hash = buf.read(32)
    else:
        hash = buf.read(16)
    self.hashes[class_id] = hash
    if has_type_trees:
        tree = TypeTree(format)
        tree.load(buf)
        self.type_trees[class_id] = tree
    if format >= 21:
        unk1 = buf.read(4)","for i in range(num_types):
    class_id = buf.read_int()
    if format >= 17:
        unk0 = buf.read_byte()
        script_id = buf.read_int16()
        if class_id == 114:
            if script_id >= 0:
                class_id = -2 - script_id
            else:
                class_id = -1
    self.class_ids.append(class_id)
    if class_id < 0:
        hash = buf.read(32)
    else:
        hash = buf.read(16)
    self.hashes[class_id] = hash
    if has_type_trees:
        tree = TypeTree(format)
        tree.load(buf)
        self.type_trees[class_id] = tree
    if format >= 21:
        unk1 = buf.read(4)",2,code no change,,,,,,,,,
jqdatasdk,https://github.com/JoinQuant/jqdatasdk/tree/master/jqdatasdk/client.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jqdatasdk/jqdatasdk/client.py,JQDataClient,"def ensure_auth(self):
    if self.inited:
        return
    if not self.username and (not self.token):
        raise RuntimeError('not inited')
    if self.username:
        (error, response) = (None, None)
        for _ in range(self.request_attempt_count):
            try:
                self._create_client()
                response = self.client.auth(self.username, self.password, self.compress, get_mac_address(), current_version)
                break
            except socket_error as ex:
                error = ex
                time.sleep(0.5)
                if self.client:
                    self.client.close()
                    self.client = None
                continue
        else:
            if error and (not response):
                raise error
        if response and response.error:
            self.data_api_url = response.error
        else:
            self.data_api_url = AUTH_API_URL
    else:
        response = self.client.auth_by_token(self.token)
    auth_message = response.msg
    if not isatty():
        auth_message = ''
    if not response.status:
        self._threading_local._instance = None
        raise self.get_error(response)
    elif self.not_auth:
        print('auth success %s' % auth_message)
        self.not_auth = False
    self.inited = True","for _ in range(self.request_attempt_count):
    try:
        self._create_client()
        response = self.client.auth(self.username, self.password, self.compress, get_mac_address(), current_version)
        break
    except socket_error as ex:
        error = ex
        time.sleep(0.5)
        if self.client:
            self.client.close()
            self.client = None
        continue
else:
    if error and (not response):
        raise error","for i in range(self.request_attempt_count):
    try:
        self._create_client()
        response = self.client.auth(self.username, self.password, self.compress, get_mac_address(), current_version)
        break
    except socket_error as ex:
        error = ex
        time.sleep(0.5)
        if self.client:
            self.client.close()
            self.client = None
        continue
else:
    if error and (not response):
        raise error",2,code no change,,,,,,,,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/doctype/payment_entry/payment_entry.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/doctype/payment_entry/payment_entry.py,PaymentEntry,"def validate_paid_invoices(self):
    no_oustanding_refs = {}
    for d in self.get('references'):
        if not d.allocated_amount:
            continue
        if d.reference_doctype in ('Sales Invoice', 'Purchase Invoice'):
            (outstanding_amount, is_return) = frappe.get_cached_value(d.reference_doctype, d.reference_name, ['outstanding_amount', 'is_return'])
            if outstanding_amount <= 0 and (not is_return):
                no_oustanding_refs.setdefault(d.reference_doctype, []).append(d)
    for (k, v) in no_oustanding_refs.items():
        frappe.msgprint(_('{} - {} now have {} as they had no outstanding amount left before submitting the Payment Entry.').format(_(k), frappe.bold(', '.join((d.reference_name for d in v))), frappe.bold(_('negative outstanding amount'))) + '<br><br>' + _('If this is undesirable please cancel the corresponding Payment Entry.'), title=_('Warning'), indicator='orange')","for (k, v) in no_oustanding_refs.items():
    frappe.msgprint(_('{} - {} now have {} as they had no outstanding amount left before submitting the Payment Entry.').format(_(k), frappe.bold(', '.join((d.reference_name for d in v))), frappe.bold(_('negative outstanding amount'))) + '<br><br>' + _('If this is undesirable please cancel the corresponding Payment Entry.'), title=_('Warning'), indicator='orange')","for i, ((k, v)) in enumerate(no_oustanding_refs.items()):
    frappe.msgprint(_('{} - {} now have {} as they had no outstanding amount left before submitting the Payment Entry.').format(_(k), frappe.bold(', '.join((d.reference_name for d in v))), frappe.bold(_('negative outstanding amount'))) + '<br><br>' + _('If this is undesirable please cancel the corresponding Payment Entry.'), title=_('Warning'), indicator='orange')",1,,,,,,,,,,
skll,https://github.com/EducationalTestingService/skll/tree/master/tests/test_input.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/skll/tests/test_input.py,,"def test_cv_folds_and_grid_search_folds():
    for ((task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search), (chosen_cv_folds, chosen_grid_search_folds)) in zip(product(['train', 'evaluate', 'predict', 'cross_validate'], [None, 5, join(train_dir, 'folds_file_test.csv')], [None, 7], [None, True, False]), [(None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (10, 5), (10, 5), (10, 5), (10, 7), (10, 7), (10, 7), (5, 5), (5, 5), (5, 5), (5, 7), (5, 7), (5, 7), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 5), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 7)]):
        yield (check_cv_folds_and_grid_search_folds, task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search, chosen_cv_folds, chosen_grid_search_folds)","for ((task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search), (chosen_cv_folds, chosen_grid_search_folds)) in zip(product(['train', 'evaluate', 'predict', 'cross_validate'], [None, 5, join(train_dir, 'folds_file_test.csv')], [None, 7], [None, True, False]), [(None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (10, 5), (10, 5), (10, 5), (10, 7), (10, 7), (10, 7), (5, 5), (5, 5), (5, 5), (5, 7), (5, 7), (5, 7), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 5), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 7)]):
    yield (check_cv_folds_and_grid_search_folds, task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search, chosen_cv_folds, chosen_grid_search_folds)","for i, ((task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search), (chosen_cv_folds, chosen_grid_search_folds)) in enumerate(zip(product(['train', 'evaluate', 'predict', 'cross_validate'], [None, 5, join(train_dir, 'folds_file_test.csv')], [None, 7], [None, True, False]), [(None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (10, 5), (10, 5), (10, 5), (10, 7), (10, 7), (10, 7), (5, 5), (5, 5), (5, 5), (5, 7), (5, 7), (5, 7), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 5), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 7)])):
    yield (check_cv_folds_and_grid_search_folds, task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search, chosen_cv_folds, chosen_grid_search_folds)",1,,,,,,,,,,
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/models/v1beta1_priority_level_configuration_condition.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python/kubernetes/client/models/v1beta1_priority_level_configuration_condition.py,V1beta1PriorityLevelConfigurationCondition,"def to_dict(self):
    """"""Returns the model properties as a dict""""""
    result = {}
    for (attr, _) in six.iteritems(self.openapi_types):
        value = getattr(self, attr)
        if isinstance(value, list):
            result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
        elif hasattr(value, 'to_dict'):
            result[attr] = value.to_dict()
        elif isinstance(value, dict):
            result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
        else:
            result[attr] = value
    return result","for (attr, _) in six.iteritems(self.openapi_types):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value","for i, (attr, _) in enumerate(six.iteritems(self.openapi_types)):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value",1,,,,,,,,,,
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/models/v1alpha1_volume_attachment_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python/kubernetes/client/models/v1alpha1_volume_attachment_status.py,V1alpha1VolumeAttachmentStatus,"def to_dict(self):
    """"""Returns the model properties as a dict""""""
    result = {}
    for (attr, _) in six.iteritems(self.openapi_types):
        value = getattr(self, attr)
        if isinstance(value, list):
            result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
        elif hasattr(value, 'to_dict'):
            result[attr] = value.to_dict()
        elif isinstance(value, dict):
            result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
        else:
            result[attr] = value
    return result","for (attr, _) in six.iteritems(self.openapi_types):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value","for i, (attr, _) in enumerate(six.iteritems(self.openapi_types)):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value",1,,,,,,,,,,
TSD,https://github.com/Sense-X/TSD/tree/master/mmdet/models/detectors/reppoints_detector.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/detectors/reppoints_detector.py,RepPointsDetector,"def merge_aug_results(self, aug_bboxes, aug_scores, img_metas):
    """"""Merge augmented detection bboxes and scores.

        Args:
            aug_bboxes (list[Tensor]): shape (n, 4*#class)
            aug_scores (list[Tensor] or None): shape (n, #class)
            img_shapes (list[Tensor]): shape (3, ).

        Returns:
            tuple: (bboxes, scores)
        """"""
    recovered_bboxes = []
    for (bboxes, img_info) in zip(aug_bboxes, img_metas):
        img_shape = img_info[0]['img_shape']
        scale_factor = img_info[0]['scale_factor']
        flip = img_info[0]['flip']
        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
        recovered_bboxes.append(bboxes)
    bboxes = torch.cat(recovered_bboxes, dim=0)
    if aug_scores is None:
        return bboxes
    else:
        scores = torch.cat(aug_scores, dim=0)
        return (bboxes, scores)","for (bboxes, img_info) in zip(aug_bboxes, img_metas):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)","for i, (bboxes, img_info) in enumerate(zip(aug_bboxes, img_metas)):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)",1,,,,,,,,,,
swift,https://github.com/openstack/swift/tree/master/swift/common/ring/composite_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/common/ring/composite_builder.py,CompositeRingBuilder,"def rebalance(self):
    """"""
        Cooperatively rebalances all component ring builders.

        This method does not change the state of the composite ring; a
        subsequent call to :meth:`compose` is required to generate updated
        composite :class:`RingData`.

        :return: A list of dicts, one per component builder, each having the
            following keys:

            * 'builder_file' maps to the component builder file;
            * 'builder' maps to the corresponding instance of
              :class:`swift.common.ring.builder.RingBuilder`;
            * 'result' maps to the results of the rebalance of that component
              i.e. a tuple of: `(number_of_partitions_altered,
              resulting_balance, number_of_removed_devices)`

            The list has the same order as components in the composite ring.
        :raises RingBuilderError: if there is an error while rebalancing any
            component builder.
        """"""
    self._load_components()
    self.update_last_part_moves()
    component_builders = list(zip(self._builder_files, self._builders))
    shuffle(component_builders)
    results = {}
    for (builder_file, builder) in component_builders:
        try:
            results[builder] = {'builder': builder, 'builder_file': builder_file, 'result': builder.rebalance()}
            builder.validate()
        except RingBuilderError as err:
            self._builders = None
            raise RingBuilderError('An error occurred while rebalancing component %s: %s' % (builder_file, err))
    for (builder_file, builder) in component_builders:
        builder.save(builder_file)
    return [results[builder] for builder in self._builders]","for (builder_file, builder) in component_builders:
    try:
        results[builder] = {'builder': builder, 'builder_file': builder_file, 'result': builder.rebalance()}
        builder.validate()
    except RingBuilderError as err:
        self._builders = None
        raise RingBuilderError('An error occurred while rebalancing component %s: %s' % (builder_file, err))","for i, (builder_file, builder) in enumerate(component_builders):
    try:
        results[builder] = {'builder': builder, 'builder_file': builder_file, 'result': builder.rebalance()}
        builder.validate()
    except RingBuilderError as err:
        self._builders = None
        raise RingBuilderError('An error occurred while rebalancing component %s: %s' % (builder_file, err))",1,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/magnum-plugins/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/magnum-plugins/all/conanfile.py,MagnumConan,"def package(self):
    cm = self._configure_cmake()
    cm.install()
    if not self.options.shared_plugins:
        build_modules_folder = os.path.join(self.package_folder, 'lib', 'cmake')
        os.makedirs(build_modules_folder)
        for (component, target, library, folder, deps) in self._plugins:
            build_module_path = os.path.join(build_modules_folder, 'conan-magnum-plugins-{}.cmake'.format(component))
            with open(build_module_path, 'w+') as f:
                f.write(textwrap.dedent('                        if(NOT ${{CMAKE_VERSION}} VERSION_LESS ""3.0"")\n                            if(TARGET MagnumPlugins::{target})\n                                set_target_properties(MagnumPlugins::{target} PROPERTIES INTERFACE_SOURCES \n                                                    ""${{CMAKE_CURRENT_LIST_DIR}}/../../include/MagnumPlugins/{library}/importStaticPlugin.cpp"")\n                            endif()\n                        endif()\n                    '.format(target=target, library=library)))
    tools.rmdir(os.path.join(self.package_folder, 'share'))
    self.copy('*.cmake', src=os.path.join(self.source_folder, 'cmake'), dst=os.path.join('lib', 'cmake'))
    self.copy('COPYING', src=self._source_subfolder, dst='licenses')","for (component, target, library, folder, deps) in self._plugins:
    build_module_path = os.path.join(build_modules_folder, 'conan-magnum-plugins-{}.cmake'.format(component))
    with open(build_module_path, 'w+') as f:
        f.write(textwrap.dedent('                        if(NOT ${{CMAKE_VERSION}} VERSION_LESS ""3.0"")\n                            if(TARGET MagnumPlugins::{target})\n                                set_target_properties(MagnumPlugins::{target} PROPERTIES INTERFACE_SOURCES \n                                                    ""${{CMAKE_CURRENT_LIST_DIR}}/../../include/MagnumPlugins/{library}/importStaticPlugin.cpp"")\n                            endif()\n                        endif()\n                    '.format(target=target, library=library)))","for i, (component, target, library, folder, deps) in enumerate(self._plugins):
    build_module_path = os.path.join(build_modules_folder, 'conan-magnum-plugins-{}.cmake'.format(component))
    with open(build_module_path, 'w+') as f:
        f.write(textwrap.dedent('                        if(NOT ${{CMAKE_VERSION}} VERSION_LESS ""3.0"")\n                            if(TARGET MagnumPlugins::{target})\n                                set_target_properties(MagnumPlugins::{target} PROPERTIES INTERFACE_SOURCES \n                                                    ""${{CMAKE_CURRENT_LIST_DIR}}/../../include/MagnumPlugins/{library}/importStaticPlugin.cpp"")\n                            endif()\n                        endif()\n                    '.format(target=target, library=library)))",1,,,,,,,,,,
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for (component_name, component_specs) in rte_components.items():
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for i, (component_name, component_specs) in enumerate(rte_components.items()):
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o","for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))","for i, (conf_label, conf) in enumerate((('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf))):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))",1,,,,,,,,,,
ansible,https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_internal/containers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/lib/ansible_test/_internal/containers.py,,"def create_support_container_context(args, ssh, containers):
    """"""Context manager that provides SSH port forwards. Returns updated container metadata.""""""
    host_type = HostType.control
    revised = ContainerDatabase(containers.data.copy())
    source = revised.data.pop(HostType.origin, None)
    container_map = {}
    if host_type not in revised.data:
        if not source:
            raise Exception('Missing origin container details.')
        for (context_name, context) in source.items():
            for (container_name, container) in context.items():
                if '-controller-' in container_name:
                    continue
                for (port, access_port) in container.port_map():
                    container_map[container.host_ip, access_port] = (context_name, container_name, port)
    if not container_map:
        return SupportContainerContext(revised, None)
    if not ssh:
        raise Exception('The %s host was not pre-configured for container access and SSH forwarding is not available.' % host_type)
    forwards = list(container_map.keys())
    process = create_ssh_port_forwards(args, ssh, forwards)
    result = SupportContainerContext(revised, process)
    try:
        port_forwards = process.collect_port_forwards()
        contexts = {}
        for (forward, forwarded_port) in port_forwards.items():
            (access_host, access_port) = forward
            (context_name, container_name, container_port) = container_map[access_host, access_port]
            container = source[context_name][container_name]
            context = contexts.setdefault(context_name, {})
            forwarded_container = context.setdefault(container_name, ContainerAccess('127.0.0.1', container.names, None, {}))
            forwarded_container.forwards[container_port] = forwarded_port
            display.info('Container ""%s"" port %d available at %s:%d is forwarded over SSH as port %d.' % (container_name, container_port, access_host, access_port, forwarded_port), verbosity=1)
        revised.data[host_type] = contexts
        return result
    except Exception:
        result.close()
        raise","for (context_name, context) in source.items():
    for (container_name, container) in context.items():
        if '-controller-' in container_name:
            continue
        for (port, access_port) in container.port_map():
            container_map[container.host_ip, access_port] = (context_name, container_name, port)","for i, (context_name, context) in enumerate(source.items()):
    for j, (container_name, container) in enumerate(context.items()):
        if '-controller-' in container_name:
            continue
        for k, (port, access_port) in enumerate(container.port_map()):
            container_map[container.host_ip, access_port] = (context_name, container_name, port)",1,,,,,,,,,,
softlearning,https://github.com/rail-berkeley/softlearning/tree/master/softlearning/environments/adapters/robosuite_adapter_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/softlearning/softlearning/environments/adapters/robosuite_adapter_test.py,TestRobosuiteAdapter,"def test_environments(self):
    TEST_ENVIRONMENTS = [('Sawyer', 'Lift')]

    def verify_reset_and_step(domain, task):
        env = RobosuiteAdapter(domain=domain, task=task, has_renderer=True, has_offscreen_renderer=True, use_camera_obs=False)
        env.reset()
        env.step(env.action_space.sample())
    for (domain, task) in TEST_ENVIRONMENTS:
        verify_reset_and_step(domain, task)","for (domain, task) in TEST_ENVIRONMENTS:
    verify_reset_and_step(domain, task)","for i, (domain, task) in enumerate(TEST_ENVIRONMENTS):
    verify_reset_and_step(domain, task)",1,,,,,,,,,,
html5lib-python,https://github.com/html5lib/html5lib-python/tree/master/utils/entities.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/html5lib-python/utils/entities.py,,"def make_test_list(entities):
    tests = []
    for (entity_name, characters) in entities.items():
        if entity_name.endswith(';') and (not subentity_exists(entity_name, entities)):
            tests.append((entity_name[:-1], '&' + entity_name[:-1], False))
        tests.append((entity_name, characters, True))
    return sorted(tests)","for (entity_name, characters) in entities.items():
    if entity_name.endswith(';') and (not subentity_exists(entity_name, entities)):
        tests.append((entity_name[:-1], '&' + entity_name[:-1], False))
    tests.append((entity_name, characters, True))","for i, (entity_name, characters) in enumerate(entities.items()):
    if entity_name.endswith(';') and (not subentity_exists(entity_name, entities)):
        tests.append((entity_name[:-1], '&' + entity_name[:-1], False))
    tests.append((entity_name, characters, True))",1,,,,,,,,,,
oj,https://github.com/online-judge-tools/oj/tree/master/onlinejudge_command/subcommand/generate_input.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oj/onlinejudge_command/subcommand/generate_input.py,,"def BufferedExecutor(lock: Optional[threading.Lock]):
    buf: List[Tuple[Callable, List[Any], Dict[str, Any]]] = []

    def submit(f, *args, **kwargs):
        nonlocal buf
        if lock is None:
            f(*args, **kwargs)
        else:
            buf += [(f, args, kwargs)]
    result = (yield submit)
    if lock is not None:
        with lock:
            for (f, args, kwargs) in buf:
                f(*args, **kwargs)
    return result","for (f, args, kwargs) in buf:
    f(*args, **kwargs)","for i, (f, args, kwargs) in enumerate(buf):
    f(*args, **kwargs)",1,,,,,,,,,,
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)","for (img_path, annotation_path) in tqdm(filename_pairs):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)","for i, (img_path, annotation_path) in enumerate(tqdm(filename_pairs)):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)",1,,,,,,,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for (k, v) in cache_keys.items():
    temp_cache_key = v['temp_cache_key']
    red.delete(temp_cache_key)","for i, (k, v) in enumerate(cache_keys.items()):
    temp_cache_key = v['temp_cache_key']
    red.delete(temp_cache_key)",1,,,,,,,,,,
nlp-beginner-finish,https://github.com/Alic-yuan/nlp-beginner-finish/tree/master/task5/dataHandler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-beginner-finish/task5/dataHandler.py,,"if __name__ == '__main__':
    from config import Config
    config = Config()
    (pad_data, char_to_ix, ix_to_chars) = get_data(config)
    for l in pad_data[:10]:
        print(l)
    n = 0
    for (k, v) in char_to_ix.items():
        print(k, v)
        if n > 10:
            break
        n += 1
    n = 0
    for (k, v) in ix_to_chars.items():
        print(k, v)
        if n > 10:
            break
        n += 1","for (k, v) in char_to_ix.items():
    print(k, v)
    if n > 10:
        break
    n += 1","for i, (k, v) in enumerate(char_to_ix.items()):
    print(k, v)
    if i > 10:
        break",1,实际是对的看context之后,,,,,,,,,
joinmarket-clientserver,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmclient/jmclient/wallet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmclient/jmclient/wallet.py,PSBTWalletMixin,"def sign_psbt(self, in_psbt, with_sign_result=False):
    """""" Given a serialized PSBT in raw binary format,
        iterate over the inputs and sign all that we can sign with this wallet.
        NB IT IS UP TO CALLERS TO ENSURE THAT THEY ACTUALLY WANT TO SIGN
        THIS TRANSACTION!
        The above is important especially in coinjoin scenarios.
        Return: (psbt, msg)
        msg: error message or None
        if not `with_sign_result`:
        psbt: signed psbt in binary serialization, or None if error.
        if `with_sign_result` True:
        psbt: (PSBT_SignResult object, psbt (deserialized) object)
        """"""
    try:
        new_psbt = btc.PartiallySignedTransaction.from_binary(in_psbt)
    except Exception as e:
        return (None, 'Unable to deserialize binary PSBT, error: ' + repr(e))
    privkeys = []
    for (k, v) in self._utxos._utxo.items():
        for (k2, v2) in v.items():
            key = self._get_key_from_path(v2[0])
            if FidelityBondMixin.is_timelocked_path(v2[0]) and len(key[0]) == 2:
                key = (key[0][0], key[1])
            privkeys.append(key)
    for vin in new_psbt.inputs:
        try:
            path = self.script_to_path(vin.utxo.scriptPubKey)
            key = self._get_key_from_path(path)
            if key not in privkeys:
                privkeys.append(key)
        except AssertionError:
            continue
        except AttributeError:
            continue
    jmckeys = list((btc.JMCKey(x[0][:-1]) for x in privkeys))
    new_keystore = btc.KeyStore.from_iterable(jmckeys)
    if isinstance(self, SegwitLegacyWallet):
        for (i, txinput) in enumerate(new_psbt.inputs):
            tu = txinput.witness_utxo
            if isinstance(tu, btc.CTxOut):
                if tu.scriptPubKey.is_witness_scriptpubkey():
                    continue
                elif tu.scriptPubKey.is_p2sh():
                    try:
                        path = self.script_to_path(tu.scriptPubKey)
                    except AssertionError:
                        continue
                    (privkey, _) = self._get_key_from_path(path)
                    txinput.redeem_script = btc.pubkey_to_p2wpkh_script(btc.privkey_to_pubkey(privkey))
    try:
        signresult = new_psbt.sign(new_keystore)
    except Exception as e:
        return (None, repr(e))
    if not with_sign_result:
        return (new_psbt.serialize(), None)
    else:
        return ((signresult, new_psbt), None)","for (k2, v2) in v.items():
    key = self._get_key_from_path(v2[0])
    if FidelityBondMixin.is_timelocked_path(v2[0]) and len(key[0]) == 2:
        key = (key[0][0], key[1])
    privkeys.append(key)","for i, (k2, v2) in enumerate(v.items()):
    key = self._get_key_from_path(v2[0])
    if FidelityBondMixin.is_timelocked_path(v2[0]) and len(key[0]) == 2:
        key = (key[0][0], key[1])
    privkeys.append(key)",1,,,,,,,,,,
neural_prophet,https://github.com/ourownstory/neural_prophet/tree/master/neuralprophet/time_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_prophet/neuralprophet/time_dataset.py,TimeDataset,"def __getitem__(self, index):
    """"""Overrides parent class method to get an item at index.

        Args:
            index (int): sample location in dataset

        Returns:
            sample (OrderedDict): model inputs
                time (torch tensor, float), dims: (1)
                seasonalities (OrderedDict), named seasonalities, each with features
                    (torch tensor, float) of dims: (n_features[name])
                lags (torch tensor, float), dims: (n_lags)
                covariates (OrderedDict), named covariates, each with features
                    (np.array, float) of dims: (n_lags)
                events (OrderedDict), all events both additive and multiplicative,
                    each with features (np.array, float) of dims: (n_lags)
                regressors (OrderedDict), all regressors both additive and multiplicative,
                    each with features (np.array, float) of dims: (n_lags)
            targets (torch tensor, float): targets to be predicted, dims: (n_forecasts)
        """"""
    sample = OrderedDict({})
    for (key, data) in self.inputs.items():
        if key in self.two_level_inputs:
            sample[key] = OrderedDict({})
            for (name, period_features) in self.inputs[key].items():
                sample[key][name] = period_features[index]
        elif key == 'events' or key == 'regressors':
            sample[key] = OrderedDict({})
            for (mode, features) in self.inputs[key].items():
                sample[key][mode] = features[index, :, :]
        else:
            sample[key] = data[index]
    targets = self.targets[index]
    return (sample, targets)","for (key, data) in self.inputs.items():
    if key in self.two_level_inputs:
        sample[key] = OrderedDict({})
        for (name, period_features) in self.inputs[key].items():
            sample[key][name] = period_features[index]
    elif key == 'events' or key == 'regressors':
        sample[key] = OrderedDict({})
        for (mode, features) in self.inputs[key].items():
            sample[key][mode] = features[index, :, :]
    else:
        sample[key] = data[index]","for i, (key, data) in enumerate(self.inputs.items()):
    if key in self.two_level_inputs:
        sample[key] = OrderedDict({})
        for (name, period_features) in self.inputs[key].items():
            sample[key][name] = period_features[index]
    elif key == 'events' or key == 'regressors':
        sample[key] = OrderedDict({})
        for (mode, features) in self.inputs[key].items():
            sample[key][mode] = features[index, :, :]
    else:
        sample[key] = data[index]",1,,,,,,,,,,
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break",1,,,,,,,,,,
sentinelsat,https://github.com/sentinelsat/sentinelsat/tree/master/tests/test_docs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentinelsat/tests/test_docs.py,,"def test_rst(rst_file):
    with open(rst_file) as input_file:
        contents = input_file.read()
    all_errors = []
    errors = rstcheck.check(contents, report_level=2, ignore={'languages': ['python', 'bash']})
    for (line_number, error) in errors:
        if 'Title underline too short' in error:
            continue
        m = re.search('Unknown interpreted text role ""([^""]+)""', error)
        if m and m.group(1) in ['program', 'paramref']:
            continue
        m = re.search('Unknown directive type ""([^""]+)""', error)
        if m and m.group(1) in ['automodule']:
            continue
        all_errors.append((line_number, error))
    assert len(all_errors) == 0","for (line_number, error) in errors:
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))","for i, (line_number, error) in enumerate(errors):
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))",1,,,,,,,,,,
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for (marker_type, active_markers) in active_markers_by_type.items():
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, (marker_type, active_markers) in enumerate(active_markers_by_type.items()):
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))",1,,,,,,,,,,
boto3,https://github.com/boto/boto3/tree/master/boto3/resources/model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/boto3/boto3/resources/model.py,ResourceModel,"def _get_related_resources(self, subresources):
    """"""
        Get a list of sub-resources or references.

        :type subresources: bool
        :param subresources: ``True`` to get sub-resources, ``False`` to
                             get references.
        :rtype: list(:py:class:`Action`)
        """"""
    resources = []
    for (name, definition) in self._get_has_definition().items():
        if subresources:
            name = self._get_name('subresource', name, snake_case=False)
        else:
            name = self._get_name('reference', name)
        action = Action(name, definition, self._resource_defs)
        data_required = False
        for identifier in action.resource.identifiers:
            if identifier.source == 'data':
                data_required = True
                break
        if subresources and (not data_required):
            resources.append(action)
        elif not subresources and data_required:
            resources.append(action)
    return resources","for (name, definition) in self._get_has_definition().items():
    if subresources:
        name = self._get_name('subresource', name, snake_case=False)
    else:
        name = self._get_name('reference', name)
    action = Action(name, definition, self._resource_defs)
    data_required = False
    for identifier in action.resource.identifiers:
        if identifier.source == 'data':
            data_required = True
            break
    if subresources and (not data_required):
        resources.append(action)
    elif not subresources and data_required:
        resources.append(action)","for i, (name, definition) in enumerate(self._get_has_definition().items()):
    if subresources:
        name = self._get_name('subresource', name, snake_case=False)
    else:
        name = self._get_name('reference', name)
    action = Action(name, definition, self._resource_defs)
    data_required = False
    for identifier in action.resource.identifiers:
        if identifier.source == 'data':
            data_required = True
            break
    if subresources and (not data_required):
        resources.append(action)
    elif not subresources and data_required:
        resources.append(action)",1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/repository_dependencies/repository_dependency_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/repository_dependencies/repository_dependency_manager.py,RepositoryDependencyInstallManager,"def create_repository_dependency_objects(self, tool_path, tool_shed_url, repo_info_dicts, install_repository_dependencies=False, no_changes_checked=False, tool_panel_section_id=None, new_tool_panel_section_label=None):
    """"""
        Discover all repository dependencies and make sure all tool_shed_repository and
        associated repository_dependency records exist as well as the dependency relationships
        between installed repositories.  This method is called when uninstalled repositories
        are being reinstalled.  If the user elected to install repository dependencies, all
        items in the all_repo_info_dicts list will be processed.  However, if repository
        dependencies are not to be installed, only those items contained in the received
        repo_info_dicts list will be processed.
        """"""
    install_model = self.app.install_model
    log.debug('Creating repository dependency objects...')
    all_created_or_updated_tool_shed_repositories = []
    created_or_updated_tool_shed_repositories = []
    tool_panel_section_keys = []
    filtered_repo_info_dicts = []
    all_required_repo_info_dict = self.get_required_repo_info_dicts(tool_shed_url, repo_info_dicts)
    all_repo_info_dicts = all_required_repo_info_dict.get('all_repo_info_dicts', [])
    if not all_repo_info_dicts:
        all_repo_info_dicts = [rid for rid in repo_info_dicts]
    for repo_info_dict in all_repo_info_dicts:
        if self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts) or install_repository_dependencies:
            for (name, repo_info_tuple) in repo_info_dict.items():
                can_update_db_record = False
                clear_metadata = True
                (description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
                (repository_db_record, installed_changeset_revision) = repository_util.repository_was_previously_installed(self.app, tool_shed_url, name, repo_info_tuple, from_tip=False)
                if repository_db_record:
                    if installed_changeset_revision != changeset_revision and repository_db_record.status == install_model.ToolShedRepository.installation_status.INSTALLED:
                        log.info(""Repository '%s' already present at revision %s, will be updated to revision %s"", repository_db_record.name, installed_changeset_revision, changeset_revision)
                        can_update_db_record = True
                        clear_metadata = False
                    elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.INSTALLED, install_model.ToolShedRepository.installation_status.CLONING, install_model.ToolShedRepository.installation_status.SETTING_TOOL_VERSIONS, install_model.ToolShedRepository.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES, install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES, install_model.ToolShedRepository.installation_status.LOADING_PROPRIETARY_DATATYPES]:
                        info_msg = ""Skipping installation of revision %s of repository '%s' because it was installed "" % (changeset_revision, repository_db_record.name)
                        info_msg += ""with the (possibly updated) revision %s and its current installation status is '%s'."" % (installed_changeset_revision, repository_db_record.status)
                        log.info(info_msg)
                        can_update_db_record = False
                    elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.ERROR, install_model.ToolShedRepository.installation_status.NEW, install_model.ToolShedRepository.installation_status.UNINSTALLED]:
                        name = repository_db_record.name
                        installed_changeset_revision = repository_db_record.installed_changeset_revision
                        can_update_db_record = True
                    elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.DEACTIVATED]:
                        log.info(f""Reactivating deactivated tool_shed_repository '{str(repository_db_record.name)}'."")
                        self.app.installed_repository_manager.activate_repository(repository_db_record)
                        can_update_db_record = False
                    elif repository_db_record.status not in [install_model.ToolShedRepository.installation_status.NEW]:
                        changeset_revision = repository_db_record.installed_changeset_revision
                        self.reset_previously_installed_repository(repository_db_record)
                        can_update_db_record = True
                else:
                    installed_changeset_revision = changeset_revision
                    can_update_db_record = True
                if can_update_db_record:
                    tpm = tool_panel_manager.ToolPanelManager(self.app)
                    if repository_db_record and repository_db_record.metadata_:
                        (_, tool_panel_section_key) = tpm.handle_tool_panel_selection(toolbox=self.app.toolbox, metadata=repository_db_record.metadata_, no_changes_checked=no_changes_checked, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
                    else:
                        (tool_panel_section_key, _) = tpm.handle_tool_panel_section(self.app.toolbox, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
                    metadata_dict = {} if clear_metadata else None
                    current_changeset_revision = changeset_revision if clear_metadata else None
                    tool_shed_repository = repository_util.create_or_update_tool_shed_repository(app=self.app, name=name, description=description, installed_changeset_revision=installed_changeset_revision, ctx_rev=ctx_rev, repository_clone_url=repository_clone_url, status=install_model.ToolShedRepository.installation_status.NEW, metadata_dict=metadata_dict, current_changeset_revision=current_changeset_revision, owner=repository_owner, dist_to_shed=False)
                    if tool_shed_repository not in all_created_or_updated_tool_shed_repositories:
                        all_created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                    if install_repository_dependencies or self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts):
                        if tool_shed_repository not in created_or_updated_tool_shed_repositories:
                            created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                            tool_panel_section_keys.append(tool_panel_section_key)
                            filtered_repo_info_dicts.append(repo_info_dict)
    self.build_repository_dependency_relationships(all_repo_info_dicts, all_created_or_updated_tool_shed_repositories)
    return (created_or_updated_tool_shed_repositories, tool_panel_section_keys, all_repo_info_dicts, filtered_repo_info_dicts)","for (name, repo_info_tuple) in repo_info_dict.items():
    can_update_db_record = False
    clear_metadata = True
    (description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
    (repository_db_record, installed_changeset_revision) = repository_util.repository_was_previously_installed(self.app, tool_shed_url, name, repo_info_tuple, from_tip=False)
    if repository_db_record:
        if installed_changeset_revision != changeset_revision and repository_db_record.status == install_model.ToolShedRepository.installation_status.INSTALLED:
            log.info(""Repository '%s' already present at revision %s, will be updated to revision %s"", repository_db_record.name, installed_changeset_revision, changeset_revision)
            can_update_db_record = True
            clear_metadata = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.INSTALLED, install_model.ToolShedRepository.installation_status.CLONING, install_model.ToolShedRepository.installation_status.SETTING_TOOL_VERSIONS, install_model.ToolShedRepository.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES, install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES, install_model.ToolShedRepository.installation_status.LOADING_PROPRIETARY_DATATYPES]:
            info_msg = ""Skipping installation of revision %s of repository '%s' because it was installed "" % (changeset_revision, repository_db_record.name)
            info_msg += ""with the (possibly updated) revision %s and its current installation status is '%s'."" % (installed_changeset_revision, repository_db_record.status)
            log.info(info_msg)
            can_update_db_record = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.ERROR, install_model.ToolShedRepository.installation_status.NEW, install_model.ToolShedRepository.installation_status.UNINSTALLED]:
            name = repository_db_record.name
            installed_changeset_revision = repository_db_record.installed_changeset_revision
            can_update_db_record = True
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.DEACTIVATED]:
            log.info(f""Reactivating deactivated tool_shed_repository '{str(repository_db_record.name)}'."")
            self.app.installed_repository_manager.activate_repository(repository_db_record)
            can_update_db_record = False
        elif repository_db_record.status not in [install_model.ToolShedRepository.installation_status.NEW]:
            changeset_revision = repository_db_record.installed_changeset_revision
            self.reset_previously_installed_repository(repository_db_record)
            can_update_db_record = True
    else:
        installed_changeset_revision = changeset_revision
        can_update_db_record = True
    if can_update_db_record:
        tpm = tool_panel_manager.ToolPanelManager(self.app)
        if repository_db_record and repository_db_record.metadata_:
            (_, tool_panel_section_key) = tpm.handle_tool_panel_selection(toolbox=self.app.toolbox, metadata=repository_db_record.metadata_, no_changes_checked=no_changes_checked, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        else:
            (tool_panel_section_key, _) = tpm.handle_tool_panel_section(self.app.toolbox, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        metadata_dict = {} if clear_metadata else None
        current_changeset_revision = changeset_revision if clear_metadata else None
        tool_shed_repository = repository_util.create_or_update_tool_shed_repository(app=self.app, name=name, description=description, installed_changeset_revision=installed_changeset_revision, ctx_rev=ctx_rev, repository_clone_url=repository_clone_url, status=install_model.ToolShedRepository.installation_status.NEW, metadata_dict=metadata_dict, current_changeset_revision=current_changeset_revision, owner=repository_owner, dist_to_shed=False)
        if tool_shed_repository not in all_created_or_updated_tool_shed_repositories:
            all_created_or_updated_tool_shed_repositories.append(tool_shed_repository)
        if install_repository_dependencies or self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts):
            if tool_shed_repository not in created_or_updated_tool_shed_repositories:
                created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                tool_panel_section_keys.append(tool_panel_section_key)
                filtered_repo_info_dicts.append(repo_info_dict)","for i, (name, repo_info_tuple) in enumerate(repo_info_dict.items()):
    can_update_db_record = False
    clear_metadata = True
    (description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
    (repository_db_record, installed_changeset_revision) = repository_util.repository_was_previously_installed(self.app, tool_shed_url, name, repo_info_tuple, from_tip=False)
    if repository_db_record:
        if installed_changeset_revision != changeset_revision and repository_db_record.status == install_model.ToolShedRepository.installation_status.INSTALLED:
            log.info(""Repository '%s' already present at revision %s, will be updated to revision %s"", repository_db_record.name, installed_changeset_revision, changeset_revision)
            can_update_db_record = True
            clear_metadata = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.INSTALLED, install_model.ToolShedRepository.installation_status.CLONING, install_model.ToolShedRepository.installation_status.SETTING_TOOL_VERSIONS, install_model.ToolShedRepository.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES, install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES, install_model.ToolShedRepository.installation_status.LOADING_PROPRIETARY_DATATYPES]:
            info_msg = ""Skipping installation of revision %s of repository '%s' because it was installed "" % (changeset_revision, repository_db_record.name)
            info_msg += ""with the (possibly updated) revision %s and its current installation status is '%s'."" % (installed_changeset_revision, repository_db_record.status)
            log.info(info_msg)
            can_update_db_record = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.ERROR, install_model.ToolShedRepository.installation_status.NEW, install_model.ToolShedRepository.installation_status.UNINSTALLED]:
            name = repository_db_record.name
            installed_changeset_revision = repository_db_record.installed_changeset_revision
            can_update_db_record = True
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.DEACTIVATED]:
            log.info(f""Reactivating deactivated tool_shed_repository '{str(repository_db_record.name)}'."")
            self.app.installed_repository_manager.activate_repository(repository_db_record)
            can_update_db_record = False
        elif repository_db_record.status not in [install_model.ToolShedRepository.installation_status.NEW]:
            changeset_revision = repository_db_record.installed_changeset_revision
            self.reset_previously_installed_repository(repository_db_record)
            can_update_db_record = True
    else:
        installed_changeset_revision = changeset_revision
        can_update_db_record = True
    if can_update_db_record:
        tpm = tool_panel_manager.ToolPanelManager(self.app)
        if repository_db_record and repository_db_record.metadata_:
            (_, tool_panel_section_key) = tpm.handle_tool_panel_selection(toolbox=self.app.toolbox, metadata=repository_db_record.metadata_, no_changes_checked=no_changes_checked, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        else:
            (tool_panel_section_key, _) = tpm.handle_tool_panel_section(self.app.toolbox, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        metadata_dict = {} if clear_metadata else None
        current_changeset_revision = changeset_revision if clear_metadata else None
        tool_shed_repository = repository_util.create_or_update_tool_shed_repository(app=self.app, name=name, description=description, installed_changeset_revision=installed_changeset_revision, ctx_rev=ctx_rev, repository_clone_url=repository_clone_url, status=install_model.ToolShedRepository.installation_status.NEW, metadata_dict=metadata_dict, current_changeset_revision=current_changeset_revision, owner=repository_owner, dist_to_shed=False)
        if tool_shed_repository not in all_created_or_updated_tool_shed_repositories:
            all_created_or_updated_tool_shed_repositories.append(tool_shed_repository)
        if install_repository_dependencies or self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts):
            if tool_shed_repository not in created_or_updated_tool_shed_repositories:
                created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                tool_panel_section_keys.append(tool_panel_section_key)
                filtered_repo_info_dicts.append(repo_info_dict)",1,,,,,,,,,,
nodemcu-pyflasher,https://github.com/marcelstoer/nodemcu-pyflasher/tree/master//Main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nodemcu-pyflasher//Main.py,NodeMcuFlasher,"def _get_serial_ports():
    ports = [__auto_select__ + ' ' + __auto_select_explanation__]
    for (port, desc, hwid) in sorted(list_ports.comports()):
        ports.append(port)
    return ports","for (port, desc, hwid) in sorted(list_ports.comports()):
    ports.append(port)","for i, (port, desc, hwid) in enumerate(sorted(list_ports.comports())):
    ports.append(port)",1,,,,,,,,,,
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])","for i, (s, ind, walk_id) in enumerate(zip(succ, sample_index, cur_walk_ids)):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])",1,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]","for (s, p) in zip(new_scores, new_points):
    if s > heap_items[0][0] and p not in in_heap:
        pop = heapq.heapreplace(heap_items, (s, p))
        in_heap.remove(pop[1])
        in_heap.add(p)
        k_last_modify = k","for i, (s, p) in enumerate(zip(new_scores, new_points)):
    if s > heap_items[0][0] and p not in in_heap:
        pop = heapq.heapreplace(heap_items, (s, p))
        in_heap.remove(pop[1])
        in_heap.add(p)
        k_last_modify = k",1,,,,,,,,,,
torchdiffeq,https://github.com/rtqichen/torchdiffeq/tree/master/tests/norm_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchdiffeq/tests/norm_tests.py,TestNorms,"def test_adjoint_norm(self):

    def f(t, x):
        return x
    t = torch.tensor([0.0, 1.0])
    adjoint_params = (torch.rand(7, requires_grad=True), torch.rand((), requires_grad=True))

    def make_spy_on_adjoint_norm(adjoint_norm, actual_norm):
        is_spy_called = [False]

        def spy_on_adjoint_norm(tensor):
            nonlocal is_spy_called
            is_spy_called[0] = True
            norm_result = adjoint_norm(tensor)
            true_norm_result = actual_norm(tensor)
            self.assertIsInstance(norm_result, torch.Tensor)
            self.assertEqual(norm_result.shape, true_norm_result.shape)
            self.assertLess((norm_result - true_norm_result).abs().max(), 1e-06)
            return norm_result
        return (spy_on_adjoint_norm, is_spy_called)
    for shape in ((), (1,), (2, 2)):
        for (use_adjoint_options, seminorm) in ((False, False), (True, False), (True, True)):
            with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
                x0 = torch.full(shape, 1.0)
                if use_adjoint_options:
                    if seminorm:
                        kwargs = dict(adjoint_options=dict(norm='seminorm'))
                    else:
                        kwargs = dict(adjoint_options={})
                else:
                    kwargs = {}
                xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
                _adjoint_norm = xs.grad_fn.adjoint_options['norm']
                is_called = False

                def actual_norm(tensor_tuple):
                    nonlocal is_called
                    is_called = True
                    self.assertIsInstance(tensor_tuple, tuple)
                    (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
                    self.assertEqual(t.shape, ())
                    self.assertEqual(y.shape, shape)
                    self.assertEqual(adj_y.shape, shape)
                    self.assertEqual(adj_param1.shape, (7,))
                    self.assertEqual(adj_param2.shape, ())
                    out = max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt())
                    if not seminorm:
                        out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
                    return out
                (xs.grad_fn.adjoint_options['norm'], is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
                xs.sum().backward()
                self.assertTrue(is_called)
                self.assertTrue(is_spy_called[0])
    for (use_adjoint_options, seminorm) in ((False, False), (True, False), (True, True)):
        with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
            x0 = (torch.tensor(1.0), torch.tensor([[0.5, 0.5], [0.1, 0.1]]))
            if use_adjoint_options:
                if seminorm:
                    kwargs = dict(adjoint_options=dict(norm='seminorm'))
                else:
                    kwargs = dict(adjoint_options={})
            else:
                kwargs = {}
            xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
            adjoint_options_dict = xs[0].grad_fn.next_functions[0][0].next_functions[0][0].adjoint_options
            _adjoint_norm = adjoint_options_dict['norm']
            is_called = False

            def actual_norm(tensor_tuple):
                nonlocal is_called
                is_called = True
                self.assertIsInstance(tensor_tuple, tuple)
                (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
                self.assertEqual(t.shape, ())
                self.assertEqual(y.shape, (5,))
                self.assertEqual(adj_y.shape, (5,))
                self.assertEqual(adj_param1.shape, (7,))
                self.assertEqual(adj_param2.shape, ())
                ya = y[0]
                yb = y[1:]
                adj_ya = adj_y[0]
                adj_yb = adj_y[1:4]
                out = max(t.abs(), ya.abs(), yb.pow(2).mean().sqrt(), adj_ya.abs(), adj_yb.pow(2).mean().sqrt())
                if not seminorm:
                    out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
                return out
            (spy_on_adjoint_norm, is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
            adjoint_options_dict['norm'] = spy_on_adjoint_norm
            xs[0].sum().backward()
            self.assertTrue(is_called)
            self.assertTrue(is_spy_called[0])
    is_called = False

    def adjoint_norm(tensor_tuple):
        nonlocal is_called
        is_called = True
        self.assertIsInstance(tensor_tuple, tuple)
        (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
        self.assertEqual(t.shape, ())
        self.assertEqual(y.shape, ())
        self.assertEqual(adj_y.shape, ())
        self.assertEqual(adj_param1.shape, (7,))
        self.assertEqual(adj_param2.shape, ())
        return max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt(), adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
    x0 = torch.tensor(1.0)
    xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, adjoint_options=dict(norm=adjoint_norm))
    xs.sum().backward()
    self.assertTrue(is_called)
    is_called = False

    def adjoint_norm(tensor_tuple):
        nonlocal is_called
        is_called = True
        self.assertIsInstance(tensor_tuple, tuple)
        (t, ya, yb, adj_ya, adj_yb, adj_param1, adj_param2) = tensor_tuple
        self.assertEqual(t.shape, ())
        self.assertEqual(ya.shape, ())
        self.assertEqual(yb.shape, (2, 2))
        self.assertEqual(adj_ya.shape, ())
        self.assertEqual(adj_yb.shape, (2, 2))
        self.assertEqual(adj_param1.shape, (7,))
        self.assertEqual(adj_param2.shape, ())
        return max(t.abs(), ya.abs(), yb.pow(2).mean().sqrt(), adj_ya.abs(), adj_yb.pow(2).mean().sqrt(), adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
    x0 = (torch.tensor(1.0), torch.tensor([[0.5, 0.5], [0.1, 0.1]]))
    xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, adjoint_options=dict(norm=adjoint_norm))
    xs[0].sum().backward()
    self.assertTrue(is_called)","for (use_adjoint_options, seminorm) in ((False, False), (True, False), (True, True)):
    with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
        x0 = torch.full(shape, 1.0)
        if use_adjoint_options:
            if seminorm:
                kwargs = dict(adjoint_options=dict(norm='seminorm'))
            else:
                kwargs = dict(adjoint_options={})
        else:
            kwargs = {}
        xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
        _adjoint_norm = xs.grad_fn.adjoint_options['norm']
        is_called = False

        def actual_norm(tensor_tuple):
            nonlocal is_called
            is_called = True
            self.assertIsInstance(tensor_tuple, tuple)
            (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
            self.assertEqual(t.shape, ())
            self.assertEqual(y.shape, shape)
            self.assertEqual(adj_y.shape, shape)
            self.assertEqual(adj_param1.shape, (7,))
            self.assertEqual(adj_param2.shape, ())
            out = max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt())
            if not seminorm:
                out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
            return out
        (xs.grad_fn.adjoint_options['norm'], is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
        xs.sum().backward()
        self.assertTrue(is_called)
        self.assertTrue(is_spy_called[0])","for i, (use_adjoint_options, seminorm) in enumerate(((False, False), (True, False), (True, True))):
    with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
        x0 = torch.full(shape, 1.0)
        if use_adjoint_options:
            if seminorm:
                kwargs = dict(adjoint_options=dict(norm='seminorm'))
            else:
                kwargs = dict(adjoint_options={})
        else:
            kwargs = {}
        xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
        _adjoint_norm = xs.grad_fn.adjoint_options['norm']
        is_called = False

        def actual_norm(tensor_tuple):
            nonlocal is_called
            self.assertIsInstance(tensor_tuple, tuple)
            (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
            self.assertEqual(t.shape, ())
            self.assertEqual(y.shape, shape)
            self.assertEqual(adj_y.shape, shape)
            self.assertEqual(adj_param1.shape, (7,))
            self.assertEqual(adj_param2.shape, ())
            out = max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt())
            if not seminorm:
                out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
            is_called = True
            return out
        (xs.grad_fn.adjoint_options['norm'], is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
        xs.sum().backward()
        self.assertTrue(is_called)
        self.assertTrue(is_spy_called[0])",1,,,,,,,,,,
python-shortcuts,https://github.com/alexander-akhmetov/python-shortcuts/tree/master/shortcuts/loader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-shortcuts/shortcuts/loader.py,WFVariableStringField,"def deserialized_data(self) -> str:
    """"""
        Raises:
            shortcuts.exceptions.UnknownVariableError: if variable's type is not supported
        """"""
    value = self._data['Value']
    value_string = value['string']
    positions = {}
    supported_types = list(SYSTEM_VARIABLES_TYPE_TO_VAR.keys()) + ['Variable']
    for (variable_range, variable_data) in value['attachmentsByRange'].items():
        if variable_data['Type'] not in supported_types:
            raise exceptions.UnknownVariableError(f""Unknown variable type: {variable_data['Type']} (possibly it is a magic variable)"")
        variable_type = variable_data['Type']
        if variable_type == 'Variable':
            variable_name = variable_data['VariableName']
        elif variable_type in SYSTEM_VARIABLES_TYPE_TO_VAR:
            variable_name = SYSTEM_VARIABLES_TYPE_TO_VAR[variable_type]
        position = self._get_position(variable_range)
        positions[position] = '{{%s}}' % variable_name
    offset = 0
    for (pos, variable) in collections.OrderedDict(sorted(positions.items())).items():
        value_string = value_string[:pos + offset] + variable + value_string[pos + offset:]
        offset += len(variable)
    return value_string","for (variable_range, variable_data) in value['attachmentsByRange'].items():
    if variable_data['Type'] not in supported_types:
        raise exceptions.UnknownVariableError(f""Unknown variable type: {variable_data['Type']} (possibly it is a magic variable)"")
    variable_type = variable_data['Type']
    if variable_type == 'Variable':
        variable_name = variable_data['VariableName']
    elif variable_type in SYSTEM_VARIABLES_TYPE_TO_VAR:
        variable_name = SYSTEM_VARIABLES_TYPE_TO_VAR[variable_type]
    position = self._get_position(variable_range)
    positions[position] = '{{%s}}' % variable_name","for i, (variable_range, variable_data) in enumerate(value['attachmentsByRange'].items()):
    if variable_data['Type'] not in supported_types:
        raise exceptions.UnknownVariableError(f""Unknown variable type: {variable_data['Type']} (possibly it is a magic variable)"")
    variable_type = variable_data['Type']
    if variable_type == 'Variable':
        variable_name = variable_data['VariableName']
    elif variable_type in SYSTEM_VARIABLES_TYPE_TO_VAR:
        variable_name = SYSTEM_VARIABLES_TYPE_TO_VAR[variable_type]
    position = self._get_position(variable_range)
    positions[position] = '{{%s}}' % variable_name",1,,,,,,,,,,
PythonRobotics,https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/AStar/a_star_variants.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/PathPlanning/AStar/a_star_variants.py,,"def main():
    obs_dict = {}
    for i in range(51):
        for j in range(51):
            obs_dict[i, j] = False
    (o_x, o_y) = ([], [])
    s_x = 5.0
    s_y = 5.0
    g_x = 35.0
    g_y = 45.0
    draw_vertical_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_vertical_line(48, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 48, 50, o_x, o_y, obs_dict)
    all_x = [10, 10, 10, 15, 20, 20, 30, 30, 35, 30, 40, 45]
    all_y = [10, 30, 45, 20, 5, 40, 10, 40, 5, 40, 10, 25]
    all_len = [10, 10, 5, 10, 10, 5, 20, 10, 25, 10, 35, 15]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_vertical_line(x, y, l, o_x, o_y, obs_dict)
    (all_x[:], all_y[:], all_len[:]) = ([], [], [])
    all_x = [35, 40, 15, 10, 45, 20, 10, 15, 25, 45, 10, 30, 10, 40]
    all_y = [5, 10, 15, 20, 20, 25, 30, 35, 35, 35, 40, 40, 45, 45]
    all_len = [10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 5]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)
    if show_animation:
        plt.plot(o_x, o_y, '.k')
        plt.plot(s_x, s_y, 'og')
        plt.plot(g_x, g_y, 'xb')
        plt.grid(True)
    if use_jump_point:
        keypoint_list = key_points(obs_dict)
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101, keypoint_list)
        search_obj.jump_point()
    else:
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101)
        search_obj.a_star()","for (x, y, l) in zip(all_x, all_y, all_len):
    draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)","for i, (x, y, l) in enumerate(zip(all_x, all_y, all_len)):
    draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)",1,,,,,,,,,,
atomic,https://github.com/projectatomic/atomic/tree/master/Atomic/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/atomic/Atomic/util.py,,"def have_match_registry(fq_name, reg_config):
    search_obj = fq_name
    for _ in fq_name.split('/'):
        if search_obj in reg_config:
            return reg_config[search_obj]
        search_obj = search_obj.rsplit('/', 1)[0]
    return None","for _ in fq_name.split('/'):
    if search_obj in reg_config:
        return reg_config[search_obj]
    search_obj = search_obj.rsplit('/', 1)[0]","for i, _ in enumerate(fq_name.split('/')):
    if search_obj in reg_config:
        return reg_config[search_obj]
    search_obj = search_obj.rsplit('/', 1)[0]",1,,,,,,,,,,
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master/Target Offer/带锁的门.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython/Target Offer/带锁的门.py,,"def openDoor(n):
    if n == None or n <= 0:
        return
    doorList = [0] * (n + 1)
    for i in range(1, len(doorList)):
        j = i
        while j <= n:
            doorList[j] = 1 - doorList[j]
            j += i
    output = [i for (i, x) in enumerate(doorList) if x != 0]
    return output","for i in range(1, len(doorList)):
    j = i
    while j <= n:
        doorList[j] = 1 - doorList[j]
        j += i","for i, _ in enumerate(range(1, len(doorList)), start=1):
    j = i
    while j <= n:
        doorList[j] = 1 - doorList[j]
        j += i",1,,,,,,,,,,
listenbrainz-server,https://github.com/metabrainz/listenbrainz-server/tree/master/listenbrainz/tests/integration/test_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/listenbrainz-server/listenbrainz/tests/integration/test_api.py,APITestCase,"def test_get_listens_order(self):
    """""" Test to make sure that the api sends listens in valid order.
        """"""
    with open(self.path_to_data_file('valid_single.json'), 'r') as f:
        payload = json.load(f)
    ts = 1400000000
    user = db_user.get_or_create(1, 'test_order')
    for i in range(3):
        payload['payload'][0]['listened_at'] = ts + 100 * i
        response = self.send_data(payload, user)
        self.assert200(response)
        self.assertEqual(response.json['status'], 'ok')
    expected_count = 3
    url = url_for('api_v1.get_listens', user_name=user['musicbrainz_id'])
    response = self.wait_for_query_to_have_items(url, expected_count)
    data = json.loads(response.data)['payload']
    self.assert200(response)
    self.assertEqual(data['count'], expected_count)
    self.assertEqual(data['listens'][0]['listened_at'], 1400000200)
    self.assertEqual(data['listens'][1]['listened_at'], 1400000100)
    self.assertEqual(data['listens'][2]['listened_at'], 1400000000)
    url = url_for('api_v1.get_listens', user_name=self.user['musicbrainz_id'])
    response = self.client.get(url, query_string={'count': '3', 'from_ts': ts - 500})
    self.assert200(response)
    data = json.loads(response.data)['payload']
    self.assertEqual(data['count'], expected_count)
    self.assertEqual(data['listens'][0]['listened_at'], 1400000200)
    self.assertEqual(data['listens'][1]['listened_at'], 1400000100)
    self.assertEqual(data['listens'][2]['listened_at'], 1400000000)","for i in range(3):
    payload['payload'][0]['listened_at'] = ts + 100 * i
    response = self.send_data(payload, user)
    self.assert200(response)
    self.assertEqual(response.json['status'], 'ok')","for i, _ in enumerate(range(3)):
    payload['payload'][0]['listened_at'] = ts + 100 * i
    response = self.send_data(payload, user)
    self.assert200(response)
    self.assertEqual(response.json['status'], 'ok')",1,,,,,,,,,,
fairscale,https://github.com/facebookresearch/fairscale/tree/master/tests/optim/test_single_node_adascale.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairscale/tests/optim/test_single_node_adascale.py,,"def run_a_bit(replay_data=None):
    data = []
    replay_data_idx = 0
    for _ in range(6):
        for i in range(num_grads_to_accum):
            if replay_data is None:
                in_data = torch.rand(in_dim).cuda()
                data.append(in_data)
            else:
                in_data = replay_data[replay_data_idx]
                replay_data_idx += 1
            out = model(in_data)
            out.sum().backward()
            if i == num_grads_to_accum - 1:
                optim.step()
                optim.zero_grad()
    return (out, data)","for i in range(num_grads_to_accum):
    if replay_data is None:
        in_data = torch.rand(in_dim).cuda()
        data.append(in_data)
    else:
        in_data = replay_data[replay_data_idx]
        replay_data_idx += 1
    out = model(in_data)
    out.sum().backward()
    if i == num_grads_to_accum - 1:
        optim.step()
        optim.zero_grad()","for i, _ in enumerate(range(num_grads_to_accum)):
    if replay_data is None:
        in_data = torch.rand(in_dim).cuda()
        data.append(in_data)
    else:
        in_data = replay_data[replay_data_idx]
        replay_data_idx += 1
    out = model(in_data)
    out.sum().backward()
    if i == num_grads_to_accum - 1:
        optim.step()
        optim.zero_grad()",1,,,,,,,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/core/layers_with_gpipe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/core/layers_with_gpipe.py,GPipeBatchMajorTransformerStack,"def __init__(self, params):
    p = params.Copy()
    num_layers = p.num_encoder_layers + p.num_decoder_layers
    if isinstance(p.splits, (list, tuple)):
        assert p.splits[-1] == num_layers
        for (i, j) in zip(p.splits[:-1], p.splits[1:]):
            assert i <= j, 'Splits must be in increasing order.'
    else:
        num_splits = p.splits
        layers_per_split = (num_layers - 1) // num_splits + 1
        p.splits = []
        for i in range(num_splits):
            p.splits.append((i + 1) * layers_per_split)
        p.splits[-1] = num_layers
    p.state_dtype = p.dtype
    if p.fprop_dtype:
        p.state_dtype = p.fprop_dtype
    transformers = []
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        p.emb_tpl.dropout_tpl = layers.DeterministicDropoutLayer.Params()
    p.emb_tpl.packed_input = p.packed_input
    p.emb_tpl.add_tgt_embedding_layer = p.num_decoder_layers > 0
    p.emb_tpl.name = 'emb'
    transformers.append(p.emb_tpl)
    if p.softmax_tpl:
        p.softmax_tpl.name = 'softmax'
        p.softmax_tpl.inputs_from_decoder = p.num_decoder_layers > 0
    for i in range(p.num_encoder_layers):
        params = p.encoder_tpl.Copy()
        params.name = 'encoder_%d' % i
        if i == p.num_encoder_layers - 1:
            params.output_layer_norm = True
        params.packed_input = p.packed_input
        if len(p.splits) > 1 or p.num_micro_batches > 1:
            params = params.cls.SetupDeterministicDropout(params)
        assert not params.has_aux_atten
        transformers.append(params)
    for i in range(p.num_decoder_layers):
        params = p.decoder_tpl.Copy()
        params.name = 'decoder_%d' % i
        params.mask_self_atten = True
        if i == p.num_decoder_layers - 1:
            params.output_layer_norm = True
        params.packed_input = p.packed_input
        if len(p.splits) > 1 or p.num_micro_batches > 1:
            params = params.cls.SetupDeterministicDropout(params)
        assert params.has_aux_atten
        transformers.append(params)
    cells = []
    cell_start = 0
    offset = 1
    for (split, cell_end) in enumerate(p.splits):
        sub = transformers[cell_start:cell_end + offset]
        if split == len(p.splits) - 1 and p.softmax_tpl:
            sub.append(p.softmax_tpl)
        cell = FeatureExtractionLayer.Params().Set(name='cell_{}'.format(split), sub=sub)
        cells.append(cell)
        cell_start = cell_end + offset
    p.cell_tpl = cells
    super().__init__(p)
    if p.label_smoothing:
        self.CreateChild('smoother', p.label_smoothing)","for i in range(p.num_encoder_layers):
    params = p.encoder_tpl.Copy()
    params.name = 'encoder_%d' % i
    if i == p.num_encoder_layers - 1:
        params.output_layer_norm = True
    params.packed_input = p.packed_input
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        params = params.cls.SetupDeterministicDropout(params)
    assert not params.has_aux_atten
    transformers.append(params)","for i, _ in enumerate(range(p.num_encoder_layers)):
    params = p.encoder_tpl.Copy()
    params.name = 'encoder_%d' % i
    if i == p.num_encoder_layers - 1:
        params.output_layer_norm = True
    params.packed_input = p.packed_input
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        params = params.cls.SetupDeterministicDropout(params)
    assert not params.has_aux_atten
    transformers.append(params)",1,,,,,,,,,,
pytorch-pose-hg-3d,https://github.com/xingyizhou/pytorch-pose-hg-3d/tree/master/src/lib/datasets/mpii.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-pose-hg-3d/src/lib/datasets/mpii.py,MPII,"def convert_eval_format(self, pred, conf, meta):
    ret = np.zeros((pred.shape[0], pred.shape[1], 2))
    for i in range(pred.shape[0]):
        ret[i] = transform_preds(pred[i], meta['center'][i].numpy(), meta['scale'][i].numpy(), [self.opt.output_h, self.opt.output_w])
    return ret","for i in range(pred.shape[0]):
    ret[i] = transform_preds(pred[i], meta['center'][i].numpy(), meta['scale'][i].numpy(), [self.opt.output_h, self.opt.output_w])","for i, _ in enumerate(range(pred.shape[0])):
    ret[i] = transform_preds(pred[i], meta['center'][i].numpy(), meta['scale'][i].numpy(), [self.opt.output_h, self.opt.output_w])",1,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])","for i in range(RUN_STEP):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)","for i, _ in enumerate(range(RUN_STEP)):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)",1,,,,,,,,,,
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/data/transform/operators.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/ppdet/data/transform/operators.py,CropWithSampling,"def apply(self, sample, context):
    """"""
        Crop the image and modify bounding box.
        Operators:
            1. Scale the image width and height.
            2. Crop the image according to a radom sample.
            3. Rescale the bounding box.
            4. Determine if the new bbox is satisfied in the new image.
        Returns:
            sample: the image, bounding box are replaced.
        """"""
    assert 'image' in sample, 'image data not found'
    im = sample['image']
    gt_bbox = sample['gt_bbox']
    gt_class = sample['gt_class']
    (im_height, im_width) = im.shape[:2]
    gt_score = None
    if 'gt_score' in sample:
        gt_score = sample['gt_score']
    sampled_bbox = []
    gt_bbox = gt_bbox.tolist()
    for sampler in self.batch_sampler:
        found = 0
        for i in range(sampler[1]):
            if found >= sampler[0]:
                break
            sample_bbox = generate_sample_bbox(sampler)
            if satisfy_sample_constraint(sampler, sample_bbox, gt_bbox, self.satisfy_all):
                sampled_bbox.append(sample_bbox)
                found = found + 1
    im = np.array(im)
    while sampled_bbox:
        idx = int(np.random.uniform(0, len(sampled_bbox)))
        sample_bbox = sampled_bbox.pop(idx)
        sample_bbox = clip_bbox(sample_bbox)
        (crop_bbox, crop_class, crop_score) = filter_and_process(sample_bbox, gt_bbox, gt_class, scores=gt_score)
        if self.avoid_no_bbox:
            if len(crop_bbox) < 1:
                continue
        xmin = int(sample_bbox[0] * im_width)
        xmax = int(sample_bbox[2] * im_width)
        ymin = int(sample_bbox[1] * im_height)
        ymax = int(sample_bbox[3] * im_height)
        im = im[ymin:ymax, xmin:xmax]
        sample['image'] = im
        sample['gt_bbox'] = crop_bbox
        sample['gt_class'] = crop_class
        sample['gt_score'] = crop_score
        return sample
    return sample","for i in range(sampler[1]):
    if found >= sampler[0]:
        break
    sample_bbox = generate_sample_bbox(sampler)
    if satisfy_sample_constraint(sampler, sample_bbox, gt_bbox, self.satisfy_all):
        sampled_bbox.append(sample_bbox)
        found = found + 1","for i, _ in enumerate(range(sampler[1])):
    if found >= sampler[0]:
        break
    sample_bbox = generate_sample_bbox(sampler)
    if satisfy_sample_constraint(sampler, sample_bbox, gt_bbox, self.satisfy_all):
        sampled_bbox.append(sample_bbox)
        found = found + 1",1,,,,,,,,,,
pororo,https://github.com/kakaobrain/pororo/tree/master/pororo/tasks/image_captioning.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pororo/pororo/tasks/image_captioning.py,PororoCaptionBrainCaption,"def _generate(self, features, boxes, caption, caption_mask):
    """"""
        Generate caption using decoding steps

        Args:
            features (torch.tensor): image feature tensor
            boxes (torch.tensor): bounding box features
            caption (torch.tensor): dummy caption template
            caption_mask (torch.tensor): mask template

        Returns:
            torch.tensor : generate token tensor

        """"""
    for i in range(self._max_len - 1):
        pred = self._generator(features, boxes, caption, caption_mask)
        pred = pred[:, i, :]
        pred_id = torch.argmax(pred, axis=-1)
        if pred_id[0] == self._end_token:
            return caption
        caption[:, i + 1] = pred_id[0]
        caption_mask[:, i + 1] = False
    return caption","for i in range(self._max_len - 1):
    pred = self._generator(features, boxes, caption, caption_mask)
    pred = pred[:, i, :]
    pred_id = torch.argmax(pred, axis=-1)
    if pred_id[0] == self._end_token:
        return caption
    caption[:, i + 1] = pred_id[0]
    caption_mask[:, i + 1] = False","for i, _ in enumerate(range(self._max_len - 1)):
    pred = self._generator(features, boxes, caption, caption_mask)
    pred = pred[:, i, :]
    pred_id = torch.argmax(pred, axis=-1)
    if pred_id[0] == self._end_token:
        return caption
    caption[:, i + 1] = pred_id[0]
    caption_mask[:, i + 1] = False",1,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/datasets/coco_hpe1_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/datasets/coco_hpe1_dataset.py,CocoHpe1Dataset,"def _check_load_keypoints(self, coco, entry):
    """"""
        Check and load ground-truth keypoints.
        """"""
    ann_ids = coco.getAnnIds(imgIds=entry['id'], iscrowd=False)
    objs = coco.loadAnns(ann_ids)
    valid_objs = []
    width = entry['width']
    height = entry['height']
    for obj in objs:
        contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
        if contiguous_cid >= self.num_class:
            continue
        if max(obj['keypoints']) == 0:
            continue
        (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
        if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
            continue
        joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
        for i in range(self.num_joints):
            joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
            joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
            visible = min(1, obj['keypoints'][i * 3 + 2])
            joints_3d[i, :2, 1] = visible
        if np.sum(joints_3d[:, 0, 1]) < 1:
            continue
        if self._check_centers:
            (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
            (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
            ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
            if num_vis / 80.0 + 47 / 80.0 > ks:
                continue
        valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})
    if not valid_objs:
        if not self._skip_empty:
            valid_objs.append({'bbox': np.array([-1, -1, 0, 0]), 'joints_3d': np.zeros((self.num_joints, 3, 2), dtype=np.float32)})
    return valid_objs","for i in range(self.num_joints):
    joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
    joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
    visible = min(1, obj['keypoints'][i * 3 + 2])
    joints_3d[i, :2, 1] = visible","for i, _ in enumerate(range(self.num_joints)):
    joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
    joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
    visible = min(1, obj['keypoints'][i * 3 + 2])
    joints_3d[i, :2, 1] = visible",1,,,,,,,,,,
hmmlearn,https://github.com/hmmlearn/hmmlearn/tree/master/lib/hmmlearn/tests/test_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hmmlearn/lib/hmmlearn/tests/test_base.py,TestMonitor,"def test_report(self, capsys):
    n_iter = 10
    m = ConvergenceMonitor(tol=0.001, n_iter=n_iter, verbose=True)
    for i in reversed(range(n_iter)):
        m.report(-0.01 * i)
    (out, err) = capsys.readouterr()
    assert not out
    assert len(err.splitlines()) == n_iter
    assert len(m.history) == n_iter","for i in reversed(range(n_iter)):
    m.report(-0.01 * i)","for i, _ in enumerate(reversed(range(n_iter))):
    m.report(-0.01 * i)",1,,,,,,,,,,
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/ClientGUIDialogs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/gui/ClientGUIDialogs.py,DialogGenerateNewAccounts,"def __init__(self, parent, service_key):
    Dialog.__init__(self, parent, 'configure new accounts')
    self._service_key = service_key
    self._num = QP.MakeQSpinBox(self, min=1, max=10000, width=80)
    self._account_types = ClientGUICommon.BetterChoice(self)
    self._lifetime = ClientGUICommon.BetterChoice(self)
    self._ok = QW.QPushButton('OK', self)
    self._ok.clicked.connect(self.EventOK)
    self._ok.setObjectName('HydrusAccept')
    self._cancel = QW.QPushButton('Cancel', self)
    self._cancel.clicked.connect(self.reject)
    self._cancel.setObjectName('HydrusCancel')
    self._num.setValue(1)
    service = HG.client_controller.services_manager.GetService(service_key)
    response = service.Request(HC.GET, 'account_types')
    account_types = response['account_types']
    for account_type in account_types:
        self._account_types.addItem(account_type.GetTitle(), account_type)
    self._account_types.setCurrentIndex(0)
    for (s, value) in HC.lifetimes:
        self._lifetime.addItem(s, value)
    self._lifetime.setCurrentIndex(3)
    ctrl_box = QP.HBoxLayout()
    QP.AddToLayout(ctrl_box, ClientGUICommon.BetterStaticText(self, 'generate'), CC.FLAGS_CENTER_PERPENDICULAR)
    QP.AddToLayout(ctrl_box, self._num, CC.FLAGS_CENTER_PERPENDICULAR)
    QP.AddToLayout(ctrl_box, self._account_types, CC.FLAGS_CENTER_PERPENDICULAR)
    QP.AddToLayout(ctrl_box, ClientGUICommon.BetterStaticText(self, 'accounts, to expire in'), CC.FLAGS_CENTER_PERPENDICULAR)
    QP.AddToLayout(ctrl_box, self._lifetime, CC.FLAGS_CENTER_PERPENDICULAR)
    b_box = QP.HBoxLayout()
    QP.AddToLayout(b_box, self._ok, CC.FLAGS_CENTER_PERPENDICULAR)
    QP.AddToLayout(b_box, self._cancel, CC.FLAGS_CENTER_PERPENDICULAR)
    vbox = QP.VBoxLayout()
    QP.AddToLayout(vbox, ctrl_box, CC.FLAGS_EXPAND_SIZER_PERPENDICULAR)
    QP.AddToLayout(vbox, b_box, CC.FLAGS_ON_RIGHT)
    self.setLayout(vbox)
    size_hint = self.sizeHint()
    QP.SetInitialSize(self, size_hint)
    ClientGUIFunctions.SetFocusLater(self._ok)","for account_type in account_types:
    self._account_types.addItem(account_type.GetTitle(), account_type)","for i, account_type in enumerate(account_types):
    self._account_types.addItem(account_type.GetTitle(), account_type)",1,,,,,,,,,,
Tuxemon,https://github.com/Tuxemon/Tuxemon/tree/master/tuxemon/states/combat/combat.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tuxemon/tuxemon/states/combat/combat.py,CombatState,"def remove_monster_actions_from_queue(self, monster: Monster) -> None:
    """"""
        Remove all queued actions for a particular monster.

        This is used mainly for removing actions after monster is fainted.

        Parameters:
            monster: Monster whose actions will be removed.

        """"""
    to_remove = set()
    for action in self._action_queue:
        if action.user is monster or action.target is monster:
            to_remove.add(action)
    for action in to_remove:
        self._action_queue.remove(action)","for action in self._action_queue:
    if action.user is monster or action.target is monster:
        to_remove.add(action)","for i, action in enumerate(self._action_queue):
    if action.user is monster or action.target is monster:
        to_remove.add(action)",1,,,,,,,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/policy.py,LambdaMode,"def run_resource_set(self, event, resources):
    from c7n.actions import EventAction
    with self.policy.ctx:
        self.policy.ctx.metrics.put_metric('ResourceCount', len(resources), 'Count', Scope='Policy', buffer=False)
        if 'debug' in event:
            self.policy.log.info('Invoking actions %s', self.policy.resource_manager.actions)
        self.policy._write_file('resources.json', utils.dumps(resources, indent=2))
        for action in self.policy.resource_manager.actions:
            self.policy.log.info('policy:%s invoking action:%s resources:%d', self.policy.name, action.name, len(resources))
            if isinstance(action, EventAction):
                results = action.process(resources, event)
            else:
                results = action.process(resources)
            self.policy._write_file('action-%s' % action.name, utils.dumps(results))
    return resources","for action in self.policy.resource_manager.actions:
    self.policy.log.info('policy:%s invoking action:%s resources:%d', self.policy.name, action.name, len(resources))
    if isinstance(action, EventAction):
        results = action.process(resources, event)
    else:
        results = action.process(resources)
    self.policy._write_file('action-%s' % action.name, utils.dumps(results))","for i, action in enumerate(self.policy.resource_manager.actions):
    self.policy.log.info('policy:%s invoking action:%s resources:%d', self.policy.name, action.name, len(resources))
    if isinstance(action, EventAction):
        results = action.process(resources, event)
    else:
        results = action.process(resources)
    self.policy._write_file('action-%s' % action.name, utils.dumps(results))",1,,,,,,,,,,
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ufoLib/glifLib.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ufoLib/glifLib.py,,"def _readGlyphFromTreeFormat2(tree, glyphObject=None, pointPen=None, validate=None, formatMinor=0):
    _readName(glyphObject, tree, validate)
    unicodes = []
    guidelines = []
    anchors = []
    haveSeenAdvance = haveSeenImage = haveSeenOutline = haveSeenLib = haveSeenNote = False
    identifiers = set()
    for element in tree:
        if element.tag == 'outline':
            if validate:
                if haveSeenOutline:
                    raise GlifLibError('The outline element occurs more than once.')
                if element.attrib:
                    raise GlifLibError('The outline element contains unknown attributes.')
                if element.text and element.text.strip() != '':
                    raise GlifLibError('Invalid outline structure.')
            haveSeenOutline = True
            if pointPen is not None:
                buildOutlineFormat2(glyphObject, pointPen, element, identifiers, validate)
        elif glyphObject is None:
            continue
        elif element.tag == 'advance':
            if validate and haveSeenAdvance:
                raise GlifLibError('The advance element occurs more than once.')
            haveSeenAdvance = True
            _readAdvance(glyphObject, element)
        elif element.tag == 'unicode':
            try:
                v = element.get('hex')
                v = int(v, 16)
                if v not in unicodes:
                    unicodes.append(v)
            except ValueError:
                raise GlifLibError('Illegal value for hex attribute of unicode element.')
        elif element.tag == 'guideline':
            if validate and len(element):
                raise GlifLibError('Unknown children in guideline element.')
            attrib = dict(element.attrib)
            for attr in ('x', 'y', 'angle'):
                if attr in attrib:
                    attrib[attr] = _number(attrib[attr])
            guidelines.append(attrib)
        elif element.tag == 'anchor':
            if validate and len(element):
                raise GlifLibError('Unknown children in anchor element.')
            attrib = dict(element.attrib)
            for attr in ('x', 'y'):
                if attr in element.attrib:
                    attrib[attr] = _number(attrib[attr])
            anchors.append(attrib)
        elif element.tag == 'image':
            if validate:
                if haveSeenImage:
                    raise GlifLibError('The image element occurs more than once.')
                if len(element):
                    raise GlifLibError('Unknown children in image element.')
            haveSeenImage = True
            _readImage(glyphObject, element, validate)
        elif element.tag == 'note':
            if validate and haveSeenNote:
                raise GlifLibError('The note element occurs more than once.')
            haveSeenNote = True
            _readNote(glyphObject, element)
        elif element.tag == 'lib':
            if validate and haveSeenLib:
                raise GlifLibError('The lib element occurs more than once.')
            haveSeenLib = True
            _readLib(glyphObject, element, validate)
        else:
            raise GlifLibError('Unknown element in GLIF: %s' % element)
    if unicodes:
        _relaxedSetattr(glyphObject, 'unicodes', unicodes)
    if guidelines:
        if validate and (not guidelinesValidator(guidelines, identifiers)):
            raise GlifLibError('The guidelines are improperly formatted.')
        _relaxedSetattr(glyphObject, 'guidelines', guidelines)
    if anchors:
        if validate and (not anchorsValidator(anchors, identifiers)):
            raise GlifLibError('The anchors are improperly formatted.')
        _relaxedSetattr(glyphObject, 'anchors', anchors)","for attr in ('x', 'y'):
    if attr in element.attrib:
        attrib[attr] = _number(attrib[attr])","for i, attr in enumerate(('x', 'y')):
    if attr in element.attrib:
        attrib[attr] = _number(attrib[attr])",1,,,,,,,,,,
rewriting,https://github.com/davidbau/rewriting/tree/master/utils/tally.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/utils/tally.py,,"def tally_topk_and_quantile(compute, dataset, sample_size=None, batch_size=10, k=100, r=4096, cachefile=None, **kwargs):
    """"""
    Computes both topk and quantile statistics in one pass over the
    data.  The compute function should return a pair (first for topk
    and second for quantile stats).
    """"""
    args = dict(sample_size=sample_size, k=k, r=r)
    cached_state = load_cached_state(cachefile, args)
    cs = CombinedState(state=cached_state, rtk=runningstats.RunningTopK(k=k), rq=runningstats.RunningQuantile(r=r))
    if cached_state is not None:
        return (cs.rtx, cs.rq)
    loader = make_loader(dataset, sample_size, batch_size, **kwargs)
    for batch in pbar(loader):
        (sample_tk, sample_q) = call_compute(compute, batch)
        cs.rtk.add(sample_tk)
        cs.rq.add(sample_q)
    cs.rtk.to_('cpu')
    cs.rq.to_('cpu')
    save_cached_state(cachefile, cs, args)
    return (cs.rtk, cs.rq)","for batch in pbar(loader):
    (sample_tk, sample_q) = call_compute(compute, batch)
    cs.rtk.add(sample_tk)
    cs.rq.add(sample_q)","for i, batch in enumerate(pbar(loader)):
    (sample_tk, sample_q) = call_compute(compute, batch)
    cs.rtk.add(sample_tk)
    cs.rq.add(sample_q)",1,,,,,,,,,,
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)","for cell_block in mesh.cells:
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))","for i, cell_block in enumerate(mesh.cells):
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))",1,,,,,,,,,,
drf-spectacular,https://github.com/tfranzel/drf-spectacular/tree/master/docs/blueprints/rollup.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/drf-spectacular/docs/blueprints/rollup.py,,"def rollup_properties(component, resolved_sub_serializers):
    if any(('allOf' in r.schema for r in resolved_sub_serializers)):
        return
    all_field_sets = [set(list(r.schema['properties'])) for r in resolved_sub_serializers]
    common_fields = all_field_sets[0].intersection(*all_field_sets[1:])
    common_schema = {'properties': {}, 'required': set()}
    for r in resolved_sub_serializers:
        for cf in sorted(common_fields):
            if cf in r.schema['properties']:
                common_schema['properties'][cf] = r.schema['properties'][cf]
                del r.schema['properties'][cf]
                if cf in r.schema.get('required', []):
                    common_schema['required'].add(cf)
        r.schema = {'allOf': [component.ref, r.schema]}
    del component.schema['oneOf']
    component.schema['properties'] = common_schema['properties']
    if common_schema['required']:
        component.schema['required'] = sorted(common_schema['required'])","for cf in sorted(common_fields):
    if cf in r.schema['properties']:
        common_schema['properties'][cf] = r.schema['properties'][cf]
        del r.schema['properties'][cf]
        if cf in r.schema.get('required', []):
            common_schema['required'].add(cf)","for i, cf in enumerate(sorted(common_fields)):
    if cf in r.schema['properties']:
        common_schema['properties'][cf] = r.schema['properties'][cf]
        del r.schema['properties'][cf]
        if cf in r.schema.get('required', []):
            common_schema['required'].add(cf)",1,,,,,,,,,,
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines","for check in valid_checks:
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')","for i, check in enumerate(valid_checks):
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')",1,,,,,,,,,,
FakeNewsNet,https://github.com/KaiDMML/FakeNewsNet/tree/master/code/tweet_collection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FakeNewsNet/code/tweet_collection.py,TweetCollector,"def collect_data(self, choices):
    for choice in choices:
        news_list = self.load_news_file(choice)
        collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for choice in choices:
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for i, choice in enumerate(choices):
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)",1,,,,,,,,,,
docker-py,https://github.com/docker/docker-py/tree/master/tests/integration/models_images_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docker-py/tests/integration/models_images_test.py,ImageCollectionTest,"def test_save_and_load(self):
    client = docker.from_env(version=TEST_API_VERSION)
    image = client.images.get(TEST_IMG)
    with tempfile.TemporaryFile() as f:
        stream = image.save()
        for chunk in stream:
            f.write(chunk)
        f.seek(0)
        result = client.images.load(f.read())
    assert len(result) == 1
    assert result[0].id == image.id","for chunk in stream:
    f.write(chunk)","for i, chunk in enumerate(stream):
    f.write(chunk)",1,,,,,,,,,,
ezdxf,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/math/clipping.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/math/clipping.py,GHPolygon,"def clip(self, clip: GHPolygon, s_entry, c_entry) -> list[list[Vec2]]:
    """"""Clip this polygon using another one as a clipper.

        This is where the algorithm is executed. It allows you to make
        a UNION, INTERSECT or DIFFERENCE operation between two polygons.

        Given two polygons A, B the following operations may be performed:

        A|B ... A OR B  (Union of A and B)
        A&B ... A AND B (Intersection of A and B)
        A\\B ... A - B
        B\\A ... B - A

        The entry records store the direction the algorithm should take when
        it arrives at that entry point in an intersection. Depending on the
        operation requested, the direction is set as follows for entry points
        (f=forward, b=backward; exit points are always set to the opposite):

              Entry
              A   B
              -----
        A|B   b   b
        A&B   f   f
        A\\B  b   f
        B\\A  f   b

        f = True, b = False when stored in the entry record
        """"""
    for subject_vertex in self:
        if not subject_vertex.intersect:
            for clipper_vertex in clip:
                if not clipper_vertex.intersect:
                    (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
                    if ip is None:
                        continue
                    subject_node = _Node(ip, us, intersect=True, entry=False)
                    clipper_node = _Node(ip, uc, intersect=True, entry=False)
                    subject_node.neighbour = clipper_node
                    clipper_node.neighbour = subject_node
                    self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
                    clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))
    s_entry ^= is_inside_polygon(self.first.vtx, clip)
    for subject_vertex in self:
        if subject_vertex.intersect:
            subject_vertex.entry = s_entry
            s_entry = not s_entry
    c_entry ^= is_inside_polygon(clip.first.vtx, self)
    for clipper_vertex in clip:
        if clipper_vertex.intersect:
            clipper_vertex.entry = c_entry
            c_entry = not c_entry
    clipped_polygons: list[list[Vec2]] = []
    while self.unprocessed():
        current: _Node = self.first_intersect
        clipped = GHPolygon()
        clipped.add(_Node(current))
        while True:
            current.set_checked()
            if current.entry:
                while True:
                    current = current.next
                    clipped.add(_Node(current))
                    if current.intersect:
                        break
            else:
                while True:
                    current = current.prev
                    clipped.add(_Node(current))
                    if current.intersect:
                        break
            current = current.neighbour
            if current.checked:
                break
        clipped_polygons.append(clipped.points)
    return clipped_polygons","for clipper_vertex in clip:
    if clipper_vertex.intersect:
        clipper_vertex.entry = c_entry
        c_entry = not c_entry","for i, clipper_vertex in enumerate(clip):
    if clipper_vertex.intersect:
        clipper_vertex.entry = c_entry
        c_entry = not c_entry",1,,,,,,,,,,
ezdxf,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/math/clipping.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/math/clipping.py,GHPolygon,"def clip(self, clip: GHPolygon, s_entry, c_entry) -> list[list[Vec2]]:
    """"""Clip this polygon using another one as a clipper.

        This is where the algorithm is executed. It allows you to make
        a UNION, INTERSECT or DIFFERENCE operation between two polygons.

        Given two polygons A, B the following operations may be performed:

        A|B ... A OR B  (Union of A and B)
        A&B ... A AND B (Intersection of A and B)
        A\\B ... A - B
        B\\A ... B - A

        The entry records store the direction the algorithm should take when
        it arrives at that entry point in an intersection. Depending on the
        operation requested, the direction is set as follows for entry points
        (f=forward, b=backward; exit points are always set to the opposite):

              Entry
              A   B
              -----
        A|B   b   b
        A&B   f   f
        A\\B  b   f
        B\\A  f   b

        f = True, b = False when stored in the entry record
        """"""
    for subject_vertex in self:
        if not subject_vertex.intersect:
            for clipper_vertex in clip:
                if not clipper_vertex.intersect:
                    (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
                    if ip is None:
                        continue
                    subject_node = _Node(ip, us, intersect=True, entry=False)
                    clipper_node = _Node(ip, uc, intersect=True, entry=False)
                    subject_node.neighbour = clipper_node
                    clipper_node.neighbour = subject_node
                    self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
                    clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))
    s_entry ^= is_inside_polygon(self.first.vtx, clip)
    for subject_vertex in self:
        if subject_vertex.intersect:
            subject_vertex.entry = s_entry
            s_entry = not s_entry
    c_entry ^= is_inside_polygon(clip.first.vtx, self)
    for clipper_vertex in clip:
        if clipper_vertex.intersect:
            clipper_vertex.entry = c_entry
            c_entry = not c_entry
    clipped_polygons: list[list[Vec2]] = []
    while self.unprocessed():
        current: _Node = self.first_intersect
        clipped = GHPolygon()
        clipped.add(_Node(current))
        while True:
            current.set_checked()
            if current.entry:
                while True:
                    current = current.next
                    clipped.add(_Node(current))
                    if current.intersect:
                        break
            else:
                while True:
                    current = current.prev
                    clipped.add(_Node(current))
                    if current.intersect:
                        break
            current = current.neighbour
            if current.checked:
                break
        clipped_polygons.append(clipped.points)
    return clipped_polygons","for clipper_vertex in clip:
    if not clipper_vertex.intersect:
        (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
        if ip is None:
            continue
        subject_node = _Node(ip, us, intersect=True, entry=False)
        clipper_node = _Node(ip, uc, intersect=True, entry=False)
        subject_node.neighbour = clipper_node
        clipper_node.neighbour = subject_node
        self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
        clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))","for i, clipper_vertex in enumerate(clip):
    if not clipper_vertex.intersect:
        (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
        if ip is None:
            continue
        subject_node = _Node(ip, us, intersect=True, entry=False)
        clipper_node = _Node(ip, uc, intersect=True, entry=False)
        subject_node.neighbour = clipper_node
        clipper_node.neighbour = subject_node
        self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
        clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))",1,,,,,,,,,,
Kats,https://github.com/facebookresearch/Kats/tree/master/kats/tests/test_consts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kats/kats/tests/test_consts.py,TimeSeriesDataOpsTest,"def test_get_item(self) -> None:
    self.assertEqual(self.ts_date_transform_concat_univ[:len(self.ts_univ_1)], self.ts_univ_1)
    self.assertEqual(self.ts_date_transform_concat_multi[:len(self.ts_multi_1)], self.ts_multi_1)
    for col in self.ts_date_transform_concat_multi.value.columns:
        ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
        self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)
    self.assertEqual(self.ts_date_transform_concat_multi[MULTIVAR_VALUE_DF_COLS], self.ts_date_transform_concat_multi)
    self.assertEqual(self.ts_univ_1[:], self.ts_univ_1)
    self.assertEqual(self.ts_univ_1[0:0], TimeSeriesData(time=pd.Series(name=TIME_COL_NAME), value=pd.Series(name=VALUE_COL_NAME), time_col_name=TIME_COL_NAME))","for col in self.ts_date_transform_concat_multi.value.columns:
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)","for i, col in enumerate(self.ts_date_transform_concat_multi.value.columns):
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)",1,,,,,,,,,,
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)","for credential in server_object.credentials.all():
    if credential.username == loginuser:
        port = credential.port
        method = credential.method
        if method == 'password':
            password = credential.password
        else:
            password = credential.password
            key = credential.key","for i, credential in enumerate(server_object.credentials.all()):
    if credential.username == loginuser:
        port = credential.port
        method = credential.method
        if method == 'password':
            password = credential.password
        else:
            password = credential.password
            key = credential.key",1,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/bpath.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/bpath.py,Path,"def intersectPath(self, path, setinside=None):
    points = []

    def addPoint(i, P):
        if eq(P, self[i].A, EPS):
            return
        if eq(P, self[i].B, EPS):
            return
        oi = self[i].order(P)
        points.append((i, oi, P))
    for (i, si) in enumerate(self):
        for cut in path:
            (P1, P2) = si.intersect(cut)
            if P1 is not None and P2 is not None and eq(P1, P2, EPS):
                P2 = None
            if P1:
                addPoint(i, P1)
            if P2:
                addPoint(i, P2)
    points.sort(key=itemgetter(0, 1))
    for (i, o, P) in reversed(points):
        split = self[i].split(P)
        if not isinstance(split, int):
            self.insert(i + 1, split)
            self[i]._cross = True
    if setinside is not None:
        self.markInside(path, setinside)
    return points","for cut in path:
    (P1, P2) = si.intersect(cut)
    if P1 is not None and P2 is not None and eq(P1, P2, EPS):
        P2 = None
    if P1:
        addPoint(i, P1)
    if P2:
        addPoint(i, P2)","for i, cut in enumerate(path):
    (P1, P2) = si.intersect(cut)
    if P1 is not None and P2 is not None and eq(P1, P2, EPS):
        P2 = None
    if P1:
        addPoint(i, P1)
    if P2:
        addPoint(i, P2)",0,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","for data in ori_data:
    vertexSet = data['vertexSet']
    for label in data['labels']:
        rel = label['r']
        for n1 in vertexSet[label['h']]:
            for n2 in vertexSet[label['t']]:
                fact_in_train.add((n1['name'], n2['name'], rel))","for i, data in enumerate(ori_data):
    vertexSet = data['vertexSet']
    for label in data['labels']:
        rel = label['r']
        for n1 in vertexSet[label['h']]:
            for n2 in vertexSet[label['t']]:
                fact_in_train.add((n1['name'], n2['name'], rel))",1,,,,,,,,,,
videoflow,https://github.com/videoflow/videoflow/tree/master/tests/test_release_resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videoflow/tests/test_release_resources.py,,"def test_bboxannotator_resources():
    for datasetid in BoundingBoxAnnotator.supported_datasets:
        filename = f'labels_{datasetid}.pbtxt'
        url_path = BASE_URL_DETECTION + filename
        get_file(filename, url_path)","for datasetid in BoundingBoxAnnotator.supported_datasets:
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)","for i, datasetid in enumerate(BoundingBoxAnnotator.supported_datasets):
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)",1,,,,,,,,,,
pyarmor,https://github.com/dashingsoft/pyarmor/tree/master/src/polyfills/argparse.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyarmor/src/polyfills/argparse.py,ArgumentParser,"def parse_known_args(self, args=None, namespace=None):
    if args is None:
        args = _sys.argv[1:]
    if namespace is None:
        namespace = Namespace()
    for action in self._actions:
        if action.dest is not SUPPRESS:
            if not hasattr(namespace, action.dest):
                if action.default is not SUPPRESS:
                    default = action.default
                    if isinstance(action.default, str):
                        default = self._get_value(action, default)
                    setattr(namespace, action.dest, default)
    for dest in self._defaults:
        if not hasattr(namespace, dest):
            setattr(namespace, dest, self._defaults[dest])
    try:
        (namespace, args) = self._parse_known_args(args, namespace)
        if hasattr(namespace, _UNRECOGNIZED_ARGS_ATTR):
            args.extend(getattr(namespace, _UNRECOGNIZED_ARGS_ATTR))
            delattr(namespace, _UNRECOGNIZED_ARGS_ATTR)
        return (namespace, args)
    except ArgumentError:
        err = _sys.exc_info()[1]
        self.error(str(err))","for dest in self._defaults:
    if not hasattr(namespace, dest):
        setattr(namespace, dest, self._defaults[dest])","for i, dest in enumerate(self._defaults):
    if not hasattr(namespace, dest):
        setattr(namespace, dest, self._defaults[dest])",1,,,,,,,,,,
taurus,https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/_selenium/test_selenium_executor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/tests/unit/modules/_selenium/test_selenium_executor.py,TestSeleniumStuff,"def obj_prepare_runner(self):
    super(SeleniumExecutor, self.obj).prepare()
    self.obj.install_required_tools()
    for driver in self.obj.webdrivers:
        self.obj.env.add_path({'PATH': driver.get_driver_dir()})
    self.obj.create_runner()
    self.obj.runner._check_tools = lambda *args: None
    self.obj.runner._compile_scripts = lambda : None
    tmp_tool = bzt.modules._apiritif.executor.Apiritif
    try:
        bzt.modules._apiritif.executor.Apiritif = MockPythonTool
        bzt.modules._selenium.Selenium.version = '3'
        self.obj.runner.prepare()
    finally:
        bzt.modules._apiritif.executor.Apiritif = tmp_tool
    self.obj.script = self.obj.runner.script","for driver in self.obj.webdrivers:
    self.obj.env.add_path({'PATH': driver.get_driver_dir()})","for i, driver in enumerate(self.obj.webdrivers):
    self.obj.env.add_path({'PATH': driver.get_driver_dir()})",1,,,,,,,,,,
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/pennylane/qaoa/cycle.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/pennylane/qaoa/cycle.py,,"def _inner_out_flow_constraint_hamiltonian(graph: Union[nx.DiGraph, rx.PyDiGraph], node: int) -> Hamiltonian:
    """"""Calculates the inner portion of the Hamiltonian in :func:`out_flow_constraint`.
    For a given :math:`i`, this function returns:

    .. math::

        d_{i}^{out}(d_{i}^{out} - 2)\\mathbb{I}
        - 2(d_{i}^{out}-1)\\sum_{j,(i,j)\\in E}\\hat{Z}_{ij} +
        ( \\sum_{j,(i,j)\\in E}\\hat{Z}_{ij} )^{2}

    Args:
        graph (nx.DiGraph or rx.PyDiGraph): the directed graph specifying possible edges
        node: a fixed node

    Returns:
        qml.Hamiltonian: The inner part of the out-flow constraint Hamiltonian.
    """"""
    if not isinstance(graph, (nx.DiGraph, rx.PyDiGraph)):
        raise ValueError(f'Input graph must be a nx.DiGraph or rx.PyDiGraph, got {type(graph).__name__}')
    coeffs = []
    ops = []
    is_rx = isinstance(graph, rx.PyDiGraph)
    get_nvalues = lambda T: (graph.nodes().index(T[0]), graph.nodes().index(T[1])) if is_rx else T
    edges_to_qubits = edges_to_wires(graph)
    out_edges = graph.out_edges(node)
    d = len(out_edges)
    if is_rx:
        out_edges = sorted(out_edges)
    for edge in out_edges:
        if len(edge) > 2:
            edge = tuple(edge[:2])
        wire = (edges_to_qubits[get_nvalues(edge)],)
        coeffs.append(1)
        ops.append(qml.PauliZ(wire))
    (coeffs, ops) = _square_hamiltonian_terms(coeffs, ops)
    for edge in out_edges:
        if len(edge) > 2:
            edge = tuple(edge[:2])
        wire = (edges_to_qubits[get_nvalues(edge)],)
        coeffs.append(-2 * (d - 1))
        ops.append(qml.PauliZ(wire))
    coeffs.append(d * (d - 2))
    ops.append(qml.Identity(0))
    H = Hamiltonian(coeffs, ops)
    H.simplify()
    H.grouping_indices = [list(range(len(H.ops)))]
    return H","for edge in out_edges:
    if len(edge) > 2:
        edge = tuple(edge[:2])
    wire = (edges_to_qubits[get_nvalues(edge)],)
    coeffs.append(-2 * (d - 1))
    ops.append(qml.PauliZ(wire))","for i, edge in enumerate(out_edges):
    if len(edge) > 2:
        edge = tuple(edge[:2])
    wire = (edges_to_qubits[get_nvalues(edge)],)
    coeffs.append(-2 * (d - 1))
    ops.append(qml.PauliZ(wire))",1,,,,,,,,,,
ezdxf,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/entities/ltype.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/entities/ltype.py,Linetype,"def setup_pattern(self, pattern: Union[Sequence[float], str], length: float=0) -> None:
    complex_line_type = True if isinstance(pattern, str) else False
    if complex_line_type:
        tags = self._setup_complex_pattern(pattern, length)
    else:
        tags = Tags([DXFTag(72, 65), DXFTag(73, len(pattern) - 1), DXFTag(40, float(pattern[0]))])
        for element in pattern[1:]:
            tags.append(DXFTag(49, float(element)))
            tags.append(DXFTag(74, 0))
    self.pattern_tags = LinetypePattern(tags)","for element in pattern[1:]:
    tags.append(DXFTag(49, float(element)))
    tags.append(DXFTag(74, 0))","for i, element in enumerate(pattern[1:]):
    tags.append(DXFTag(49, float(element)))
    tags.append(DXFTag(74, 0))",1,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/utils/data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/data.py,,"def traverse_dict_and_list(data, key, default=None, delimiter=DEFAULT_TARGET_DELIM):
    """"""
    Traverse a dict or list using a colon-delimited (or otherwise delimited,
    using the 'delimiter' param) target string. The target 'foo:bar:0' will
    return data['foo']['bar'][0] if this value exists, and will otherwise
    return the dict in the default argument.
    Function will automatically determine the target type.
    The target 'foo:bar:0' will return data['foo']['bar'][0] if data like
    {'foo':{'bar':['baz']}} , if data like {'foo':{'bar':{'0':'baz'}}}
    then return data['foo']['bar']['0']
    """"""
    ptr = data
    if isinstance(key, str):
        key = key.split(delimiter)
    if isinstance(key, int):
        key = [key]
    for each in key:
        if isinstance(ptr, list):
            try:
                idx = int(each)
            except ValueError:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[each]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    return default
            else:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[idx]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    try:
                        ptr = ptr[idx]
                    except IndexError:
                        return default
        else:
            try:
                ptr = ptr[each]
            except KeyError:
                import salt.utils.args
                try:
                    loaded_key = salt.utils.args.yamlify_arg(each)
                except Exception:
                    return default
                if loaded_key == each:
                    return default
                else:
                    try:
                        ptr = ptr[loaded_key]
                    except (KeyError, TypeError):
                        return default
            except TypeError:
                return default
    return ptr","for embedded in (x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass","for i, embedded in enumerate(x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass",1,,,,,,,,,,
jc,https://github.com/kellyjonbrazil/jc/tree/master/jc/parsers/hciconfig.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jc/jc/parsers/hciconfig.py,,"def _process(proc_data):
    """"""
    Final processing to conform to the schema.

    Parameters:

        proc_data:   (List of Dictionaries) raw structured data to process

    Returns:

        List of Dictionaries. Structured data to conform to the schema.
    """"""
    for entry in proc_data:
        int_list = ['acl_mtu', 'acl_mtu_packets', 'sco_mtu', 'sco_mtu_packets', 'rx_bytes', 'rx_acl', 'rx_sco', 'rx_events', 'rx_errors', 'tx_bytes', 'tx_acl', 'tx_sco', 'tx_commands', 'tx_errors']
        for key in entry:
            if key in int_list:
                entry[key] = jc.utils.convert_to_int(entry[key])
        if 'service_classes' in entry and len(entry['service_classes']) == 1 and ('Unspecified' in entry['service_classes']):
            entry['service_classes'] = None
    return proc_data","for entry in proc_data:
    int_list = ['acl_mtu', 'acl_mtu_packets', 'sco_mtu', 'sco_mtu_packets', 'rx_bytes', 'rx_acl', 'rx_sco', 'rx_events', 'rx_errors', 'tx_bytes', 'tx_acl', 'tx_sco', 'tx_commands', 'tx_errors']
    for key in entry:
        if key in int_list:
            entry[key] = jc.utils.convert_to_int(entry[key])
    if 'service_classes' in entry and len(entry['service_classes']) == 1 and ('Unspecified' in entry['service_classes']):
        entry['service_classes'] = None","for i, entry in enumerate(proc_data):
    int_list = ['acl_mtu', 'acl_mtu_packets', 'sco_mtu', 'sco_mtu_packets', 'rx_bytes', 'rx_acl', 'rx_sco', 'rx_events', 'rx_errors', 'tx_bytes', 'tx_acl', 'tx_sco', 'tx_commands', 'tx_errors']
    for key in entry:
        if key in int_list:
            entry[key] = jc.utils.convert_to_int(entry[key])
    if 'service_classes' in entry and len(entry['service_classes']) == 1 and ('Unspecified' in entry['service_classes']):
        entry['service_classes'] = None",1,,,,,,,,,,
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for entry in vOriginTable:
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)","for i, entry in enumerate(vOriginTable):
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)",1,,,,,,,,,,
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,,"def test_compute_log_manager(mock_s3_bucket):

    @op
    def easy(context):
        context.log.info('easy')
        print(HELLO_WORLD)
        return 'easy'

    @job
    def simple():
        easy()
    with tempfile.TemporaryDirectory() as temp_dir:
        with environ({'DAGSTER_HOME': temp_dir}):
            run_store = SqliteRunStorage.from_local(temp_dir)
            event_store = SqliteEventLogStorage(temp_dir)
            manager = S3ComputeLogManager(bucket=mock_s3_bucket.name, prefix='my_prefix', local_dir=temp_dir)
            instance = DagsterInstance(instance_type=InstanceType.PERSISTENT, local_artifact_storage=LocalArtifactStorage(temp_dir), run_storage=run_store, event_storage=event_store, compute_log_manager=manager, run_coordinator=DefaultRunCoordinator(), run_launcher=DefaultRunLauncher(), ref=InstanceRef.from_dir(temp_dir))
            result = simple.execute_in_process(instance=instance)
            capture_events = [event for event in result.all_events if event.event_type == DagsterEventType.LOGS_CAPTURED]
            assert len(capture_events) == 1
            event = capture_events[0]
            file_key = event.logs_captured_data.file_key
            log_key = manager.build_log_key_for_run(result.run_id, file_key)
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr
            s3_object = mock_s3_bucket.Object(key=manager._s3_key(log_key, ComputeIOType.STDERR))
            stderr_s3 = s3_object.get()['Body'].read().decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr_s3
            local_dir = os.path.dirname(manager._local_manager.get_captured_local_path(log_key, IO_TYPE_EXTENSION[ComputeIOType.STDOUT]))
            for filename in os.listdir(local_dir):
                os.unlink(os.path.join(local_dir, filename))
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr","for expected in EXPECTED_LOGS:
    assert expected in stderr","for i, expected in enumerate(EXPECTED_LOGS):
    assert expected in stderr",1,,,,,,,,,,
UNetPlusPlus,https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/preprocessing/sanity_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNetPlusPlus/pytorch/nnunet/preprocessing/sanity_checks.py,,"def verify_dataset_integrity(folder):
    """"""
    folder needs the imagesTr, imagesTs and labelsTr subfolders. There also needs to be a dataset.json
    checks if all training cases and labels are present
    checks if all test cases (if any) are present
    for each case, checks whether all modalities apre present
    for each case, checks whether the pixel grids are aligned
    checks whether the labels really only contain values they should
    :param folder:
    :return:
    """"""
    assert isfile(join(folder, 'dataset.json')), 'There needs to be a dataset.json file in folder, folder=%s' % folder
    assert isdir(join(folder, 'imagesTr')), 'There needs to be a imagesTr subfolder in folder, folder=%s' % folder
    assert isdir(join(folder, 'labelsTr')), 'There needs to be a labelsTr subfolder in folder, folder=%s' % folder
    dataset = load_json(join(folder, 'dataset.json'))
    training_cases = dataset['training']
    num_modalities = len(dataset['modality'].keys())
    test_cases = dataset['test']
    expected_train_identifiers = [i['image'].split('/')[-1][:-7] for i in training_cases]
    expected_test_identifiers = [i.split('/')[-1][:-7] for i in test_cases]
    nii_files_in_imagesTr = subfiles(join(folder, 'imagesTr'), suffix='.nii.gz', join=False)
    nii_files_in_labelsTr = subfiles(join(folder, 'labelsTr'), suffix='.nii.gz', join=False)
    label_files = []
    geometries_OK = True
    has_nan = False
    if len(expected_train_identifiers) != len(np.unique(expected_train_identifiers)):
        raise RuntimeError('found duplicate training cases in dataset.json')
    print('Verifying training set')
    for c in expected_train_identifiers:
        print('checking case', c)
        expected_label_file = join(folder, 'labelsTr', c + '.nii.gz')
        label_files.append(expected_label_file)
        expected_image_files = [join(folder, 'imagesTr', c + '_%04.0d.nii.gz' % i) for i in range(num_modalities)]
        assert isfile(expected_label_file), 'could not find label file for case %s. Expected file: \n%s' % (c, expected_label_file)
        assert all([isfile(i) for i in expected_image_files]), 'some image files are missing for case %s. Expected files:\n %s' % (c, expected_image_files)
        label_itk = sitk.ReadImage(expected_label_file)
        nans_in_seg = np.any(np.isnan(sitk.GetArrayFromImage(label_itk)))
        has_nan = has_nan | nans_in_seg
        if nans_in_seg:
            print('There are NAN values in segmentation %s' % expected_label_file)
        images_itk = [sitk.ReadImage(i) for i in expected_image_files]
        for (i, img) in enumerate(images_itk):
            nans_in_image = np.any(np.isnan(sitk.GetArrayFromImage(img)))
            has_nan = has_nan | nans_in_image
            same_geometry = verify_same_geometry(img, label_itk)
            if not same_geometry:
                geometries_OK = False
                print('The geometry of the image %s does not match the geometry of the label file. The pixel arrays will not be aligned and nnU-Net cannot use this data. Please make sure your image modalities are coregistered and have the same geometry as the label' % expected_image_files[0][:-12])
            if nans_in_image:
                print('There are NAN values in image %s' % expected_image_files[i])
        for i in expected_image_files:
            nii_files_in_imagesTr.remove(os.path.basename(i))
        nii_files_in_labelsTr.remove(os.path.basename(expected_label_file))
    assert len(nii_files_in_imagesTr) == 0, 'there are training cases in imagesTr that are not listed in dataset.json: %s' % nii_files_in_imagesTr
    assert len(nii_files_in_labelsTr) == 0, 'there are training cases in labelsTr that are not listed in dataset.json: %s' % nii_files_in_labelsTr
    print('Verifying label values')
    expected_labels = list((int(i) for i in dataset['labels'].keys()))
    p = Pool(default_num_threads)
    results = p.starmap(verify_contains_only_expected_labels, zip(label_files, [expected_labels] * len(label_files)))
    p.close()
    p.join()
    fail = False
    print('Expected label values are', expected_labels)
    for (i, r) in enumerate(results):
        if not r[0]:
            print('Unexpected labels found in file %s. Found these unexpected values (they should not be there) %s' % (label_files[i], r[1]))
            fail = True
    if fail:
        raise AssertionError('Found unexpected labels in the training dataset. Please correct that or adjust your dataset.json accordingly')
    else:
        print('Labels OK')
    if len(expected_test_identifiers) > 0:
        print('Verifying test set')
        nii_files_in_imagesTs = subfiles(join(folder, 'imagesTs'), suffix='.nii.gz', join=False)
        for c in expected_test_identifiers:
            expected_image_files = [join(folder, 'imagesTs', c + '_%04.0d.nii.gz' % i) for i in range(num_modalities)]
            assert all([isfile(i) for i in expected_image_files]), 'some image files are missing for case %s. Expected files:\n %s' % (c, expected_image_files)
            if num_modalities > 1:
                images_itk = [sitk.ReadImage(i) for i in expected_image_files]
                reference_img = images_itk[0]
                for (i, img) in enumerate(images_itk[1:]):
                    assert verify_same_geometry(img, reference_img), 'The modalities of the image %s do not seem to be registered. Please coregister your modalities.' % expected_image_files[i]
            for i in expected_image_files:
                nii_files_in_imagesTs.remove(os.path.basename(i))
        assert len(nii_files_in_imagesTs) == 0, 'there are training cases in imagesTs that are not listed in dataset.json: %s' % nii_files_in_imagesTr
    (all_same, unique_orientations) = verify_all_same_orientation(join(folder, 'imagesTr'))
    if not all_same:
        print('WARNING: Not all images in the dataset have the same axis ordering. We very strongly recommend you correct that by reorienting the data. fslreorient2std should do the trick')
    if not geometries_OK:
        raise Warning('GEOMETRY MISMATCH FOUND! CHECK THE TEXT OUTPUT! This does not cause an error at this point  but you should definitely check whether your geometries are alright!')
    else:
        print('Dataset OK')
    if has_nan:
        raise RuntimeError('Some images have nan values in them. This will break the training. See text output above to see which ones')","for i in expected_image_files:
    nii_files_in_imagesTs.remove(os.path.basename(i))","for i, expected_image_file in enumerate(expected_image_files):
    nii_files_in_imagesTs.remove(os.path.basename(expected_image_file))",1,,,,,,,,,,
wagtail,https://github.com/wagtail/wagtail/tree/master/wagtail/core/management/commands/replace_text.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wagtail/wagtail/core/management/commands/replace_text.py,,"def replace_in_model(model, from_text, to_text):
    text_field_names = [field.name for field in model._meta.fields if isinstance(field, models.TextField) or isinstance(field, models.CharField)]
    updated_fields = []
    for field in text_field_names:
        field_value = getattr(model, field)
        if field_value and from_text in field_value:
            updated_fields.append(field)
            setattr(model, field, field_value.replace(from_text, to_text))
    if updated_fields:
        model.save(update_fields=updated_fields)","for field in text_field_names:
    field_value = getattr(model, field)
    if field_value and from_text in field_value:
        updated_fields.append(field)
        setattr(model, field, field_value.replace(from_text, to_text))","for i, field in enumerate(text_field_names):
    field_value = getattr(model, field)
    if field_value and from_text in field_value:
        updated_fields.append(field)
        setattr(model, field, field_value.replace(from_text, to_text))",1,,,,,,,,,,
stm32-rs,https://github.com/stm32-rs/stm32-rs/tree/master/scripts/group.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stm32-rs/scripts/group.py,,"def main(devices, output):
    print('Stage 1: Enumerating all fields in all devices')
    peripherals = {}
    for device_path in tqdm(glob.glob(os.path.join(devices, '*.yaml'))):
        device_name = os.path.splitext(os.path.basename(device_path))[0]
        with open(device_path, encoding='utf-8') as f:
            device = yaml.safe_load(f)
            device['_path'] = device_path
        if '_svd' not in device:
            raise RuntimeError('You must have an _svd key in the YAML file')
        svdpath = patch.abspath(device_path, device['_svd'])
        svd = ET.parse(svdpath)
        patch.process_device(svd, device)
        for ptag in svd.iter('peripheral'):
            if 'derivedFrom' in ptag.attrib:
                continue
            pname = ptag.find('name').text
            device_members = []
            for rtag in ptag.iter('register'):
                rname = rtag.find('name').text
                roffset = rtag.find('addressOffset').text
                for ftag in rtag.iter('field'):
                    fname = ftag.find('name').text
                    foffset = ftag.find('bitOffset').text
                    fwidth = ftag.find('bitWidth').text
                    device_members.append('{}__{}_{}__{}_{}_{}'.format(pname, roffset, rname, foffset, fwidth, fname))
            if pname not in peripherals:
                peripherals[pname] = {}
            peripherals[pname][device_name] = device_members
    print('Stage 2: Inverting to find all devices for each field and merging')
    fields = {}
    merged_fields = {}
    for (pname, devices) in tqdm(peripherals.items()):
        fields[pname] = {}
        for (device, device_fields) in devices.items():
            for fname in device_fields:
                if fname not in fields[pname]:
                    fields[pname][fname] = set()
                fields[pname][fname].add(device)
        merged_fields[pname] = {}
        skip_fields = set()
        for (field, devices) in fields[pname].items():
            if field in skip_fields:
                continue
            merged_key = [field]
            for (field2, devices2) in fields[pname].items():
                if field != field2 and devices == devices2:
                    merged_key.append(field2)
                    skip_fields.add(field2)
            merged_fields[pname][','.join(sorted(merged_key))] = devices
    print('Stage 3: Building tree of subsets')
    field_tree = {}
    for pname in tqdm(merged_fields):
        field_tree[pname] = {}
        for (fieldset, devices) in merged_fields[pname].items():
            field_tree[pname][fieldset] = (devices, {})

        def treeify(fieldset, devices, children, siblings):
            moved_siblings = []
            for (fieldset2, (devices2, children2)) in siblings.items():
                if fieldset2 != fieldset and devices2 < devices:
                    children[fieldset2] = (devices2, children2)
                    moved_siblings.append(fieldset2)
            for fieldset2 in moved_siblings:
                del siblings[fieldset2]
            moved_children = []
            for fieldset2 in moved_siblings:
                if fieldset2 in moved_children:
                    continue
                (devices2, children2) = children[fieldset2]
                moved_children += treeify(fieldset2, devices2, children2, children)
            return moved_siblings
        fieldsets = list(field_tree[pname].keys())
        moved_fieldsets = []
        for fieldset in fieldsets:
            if fieldset in moved_fieldsets:
                continue
            (devices, children) = field_tree[pname][fieldset]
            moved_fieldsets += treeify(fieldset, devices, children, field_tree[pname])

        def strip_devices(siblings):
            fieldsets = list(siblings.keys())
            for fieldset in fieldsets:
                (devices, children) = siblings[fieldset]
                if children:
                    strip_devices(children)
                    siblings[fieldset] = children
                else:
                    siblings[fieldset] = list(devices)
        strip_devices(field_tree[pname])
    print('Stage 4: Writing results JSON')
    with open(output, 'w') as f:
        json.dump(field_tree, f, indent=2, sort_keys=True)","for fieldset2 in moved_siblings:
    del siblings[fieldset2]","for i, fieldset2 in enumerate(moved_siblings):
    del siblings[moved_siblings[i]]",1,,,,,,,,,,
pre-commit,https://github.com/pre-commit/pre-commit/tree/master/pre_commit/languages/pygrep.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pre-commit/pre_commit/languages/pygrep.py,,"def main(argv: Sequence[str] | None=None) -> int:
    parser = argparse.ArgumentParser(description='grep-like finder using python regexes.  Unlike grep, this tool returns nonzero when it finds a match and zero otherwise.  The idea here being that matches are ""problems"".')
    parser.add_argument('-i', '--ignore-case', action='store_true')
    parser.add_argument('--multiline', action='store_true')
    parser.add_argument('--negate', action='store_true')
    parser.add_argument('pattern', help='python regex pattern.')
    parser.add_argument('filenames', nargs='*')
    args = parser.parse_args(argv)
    flags = re.IGNORECASE if args.ignore_case else 0
    if args.multiline:
        flags |= re.MULTILINE | re.DOTALL
    pattern = re.compile(args.pattern.encode(), flags)
    retv = 0
    process_fn = FNS[Choice(multiline=args.multiline, negate=args.negate)]
    for filename in args.filenames:
        retv |= process_fn(pattern, filename)
    return retv","for filename in args.filenames:
    retv |= process_fn(pattern, filename)","for i, filename in enumerate(args.filenames):
    retv |= process_fn(pattern, filename)",1,,,,,,,,,,
DeepLabCut,https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/utils/visualization.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepLabCut/deeplabcut/utils/visualization.py,,"def make_labeled_images_from_dataframe(df, cfg, destfolder='', scale=1.0, dpi=100, keypoint='+', draw_skeleton=True, color_by='bodypart'):
    """"""
    Write labeled frames to disk from a DataFrame.
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the labeled data. Typically, the DataFrame is obtained
        through pandas.read_csv() or pandas.read_hdf().
    cfg : dict
        Project configuration.
    destfolder : string, optional
        Destination folder into which images will be stored. By default, same location as the labeled data.
        Note that the folder will be created if it does not exist.
    scale : float, optional
        Up/downscale the output dimensions.
        By default, outputs are of the same dimensions as the original images.
    dpi : int, optional
        Output resolution. 100 dpi by default.
    keypoint : str, optional
        Keypoint appearance. By default, keypoints are marked by a + sign.
        Refer to https://matplotlib.org/3.2.1/api/markers_api.html for a list of all possible options.
    draw_skeleton : bool, optional
        Whether to draw the animal skeleton as defined in *cfg*. True by default.
    color_by : str, optional
        Color scheme of the keypoints. Must be either 'bodypart' or 'individual'.
        By default, keypoints are colored relative to the bodypart they represent.
    """"""
    bodyparts = df.columns.get_level_values('bodyparts')
    bodypart_names = bodyparts.unique()
    nbodyparts = len(bodypart_names)
    bodyparts = bodyparts[::2]
    draw_skeleton = draw_skeleton and cfg['skeleton']
    if color_by == 'bodypart':
        map_ = bodyparts.map(dict(zip(bodypart_names, range(nbodyparts))))
        cmap = get_cmap(nbodyparts, cfg['colormap'])
        colors = cmap(map_)
    elif color_by == 'individual':
        try:
            individuals = df.columns.get_level_values('individuals')
            individual_names = individuals.unique().to_list()
            nindividuals = len(individual_names)
            individuals = individuals[::2]
            map_ = individuals.map(dict(zip(individual_names, range(nindividuals))))
            cmap = get_cmap(nindividuals, cfg['colormap'])
            colors = cmap(map_)
        except KeyError as e:
            raise Exception('Coloring by individuals is only valid for multi-animal data') from e
    else:
        raise ValueError('`color_by` must be either `bodypart` or `individual`.')
    bones = []
    if draw_skeleton:
        for (bp1, bp2) in cfg['skeleton']:
            (match1, match2) = ([], [])
            for (j, bp) in enumerate(bodyparts):
                if bp == bp1:
                    match1.append(j)
                elif bp == bp2:
                    match2.append(j)
            bones.extend(zip(match1, match2))
    ind_bones = tuple(zip(*bones))
    sep = '/' if '/' in df.index[0] else '\\'
    images = cfg['project_path'] + sep + df.index
    if sep != os.path.sep:
        images = images.str.replace(sep, os.path.sep)
    if not destfolder:
        destfolder = os.path.dirname(images[0])
    tmpfolder = destfolder + '_labeled'
    attempttomakefolder(tmpfolder)
    images_list = images.to_list()
    ic = io.imread_collection(images_list)
    (h, w) = ic[0].shape[:2]
    all_same_shape = True
    for array in ic[1:]:
        if array.shape[:2] != (h, w):
            all_same_shape = False
            break
    xy = df.values.reshape((df.shape[0], -1, 2))
    segs = xy[:, ind_bones].swapaxes(1, 2)
    s = cfg['dotsize']
    alpha = cfg['alphavalue']
    if all_same_shape:
        (fig, ax) = prepare_figure_axes(w, h, scale, dpi)
        im = ax.imshow(np.zeros((h, w)), 'gray')
        pts = [ax.plot([], [], keypoint, ms=s, alpha=alpha, color=c)[0] for c in colors]
        coll = LineCollection([], colors=cfg['skeleton_color'], alpha=alpha)
        ax.add_collection(coll)
        for i in trange(len(ic)):
            filename = ic.files[i]
            ind = images_list.index(filename)
            coords = xy[ind]
            img = ic[i]
            if img.ndim == 2 or img.shape[-1] == 1:
                img = color.gray2rgb(ic[i])
            im.set_data(img)
            for (pt, coord) in zip(pts, coords):
                pt.set_data(*coord)
            if ind_bones:
                coll.set_segments(segs[ind])
            imagename = os.path.basename(filename)
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            fig.savefig(os.path.join(tmpfolder, imagename.replace('.png', f'_{color_by}.png')), dpi=dpi)
        plt.close(fig)
    else:
        for i in trange(len(ic)):
            filename = ic.files[i]
            ind = images_list.index(filename)
            coords = xy[ind]
            image = ic[i]
            (h, w) = image.shape[:2]
            (fig, ax) = prepare_figure_axes(w, h, scale, dpi)
            ax.imshow(image)
            for (coord, c) in zip(coords, colors):
                ax.plot(*coord, keypoint, ms=s, alpha=alpha, color=c)
            if ind_bones:
                coll = LineCollection(segs[ind], colors=cfg['skeleton_color'], alpha=alpha)
                ax.add_collection(coll)
            imagename = os.path.basename(filename)
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            fig.savefig(os.path.join(tmpfolder, imagename.replace('.png', f'_{color_by}.png')), dpi=dpi)
            plt.close(fig)","for i in trange(len(ic)):
    filename = ic.files[i]
    ind = images_list.index(filename)
    coords = xy[ind]
    img = ic[i]
    if img.ndim == 2 or img.shape[-1] == 1:
        img = color.gray2rgb(ic[i])
    im.set_data(img)
    for (pt, coord) in zip(pts, coords):
        pt.set_data(*coord)
    if ind_bones:
        coll.set_segments(segs[ind])
    imagename = os.path.basename(filename)
    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    fig.savefig(os.path.join(tmpfolder, imagename.replace('.png', f'_{color_by}.png')), dpi=dpi)","for i, filename in enumerate(ic.files):
    ind = images_list.index(filename)
    coords = xy[ind]
    img = ic[i]
    if img.ndim == 2 or img.shape[-1] == 1:
        img = color.gray2rgb(ic[i])
    im.set_data(img)
    for (pt, coord) in zip(pts, coords):
        pt.set_data(*coord)
    if ind_bones:
        coll.set_segments(segs[ind])
    imagename = os.path.basename(filename)
    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    fig.savefig(os.path.join(tmpfolder, imagename.replace('.png', f'_{color_by}.png')), dpi=dpi)",1,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/modules/dnsmasq.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/dnsmasq.py,,"def get_config(config_file='/etc/dnsmasq.conf'):
    """"""
    Dumps all options from the config file.

    config_file
        The location of the config file from which to obtain contents.
        Defaults to ``/etc/dnsmasq.conf``.

    CLI Examples:

    .. code-block:: bash

        salt '*' dnsmasq.get_config
        salt '*' dnsmasq.get_config config_file=/etc/dnsmasq.conf
    """"""
    dnsopts = _parse_dnamasq(config_file)
    if 'conf-dir' in dnsopts:
        for filename in os.listdir(dnsopts['conf-dir']):
            if filename.startswith('.'):
                continue
            if filename.endswith('~'):
                continue
            if filename.endswith('#') and filename.endswith('#'):
                continue
            dnsopts.update(_parse_dnamasq('{}/{}'.format(dnsopts['conf-dir'], filename)))
    return dnsopts","for filename in os.listdir(dnsopts['conf-dir']):
    if filename.startswith('.'):
        continue
    if filename.endswith('~'):
        continue
    if filename.endswith('#') and filename.endswith('#'):
        continue
    dnsopts.update(_parse_dnamasq('{}/{}'.format(dnsopts['conf-dir'], filename)))","for i, filename in enumerate(os.listdir(dnsopts['conf-dir'])):
    if filename.startswith('.'):
        continue
    if filename.endswith('~'):
        continue
    if filename.endswith('#') and filename.endswith('#'):
        continue
    dnsopts.update(_parse_dnamasq('{}/{}'.format(dnsopts['conf-dir'], filename)))",1,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for fn in possibleFiles:
    mass_store.append([False, fn[len(path):]])","for i, fn in enumerate(possibleFiles):
    mass_store.append([False, fn[len(path):]])",1,,,,,,,,,,
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_saving_in_empty_folder(self):
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs:
            if isinstance(temp_fs, TempFS):
                self.eopatch.save(temp_fs.root_path)
            else:
                self.eopatch.save('/', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
            subfolder = 'new-subfolder'
            self.eopatch.save('new-subfolder', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for fs_loader in self.filesystem_loaders:
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for i, fs_loader in enumerate(self.filesystem_loaders):
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))",1,,,,,,,,,,
Deep-Reinforcement-Learning-Algorithms-with-PyTorch,https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tree/master/environments/ant_environments/maze_env.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/environments/ant_environments/maze_env.py,MazeEnv,"def __init__(self, maze_id=None, maze_height=0.5, maze_size_scaling=8, n_bins=0, sensor_range=3.0, sensor_span=2 * math.pi, observe_blocks=False, put_spin_near_agent=False, top_down_view=False, manual_collision=False, *args, **kwargs):
    self._maze_id = maze_id
    model_cls = self.__class__.MODEL_CLASS
    if model_cls is None:
        raise 'MODEL_CLASS unspecified!'
    xml_path = os.path.join(MODEL_DIR, model_cls.FILE)
    import sys
    sys.path.insert(0, '/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments')
    sys.path.insert(0, '/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments/environments')
    print(os.getcwd())
    print(xml_path)
    tree = ET.parse('/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments/ant_environments/' + xml_path[13:])
    worldbody = tree.find('.//worldbody')
    self.MAZE_HEIGHT = height = maze_height
    self.MAZE_SIZE_SCALING = size_scaling = maze_size_scaling
    self._n_bins = n_bins
    self._sensor_range = sensor_range * size_scaling
    self._sensor_span = sensor_span
    self._observe_blocks = observe_blocks
    self._put_spin_near_agent = put_spin_near_agent
    self._top_down_view = top_down_view
    self._manual_collision = manual_collision
    self.MAZE_STRUCTURE = structure = maze_env_utils.construct_maze(maze_id=self._maze_id)
    self.elevated = any((-1 in row for row in structure))
    self.blocks = any((any((maze_env_utils.can_move(r) for r in row)) for row in structure))
    (torso_x, torso_y) = self._find_robot()
    self._init_torso_x = torso_x
    self._init_torso_y = torso_y
    self._init_positions = [(x - torso_x, y - torso_y) for (x, y) in self._find_all_robots()]
    self._xy_to_rowcol = lambda x, y: (2 + (y + size_scaling / 2) / size_scaling, 2 + (x + size_scaling / 2) / size_scaling)
    self._view = np.zeros([5, 5, 3])
    height_offset = 0.0
    if self.elevated:
        height_offset = height * size_scaling
        torso = tree.find("".//body[@name='torso']"")
        torso.set('pos', '0 0 %.2f' % (0.75 + height_offset))
    if self.blocks:
        default = tree.find('.//default')
        default.find('.//geom').set('solimp', '.995 .995 .01')
    self.movable_blocks = []
    for i in range(len(structure)):
        for j in range(len(structure[0])):
            struct = structure[i][j]
            if struct == 'r' and self._put_spin_near_agent:
                struct = maze_env_utils.Move.SpinXY
            if self.elevated and struct not in [-1]:
                ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
            if struct == 1:
                ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
            elif maze_env_utils.can_move(struct):
                name = 'movable_%d_%d' % (i, j)
                self.movable_blocks.append((name, struct))
                falling = maze_env_utils.can_move_z(struct)
                spinning = maze_env_utils.can_spin(struct)
                x_offset = 0.25 * size_scaling if spinning else 0.0
                y_offset = 0.0
                shrink = 0.1 if spinning else 0.99 if falling else 1.0
                height_shrink = 0.1 if spinning else 1.0
                movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
                ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
                if maze_env_utils.can_move_x(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_move_y(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_move_z(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_spin(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')
    torso = tree.find("".//body[@name='torso']"")
    geoms = torso.findall('.//geom')
    for geom in geoms:
        if 'name' not in geom.attrib:
            raise Exception('Every geom of the torso must have a name defined')
    (_, file_path) = tempfile.mkstemp(text=True, suffix='.xml')
    tree.write(file_path)
    self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)","for geom in geoms:
    if 'name' not in geom.attrib:
        raise Exception('Every geom of the torso must have a name defined')","for i, geom in enumerate(geoms):
    if 'name' not in geom.attrib:
        raise Exception(f'Every geom {i} of the torso must have a name defined')",1,,,,,,,,,,
Pillow,https://github.com/python-pillow/Pillow/tree/master/src/PIL/FontFile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pillow/src/PIL/FontFile.py,FontFile,"def compile(self):
    """"""Create metrics and bitmap""""""
    if self.bitmap:
        return
    h = w = maxwidth = 0
    lines = 1
    for glyph in self:
        if glyph:
            (d, dst, src, im) = glyph
            h = max(h, src[3] - src[1])
            w = w + (src[2] - src[0])
            if w > WIDTH:
                lines += 1
                w = src[2] - src[0]
            maxwidth = max(maxwidth, w)
    xsize = maxwidth
    ysize = lines * h
    if xsize == 0 and ysize == 0:
        return ''
    self.ysize = h
    self.bitmap = Image.new('1', (xsize, ysize))
    self.metrics = [None] * 256
    x = y = 0
    for i in range(256):
        glyph = self[i]
        if glyph:
            (d, dst, src, im) = glyph
            xx = src[2] - src[0]
            (x0, y0) = (x, y)
            x = x + xx
            if x > WIDTH:
                (x, y) = (0, y + h)
                (x0, y0) = (x, y)
                x = xx
            s = (src[0] + x0, src[1] + y0, src[2] + x0, src[3] + y0)
            self.bitmap.paste(im.crop(src), s)
            self.metrics[i] = (d, dst, s)","for glyph in self:
    if glyph:
        (d, dst, src, im) = glyph
        h = max(h, src[3] - src[1])
        w = w + (src[2] - src[0])
        if w > WIDTH:
            lines += 1
            w = src[2] - src[0]
        maxwidth = max(maxwidth, w)","for i, glyph in enumerate(self):
    if glyph:
        (d, dst, src, im) = glyph
        h = max(h, src[3] - src[1])
        w = w + (src[2] - src[0])
        if w > WIDTH:
            lines += 1
            w = src[2] - src[0]
        maxwidth = max(maxwidth, w)",1,,,,,,,,,,
videos,https://github.com/3b1b/videos/tree/master/_2017/eoc/chapter7.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/eoc/chapter7.py,Goals,"def construct(self):
    goals = [TexText('Goal %d:' % d, s) for (d, s) in [(1, 'Formal definition of derivatives'), (2, '$(\\epsilon, \\delta)$ definition of a limit'), (3, ""L'Hôpital's rule"")]]
    for goal in goals:
        goal.scale(1.3)
        goal.shift(3 * DOWN).to_edge(LEFT)
    curr_goal = goals[0]
    self.play(FadeIn(curr_goal))
    self.wait(2)
    for goal in goals[1:]:
        self.play(Transform(curr_goal, goal))
        self.wait(2)","for goal in goals[1:]:
    self.play(Transform(curr_goal, goal))
    self.wait(2)","for i, goal in enumerate(goals[1:]):
    self.play(Transform(curr_goal, goal))
    self.wait(2)",1,,,,,,,,,,
neon,https://github.com/NervanaSystems/neon/tree/master/neon/backends/autodiff.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neon/neon/backends/autodiff.py,Autodiff,"def get_grad_tensor(self, tensors):
    """"""
        Get gradient values in type Tensor w.r.t the list of `tensors`. If a
        tensor is not used, its gradient will be set to zero.

        Arguments:
            Tensors (list): List of Tensors to compute gradients on.

        Returns
            list: A list of Tensors, each of them is the gradent of the input
                  tensor.
        """"""
    grad_op_trees = self.get_grad_op_tree(tensors)
    grad_vals = []
    for grad_op_tree in grad_op_trees:
        grad_val = self.be.empty(grad_op_tree.shape)
        grad_val[:] = grad_op_tree
        grad_vals.append(grad_val)
    return grad_vals","for grad_op_tree in grad_op_trees:
    grad_val = self.be.empty(grad_op_tree.shape)
    grad_val[:] = grad_op_tree
    grad_vals.append(grad_val)","for i, grad_op_tree in enumerate(grad_op_trees):
    grad_val = self.be.empty(grad_op_tree.shape)
    grad_val[:] = grad_op_tree
    grad_vals.append(grad_val)",1,,,,,,,,,,
ImageCaptioning.pytorch,https://github.com/ruotianluo/ImageCaptioning.pytorch/tree/master/captioning/utils/misc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ImageCaptioning.pytorch/captioning/utils/misc.py,,"def get_lr(optimizer):
    for group in optimizer.param_groups:
        return group['lr']","for group in optimizer.param_groups:
    return group['lr']","for i, group in enumerate(optimizer.param_groups):
    return group['lr']",1,,,,,,,,,,
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/db/ClientDB.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/db/ClientDB.py,DB,"def _DuplicatesDissolveMediaIdFromHashes(self, hashes):
    hash_ids = self.modules_hashes_local_cache.GetHashIds(hashes)
    for hash_id in hash_ids:
        media_id = self._DuplicatesGetMediaId(hash_id, do_not_create=True)
        if media_id is not None:
            self._DuplicatesDissolveMediaId(media_id)","for hash_id in hash_ids:
    media_id = self._DuplicatesGetMediaId(hash_id, do_not_create=True)
    if media_id is not None:
        self._DuplicatesDissolveMediaId(media_id)","for i, hash_id in enumerate(hash_ids):
    media_id = self._DuplicatesGetMediaId(hash_id, do_not_create=True)
    if media_id is not None:
        self._DuplicatesDissolveMediaId(media_id)",1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/base/webapp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/base/webapp.py,GalaxyWebTransaction,"def get_or_create_default_history(self):
    """"""
        Gets or creates a default history and associates it with the current
        session.
        """"""
    if not self.galaxy_session.user:
        return self.new_history()
    unnamed_histories = self.sa_session.query(self.app.model.History).filter_by(user=self.galaxy_session.user, name=self.app.model.History.default_name, deleted=False)
    default_history = None
    for history in unnamed_histories:
        if len(history.datasets) == 0:
            default_history = history
            break
    if default_history:
        history = default_history
        self.set_history(history)
    else:
        history = self.new_history()
    return history","for history in unnamed_histories:
    if len(history.datasets) == 0:
        default_history = history
        break","for i, history in enumerate(unnamed_histories):
    if len(history.datasets) == 0:
        default_history = history
        break",1,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/digitalocean.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/digitalocean.py,,"def destroy_dns_records(fqdn):
    """"""
    Deletes DNS records for the given hostname if the domain is managed with DO.
    """"""
    domain = '.'.join(fqdn.split('.')[-2:])
    hostname = '.'.join(fqdn.split('.')[:-2])
    try:
        response = query(method='domains', droplet_id=domain, command='records')
    except SaltCloudSystemExit:
        log.debug('Failed to find domains.')
        return False
    log.debug('found DNS records: %s', pprint.pformat(response))
    records = response['domain_records']
    if records:
        record_ids = [r['id'] for r in records if r['name'].decode() == hostname]
        log.debug('deleting DNS record IDs: %s', record_ids)
        for id_ in record_ids:
            try:
                log.info('deleting DNS record %s', id_)
                ret = query(method='domains', droplet_id=domain, command='records/{}'.format(id_), http_method='delete')
            except SaltCloudSystemExit:
                log.error('failed to delete DNS domain %s record ID %s.', domain, hostname)
            log.debug('DNS deletion REST call returned: %s', pprint.pformat(ret))
    return False","for id_ in record_ids:
    try:
        log.info('deleting DNS record %s', id_)
        ret = query(method='domains', droplet_id=domain, command='records/{}'.format(id_), http_method='delete')
    except SaltCloudSystemExit:
        log.error('failed to delete DNS domain %s record ID %s.', domain, hostname)
    log.debug('DNS deletion REST call returned: %s', pprint.pformat(ret))","for i, id_ in enumerate(record_ids):
    try:
        log.info('deleting DNS record %s', id_)
        ret = query(method='domains', droplet_id=domain, command='records/{}'.format(id_), http_method='delete')
    except SaltCloudSystemExit:
        log.error('failed to delete DNS domain %s record ID %s.', domain, hostname)
    log.debug('DNS deletion REST call returned: %s', pprint.pformat(ret))",1,,,,,,,,,,
spaCy,https://github.com/explosion/spaCy/tree/master/spacy/training/batchers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/training/batchers.py,,"def minibatch_by_padded_size(seqs: Iterable[ItemT], size: Sizing, buffer: int=256, discard_oversize: bool=False, get_length: Callable=len) -> Iterable[List[ItemT]]:
    """"""Minibatch a sequence by the size of padded batches that would result,
    with sequences binned by length within a window.

    The padded size is defined as the maximum length of sequences within the
    batch multiplied by the number of sequences in the batch.

    size (int or Sequence[int]): The largest padded size to batch sequences into.
    buffer (int): The number of sequences to accumulate before sorting by length.
        A larger buffer will result in more even sizing, but if the buffer is
        very large, the iteration order will be less random, which can result
        in suboptimal training.
    discard_oversize (bool): Whether to discard sequences that are by themselves
        longer than the largest padded batch size.
    get_length (Callable or None): Function to get the length of a sequence item.
        The `len` function is used by default.
    """"""
    if isinstance(size, int):
        size_ = itertools.repeat(size)
    else:
        size_ = iter(size)
    for outer_batch in minibatch(seqs, size=buffer):
        outer_batch = list(outer_batch)
        target_size = next(size_)
        for indices in _batch_by_length(outer_batch, target_size, get_length):
            subbatch = [outer_batch[i] for i in indices]
            padded_size = max((len(seq) for seq in subbatch)) * len(subbatch)
            if discard_oversize and padded_size >= target_size:
                pass
            else:
                yield subbatch","for indices in _batch_by_length(outer_batch, target_size, get_length):
    subbatch = [outer_batch[i] for i in indices]
    padded_size = max((len(seq) for seq in subbatch)) * len(subbatch)
    if discard_oversize and padded_size >= target_size:
        pass
    else:
        yield subbatch","for i, indices in enumerate(_batch_by_length(outer_batch, target_size, get_length)):
    subbatch = [outer_batch[i] for i in indices]
    padded_size = max((len(seq) for seq in subbatch)) * len(subbatch)
    if discard_oversize and padded_size >= target_size:
        pass
    else:
        yield subbatch",1,,,,,,,,,,
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for ip in myip_addr or remote_addr:
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()","for i, ip in enumerate(myip_addr or remote_addr):
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()",1,,,,,,,,,,
weevely3,https://github.com/epinna/weevely3/tree/master/modules/net/scan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/weevely3/modules/net/scan.py,Scan,"def run(self):
    IPs = []
    for ip_or_network in self.args['addresses'].split(','):
        if ip_or_network.count('-') == 1:
            IPs += list(utils.iputil.ip_range(ip_or_network))
        elif ip_or_network.count('/') == 1:
            IPs += [str(utils.ipaddr.IPAddress(ip)) for ip in utils.ipaddr.IPNetwork(ip_or_network)]
        else:
            IPs.append(ip_or_network)
    prts = utils.iputil.port_range(self.args['ports'])
    results_string = ''
    for ips_chunk in list(utils.strings.chunks(IPs, self.args['addresses_per_request'])):
        for prts_chunk in list(utils.strings.chunks(prts, self.args['ports_per_request'])):
            results_string += self.vectors.get_result(name='fsockopen', format_args={'ips': ips_chunk, 'prts': prts_chunk, 'timeout': self.args['timeout']})
            log.warn('Scanning addresses %s-%s:%i-%i' % (ips_chunk[0], ips_chunk[-1], prts_chunk[0], prts_chunk[-1]))
    result = []
    for result_string in results_string.split('\n'):
        addr_string_splitted = result_string.split(' ')
        if addr_string_splitted[0] == 'OPN':
            address = addr_string_splitted[1]
            error = 'OPEN'
        elif addr_string_splitted[0] == 'ERR':
            address = addr_string_splitted[1]
            error = '%s (%s)' % (' '.join(addr_string_splitted[2:-1]), addr_string_splitted[-1])
        else:
            log.debug(messages.module_net_scan.unexpected_response)
            continue
        if self.args.get('print'):
            result.append((address, error))
        elif error == 'OPEN':
            result.append(address)
    return result","for ip_or_network in self.args['addresses'].split(','):
    if ip_or_network.count('-') == 1:
        IPs += list(utils.iputil.ip_range(ip_or_network))
    elif ip_or_network.count('/') == 1:
        IPs += [str(utils.ipaddr.IPAddress(ip)) for ip in utils.ipaddr.IPNetwork(ip_or_network)]
    else:
        IPs.append(ip_or_network)","for i, ip_or_network in enumerate(self.args['addresses'].split(',')):
    if ip_or_network.count('-') == 1:
        IPs += list(utils.iputil.ip_range(ip_or_network))
    elif ip_or_network.count('/') == 1:
        IPs += [str(utils.ipaddr.IPAddress(ip)) for ip in utils.ipaddr.IPNetwork(ip_or_network)]
    else:
        IPs.append(ip_or_network)",1,,,,,,,,,,
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)","for item in builder.widgets.items():
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)","for i, item in enumerate(builder.widgets.items()):
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect((i, item), sig, handler_names, callback_handler_dict, builder.connections)",0,,,,,,,,,,
TuChart,https://github.com/Seedarchangel/TuChart/tree/master/Tuchart/Graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TuChart/Tuchart/Graph.py,,"def graphpage(items, startdate, enddate, option, width1, height1):
    page = Page()
    for i in items:
        j = re.split('-', i)
        if len(j) == 3:
            a = generateline(j[1], j[2], startdate, enddate, option)
            if a is None:
                continue
            time = [d[0] for d in a]
            if j[2] != 'Kline':
                if len(a[0]) == 4 and a[0][2] == 'bar':
                    overlap = Overlap()
                    form = [e[1] for e in a]
                    bar = Bar(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    bar.add(j[0] + '-' + j[2], time, form, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider')
                    overlap.add(bar)
                    line = Line(j[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    price = [e[3] for e in a]
                    line.add(j[0] + 'price', time, price, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider', yaxis_type='value')
                    overlap.add(line, yaxis_index=1, is_add_yaxis=True)
                    page.add(overlap)
                if len(a[0]) == 5 and a[0][3] == 'pie':
                    overlap = Overlap()
                    timeline = Timeline(is_auto_play=False, timeline_bottom=0)
                    namearray = [c[0] for c in a]
                    valuearray = [d[1] for d in a]
                    quarter = [e[2] for e in a]
                    num = a[0][4]
                    for x in range(0, num / 10):
                        list1 = valuearray[x]
                        names = namearray[x]
                        quarters = quarter[x][0]
                        for (idx, val) in enumerate(list1):
                            list1[idx] = float(val)
                        pie = Pie(j[0] + '-' + '前十股东'.decode('utf-8'), width=width1 * 10 / 11, height=height1 * 10 / 11)
                        pie.add(j[0] + '-' + '前十股东'.decode('utf-8'), names, list1, radius=[30, 55], is_legend_show=False, is_label_show=True, label_formatter='{b}: {c}\n{d}%')
                        timeline.add(pie, quarters)
                    timeline.render()
                    return
                else:
                    form = [e[1] for e in a]
                    line = Line(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    line.add(j[0] + '-' + j[2], time, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                    page.add(line)
            else:
                overlap = Overlap()
                close = zip(*a)[2]
                candle = [[x[1], x[2], x[3], x[4]] for x in a]
                candlestick = Kline(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                candlestick.add(j[0], time, candle, is_datazoom_show=True, datazoom_type='slider', yaxis_interval=1)
                overlap.add(candlestick)
                if len(close) > 10:
                    ma10 = calculateMa(close, 10)
                    line1 = Line(title_color='#C0C0C0')
                    line1.add(j[0] + '-' + 'MA10', time, ma10)
                    overlap.add(line1)
                if len(close) > 20:
                    ma20 = calculateMa(close, 20)
                    line2 = Line(title_color='#C0C0C0')
                    line2.add(j[0] + '-' + 'MA20', time, ma20)
                    overlap.add(line2)
                if len(close) > 30:
                    ma30 = calculateMa(close, 30)
                    line3 = Line(title_color='#C0C0C0')
                    line3.add(j[0] + '-' + 'MA30', time, ma30)
                    overlap.add(line3)
                page.add(overlap)
        else:
            for k in range(1, len(j) / 3):
                j[3 * k - 1] = re.sub('\n&', '', j[3 * k - 1])
            sizearray = []
            layout = Overlap()
            for i in xrange(0, len(j), 3):
                array = j[i:i + 3]
                b = generateline(array[1], array[2], startdate, enddate, option)
                if b is None:
                    continue
                btime = [d[0] for d in b]
                if array[2] != 'Kline':
                    if len(b[0]) == 4 and b[0][2] == 'bar':
                        form = [e[1] for e in b]
                        bar = Bar(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                        bar.add(array[0] + '-' + array[2], btime, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                        layout.add(bar)
                        line = Line(array[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                        price = [e[3] for e in b]
                        line.add(array[0] + 'price', btime, price, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_type='value')
                        layout.add(line, yaxis_index=1, is_add_yaxis=True)
                    else:
                        line = Line(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                        line.add(array[0] + '-' + array[2], btime, b, is_datazoom_show=True, yaxis_max='dataMax', yaxis_min='dataMin', datazoom_type='slider')
                        layout.add(line)
                else:
                    candle = [[x[1], x[2], x[3], x[4]] for x in b]
                    candlestick = Kline(array[0] + '-' + array[1], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    candlestick.add(array[0], btime, candle, is_datazoom_show=True, datazoom_type=['slider'])
                    close = zip(*b)[2]
                    if len(close) > 10:
                        ma10 = calculateMa(close, 10)
                        line4 = Line(title_color='#C0C0C0')
                        line4.add(array[0] + '-' + 'MA10', btime, ma10)
                        layout.add(line4)
                    if len(close) > 20:
                        ma20 = calculateMa(close, 20)
                        line5 = Line(title_color='#C0C0C0')
                        line5.add(array[0] + '-' + 'MA20', btime, ma20)
                        layout.add(line5)
                    if len(close) > 30:
                        ma30 = calculateMa(close, 30)
                        line6 = Line(title_color='#C0C0C0')
                        line6.add(array[0] + '-' + 'MA30', btime, ma30)
                        layout.add(line6)
                    layout.add(candlestick)
            page.add(layout)
    page.render()","for i in items:
    j = re.split('-', i)
    if len(j) == 3:
        a = generateline(j[1], j[2], startdate, enddate, option)
        if a is None:
            continue
        time = [d[0] for d in a]
        if j[2] != 'Kline':
            if len(a[0]) == 4 and a[0][2] == 'bar':
                overlap = Overlap()
                form = [e[1] for e in a]
                bar = Bar(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                bar.add(j[0] + '-' + j[2], time, form, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider')
                overlap.add(bar)
                line = Line(j[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                price = [e[3] for e in a]
                line.add(j[0] + 'price', time, price, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider', yaxis_type='value')
                overlap.add(line, yaxis_index=1, is_add_yaxis=True)
                page.add(overlap)
            if len(a[0]) == 5 and a[0][3] == 'pie':
                overlap = Overlap()
                timeline = Timeline(is_auto_play=False, timeline_bottom=0)
                namearray = [c[0] for c in a]
                valuearray = [d[1] for d in a]
                quarter = [e[2] for e in a]
                num = a[0][4]
                for x in range(0, num / 10):
                    list1 = valuearray[x]
                    names = namearray[x]
                    quarters = quarter[x][0]
                    for (idx, val) in enumerate(list1):
                        list1[idx] = float(val)
                    pie = Pie(j[0] + '-' + '前十股东'.decode('utf-8'), width=width1 * 10 / 11, height=height1 * 10 / 11)
                    pie.add(j[0] + '-' + '前十股东'.decode('utf-8'), names, list1, radius=[30, 55], is_legend_show=False, is_label_show=True, label_formatter='{b}: {c}\n{d}%')
                    timeline.add(pie, quarters)
                timeline.render()
                return
            else:
                form = [e[1] for e in a]
                line = Line(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                line.add(j[0] + '-' + j[2], time, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                page.add(line)
        else:
            overlap = Overlap()
            close = zip(*a)[2]
            candle = [[x[1], x[2], x[3], x[4]] for x in a]
            candlestick = Kline(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
            candlestick.add(j[0], time, candle, is_datazoom_show=True, datazoom_type='slider', yaxis_interval=1)
            overlap.add(candlestick)
            if len(close) > 10:
                ma10 = calculateMa(close, 10)
                line1 = Line(title_color='#C0C0C0')
                line1.add(j[0] + '-' + 'MA10', time, ma10)
                overlap.add(line1)
            if len(close) > 20:
                ma20 = calculateMa(close, 20)
                line2 = Line(title_color='#C0C0C0')
                line2.add(j[0] + '-' + 'MA20', time, ma20)
                overlap.add(line2)
            if len(close) > 30:
                ma30 = calculateMa(close, 30)
                line3 = Line(title_color='#C0C0C0')
                line3.add(j[0] + '-' + 'MA30', time, ma30)
                overlap.add(line3)
            page.add(overlap)
    else:
        for k in range(1, len(j) / 3):
            j[3 * k - 1] = re.sub('\n&', '', j[3 * k - 1])
        sizearray = []
        layout = Overlap()
        for i in xrange(0, len(j), 3):
            array = j[i:i + 3]
            b = generateline(array[1], array[2], startdate, enddate, option)
            if b is None:
                continue
            btime = [d[0] for d in b]
            if array[2] != 'Kline':
                if len(b[0]) == 4 and b[0][2] == 'bar':
                    form = [e[1] for e in b]
                    bar = Bar(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    bar.add(array[0] + '-' + array[2], btime, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                    layout.add(bar)
                    line = Line(array[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    price = [e[3] for e in b]
                    line.add(array[0] + 'price', btime, price, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_type='value')
                    layout.add(line, yaxis_index=1, is_add_yaxis=True)
                else:
                    line = Line(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    line.add(array[0] + '-' + array[2], btime, b, is_datazoom_show=True, yaxis_max='dataMax', yaxis_min='dataMin', datazoom_type='slider')
                    layout.add(line)
            else:
                candle = [[x[1], x[2], x[3], x[4]] for x in b]
                candlestick = Kline(array[0] + '-' + array[1], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                candlestick.add(array[0], btime, candle, is_datazoom_show=True, datazoom_type=['slider'])
                close = zip(*b)[2]
                if len(close) > 10:
                    ma10 = calculateMa(close, 10)
                    line4 = Line(title_color='#C0C0C0')
                    line4.add(array[0] + '-' + 'MA10', btime, ma10)
                    layout.add(line4)
                if len(close) > 20:
                    ma20 = calculateMa(close, 20)
                    line5 = Line(title_color='#C0C0C0')
                    line5.add(array[0] + '-' + 'MA20', btime, ma20)
                    layout.add(line5)
                if len(close) > 30:
                    ma30 = calculateMa(close, 30)
                    line6 = Line(title_color='#C0C0C0')
                    line6.add(array[0] + '-' + 'MA30', btime, ma30)
                    layout.add(line6)
                layout.add(candlestick)
        page.add(layout)","for i, item in enumerate(items):
    j = re.split('-', item)
    if len(j) == 3:
        a = generateline(j[1], j[2], startdate, enddate, option)
        if a is None:
            continue
        time = [d[0] for d in a]
        if j[2] != 'Kline':
            if len(a[0]) == 4 and a[0][2] == 'bar':
                overlap = Overlap()
                form = [e[1] for e in a]
                bar = Bar(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                bar.add(j[0] + '-' + j[2], time, form, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider')
                overlap.add(bar)
                line = Line(j[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                price = [e[3] for e in a]
                line.add(j[0] + 'price', time, price, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider', yaxis_type='value')
                overlap.add(line, yaxis_index=1, is_add_yaxis=True)
                page.add(overlap)
            if len(a[0]) == 5 and a[0][3] == 'pie':
                overlap = Overlap()
                timeline = Timeline(is_auto_play=False, timeline_bottom=0)
                namearray = [c[0] for c in a]
                valuearray = [d[1] for d in a]
                quarter = [e[2] for e in a]
                num = a[0][4]
                for x in range(0, num / 10):
                    list1 = valuearray[x]
                    names = namearray[x]
                    quarters = quarter[x][0]
                    for (idx, val) in enumerate(list1):
                        list1[idx] = float(val)
                    pie = Pie(j[0] + '-' + '前十股东'.decode('utf-8'), width=width1 * 10 / 11, height=height1 * 10 / 11)
                    pie.add(j[0] + '-' + '前十股东'.decode('utf-8'), names, list1, radius=[30, 55], is_legend_show=False, is_label_show=True, label_formatter='{b}: {c}\n{d}%')
                    timeline.add(pie, quarters)
                timeline.render()
                return
            else:
                form = [e[1] for e in a]
                line = Line(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                line.add(j[0] + '-' + j[2], time, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                page.add(line)
        else:
            overlap = Overlap()
            close = zip(*a)[2]
            candle = [[x[1], x[2], x[3], x[4]] for x in a]
            candlestick = Kline(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
            candlestick.add(j[0], time, candle, is_datazoom_show=True, datazoom_type='slider', yaxis_interval=1)
            overlap.add(candlestick)
            if len(close) > 10:
                ma10 = calculateMa(close, 10)
                line1 = Line(title_color='#C0C0C0')
                line1.add(j[0] + '-' + 'MA10', time, ma10)
                overlap.add(line1)
            if len(close) > 20:
                ma20 = calculateMa(close, 20)
                line2 = Line(title_color='#C0C0C0')
                line2.add(j[0] + '-' + 'MA20', time, ma20)
                overlap.add(line2)
            if len(close) > 30:
                ma30 = calculateMa(close, 30)
                line3 = Line(title_color='#C0C0C0')
                line3.add(j[0] + '-' + 'MA30', time, ma30)
                overlap.add(line3)
            page.add(overlap)
    else:
        for k in range(1, len(j) / 3):
            j[3 * k - 1] = re.sub('\n&', '', j[3 * k - 1])
        sizearray = []
        layout = Overlap()
        for idx in xrange(0, len(j), 3):
            array = j[idx:idx + 3]
            b = generateline(array[1], array[2], startdate, enddate, option)
            if b is None:
                continue
            btime = [d[0] for d in b]
            if array[2] != 'Kline':
                if len(b[0]) == 4 and b[0][2] == 'bar':
                    form = [e[1] for e in b]
                    bar = Bar(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    bar.add(array[0] + '-' + array[2], btime, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                    layout.add(bar)
                    line = Line(array[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    price = [e[3] for e in b]
                    line.add(array[0] + 'price', btime, price, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_type='value')
                    layout.add(line, yaxis_index=1, is_add_yaxis=True)
                else:
                    line = Line(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    line.add(array[0] + '-' + array[2], btime, b, is_datazoom_show=True, yaxis_max='dataMax', yaxis_min='dataMin', datazoom_type='slider')
                    layout.add(line)
            else:
                candle = [[x[1], x[2], x[3], x[4]] for x in b]
                candlestick = Kline(array[0] + '-' + array[1], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                candlestick.add(array[0], btime, candle, is_datazoom_show=True, datazoom_type=['slider'])
                close = zip(*b)[2]
                if len(close) > 10:
                    ma10 = calculateMa(close, 10)
                    line4 = Line(title_color='#C0C0C0')
                    line4.add(array[0] + '-' + 'MA10', btime, ma10)
                    layout.add(line4)
                if len(close) > 20:
                    ma20 = calculateMa(close, 20)
                    line5 = Line(title_color='#C0C0C0')
                    line5.add(array[0] + '-' + 'MA20', btime, ma20)
                    layout.add(line5)
                if len(close) > 30:
                    ma30 = calculateMa(close, 30)
                    line6 = Line(title_color='#C0C0C0')
                    line6.add(array[0] + '-' + 'MA30', btime, ma30)
                    layout.add(line6)
                layout.add(candlestick)
        page.add(layout)",1,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for item in self.data.trainSet_i:
    if len(self.data.trainSet_i[item]) > 1:
        self.itemNet[item] = self.data.trainSet_i[item]","for i, item in enumerate(self.data.trainSet_i):
    if len(self.data.trainSet_i[item]) > 1:
        self.itemNet[item] = self.data.trainSet_i[item]",1,,,,,,,,,,
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/youtube.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/youtube.py,YoutubeIE,"def _get_automatic_captions(self, video_id, webpage):
    """"""We need the webpage for getting the captions url, pass it as an
           argument to speed up the process.""""""
    self.to_screen('%s: Looking for automatic captions' % video_id)
    player_config = self._get_ytplayer_config(video_id, webpage)
    err_msg = ""Couldn't find automatic captions for %s"" % video_id
    if not player_config:
        self._downloader.report_warning(err_msg)
        return {}
    try:
        args = player_config['args']
        caption_url = args.get('ttsurl')
        if caption_url:
            timestamp = args['timestamp']
            list_params = compat_urllib_parse_urlencode({'type': 'list', 'tlangs': 1, 'asrs': 1})
            list_url = caption_url + '&' + list_params
            caption_list = self._download_xml(list_url, video_id)
            original_lang_node = caption_list.find('track')
            if original_lang_node is None:
                self._downloader.report_warning(""Video doesn't have automatic captions"")
                return {}
            original_lang = original_lang_node.attrib['lang_code']
            caption_kind = original_lang_node.attrib.get('kind', '')
            sub_lang_list = {}
            for lang_node in caption_list.findall('target'):
                sub_lang = lang_node.attrib['lang_code']
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
                    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})
                sub_lang_list[sub_lang] = sub_formats
            return sub_lang_list

        def make_captions(sub_url, sub_langs):
            parsed_sub_url = compat_urllib_parse_urlparse(sub_url)
            caption_qs = compat_parse_qs(parsed_sub_url.query)
            captions = {}
            for sub_lang in sub_langs:
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    caption_qs.update({'tlang': [sub_lang], 'fmt': [ext]})
                    sub_url = compat_urlparse.urlunparse(parsed_sub_url._replace(query=compat_urllib_parse_urlencode(caption_qs, True)))
                    sub_formats.append({'url': sub_url, 'ext': ext})
                captions[sub_lang] = sub_formats
            return captions
        player_response = args.get('player_response')
        if player_response and isinstance(player_response, compat_str):
            player_response = self._parse_json(player_response, video_id, fatal=False)
            if player_response:
                renderer = player_response['captions']['playerCaptionsTracklistRenderer']
                base_url = renderer['captionTracks'][0]['baseUrl']
                sub_lang_list = []
                for lang in renderer['translationLanguages']:
                    lang_code = lang.get('languageCode')
                    if lang_code:
                        sub_lang_list.append(lang_code)
                return make_captions(base_url, sub_lang_list)
        caption_tracks = args['caption_tracks']
        caption_translation_languages = args['caption_translation_languages']
        caption_url = compat_parse_qs(caption_tracks.split(',')[0])['u'][0]
        sub_lang_list = []
        for lang in caption_translation_languages.split(','):
            lang_qs = compat_parse_qs(compat_urllib_parse_unquote_plus(lang))
            sub_lang = lang_qs.get('lc', [None])[0]
            if sub_lang:
                sub_lang_list.append(sub_lang)
        return make_captions(caption_url, sub_lang_list)
    except (KeyError, IndexError, ExtractorError):
        self._downloader.report_warning(err_msg)
        return {}","for lang in renderer['translationLanguages']:
    lang_code = lang.get('languageCode')
    if lang_code:
        sub_lang_list.append(lang_code)","for i, lang in enumerate(renderer['translationLanguages']):
    lang_code = lang.get('languageCode')
    if lang_code:
        sub_lang_list.append(lang_code)",1,,,,,,,,,,
MLAlgorithms,https://github.com/rushter/MLAlgorithms/tree/master/mla/neuralnet/nnet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MLAlgorithms/mla/neuralnet/nnet.py,NeuralNet,"def is_training(self, train):
    self.training = train
    for layer in self.layers:
        if isinstance(layer, PhaseMixin):
            layer.is_training = train","for layer in self.layers:
    if isinstance(layer, PhaseMixin):
        layer.is_training = train","for i, layer in enumerate(self.layers):
    if isinstance(layer, PhaseMixin):
        self.layers[i].is_training = train",1,,,,,,,,,,
texar,https://github.com/asyml/texar/tree/master/texar/tf/core/layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar/texar/tf/core/layers.py,MergeLayer,"def call(self, inputs):
    if self._layers is None:
        layer_outputs = inputs
        if not isinstance(layer_outputs, (list, tuple)):
            layer_outputs = [layer_outputs]
    else:
        layer_outputs = []
        for layer in self._layers:
            layer_output = layer(inputs)
            layer_outputs.append(layer_output)
    if self._mode == 'concat':
        outputs = tf.concat(values=layer_outputs, axis=self._axis)
    elif self._mode == 'elemwise_sum':
        outputs = layer_outputs[0]
        for i in range(1, len(layer_outputs)):
            outputs = tf.add(outputs, layer_outputs[i])
    elif self._mode == 'elemwise_mul':
        outputs = layer_outputs[0]
        for i in range(1, len(layer_outputs)):
            outputs = tf.multiply(outputs, layer_outputs[i])
    elif self._mode == 'sum':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_sum(_concat, axis=self._axis)
    elif self._mode == 'mean':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_mean(_concat, axis=self._axis)
    elif self._mode == 'prod':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_prod(_concat, axis=self._axis)
    elif self._mode == 'max':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_max(_concat, axis=self._axis)
    elif self._mode == 'min':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_min(_concat, axis=self._axis)
    elif self._mode == 'and':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_all(_concat, axis=self._axis)
    elif self._mode == 'or':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_any(_concat, axis=self._axis)
    elif self._mode == 'logsumexp':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_logsumexp(_concat, axis=self._axis)
    else:
        raise ValueError(""Unknown merge mode: '%s'"" % self._mode)
    if not self.built or not self._vars_built:
        self._collect_weights()
        self._vars_built = True
    return outputs","for i in range(1, len(layer_outputs)):
    outputs = tf.multiply(outputs, layer_outputs[i])","for i, layer_output in enumerate(layer_outputs[1:]):
    outputs = tf.multiply(outputs, layer_output)",1,,,,,,,,,,
baserow,https://github.com/bram2w/baserow/tree/master/premium/backend/src/baserow_premium/license/handler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/baserow/premium/backend/src/baserow_premium/license/handler.py,LicenseHandler,"def send_license_info_and_fetch_license_status_with_authority(cls, license_objects: List[License]):
    license_payloads = []
    extra_license_info = []
    for license_object in license_objects:
        license_payloads.append(license_object.license)
        try:
            license_type = license_object.license_type
            extra_info = {'id': license_object.license_id, 'seats_taken': license_type.get_seats_taken(license_object), 'free_users_count': license_type.get_free_users_count(license_object)}
            extra_license_info.append(extra_info)
        except (InvalidLicenseError, UnsupportedLicenseError, DatabaseError):
            pass
    return cls.fetch_license_status_with_authority(license_payloads, extra_license_info)","for license_object in license_objects:
    license_payloads.append(license_object.license)
    try:
        license_type = license_object.license_type
        extra_info = {'id': license_object.license_id, 'seats_taken': license_type.get_seats_taken(license_object), 'free_users_count': license_type.get_free_users_count(license_object)}
        extra_license_info.append(extra_info)
    except (InvalidLicenseError, UnsupportedLicenseError, DatabaseError):
        pass","for i, license_object in enumerate(license_objects):
    license_payloads.append(license_object.license)
    try:
        license_type = license_object.license_type
        extra_info = {'id': license_object.license_id, 'seats_taken': license_type.get_seats_taken(license_object), 'free_users_count': license_type.get_free_users_count(license_object)}
        extra_license_info.append(extra_info)
    except (InvalidLicenseError, UnsupportedLicenseError, DatabaseError):
        pass",,,,,,,,,,,
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for line in input_file:
    filenames.append(line.rstrip())","for i, line in enumerate(input_file):
    filenames.append(line.rstrip())",,,,,,,,,,,
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for link_span in link_spans:
    _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
    offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for i, link_span in enumerate(link_spans):
    _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
    offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])",,,,,,,,,,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/dns_data_management.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/dns_data_management.py,IPASystemRecords,"def _get_location_dns_records_for_server(self, zone_obj, hostname, locations, roles=None, include_master_role=True, include_kerberos_realm=True):
    server = self.servers_data[hostname]
    if roles:
        eff_roles = server['roles'] & roles
    else:
        eff_roles = server['roles']
    hostname_abs = DNSName(hostname).make_absolute()
    for location in locations:
        if location == self.servers_data[hostname]['location']:
            priority = self.PRIORITY_HIGH
        else:
            priority = self.PRIORITY_LOW
        if include_kerberos_realm:
            self.__add_kerberos_txt_rec(zone_obj, location)
        if include_master_role:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
            self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
        if 'AD trust controller' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
        if 'NTP server' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)
    return zone_obj","for location in locations:
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)","for i, location in enumerate(locations):
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)",,,,,,,,,,,
pgsync,https://github.com/toluaina/pgsync/tree/master/pgsync/settings.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgsync/pgsync/settings.py,,"def _get_logging_config(silent_loggers: Optional[str]=None):
    """"""
    Return the python logging configuration based on environment variables.
    """"""
    config: dict = {'version': 1, 'disable_existing_loggers': False, 'formatters': {'simple': {'format': '%(asctime)s.%(msecs)03d:%(levelname)s:%(name)s: %(message)s', 'datefmt': '%Y-%m-%d %H:%M:%S'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'level': env.str('CONSOLE_LOGGING_HANDLER_MIN_LEVEL', default='WARNING'), 'formatter': 'simple'}}, 'loggers': {'': {'handlers': env.list('LOG_HANDLERS', default=['console']), 'level': env.str('GENERAL_LOGGING_LEVEL', default='DEBUG'), 'propagate': True}}}
    if silent_loggers:
        for silent_logger in silent_loggers:
            config['loggers'][silent_logger] = {'level': 'INFO'}
    for logger_config in env.list('CUSTOM_LOGGING', default=[]):
        (logger, level) = logger_config.split('=')
        config['loggers'][logger] = {'level': level}
    return config","for logger_config in env.list('CUSTOM_LOGGING', default=[]):
    (logger, level) = logger_config.split('=')
    config['loggers'][logger] = {'level': level}","for i, logger_config in enumerate(env.list('CUSTOM_LOGGING', default=[])):
    (logger, level) = logger_config.split('=')
    config['loggers'][logger] = {'level': level}",,,,,,,,,,,
maltrail,https://github.com/stamparm/maltrail/tree/master/trails/feeds/360necurs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maltrail/trails/feeds/360necurs.py,,"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
            retval[match.group(1)] = (__info__, __reference__)
    return retval","for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
    retval[match.group(1)] = (__info__, __reference__)","for i, match in enumerate(re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content)):
    retval[match.group(1)] = (__info__, __reference__)",,,,,,,,,,,
integrations-core,https://github.com/DataDog/integrations-core/tree/master/presto/tests/test_presto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/presto/tests/test_presto.py,,"def test(dd_agent_check):
    instance = {}
    aggregator = dd_agent_check(instance, rate=True)
    for metric in METRICS + JVM_E2E_METRICS_NEW:
        aggregator.assert_metric(metric)
    aggregator.assert_all_metrics_covered()
    aggregator.assert_metrics_using_metadata(get_metadata_metrics(), exclude=JVM_E2E_METRICS_NEW)","for metric in METRICS + JVM_E2E_METRICS_NEW:
    aggregator.assert_metric(metric)","for i, metric in enumerate(METRICS + JVM_E2E_METRICS_NEW):
    aggregator.assert_metric(metric)",,,,,,,,,,,
elasticdl,https://github.com/sql-machine-learning/elasticdl/tree/master/elasticdl/python/common/evaluation_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elasticdl/elasticdl/python/common/evaluation_utils.py,EvaluationMetrics,"def reset_metric_states(self):
    """"""Resets all of the metric state variables.""""""
    for metrics in self._metrics_dict.values():
        for metric_inst in metrics.values():
            metric_inst.reset_states()","for metrics in self._metrics_dict.values():
    for metric_inst in metrics.values():
        metric_inst.reset_states()","for i, metrics in enumerate(self._metrics_dict.values()):
    for metric_inst in metrics.values():
        metric_inst.reset_states()",,,,,,,,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/f2py/f2py2e.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/f2py/f2py2e.py,,"def callcrackfortran(files, options):
    rules.options = options
    crackfortran.debug = options['debug']
    crackfortran.verbose = options['verbose']
    if 'module' in options:
        crackfortran.f77modulename = options['module']
    if 'skipfuncs' in options:
        crackfortran.skipfuncs = options['skipfuncs']
    if 'onlyfuncs' in options:
        crackfortran.onlyfuncs = options['onlyfuncs']
    crackfortran.include_paths[:] = options['include_paths']
    crackfortran.dolowercase = options['do-lower']
    postlist = crackfortran.crackfortran(files)
    if 'signsfile' in options:
        outmess('Saving signatures to file ""%s""\n' % options['signsfile'])
        pyf = crackfortran.crack2fortran(postlist)
        if options['signsfile'][-6:] == 'stdout':
            sys.stdout.write(pyf)
        else:
            with open(options['signsfile'], 'w') as f:
                f.write(pyf)
    if options['coutput'] is None:
        for mod in postlist:
            mod['coutput'] = '%smodule.c' % mod['name']
    else:
        for mod in postlist:
            mod['coutput'] = options['coutput']
    if options['f2py_wrapper_output'] is None:
        for mod in postlist:
            mod['f2py_wrapper_output'] = '%s-f2pywrappers.f' % mod['name']
    else:
        for mod in postlist:
            mod['f2py_wrapper_output'] = options['f2py_wrapper_output']
    return postlist","for mod in postlist:
    mod['coutput'] = options['coutput']","for i, mod in enumerate(postlist):
    mod['coutput'] = options['coutput']",,,,,,,,,,,
h,https://github.com/hypothesis/h/tree/master/tests/h/views/activity_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/h/tests/h/views/activity_test.py,TestGroupSearchController,"def test_search_returns_group_moderators_faceted_by(self, controller, pyramid_request, test_user, test_group):
    pyramid_request.params = {'q': 'user:does_not_matter'}
    result = controller.search()
    for moderator in result['group_users_args'][1]:
        assert not moderator['faceted_by']","for moderator in result['group_users_args'][1]:
    assert not moderator['faceted_by']","for i, moderator in enumerate(result['group_users_args'][1]):
    assert not moderator['faceted_by']",,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/testsuite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/testsuite.py,,"def create_op(scope, op_type, inputs, outputs, attrs, cache_list=None):
    kwargs = dict()
    op_maker = core.op_proto_and_checker_maker
    op_role_attr_name = op_maker.kOpRoleAttrName()
    if op_role_attr_name not in attrs:
        attrs[op_role_attr_name] = int(op_maker.OpRole.Forward)

    def __create_var__(name, var_name):
        scope.var(var_name).get_tensor()
        kwargs[name].append(var_name)
    for (in_name, in_dup) in Operator.get_op_inputs(op_type):
        if in_name in inputs:
            kwargs[in_name] = []
            if in_dup:
                sub_in = inputs[in_name]
                for item in sub_in:
                    (sub_in_name, _) = (item[0], item[1])
                    __create_var__(in_name, sub_in_name)
            else:
                __create_var__(in_name, in_name)
    if cache_list is not None and isinstance(cache_list, list):
        for name in cache_list:
            kwargs[name] = []
            scope.var(name)
            kwargs[name].append(name)
    for (out_name, out_dup) in Operator.get_op_outputs(op_type):
        if out_name in outputs:
            kwargs[out_name] = []
            if out_dup:
                sub_out = outputs[out_name]
                for item in sub_out:
                    (sub_out_name, _) = (item[0], item[1])
                    __create_var__(out_name, sub_out_name)
            else:
                __create_var__(out_name, out_name)
    for attr_name in Operator.get_op_attr_names(op_type):
        if attr_name in attrs:
            kwargs[attr_name] = attrs[attr_name]
    for extra_attr_name in Operator.get_op_extra_attr_names(op_type):
        if extra_attr_name in attrs:
            kwargs[extra_attr_name] = attrs[extra_attr_name]
    return Operator(op_type, **kwargs)","for name in cache_list:
    kwargs[name] = []
    scope.var(name)
    kwargs[name].append(name)","for i, name in enumerate(cache_list):
    kwargs[name] = []
    scope.var(name)
    kwargs[name].append(name)",,,,,,,,,,,
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/ClientGUIScrolledPanelsReview.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/gui/ClientGUIScrolledPanelsReview.py,ReviewVacuumData,"def _CanVacuum(self):
    names = self._vacuum_listctrl.GetData(only_selected=True)
    if len(names) == 0:
        return False
    for name in names:
        (result, info) = self._CanVacuumName(name)
        if not result:
            return False
    return True","for name in names:
    (result, info) = self._CanVacuumName(name)
    if not result:
        return False","for i, name in enumerate(names):
    (result, info) = self._CanVacuumName(name)
    if not result:
        return False",,,,,,,,,,,
DFDNet,https://github.com/csxmli2016/DFDNet/tree/master/models/base_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DFDNet/models/base_model.py,BaseModel,"def print_networks(self, verbose):
    for name in self.model_names:
        if isinstance(name, str):
            net = getattr(self, 'net' + name)
            num_params = 0
            for param in net.parameters():
                num_params += param.numel()","for name in self.model_names:
    if isinstance(name, str):
        net = getattr(self, 'net' + name)
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()","for i, name in enumerate(self.model_names):
    if isinstance(name, str):
        net = getattr(self, 'net' + name)
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()",,,,,,,,,,,
fairscale,https://github.com/facebookresearch/fairscale/tree/master/fairscale/nn/model_parallel/random.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairscale/fairscale/nn/model_parallel/random.py,CudaRNGStatesTracker,"def get_states(self) -> Dict[str, torch.ByteTensor]:
    """"""Get rng states. Copy the dictionary so we have direct
        pointers to the states, not just a pointer to the dictionary.""""""
    states = {}
    for name in self.states_:
        states[name] = self.states_[name]
    return states","for name in self.states_:
    states[name] = self.states_[name]","for i, name in enumerate(self.states_):
    states[name] = self.states_[name]",,,,,,,,,,,
angr,https://github.com/angr/angr/tree/master/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py,PagedMemoryMixin,"def merge(self, others: Iterable['PagedMemoryMixin'], merge_conditions, common_ancestor=None) -> bool:
    changed_pages_and_offsets: Dict[int, Optional[Set[int]]] = {}
    for o in others:
        for (changed_page, changed_offsets) in self.changed_pages(o).items():
            if changed_offsets is None:
                changed_pages_and_offsets[changed_page] = None
            elif changed_page not in changed_pages_and_offsets:
                changed_pages_and_offsets[changed_page] = changed_offsets
            elif changed_pages_and_offsets[changed_page] is None:
                pass
            else:
                changed_pages_and_offsets[changed_page] = changed_pages_and_offsets[changed_page].union(changed_offsets)
    if merge_conditions is None:
        merge_conditions = [None] * (len(list(others)) + 1)
    merged_bytes = set()
    for page_no in sorted(changed_pages_and_offsets.keys()):
        l.debug('... on page %x', page_no)
        page = self._get_page(page_no, True)
        other_pages = []
        for o in others:
            if page_no in o._pages:
                other_pages.append(o._get_page(page_no, False))
        page_addr = page_no * self.page_size
        changed_offsets = changed_pages_and_offsets[page_no]
        merged_offsets = page.merge(other_pages, merge_conditions, page_addr=page_addr, memory=self, changed_offsets=changed_offsets)
        for off in merged_offsets:
            merged_bytes.add(page_addr + off)
    return bool(merged_bytes)","for off in merged_offsets:
    merged_bytes.add(page_addr + off)","for i, off in enumerate(merged_offsets):
    merged_bytes.add(page_addr + off)",,,,,,,,,,,
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/tests/ops/qubit/test_non_parametric_ops.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/tests/ops/qubit/test_non_parametric_ops.py,TestDecompositions,"def test_SISWAP_decomposition(self, siswap_op, tol):
    """"""Tests that the decomposition of the SISWAP gate and its SQISW alias gate is correct""""""
    op = siswap_op(wires=[0, 1])
    res = op.decomposition()
    assert len(res) == 12
    assert res[0].wires == Wires([0])
    assert res[1].wires == Wires([0])
    assert res[2].wires == Wires([0, 1])
    assert res[3].wires == Wires([0])
    assert res[4].wires == Wires([0])
    assert res[5].wires == Wires([0])
    assert res[6].wires == Wires([0])
    assert res[7].wires == Wires([1])
    assert res[8].wires == Wires([1])
    assert res[9].wires == Wires([0, 1])
    assert res[10].wires == Wires([0])
    assert res[11].wires == Wires([1])
    assert res[0].name == 'SX'
    assert res[1].name == 'RZ'
    assert res[2].name == 'CNOT'
    assert res[3].name == 'SX'
    assert res[4].name == 'RZ'
    assert res[5].name == 'SX'
    assert res[6].name == 'RZ'
    assert res[7].name == 'SX'
    assert res[8].name == 'RZ'
    assert res[9].name == 'CNOT'
    assert res[10].name == 'SX'
    assert res[11].name == 'SX'
    mats = []
    for i in reversed(res):
        if i.wires == Wires([1]):
            mats.append(np.kron(np.eye(2), i.matrix()))
        elif i.wires == Wires([0]):
            mats.append(np.kron(i.matrix(), np.eye(2)))
        elif i.wires == Wires([1, 0]) and i.name == 'CNOT':
            mats.append(np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]))
        else:
            mats.append(i.matrix())
    decomposed_matrix = np.linalg.multi_dot(mats)
    assert np.allclose(decomposed_matrix, op.matrix(), atol=tol, rtol=0)","for i in reversed(res):
    if i.wires == Wires([1]):
        mats.append(np.kron(np.eye(2), i.matrix()))
    elif i.wires == Wires([0]):
        mats.append(np.kron(i.matrix(), np.eye(2)))
    elif i.wires == Wires([1, 0]) and i.name == 'CNOT':
        mats.append(np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]))
    else:
        mats.append(i.matrix())","for i, op in enumerate(reversed(res)):
    if op.wires == Wires([1]):
        mats.append(np.kron(np.eye(2), op.matrix()))
    elif op.wires == Wires([0]):
        mats.append(np.kron(op.matrix(), np.eye(2)))
    elif op.wires == Wires([1, 0]) and op.name == 'CNOT':
        mats.append(np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]))
    else:
        mats.append(op.matrix())",,,,,,,,,,,
nevergrad,https://github.com/facebookresearch/nevergrad/tree/master/nevergrad/benchmark/experiments.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nevergrad/nevergrad/benchmark/experiments.py,,"def constrained_illconditioned_parallel(seed: tp.Optional[int]=None) -> tp.Iterator[Experiment]:
    """"""Many optimizers on ill cond problems with constraints.""""""
    seedg = create_seed_generator(seed)
    functions = [ArtificialFunction(name, block_dimension=50, rotation=rotation) for name in ['cigar', 'ellipsoid'] for rotation in [True, False]]
    for func in functions:
        func.parametrization.register_cheap_constraint(_Constraint('sum', as_bool=False))
    for function in functions:
        for budget in [400, 4000, 40000]:
            optims: tp.List[str] = get_optimizers('large', seed=next(seedg))
            for optim in optims:
                yield Experiment(function, optim, budget=budget, num_workers=1, seed=next(seedg))","for optim in optims:
    yield Experiment(function, optim, budget=budget, num_workers=1, seed=next(seedg))","for i, optim in enumerate(optims):
    yield Experiment(function, optim, budget=budget, num_workers=1, seed=next(seedg))",,,,,,,,,,,
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","for overwrite in remaining_overwrites:
    if overwrite.is_role() and roles.has(overwrite.id):
        denies |= overwrite.deny
        allows |= overwrite.allow","for i, overwrite in enumerate(remaining_overwrites):
    if overwrite.is_role() and roles.has(overwrite.id):
        denies |= overwrite.deny
        allows |= overwrite.allow",,,,,,,,,,,
django,https://github.com/django/django/tree/master/django/core/paginator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/django/core/paginator.py,Paginator,"def __iter__(self):
    for page_number in self.page_range:
        yield self.page(page_number)","for page_number in self.page_range:
    yield self.page(page_number)","for i, page_number in enumerate(self.page_range):
    yield self.page(page_number)",,,,,,,,,,,
MeshCNN,https://github.com/ranahanocka/MeshCNN/tree/master/util/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MeshCNN/util/util.py,,"def print_network(net):
    """"""Print the total number of parameters in the network
    Parameters:
        network
    """"""
    print('---------- Network initialized -------------')
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print('[Network] Total number of parameters : %.3f M' % (num_params / 1000000.0))
    print('-----------------------------------------------')","for param in net.parameters():
    num_params += param.numel()","for i, param in enumerate(net.parameters()):
    num_params += param.numel()",,,,,,,,,,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/notifications/notifications/digest.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/notifications/notifications/digest.py,DigestNotification,"def send(self) -> None:
    shared_context = self.get_context()
    if should_send_as_alert_notification(shared_context):
        return send_as_alert_notification(shared_context, self.target_type, self.target_identifier)
    participants_by_provider_by_event = get_participants_by_event(self.digest, self.project, self.target_type, self.target_identifier)
    actor_ids = set()
    combined_participants_by_provider = defaultdict(set)
    for participants_by_provider in participants_by_provider_by_event.values():
        for (provider, participants) in participants_by_provider.items():
            for participant in participants:
                actor_ids.add(participant.actor_id)
                combined_participants_by_provider[provider].add(participant)
    if not actor_ids:
        return
    logger.info('mail.adapter.notify_digest', extra={'project_id': self.project.id, 'target_type': self.target_type.value, 'target_identifier': self.target_identifier, 'actor_ids': actor_ids})
    extra_context: Mapping[int, Mapping[str, Any]] = {}
    if should_get_personalized_digests(self.target_type, self.project.id):
        extra_context = self.get_extra_context(participants_by_provider_by_event)
    for (provider, participants) in combined_participants_by_provider.items():
        notify(provider, self, participants, shared_context, extra_context)","for participant in participants:
    actor_ids.add(participant.actor_id)
    combined_participants_by_provider[provider].add(participant)","for i, participant in enumerate(participants):
    actor_ids.add(participant.actor_id)
    combined_participants_by_provider[provider].add(participant)",,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for post_id in post_ids:
    for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if i % 1000000 == 0:
            print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)","for i, post_id in enumerate(post_ids):
    for (j, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if j % 1000000 == 0:
            print('read %d lines, found %d' % (j, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)",,,,,,,,,,,
indico,https://github.com/indico/indico/tree/master/indico/modules/events/abstracts/forms.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/indico/indico/modules/events/abstracts/forms.py,,"def make_review_form(event):
    """"""Extend the abstract WTForm to add the extra fields.

    Each extra field will use a field named ``custom_ID``.

    :param event: The `Event` for which to create the abstract form.
    :return: An `AbstractForm` subclass.
    """"""
    form_class = type('_AbstractReviewForm', (AbstractReviewForm,), {})
    for question in event.abstract_review_questions:
        name = f'question_{question.id}'
        setattr(form_class, name, question.field.create_wtf_field())
    return form_class","for question in event.abstract_review_questions:
    name = f'question_{question.id}'
    setattr(form_class, name, question.field.create_wtf_field())","for i, question in enumerate(event.abstract_review_questions):
    name = f'question_{question.id}'
    setattr(form_class, name, question.field.create_wtf_field())",,,,,,,,,,,
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for recorder in self.recorders.values():
    recorders.append(recorder.to_dict())","for i, recorder in enumerate(self.recorders.values()):
    recorders.append(recorder.to_dict())",,,,,,,,,,,
Cura,https://github.com/Ultimaker/Cura/tree/master/plugins/VersionUpgrade/VersionUpgrade44to45/VersionUpgrade44to45.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/plugins/VersionUpgrade/VersionUpgrade44to45/VersionUpgrade44to45.py,VersionUpgrade44to45,"def upgradeInstanceContainer(self, serialized: str, filename: str) -> Tuple[List[str], List[str]]:
    """"""Upgrades instance containers to have the new version number.

        This renames the renamed settings in the containers.
        """"""
    parser = configparser.ConfigParser(interpolation=None, comment_prefixes=())
    parser.read_string(serialized)
    parser['metadata']['setting_version'] = '11'
    if 'values' in parser:
        for (preferred, removed) in _merged_settings.items():
            if removed in parser['values']:
                if preferred not in parser['values']:
                    parser['values'][preferred] = parser['values'][removed]
                del parser['values'][removed]
        for removed in _removed_settings:
            if removed in parser['values']:
                del parser['values'][removed]
    result = io.StringIO()
    parser.write(result)
    return ([filename], [result.getvalue()])","for removed in _removed_settings:
    if removed in parser['values']:
        del parser['values'][removed]","for i, removed in enumerate(_removed_settings):
    if removed in parser['values']:
        del parser['values'][removed]",,,,,,,,,,,
volatility3,https://github.com/volatilityfoundation/volatility3/tree/master/volatility3/framework/plugins/windows/modscan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/volatility3/volatility3/framework/plugins/windows/modscan.py,ModScan,"def scan_modules(cls, context: interfaces.context.ContextInterface, layer_name: str, symbol_table: str) -> Iterable[interfaces.objects.ObjectInterface]:
    """"""Scans for modules using the poolscanner module and constraints.

        Args:
            context: The context to retrieve required elements (layers, symbol tables) from
            layer_name: The name of the layer on which to operate
            symbol_table: The name of the table containing the kernel symbols

        Returns:
            A list of Driver objects as found from the `layer_name` layer based on Driver pool signatures
        """"""
    constraints = poolscanner.PoolScanner.builtin_constraints(symbol_table, [b'MmLd'])
    for result in poolscanner.PoolScanner.generate_pool_scan(context, layer_name, symbol_table, constraints):
        (_constraint, mem_object, _header) = result
        yield mem_object","for result in poolscanner.PoolScanner.generate_pool_scan(context, layer_name, symbol_table, constraints):
    (_constraint, mem_object, _header) = result
    yield mem_object","for i, result in enumerate(poolscanner.PoolScanner.generate_pool_scan(context, layer_name, symbol_table, constraints)):
    (_constraint, mem_object, _header) = result
    yield mem_object",,,,,,,,,,,
enum4linux-ng,https://github.com/cddmp/enum4linux-ng/tree/master//enum4linux-ng.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enum4linux-ng//enum4linux-ng.py,EnumGroupsRpc,"def run(self):
    """"""
        Run module enum groups.
        """"""
    module_name = ENUM_GROUPS_RPC
    print_heading(f'Groups via RPC on {self.target.host}')
    output = {}
    groups = None
    for grouptype in ['local', 'builtin', 'domain']:
        print_info(f'Enumerating {grouptype} groups')
        enum = self.enum(grouptype)
        if enum.retval is None:
            output = process_error(enum.retmsg, ['groups'], module_name, output)
        else:
            if groups is None:
                groups = {}
            print_success(enum.retmsg)
            groups.update(enum.retval)
    if groups:
        if self.with_members:
            print_info('Enumerating group members')
            for rid in groups.keys():
                groupname = groups[rid]['groupname']
                grouptype = groups[rid]['type']
                group_members = self.get_members_from_name(groupname, grouptype, rid)
                if group_members.retval or group_members.retval == '':
                    print_success(group_members.retmsg)
                else:
                    output = process_error(group_members.retmsg, ['groups'], module_name, output)
                groups[rid]['members'] = group_members.retval
        if self.detailed:
            print_info('Enumerating group details')
            for rid in groups.keys():
                groupname = groups[rid]['groupname']
                grouptype = groups[rid]['type']
                details = self.get_details_from_rid(rid, groupname, grouptype)
                if details.retval:
                    print_success(details.retmsg)
                else:
                    output = process_error(details.retmsg, ['groups'], module_name, output)
                groups[rid]['details'] = details.retval
        print_success(f'After merging groups results we have {len(groups.keys())} group(s) total:\n{yamlize(groups, sort=True)}')
    output['groups'] = groups
    return output","for rid in groups.keys():
    groupname = groups[rid]['groupname']
    grouptype = groups[rid]['type']
    details = self.get_details_from_rid(rid, groupname, grouptype)
    if details.retval:
        print_success(details.retmsg)
    else:
        output = process_error(details.retmsg, ['groups'], module_name, output)
    groups[rid]['details'] = details.retval","for i, rid in enumerate(groups.keys()):
    groupname = groups[rid]['groupname']
    grouptype = groups[rid]['type']
    details = self.get_details_from_rid(rid, groupname, grouptype)
    if details.retval:
        print_success(details.retmsg)
    else:
        output = process_error(details.retmsg, ['groups'], module_name, output)
    groups[rid]['details'] = details.retval",,,,,,,,,,,
airflow,https://github.com/apache/airflow/tree/master/airflow/providers/mysql/transfers/vertica_to_mysql.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/providers/mysql/transfers/vertica_to_mysql.py,VerticaToMySqlOperator,"def execute(self, context):
    vertica = VerticaHook(vertica_conn_id=self.vertica_conn_id)
    mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)
    tmpfile = None
    result = None
    selected_columns = []
    count = 0
    with closing(vertica.get_conn()) as conn:
        with closing(conn.cursor()) as cursor:
            cursor.execute(self.sql)
            selected_columns = [d.name for d in cursor.description]
            if self.bulk_load:
                with NamedTemporaryFile('w') as tmpfile:
                    self.log.info('Selecting rows from Vertica to local file %s...', tmpfile.name)
                    self.log.info(self.sql)
                    csv_writer = csv.writer(tmpfile, delimiter='\t', encoding='utf-8')
                    for row in cursor.iterate():
                        csv_writer.writerow(row)
                        count += 1
                    tmpfile.flush()
            else:
                self.log.info('Selecting rows from Vertica...')
                self.log.info(self.sql)
                result = cursor.fetchall()
                count = len(result)
            self.log.info('Selected rows from Vertica %s', count)
    if self.mysql_preoperator:
        self.log.info('Running MySQL preoperator...')
        mysql.run(self.mysql_preoperator)
    try:
        if self.bulk_load:
            self.log.info('Bulk inserting rows into MySQL...')
            with closing(mysql.get_conn()) as conn:
                with closing(conn.cursor()) as cursor:
                    cursor.execute(f""LOAD DATA LOCAL INFILE '{tmpfile.name}' INTO TABLE {self.mysql_table} LINES TERMINATED BY '\r\n' ({', '.join(selected_columns)})"")
                    conn.commit()
            tmpfile.close()
        else:
            self.log.info('Inserting rows into MySQL...')
            mysql.insert_rows(table=self.mysql_table, rows=result, target_fields=selected_columns)
        self.log.info('Inserted rows into MySQL %s', count)
    except (MySQLdb.Error, MySQLdb.Warning):
        self.log.info('Inserted rows into MySQL 0')
        raise
    if self.mysql_postoperator:
        self.log.info('Running MySQL postoperator...')
        mysql.run(self.mysql_postoperator)
    self.log.info('Done')","for row in cursor.iterate():
    csv_writer.writerow(row)
    count += 1","for i, row in enumerate(cursor.iterate()):
    csv_writer.writerow(row)
    count += 1",,,,,,,,,,,
Deep-Reinforcement-Learning-Algorithms-with-PyTorch,https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tree/master/environments/ant_environments/maze_env.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/environments/ant_environments/maze_env.py,MazeEnv,"def __init__(self, maze_id=None, maze_height=0.5, maze_size_scaling=8, n_bins=0, sensor_range=3.0, sensor_span=2 * math.pi, observe_blocks=False, put_spin_near_agent=False, top_down_view=False, manual_collision=False, *args, **kwargs):
    self._maze_id = maze_id
    model_cls = self.__class__.MODEL_CLASS
    if model_cls is None:
        raise 'MODEL_CLASS unspecified!'
    xml_path = os.path.join(MODEL_DIR, model_cls.FILE)
    import sys
    sys.path.insert(0, '/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments')
    sys.path.insert(0, '/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments/environments')
    print(os.getcwd())
    print(xml_path)
    tree = ET.parse('/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments/ant_environments/' + xml_path[13:])
    worldbody = tree.find('.//worldbody')
    self.MAZE_HEIGHT = height = maze_height
    self.MAZE_SIZE_SCALING = size_scaling = maze_size_scaling
    self._n_bins = n_bins
    self._sensor_range = sensor_range * size_scaling
    self._sensor_span = sensor_span
    self._observe_blocks = observe_blocks
    self._put_spin_near_agent = put_spin_near_agent
    self._top_down_view = top_down_view
    self._manual_collision = manual_collision
    self.MAZE_STRUCTURE = structure = maze_env_utils.construct_maze(maze_id=self._maze_id)
    self.elevated = any((-1 in row for row in structure))
    self.blocks = any((any((maze_env_utils.can_move(r) for r in row)) for row in structure))
    (torso_x, torso_y) = self._find_robot()
    self._init_torso_x = torso_x
    self._init_torso_y = torso_y
    self._init_positions = [(x - torso_x, y - torso_y) for (x, y) in self._find_all_robots()]
    self._xy_to_rowcol = lambda x, y: (2 + (y + size_scaling / 2) / size_scaling, 2 + (x + size_scaling / 2) / size_scaling)
    self._view = np.zeros([5, 5, 3])
    height_offset = 0.0
    if self.elevated:
        height_offset = height * size_scaling
        torso = tree.find("".//body[@name='torso']"")
        torso.set('pos', '0 0 %.2f' % (0.75 + height_offset))
    if self.blocks:
        default = tree.find('.//default')
        default.find('.//geom').set('solimp', '.995 .995 .01')
    self.movable_blocks = []
    for i in range(len(structure)):
        for j in range(len(structure[0])):
            struct = structure[i][j]
            if struct == 'r' and self._put_spin_near_agent:
                struct = maze_env_utils.Move.SpinXY
            if self.elevated and struct not in [-1]:
                ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
            if struct == 1:
                ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
            elif maze_env_utils.can_move(struct):
                name = 'movable_%d_%d' % (i, j)
                self.movable_blocks.append((name, struct))
                falling = maze_env_utils.can_move_z(struct)
                spinning = maze_env_utils.can_spin(struct)
                x_offset = 0.25 * size_scaling if spinning else 0.0
                y_offset = 0.0
                shrink = 0.1 if spinning else 0.99 if falling else 1.0
                height_shrink = 0.1 if spinning else 1.0
                movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
                ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
                if maze_env_utils.can_move_x(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_move_y(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_move_z(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_spin(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')
    torso = tree.find("".//body[@name='torso']"")
    geoms = torso.findall('.//geom')
    for geom in geoms:
        if 'name' not in geom.attrib:
            raise Exception('Every geom of the torso must have a name defined')
    (_, file_path) = tempfile.mkstemp(text=True, suffix='.xml')
    tree.write(file_path)
    self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)","for i in range(len(structure)):
    for j in range(len(structure[0])):
        struct = structure[i][j]
        if struct == 'r' and self._put_spin_near_agent:
            struct = maze_env_utils.Move.SpinXY
        if self.elevated and struct not in [-1]:
            ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
        if struct == 1:
            ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
        elif maze_env_utils.can_move(struct):
            name = 'movable_%d_%d' % (i, j)
            self.movable_blocks.append((name, struct))
            falling = maze_env_utils.can_move_z(struct)
            spinning = maze_env_utils.can_spin(struct)
            x_offset = 0.25 * size_scaling if spinning else 0.0
            y_offset = 0.0
            shrink = 0.1 if spinning else 0.99 if falling else 1.0
            height_shrink = 0.1 if spinning else 1.0
            movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
            ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
            if maze_env_utils.can_move_x(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_y(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_z(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_spin(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')","for i, row in enumerate(structure):
    for j, struct in enumerate(row):
        if struct == 'r' and self._put_spin_near_agent:
            struct = maze_env_utils.Move.SpinXY
        if self.elevated and struct not in [-1]:
            ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
        if struct == 1:
            ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
        elif maze_env_utils.can_move(struct):
            name = 'movable_%d_%d' % (i, j)
            self.movable_blocks.append((name, struct))
            falling = maze_env_utils.can_move_z(struct)
            spinning = maze_env_utils.can_spin(struct)
            x_offset = 0.25 * size_scaling if spinning else 0.0
            y_offset = 0.0
            shrink = 0.1 if spinning else 0.99 if falling else 1.0
            height_shrink = 0.1 if spinning else 1.0
            movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
            ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
            if maze_env_utils.can_move_x(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_y(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_z(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_spin(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')",,,,,,,,,,,
stm32-rs,https://github.com/stm32-rs/stm32-rs/tree/master/scripts/group.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stm32-rs/scripts/group.py,,"def main(devices, output):
    print('Stage 1: Enumerating all fields in all devices')
    peripherals = {}
    for device_path in tqdm(glob.glob(os.path.join(devices, '*.yaml'))):
        device_name = os.path.splitext(os.path.basename(device_path))[0]
        with open(device_path, encoding='utf-8') as f:
            device = yaml.safe_load(f)
            device['_path'] = device_path
        if '_svd' not in device:
            raise RuntimeError('You must have an _svd key in the YAML file')
        svdpath = patch.abspath(device_path, device['_svd'])
        svd = ET.parse(svdpath)
        patch.process_device(svd, device)
        for ptag in svd.iter('peripheral'):
            if 'derivedFrom' in ptag.attrib:
                continue
            pname = ptag.find('name').text
            device_members = []
            for rtag in ptag.iter('register'):
                rname = rtag.find('name').text
                roffset = rtag.find('addressOffset').text
                for ftag in rtag.iter('field'):
                    fname = ftag.find('name').text
                    foffset = ftag.find('bitOffset').text
                    fwidth = ftag.find('bitWidth').text
                    device_members.append('{}__{}_{}__{}_{}_{}'.format(pname, roffset, rname, foffset, fwidth, fname))
            if pname not in peripherals:
                peripherals[pname] = {}
            peripherals[pname][device_name] = device_members
    print('Stage 2: Inverting to find all devices for each field and merging')
    fields = {}
    merged_fields = {}
    for (pname, devices) in tqdm(peripherals.items()):
        fields[pname] = {}
        for (device, device_fields) in devices.items():
            for fname in device_fields:
                if fname not in fields[pname]:
                    fields[pname][fname] = set()
                fields[pname][fname].add(device)
        merged_fields[pname] = {}
        skip_fields = set()
        for (field, devices) in fields[pname].items():
            if field in skip_fields:
                continue
            merged_key = [field]
            for (field2, devices2) in fields[pname].items():
                if field != field2 and devices == devices2:
                    merged_key.append(field2)
                    skip_fields.add(field2)
            merged_fields[pname][','.join(sorted(merged_key))] = devices
    print('Stage 3: Building tree of subsets')
    field_tree = {}
    for pname in tqdm(merged_fields):
        field_tree[pname] = {}
        for (fieldset, devices) in merged_fields[pname].items():
            field_tree[pname][fieldset] = (devices, {})

        def treeify(fieldset, devices, children, siblings):
            moved_siblings = []
            for (fieldset2, (devices2, children2)) in siblings.items():
                if fieldset2 != fieldset and devices2 < devices:
                    children[fieldset2] = (devices2, children2)
                    moved_siblings.append(fieldset2)
            for fieldset2 in moved_siblings:
                del siblings[fieldset2]
            moved_children = []
            for fieldset2 in moved_siblings:
                if fieldset2 in moved_children:
                    continue
                (devices2, children2) = children[fieldset2]
                moved_children += treeify(fieldset2, devices2, children2, children)
            return moved_siblings
        fieldsets = list(field_tree[pname].keys())
        moved_fieldsets = []
        for fieldset in fieldsets:
            if fieldset in moved_fieldsets:
                continue
            (devices, children) = field_tree[pname][fieldset]
            moved_fieldsets += treeify(fieldset, devices, children, field_tree[pname])

        def strip_devices(siblings):
            fieldsets = list(siblings.keys())
            for fieldset in fieldsets:
                (devices, children) = siblings[fieldset]
                if children:
                    strip_devices(children)
                    siblings[fieldset] = children
                else:
                    siblings[fieldset] = list(devices)
        strip_devices(field_tree[pname])
    print('Stage 4: Writing results JSON')
    with open(output, 'w') as f:
        json.dump(field_tree, f, indent=2, sort_keys=True)","for rtag in ptag.iter('register'):
    rname = rtag.find('name').text
    roffset = rtag.find('addressOffset').text
    for ftag in rtag.iter('field'):
        fname = ftag.find('name').text
        foffset = ftag.find('bitOffset').text
        fwidth = ftag.find('bitWidth').text
        device_members.append('{}__{}_{}__{}_{}_{}'.format(pname, roffset, rname, foffset, fwidth, fname))","for i, rtag in enumerate(ptag.iter('register')):
    rname = rtag.find('name').text
    roffset = rtag.find('addressOffset').text
    for j, ftag in enumerate(rtag.iter('field')):
        fname = ftag.find('name').text
        foffset = ftag.find('bitOffset').text
        fwidth = ftag.find('bitWidth').text
        device_members.append('{}__{}_{}__{}_{}_{}'.format(pname, roffset, rname, foffset, fwidth, fname))",,,,,,,,,,,
Kunlun-M,https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kunlun-M/core/detection.py,Detection,"def framework(self):
    tree = self.rule()
    root = tree.getroot()
    (frame_data, language_data) = self.parse_xml(root, self.frame_data, self.language_data)
    projects_data = self.project_information(self.target_directory, False)
    frame_name = self.dependency_scan(root)
    if frame_name is not None:
        return frame_name
    frames_name = frame_data.keys()
    for frame_name in frames_name:
        for rule_name in frame_data[frame_name]:
            for project_data in projects_data:
                if rule_name in project_data:
                    logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                    return frame_name
    logger.info('[DETECTION] [FRAMEWORK] Unknown Framework')
    return 'Unknown Framework'","for rule_name in frame_data[frame_name]:
    for project_data in projects_data:
        if rule_name in project_data:
            logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
            return frame_name","for i, rule_name in enumerate(frame_data[frame_name]):
    for project_data in projects_data:
        if rule_name in project_data:
            logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
            return frame_name",,,,,,,,,,,
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/svgcode.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/svgcode.py,SVGcode,"def path2gcode(self, path, samples_per_unit=100, d=4):
    gcode = []
    if isinstance(path, str):
        path = Path(path)

    def rv(v):
        return f'{round(v, d):{d}}'.rstrip('0').rstrip('.')
    for segment in path:
        subdiv = max(1, round(segment.length(error=1e-05) * samples_per_unit))
        if isinstance(segment, Move):
            gcode.append(f'G0 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
        elif isinstance(segment, (Line, Close)):
            gcode.append(f'G1 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
        elif isinstance(segment, Arc) and abs(segment.rx - segment.ry) < 1e-09:
            garc = 'G02' if segment.sweep > 0 else 'G03'
            gcode.append(' '.join([f'{garc}', f'X{rv(segment.end.x)}', f'Y{rv(-segment.end.y)}', f'R{rv(segment.rx)}']))
        else:
            subdiv_points = numpy.linspace(0, 1, subdiv, endpoint=True)[1:]
            points = segment.npoint(subdiv_points)
            gcode.extend([f'G1 X{rv(sp[0])} Y{rv(-sp[1])}' for sp in points])
    return '\n'.join(gcode)","for segment in path:
    subdiv = max(1, round(segment.length(error=1e-05) * samples_per_unit))
    if isinstance(segment, Move):
        gcode.append(f'G0 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, (Line, Close)):
        gcode.append(f'G1 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, Arc) and abs(segment.rx - segment.ry) < 1e-09:
        garc = 'G02' if segment.sweep > 0 else 'G03'
        gcode.append(' '.join([f'{garc}', f'X{rv(segment.end.x)}', f'Y{rv(-segment.end.y)}', f'R{rv(segment.rx)}']))
    else:
        subdiv_points = numpy.linspace(0, 1, subdiv, endpoint=True)[1:]
        points = segment.npoint(subdiv_points)
        gcode.extend([f'G1 X{rv(sp[0])} Y{rv(-sp[1])}' for sp in points])","for i, segment in enumerate(path):
    subdiv = max(1, round(segment.length(error=1e-05) * samples_per_unit))
    if isinstance(segment, Move):
        gcode.append(f'G0 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, (Line, Close)):
        gcode.append(f'G1 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, Arc) and abs(segment.rx - segment.ry) < 1e-09:
        garc = 'G02' if segment.sweep > 0 else 'G03'
        gcode.append(' '.join([f'{garc}', f'X{rv(segment.end.x)}', f'Y{rv(-segment.end.y)}', f'R{rv(segment.rx)}']))
    else:
        subdiv_points = numpy.linspace(0, 1, subdiv, endpoint=True)[1:]
        points = segment.npoint(subdiv_points)
        gcode.extend([f'G1 X{rv(sp[0])} Y{rv(-sp[1])}' for sp in points])",,,,,,,,,,,
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for sel in view.sel():
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))","for i, sel in enumerate(view.sel()):
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))",,,,,,,,,,,
pytracking,https://github.com/visionml/pytracking/tree/master/pytracking/evaluation/lasotdataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/pytracking/evaluation/lasotdataset.py,LaSOTDataset,"def clean_seq_list(self):
    clean_lst = []
    for i in range(len(self.sequence_list)):
        (cls, _) = self.sequence_list[i].split('-')
        clean_lst.append(cls)
    return clean_lst","for i in range(len(self.sequence_list)):
    (cls, _) = self.sequence_list[i].split('-')
    clean_lst.append(cls)","for i, sequence in enumerate(self.sequence_list):
    (cls, _) = sequence.split('-')
    clean_lst.append(cls)",,,,,,,,,,,
cartopy,https://github.com/SciTools/cartopy/tree/master/lib/cartopy/io/shapereader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cartopy/lib/cartopy/io/shapereader.py,BasicReader,"def records(self):
    """"""
        Return an iterator of :class:`~Record` instances.

        """"""
    fields = self._reader.fields[1:]
    for shape_record in self._reader.iterShapeRecords():
        attributes = shape_record.record.as_dict()
        yield Record(shape_record.shape, attributes, fields)","for shape_record in self._reader.iterShapeRecords():
    attributes = shape_record.record.as_dict()
    yield Record(shape_record.shape, attributes, fields)","for i, shape_record in enumerate(self._reader.iterShapeRecords()):
    attributes = shape_record.record.as_dict()
    yield Record(shape_record.shape, attributes, fields)",,,,,,,,,,,
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_dragonbones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_dragonbones.py,,"def bone_is_deform_bone(self, bone, sprites):
    for sprite in sprites:
        if sprite.type == 'MESH':
            init_mesh = sprite.data
            meshes = []
            if sprite.coa_type == 'MESH':
                meshes.append(sprite.data)
            elif sprite.coa_type == 'SLOT':
                for slot in sprite.coa_slot:
                    meshes.append(slot.mesh)
            for mesh in meshes:
                sprite.data = mesh
                if sprite.parent_bone == bone.name:
                    sprite.data = init_mesh
                    return True
                if not bone.name in sprite.vertex_groups:
                    break
                else:
                    v_group = sprite.vertex_groups[bone.name]
                    for vert in sprite.data.vertices:
                        try:
                            weight = v_group.weight(vert.index)
                            if weight > 0:
                                sprite.data = init_mesh
                                return True
                        except:
                            pass
            sprite.data = init_mesh
    return False","for slot in sprite.coa_slot:
    meshes.append(slot.mesh)","for i, slot in enumerate(sprite.coa_slot):
    meshes.append(slot.mesh)",,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/modules/virt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/virt.py,,"def revert_snapshot(name, vm_snapshot=None, cleanup=False, **kwargs):
    """"""
    Revert snapshot to the previous from current (if available) or to the specific.

    :param name: domain name
    :param vm_snapshot: name of the snapshot to revert
    :param cleanup: Remove all newer than reverted snapshots. Values: True or False (default False).
    :param connection: libvirt connection URI, overriding defaults

        .. versionadded:: 2019.2.0
    :param username: username to connect with, overriding defaults

        .. versionadded:: 2019.2.0
    :param password: password to connect with, overriding defaults

        .. versionadded:: 2019.2.0

    .. versionadded:: 2016.3.0

    CLI Example:

    .. code-block:: bash

        salt '*' virt.revert <domain>
        salt '*' virt.revert <domain> <snapshot>
    """"""
    ret = dict()
    conn = __get_conn(**kwargs)
    domain = _get_domain(conn, name)
    snapshots = domain.listAllSnapshots()
    _snapshots = list()
    for snap_obj in snapshots:
        _snapshots.append({'idx': _parse_snapshot_description(snap_obj, unix_time=True)['created'], 'ptr': snap_obj})
    snapshots = [w_ptr['ptr'] for w_ptr in sorted(_snapshots, key=lambda item: item['idx'], reverse=True)]
    del _snapshots
    if not snapshots:
        conn.close()
        raise CommandExecutionError('No snapshots found')
    elif len(snapshots) == 1:
        conn.close()
        raise CommandExecutionError('Cannot revert to itself: only one snapshot is available.')
    snap = None
    for p_snap in snapshots:
        if not vm_snapshot:
            if p_snap.isCurrent() and snapshots[snapshots.index(p_snap) + 1:]:
                snap = snapshots[snapshots.index(p_snap) + 1:][0]
                break
        elif p_snap.getName() == vm_snapshot:
            snap = p_snap
            break
    if not snap:
        conn.close()
        raise CommandExecutionError(snapshot and 'Snapshot ""{}"" not found'.format(vm_snapshot) or 'No more previous snapshots available')
    elif snap.isCurrent():
        conn.close()
        raise CommandExecutionError('Cannot revert to the currently running snapshot.')
    domain.revertToSnapshot(snap)
    ret['reverted'] = snap.getName()
    if cleanup:
        delete = list()
        for p_snap in snapshots:
            if p_snap.getName() != snap.getName():
                delete.append(p_snap.getName())
                p_snap.delete()
            else:
                break
        ret['deleted'] = delete
    else:
        ret['deleted'] = 'N/A'
    conn.close()
    return ret","for snap_obj in snapshots:
    _snapshots.append({'idx': _parse_snapshot_description(snap_obj, unix_time=True)['created'], 'ptr': snap_obj})","for i, snap_obj in enumerate(snapshots):
    _snapshots.append({'idx': _parse_snapshot_description(snap_obj, unix_time=True)['created'], 'ptr': snap_obj})",,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for span in spans:
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))","for i, span in enumerate(spans):
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))",,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/cmu_dog/agents.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/cmu_dog/agents.py,,"def _all_split_datafiles(opt: Opt) -> List[str]:
    datafiles = []
    split_type = SplitType(opt.get('cmu_dog_split_type'))
    if split_type in {SplitType.SEEN, SplitType.UNSEEN}:
        for split in ['train', 'valid', 'test']:
            datafiles.append(_datafile(split, SplitType.SEEN))
        datafiles.append(_datafile('test', SplitType.UNSEEN))
    else:
        for split in ['train', 'valid', 'test']:
            datafiles.append(_datafile(split, split_type))
    return datafiles","for split in ['train', 'valid', 'test']:
    datafiles.append(_datafile(split, SplitType.SEEN))","for i, split in enumerate(['train', 'valid', 'test']):
    datafiles.append(_datafile(split, SplitType.SEEN))",,,,,,,,,,,
mmocr,https://github.com/open-mmlab/mmocr/tree/master/tools/data/textdet/totaltext_converter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/tools/data/textdet/totaltext_converter.py,,"def main():
    args = parse_args()
    root_path = args.root_path
    img_dir = osp.join(root_path, 'imgs')
    gt_dir = osp.join(root_path, 'annotations')
    set_name = {}
    for split in ['training', 'test']:
        set_name.update({split: 'instances_' + split + '.json'})
        assert osp.exists(osp.join(img_dir, split))
    for (split, json_name) in set_name.items():
        print(f'Converting {split} into {json_name}')
        with mmcv.Timer(print_tmpl='It takes {}s to convert totaltext annotation'):
            files = collect_files(osp.join(img_dir, split), osp.join(gt_dir, split))
            image_infos = collect_annotations(files, nproc=args.nproc)
            convert_annotations(image_infos, osp.join(root_path, json_name))","for split in ['training', 'test']:
    set_name.update({split: 'instances_' + split + '.json'})
    assert osp.exists(osp.join(img_dir, split))","for i, split in enumerate(['training', 'test']):
    set_name.update({split: 'instances_' + split + '.json'})
    assert osp.exists(osp.join(img_dir, split))",,,,,,,,,,,
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/quantization_model_evaluator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/quantization_model_evaluator.py,ModelEvaluator,"def _initialize_input_shape_with_data_range(self):
    input_shapes = []
    for (_, _, batch_input, _) in self.dataset:
        input_shapes.extend(self.preprocessor.query_data_batch_shapes(batch_input))
    shapes_statistic = np.swapaxes(np.array(input_shapes), 1, 0)
    per_input_tamplates = []
    for stat_shape in shapes_statistic:
        shape_template = [-1] * len(stat_shape[0])
        undefined_shapes = np.squeeze(np.sum(shapes_statistic == -1, axis=1), 0).astype(int)
        for (i, ds) in enumerate(undefined_shapes):
            if ds > 0:
                continue
            axis_sizes = stat_shape[:, i]
            min_size = np.min(axis_sizes)
            max_size = np.max(axis_sizes)
            shape_template[i] = min_size if min_size == max_size else (min_size, max_size)
        per_input_tamplates.append(shape_template)
    self._initialize_input_shape(dynamic_shape_helper=per_input_tamplates)","for stat_shape in shapes_statistic:
    shape_template = [-1] * len(stat_shape[0])
    undefined_shapes = np.squeeze(np.sum(shapes_statistic == -1, axis=1), 0).astype(int)
    for (i, ds) in enumerate(undefined_shapes):
        if ds > 0:
            continue
        axis_sizes = stat_shape[:, i]
        min_size = np.min(axis_sizes)
        max_size = np.max(axis_sizes)
        shape_template[i] = min_size if min_size == max_size else (min_size, max_size)
    per_input_tamplates.append(shape_template)","for i, stat_shape in enumerate(shapes_statistic):
    shape_template = [-1] * len(stat_shape[0])
    undefined_shapes = np.squeeze(np.sum(shapes_statistic == -1, axis=1), 0).astype(int)
    for (j, ds) in enumerate(undefined_shapes):
        if ds > 0:
            continue
        axis_sizes = stat_shape[:, j]
        min_size = np.min(axis_sizes)
        max_size = np.max(axis_sizes)
        shape_template[j] = min_size if min_size == max_size else (min_size, max_size)
    per_input_tamplates.append(shape_template)",,,,,,,,,,,
gated-graph-neural-network-samples,https://github.com/microsoft/gated-graph-neural-network-samples/tree/master//chem_tensorflow_async.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gated-graph-neural-network-samples//chem_tensorflow_async.py,AsyncGGNNChemModel,"def __tensorise_edge_sequence(self, edges) -> Tuple[np.ndarray, List[List[np.ndarray]], List[List[np.ndarray]], List[np.ndarray]]:
    sending_nodes = []
    msg_targets = []
    receiving_nodes = []
    all_nodes = set()
    for step_edges in edges:
        msg_targets_uniq = set((w for (_, __, w) in step_edges))
        recv_nodes = list(sorted(msg_targets_uniq))
        recv_nodes_to_uniq_id = {v: i for (i, v) in enumerate(recv_nodes)}
        sending_nodes_in_step = []
        msg_targets_in_step = []
        for target_e_typ in range(self.num_edge_types):
            sending_nodes_in_step.append(np.array([v for (v, e_typ, _) in step_edges if e_typ == target_e_typ], dtype=np.int32))
            msg_targets_in_step.append(np.array([recv_nodes_to_uniq_id[w] for (_, e_typ, w) in step_edges if e_typ == target_e_typ], dtype=np.int32))
        msg_targets.append(msg_targets_in_step)
        sending_nodes.append(sending_nodes_in_step)
        receiving_nodes.append(np.array(recv_nodes, dtype=np.int32))
        all_nodes.update((v for (v, _, __) in step_edges))
        all_nodes.update((w for (_, __, w) in step_edges))
    all_updated_nodes = set()
    all_updated_nodes.update((v for step_receiving_nodes in receiving_nodes for v in step_receiving_nodes))
    initial_nodes = list(sorted(all_nodes - all_updated_nodes))
    return (np.array(initial_nodes, dtype=np.int32), sending_nodes, msg_targets, receiving_nodes)","for target_e_typ in range(self.num_edge_types):
    sending_nodes_in_step.append(np.array([v for (v, e_typ, _) in step_edges if e_typ == target_e_typ], dtype=np.int32))
    msg_targets_in_step.append(np.array([recv_nodes_to_uniq_id[w] for (_, e_typ, w) in step_edges if e_typ == target_e_typ], dtype=np.int32))","for i, target_e_typ in enumerate(range(self.num_edge_types)):
    sending_nodes_in_step.append(np.array([v for (v, e_typ, _) in step_edges if e_typ == target_e_typ], dtype=np.int32))
    msg_targets_in_step.append(np.array([recv_nodes_to_uniq_id[w] for (_, e_typ, w) in step_edges if e_typ == target_e_typ], dtype=np.int32))",,,,,,,,,,,
pafy,https://github.com/mps-youtube/pafy/tree/master/pafy/backend_shared.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pafy/pafy/backend_shared.py,BasePafy,"def getbestthumb(self):
    """""" Return the best available thumbnail.""""""
    if not self._bestthumb:
        part_url = 'http://i.ytimg.com/vi/%s/' % self.videoid
        thumbs = ('maxresdefault.jpg', 'sddefault.jpg', 'hqdefault.jpg', 'mqdefault.jpg', 'default.jpg')
        for thumb in thumbs:
            url = part_url + thumb
            if self._content_available(url):
                return url
    return self._bestthumb","for thumb in thumbs:
    url = part_url + thumb
    if self._content_available(url):
        return url","for i, thumb in enumerate(thumbs):
    url = part_url + thumb
    if self._content_available(url):
        return url",,,,,,,,,,,
Bert-Chinese-Text-Classification-Pytorch,https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch/tree/master/pytorch_pretrained/tokenization.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Bert-Chinese-Text-Classification-Pytorch/pytorch_pretrained/tokenization.py,WordpieceTokenizer,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""
    output_tokens = []
    for token in whitespace_tokenize(text):
        chars = list(token)
        if len(chars) > self.max_input_chars_per_word:
            output_tokens.append(self.unk_token)
            continue
        is_bad = False
        start = 0
        sub_tokens = []
        while start < len(chars):
            end = len(chars)
            cur_substr = None
            while start < end:
                substr = ''.join(chars[start:end])
                if start > 0:
                    substr = '##' + substr
                if substr in self.vocab:
                    cur_substr = substr
                    break
                end -= 1
            if cur_substr is None:
                is_bad = True
                break
            sub_tokens.append(cur_substr)
            start = end
        if is_bad:
            output_tokens.append(self.unk_token)
        else:
            output_tokens.extend(sub_tokens)
    return output_tokens","for token in whitespace_tokenize(text):
    chars = list(token)
    if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue
    is_bad = False
    start = 0
    sub_tokens = []
    while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
            substr = ''.join(chars[start:end])
            if start > 0:
                substr = '##' + substr
            if substr in self.vocab:
                cur_substr = substr
                break
            end -= 1
        if cur_substr is None:
            is_bad = True
            break
        sub_tokens.append(cur_substr)
        start = end
    if is_bad:
        output_tokens.append(self.unk_token)
    else:
        output_tokens.extend(sub_tokens)","for i, token in enumerate(whitespace_tokenize(text)):
    chars = list(token)
    if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue
    is_bad = False
    start = 0
    sub_tokens = []
    while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
            substr = ''.join(chars[start:end])
            if start > 0:
                substr = '##' + substr
            if substr in self.vocab:
                cur_substr = substr
                break
            end -= 1
        if cur_substr is None:
            is_bad = True
            break
        sub_tokens.append(cur_substr)
        start = end
    if is_bad:
        output_tokens.append(self.unk_token)
    else:
        output_tokens.extend(sub_tokens)",,,,,,,,,,,
oppia,https://github.com/oppia/oppia/tree/master/core/controllers/learner_goals_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/controllers/learner_goals_test.py,LearnerGoalsHandlerTests,"def test_add_topic_to_learner_goal(self) -> None:
    self.login(self.VIEWER_EMAIL)
    csrf_token = self.get_new_csrf_token()
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, self.TOPIC_ID_1), {}, csrf_token=csrf_token)
    self.assertEqual(learner_goals_services.get_all_topic_ids_to_learn(self.viewer_id), [self.TOPIC_ID_1])
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, self.TOPIC_ID_2), {}, csrf_token=csrf_token)
    self.assertEqual(learner_goals_services.get_all_topic_ids_to_learn(self.viewer_id), [self.TOPIC_ID_1, self.TOPIC_ID_2])
    learner_progress_services.mark_topic_as_learnt(self.viewer_id, self.TOPIC_ID_3)
    response = self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, self.TOPIC_ID_3), {}, csrf_token=csrf_token)
    self.assertEqual(response['belongs_to_learnt_list'], True)
    self.assertEqual(learner_goals_services.get_all_topic_ids_to_learn(self.viewer_id), [self.TOPIC_ID_1, self.TOPIC_ID_2])
    response = self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, 'InvalidActivityType', self.TOPIC_ID_1), {}, csrf_token=csrf_token, expected_status_int=400)
    self.assertIn(""Received InvalidActivityType which is not in the allowed range of choices: ['learntopic']"", response['error'])
    for topic_id in range(2, feconf.MAX_CURRENT_GOALS_COUNT + 1):
        self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % topic_id), {}, csrf_token=csrf_token)
    response = self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % str(feconf.MAX_CURRENT_GOALS_COUNT + 3)), {}, csrf_token=csrf_token)
    self.assertEqual(response['goals_limit_exceeded'], True)
    self.logout()","for topic_id in range(2, feconf.MAX_CURRENT_GOALS_COUNT + 1):
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % topic_id), {}, csrf_token=csrf_token)","for i, topic_id in enumerate(range(2, feconf.MAX_CURRENT_GOALS_COUNT + 1), start=2):
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % topic_id), {}, csrf_token=csrf_token)",,,,,,,,,,,
easytrader,https://github.com/shidenggui/easytrader/tree/master/easytrader/xq_follower.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/easytrader/easytrader/xq_follower.py,XueQiuFollower,"def project_transactions(self, transactions, assets):
    for transaction in transactions:
        weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
        initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
        transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
        transaction['stock_code'] = transaction['stock_symbol'].lower()
        transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
        transaction['amount'] = int(round(initial_amount, -2))
        if transaction['action'] == 'sell' and self._adjust_sell:
            transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])","for transaction in transactions:
    weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
    initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
    transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
    transaction['stock_code'] = transaction['stock_symbol'].lower()
    transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
    transaction['amount'] = int(round(initial_amount, -2))
    if transaction['action'] == 'sell' and self._adjust_sell:
        transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])","for i, transaction in enumerate(transactions):
    weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
    initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
    transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
    transaction['stock_code'] = transaction['stock_symbol'].lower()
    transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
    transaction['amount'] = int(round(initial_amount, -2))
    if transaction['action'] == 'sell' and self._adjust_sell:
        transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])",,,,,,,,,,,
texar-pytorch,https://github.com/asyml/texar-pytorch/tree/master/texar/torch/data/data/scalar_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/texar/torch/data/data/scalar_data.py,ScalarData,"def process(self, raw_example: List[str]) -> Union[bool, int, float]:
    assert len(raw_example) == 1
    example_: Union[int, str]
    if self._data_type == np.bool_:
        example_ = strtobool(raw_example[0])
    else:
        example_ = raw_example[0]
    example = self._data_type(example_)
    for transform in self._other_transforms:
        example = transform(example)
    return example","for transform in self._other_transforms:
    example = transform(example)","for i, transform in enumerate(self._other_transforms):
    example = transform(example)",,,,,,,,,,,
weblate,https://github.com/WeblateOrg/weblate/tree/master/weblate/addons/git.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/weblate/weblate/addons/git.py,GitSquashAddon,"def get_filenames(self, component):
    languages = defaultdict(list)
    for origin in [component] + list(component.linked_childs):
        for translation in origin.translation_set.prefetch_related('language'):
            code = translation.language.code
            if not translation.filename:
                continue
            languages[code].extend(translation.filenames)
    return languages","for translation in origin.translation_set.prefetch_related('language'):
    code = translation.language.code
    if not translation.filename:
        continue
    languages[code].extend(translation.filenames)","for i, translation in enumerate(origin.translation_set.prefetch_related('language')):
    code = translation.language.code
    if not translation.filename:
        continue
    languages[code].extend(translation.filenames)",,,,,,,,,,,
mycroft-core,https://github.com/MycroftAI/mycroft-core/tree/master/test/unittests/skills/test_mycroft_skill.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mycroft-core/test/unittests/skills/test_mycroft_skill.py,TestMycroftSkill,"def test_remove_context(self):

    def check_remove_context(result_list):
        for type in self.emitter.get_types():
            self.assertEqual(type, 'remove_context')
        self.assertEqual(sorted(self.emitter.get_results()), sorted(result_list))
        self.emitter.reset()
    s = SimpleSkill1()
    s.bind(self.emitter)
    s.remove_context('Donatello')
    expected = [{'context': 'ADonatello'}]
    check_remove_context(expected)","for type in self.emitter.get_types():
    self.assertEqual(type, 'remove_context')","for i, type in enumerate(self.emitter.get_types()):
    self.assertEqual(type, 'remove_context')",,,,,,,,,,,
portia,https://github.com/scrapinghub/portia/tree/master/slybot/slybot/tests/test_spider.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/portia/slybot/slybot/tests/test_spider.py,SpiderTest,"def test_variants(self):
    """"""Ensure variants are extracted as list of dicts""""""
    name = 'networkhealth.com'
    spider = self.smanager.create(name)
    spec = self.smanager._specs['spiders'][name]
    (template,) = spec['templates']
    target = HtmlPage(url=template['url'], body=template['original_body'])
    (items, _) = spider.plugins['Annotations'].extract_items(target)
    for item in items:
        for variant in item['variants']:
            self.assertEqual(type(variant), dict)","for variant in item['variants']:
    self.assertEqual(type(variant), dict)","for i, variant in enumerate(item['variants']):
    self.assertEqual(type(variant), dict)",,,,,,,,,,,
Screencast-Keys,https://github.com/nutti/Screencast-Keys/tree/master/src/screencast_keys/ops.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Screencast-Keys/src/screencast_keys/ops.py,SK_OT_ScreencastKeys,"def event_timer_remove(cls, context):
    wm = context.window_manager
    for win in wm.windows:
        key = win.as_pointer()
        if key in cls.timers:
            wm.event_timer_remove(cls.timers[key])
    cls.timers.clear()","for win in wm.windows:
    key = win.as_pointer()
    if key in cls.timers:
        wm.event_timer_remove(cls.timers[key])","for i, win in enumerate(wm.windows):
    key = win.as_pointer()
    if key in cls.timers:
        wm.event_timer_remove(cls.timers[key])",,,,,,,,,,,
ChatLearner,https://github.com/bshao001/ChatLearner/tree/master/chatbot/patternutils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ChatLearner/chatbot/patternutils.py,,"def _text2int(text):
    if text.isdigit():
        return int(text)
    num_words = {}
    units = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen']
    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']
    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']
    num_words['and'] = (1, 0)
    for (idx, word) in enumerate(units):
        num_words[word] = (1, idx)
    for (idx, word) in enumerate(tens):
        num_words[word] = (1, idx * 10)
    for (idx, word) in enumerate(scales):
        num_words[word] = (10 ** (idx * 3 or 2), 0)
    current = result = 0
    for word in text.replace('-', ' ').lower().split():
        if word not in num_words:
            return -1
        (scale, increment) = num_words[word]
        current = current * scale + increment
        if scale > 100:
            result += current
            current = 0
    return result + current","for word in text.replace('-', ' ').lower().split():
    if word not in num_words:
        return -1
    (scale, increment) = num_words[word]
    current = current * scale + increment
    if scale > 100:
        result += current
        current = 0","for i, word in enumerate(text.replace('-', ' ').lower().split()):
    if word not in num_words:
        return -1
    (scale, increment) = num_words[word]
    current = current * scale + increment
    if scale > 100:
        result += current
        current = 0",,,,,,,,,,,
mayavi,https://github.com/enthought/mayavi/tree/master/tvtk/indenter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/tvtk/indenter.py,VTKDocMassager,"def _rename_methods(self, doc):
    lines = doc.split('\n')
    nl = []
    for line in lines:
        words = line.split(' ')
        nw = []
        for word in words:
            if word[:3] == 'vtk':
                nw.append(word)
            else:
                nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
        nl.append(' '.join(nw))
    return '\n'.join(nl)","for word in words:
    if word[:3] == 'vtk':
        nw.append(word)
    else:
        nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))","for i, word in enumerate(words):
    if word[:3] == 'vtk':
        nw.append(word)
    else:
        nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))",,,,,,,,,,,
pyglet,https://github.com/pyglet/pyglet/tree/master/doc/ext/docstrings.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/doc/ext/docstrings.py,,"def convert(converter, start):
    (affected, result) = process_block(converter, lines, start)
    for x in range(start, start + affected + 1):
        del lines[start]
    for (i, line) in enumerate(result):
        lines.insert(start + i, line)","for x in range(start, start + affected + 1):
    del lines[start]","for i, x in enumerate(range(start, start + affected + 1)):
    del lines[start]",,,,,,,,,,,
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/tools/anchor_cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/static/tools/anchor_cluster.py,YOLOv2AnchorCluster,"def print_result(self, centers):
    logger.info('%d anchor cluster result: [w, h]' % self.n)
    for (w, h) in centers:
        logger.info('[%d, %d]' % (round(w), round(h)))","for (w, h) in centers:
    logger.info('[%d, %d]' % (round(w), round(h)))","for i,(w, h) in enumerate(centers):
    logger.info('[%d, %d]' % (round(w), round(h)))",,,,,,,,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)","for _ in batches:
    if self._should_stop(supervisor):
        return
    results = session.run(train_fetches, train_feed_dict)
    train_loss += results['loss']
    if self.compute_summaries:
        supervisor.summary_computed(session, results['summaries'], results['step'])","for i,_ in enumerate(batches):
    if self._should_stop(supervisor):
        return
    results = session.run(train_fetches, train_feed_dict)
    train_loss += results['loss']
    if self.compute_summaries:
        supervisor.summary_computed(session, results['summaries'], results['step'])",,,,,,,,,,,
PythonRobotics,https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/AStar/a_star_variants.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/PathPlanning/AStar/a_star_variants.py,,"def main():
    obs_dict = {}
    for i in range(51):
        for j in range(51):
            obs_dict[i, j] = False
    (o_x, o_y) = ([], [])
    s_x = 5.0
    s_y = 5.0
    g_x = 35.0
    g_y = 45.0
    draw_vertical_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_vertical_line(48, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 48, 50, o_x, o_y, obs_dict)
    all_x = [10, 10, 10, 15, 20, 20, 30, 30, 35, 30, 40, 45]
    all_y = [10, 30, 45, 20, 5, 40, 10, 40, 5, 40, 10, 25]
    all_len = [10, 10, 5, 10, 10, 5, 20, 10, 25, 10, 35, 15]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_vertical_line(x, y, l, o_x, o_y, obs_dict)
    (all_x[:], all_y[:], all_len[:]) = ([], [], [])
    all_x = [35, 40, 15, 10, 45, 20, 10, 15, 25, 45, 10, 30, 10, 40]
    all_y = [5, 10, 15, 20, 20, 25, 30, 35, 35, 35, 40, 40, 45, 45]
    all_len = [10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 5]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)
    if show_animation:
        plt.plot(o_x, o_y, '.k')
        plt.plot(s_x, s_y, 'og')
        plt.plot(g_x, g_y, 'xb')
        plt.grid(True)
    if use_jump_point:
        keypoint_list = key_points(obs_dict)
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101, keypoint_list)
        search_obj.jump_point()
    else:
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101)
        search_obj.a_star()","for i in range(51):
    for j in range(51):
        obs_dict[i, j] = False","for i,_ in enumerate(range(51)):
    for j,_ in enumerate(range(51)):
        obs_dict[i, j] = False",,,,,,,,,,,
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i,_ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate",,,,,,,,,,,
onnx-tensorflow,https://github.com/onnx/onnx-tensorflow/tree/master/test/backend/test_node.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnx-tensorflow/test/backend/test_node.py,TestNode,"def test_scan_v8(self):
    if legacy_opset_pre_ver(8) or not legacy_opset_pre_ver(9):
        raise unittest.SkipTest('ONNX version {} not supported.'.format(defs.onnx_opset_version()))
    initial = self._get_rnd_int(0, 100, shape=[5, 1]).astype(np.float32)
    x1 = self._get_rnd_float32(0, 1000, shape=[5, 20, 6, 2])
    x2 = self._get_rnd_float32(0, 1000, shape=[5, 20, 6, 2])
    directions = [0, 1]
    sequence_lens = np.array([15, 20, 14, 18, 20]).astype(np.int32)
    Y = initial + (np.shape(x1)[1] if sequence_lens is str else np.reshape(sequence_lens, [-1, 1]))
    x1_out = x1 + 1
    x2_out = x2[:, ::-1] + 1
    Z = np.concatenate([x1_out, x2_out], 2)
    if sequence_lens is not str:
        for batch in range(len(sequence_lens)):
            shape = list(np.shape(Z[batch]))
            seq_len = sequence_lens[batch]
            zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])
            Z[batch] = np.concatenate([Z[batch][:seq_len], zero_pad])
    output = self._run_scan_node(initial, x1, x2, [6, 4], [3, 2], sequence_lens=sequence_lens, directions=directions)
    output_z = np.concatenate([output['z1'], output['z2'], output['z3'], output['z4']], 2)
    np.testing.assert_almost_equal(output['y'], Y)
    np.testing.assert_almost_equal(output_z, Z)","for batch in range(len(sequence_lens)):
    shape = list(np.shape(Z[batch]))
    seq_len = sequence_lens[batch]
    zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])
    Z[batch] = np.concatenate([Z[batch][:seq_len], zero_pad])","for i,batch in enumerate(range(len(sequence_lens))):
    shape = list(np.shape(Z[i]))
    seq_len = sequence_lens[i]
    zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])
    Z[i] = np.concatenate([Z[i][:seq_len], zero_pad])",,,,,,,,,,,
nut,https://github.com/blawar/nut/tree/master/nut/blockchain.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nut/nut/blockchain.py,Blockchain,"def hasTitle(self, id):
    for c in self.chain:
        for t in c.transactions:
            if t.titleId == id:
                return t.titleKey
    return None","for c in self.chain:
    for t in c.transactions:
        if t.titleId == id:
            return t.titleKey","for i,c in enumerate(self.chain):
    for t in c.transactions:
        if t.titleId == id:
            return t.titleKey",,,,,,,,,,,
jetson-gpio,https://github.com/NVIDIA/jetson-gpio/tree/master/lib/python/Jetson/GPIO/gpio.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jetson-gpio/lib/python/Jetson/GPIO/gpio.py,,"def setup(channels, direction, pull_up_down=_Default(PUD_OFF), initial=None):
    if pull_up_down in setup.__defaults__:
        pull_up_down_explicit = False
        pull_up_down = pull_up_down.val
    else:
        pull_up_down_explicit = True
    ch_infos = _channels_to_infos(channels, need_gpio=True)
    if direction != OUT and direction != IN:
        raise ValueError('An invalid direction was passed to setup()')
    if direction == OUT and pull_up_down != PUD_OFF:
        raise ValueError('pull_up_down parameter is not valid for outputs')
    if pull_up_down_explicit:
        warnings.warn(""Jetson.GPIO ignores setup()'s pull_up_down parameter"")
    if pull_up_down != PUD_OFF and pull_up_down != PUD_UP and (pull_up_down != PUD_DOWN):
        raise ValueError('Invalid value for pull_up_down; should be one ofPUD_OFF, PUD_UP or PUD_DOWN')
    if _gpio_warnings:
        for ch_info in ch_infos:
            sysfs_cfg = _sysfs_channel_configuration(ch_info)
            app_cfg = _app_channel_configuration(ch_info)
            if app_cfg is None and sysfs_cfg is not None:
                warnings.warn('This channel is already in use, continuing anyway. Use GPIO.setwarnings(False) to disable warnings', RuntimeWarning)
    for ch_info in ch_infos:
        if ch_info.channel in _channel_configuration:
            _cleanup_one(ch_info)
    if direction == OUT:
        initial = _make_iterable(initial, len(ch_infos))
        if len(initial) != len(ch_infos):
            raise RuntimeError('Number of values != number of channels')
        for (ch_info, init) in zip(ch_infos, initial):
            _setup_single_out(ch_info, init)
    else:
        if initial is not None:
            raise ValueError('initial parameter is not valid for inputs')
        for ch_info in ch_infos:
            _setup_single_in(ch_info)","for ch_info in ch_infos:
    if ch_info.channel in _channel_configuration:
        _cleanup_one(ch_info)","for i,ch_info in enumerate(ch_infos):
    if ch_info.channel in _channel_configuration:
        _cleanup_one(ch_info)",,,,,,,,,,,
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)","for collider in collrates:
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table","for i,collider in enumerate(collrates):
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table",,,,,,,,,,,
listenbrainz-server,https://github.com/metabrainz/listenbrainz-server/tree/master/listenbrainz/db/tests/test_user.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/listenbrainz-server/listenbrainz/db/tests/test_user.py,UserTestCase,"def test_get_all_users_columns(self):
    """""" Tests that get_all_users only returns those columns which are asked for """"""
    users = db_user.get_all_users()
    for user in users:
        for column in db_user.USER_GET_COLUMNS:
            self.assertIn(column, user)
    users = db_user.get_all_users(columns=['id'])
    for user in users:
        self.assertIn('id', user)
        for column in db_user.USER_GET_COLUMNS:
            if column != 'id':
                self.assertNotIn(column, user)","for column in db_user.USER_GET_COLUMNS:
    self.assertIn(column, user)","for i,column in enumerate(db_user.USER_GET_COLUMNS):
    self.assertIn(column, user)",,,,,,,,,,,
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/modnas/estim/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/modnas/estim/base.py,,"def build_criterions_all(crit_configs, device_ids=None):
    """"""Build Criterions from configs.""""""
    crits_all = []
    crits_train = []
    crits_eval = []
    crits_valid = []
    for crit_conf in streamline_spec(crit_configs):
        crit = backend.get_criterion(crit_conf, device_ids=device_ids)
        crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
        if not isinstance(crit_mode, list):
            crit_mode = [crit_mode]
        if 'all' in crit_mode:
            crits_all.append(crit)
        if 'train' in crit_mode:
            crits_train.append(crit)
        if 'eval' in crit_mode:
            crits_eval.append(crit)
        if 'valid' in crit_mode:
            crits_valid.append(crit)
    return (crits_all, crits_train, crits_eval, crits_valid)","for crit_conf in streamline_spec(crit_configs):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)","for i,crit_conf in enumerate(streamline_spec(crit_configs)):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)",,,,,,,,,,,
advertools,https://github.com/eliasdabbas/advertools/tree/master/advertools/serp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/advertools/advertools/serp.py,,"def youtube_channel_details(key, channel_ids):
    """"""Return details of channels for which the ids are given.
    Assumes ``ids`` is a comma-separated list of channel ids with
    no spaces.""""""
    base_url = 'https://www.googleapis.com/youtube/v3/channels?part=snippet,contentDetails,statistics'
    channel_ids = _split_by_comma(channel_ids, length=50)
    final_df = pd.DataFrame()
    for channel_id in channel_ids:
        params = {'id': channel_id, 'key': key}
        logging.info(msg='Requesting: ' + 'channel details')
        channel_resp = requests.get(base_url, params=params)
        if channel_resp.status_code >= 400:
            raise Exception(channel_resp.json())
        items_df = pd.DataFrame(channel_resp.json()['items'])
        details = ['snippet', 'statistics', 'contentDetails']
        detail_df = pd.DataFrame()
        for detail in details:
            try:
                detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
            except KeyError:
                continue
        temp_df = pd.concat([items_df, detail_df], axis=1)
        final_df = final_df.append(temp_df, sort=False, ignore_index=True)
    return final_df","for detail in details:
    try:
        detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
    except KeyError:
        continue","for i,detail in enumerate(details):
    try:
        detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
    except KeyError:
        continue",,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/micro/model_library_format.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/micro/model_library_format.py,,"def _build_sid_map(graph_json):
    """"""Build a simpler storage id info map from graph JSON.

    Parameters
    ----------
    graph_json : str
        String representation of the graph_json created from tvm.relay.build().

    Returns
    -------
    list :
        A list with one entry per storage id describing that memory.
    """"""
    graph = json.loads(graph_json)
    seen_storage_ids = set()
    memory_map = []
    for (node_id, storage_id) in enumerate(graph['attrs']['storage_id'][1]):
        if storage_id in seen_storage_ids:
            continue
        seen_storage_ids.add(storage_id)
        num_elements = 1
        for dim in graph['attrs']['shape'][1][storage_id]:
            num_elements *= dim
        dltype = graph['attrs']['dltype'][1][storage_id]
        m = re.match('^[a-zA-Z]+([0-9]+)$', dltype)
        assert m, f'Exported graph contains unknown dltype {dltype}'
        elem_bits = int(m.group(1))
        map_entry = {'storage_id': storage_id, 'size_bytes': (num_elements * elem_bits + 7) // 8}
        if node_id in graph['arg_nodes']:
            map_entry['input_binding'] = graph['nodes'][node_id]['name']
        memory_map.append(map_entry)
    return memory_map","for dim in graph['attrs']['shape'][1][storage_id]:
    num_elements *= dim","for i,dim in enumerate(graph['attrs']['shape'][1][storage_id]):
    num_elements *= dim",,,,,,,,,,,
neon,https://github.com/NervanaSystems/neon/tree/master/tests/test_conv_layer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neon/tests/test_conv_layer.py,ConvLayerRef,"def makelinks(self, nifm, ifmsize, ifmshape, ofmshape, fshape, strides):
    ndims = len(ifmshape)
    dimsizes = np.empty(ndims, dtype='int32')
    for dim in range(ndims):
        dimsizes[dim] = np.prod(ifmshape[dim:])
    links = []
    for ofmdim in np.ndindex(ofmshape):
        src = ofmdim[-1]
        for dim in range(-1, -ndims, -1):
            src += dimsizes[dim] * ofmdim[dim - 1]
        src *= strides
        indlist = list(range(src, src + fshape[-1]))
        for dim in range(-1, -ndims, -1):
            indarray = np.array(indlist)
            for dimind in range(1, fshape[dim - 1]):
                indlist.extend(list(indarray + dimind * dimsizes[dim]))
        indarray = np.array(indlist)
        for ifm in range(1, nifm):
            indlist.extend(list(indarray + ifm * ifmsize))
        links.append(indlist)
    self.links = np.array(links, dtype='int32')","for dim in range(-1, -ndims, -1):
    src += dimsizes[dim] * ofmdim[dim - 1]","for i,dim in enumerate(range(-1, -ndims, -1)):
    src += dimsizes[dim] * ofmdim[dim - 1]",,,,,,,,,,,
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for e in el:
    self.delete(e)","for i,e in enumerate(el):
    self.delete(e)",,,,,,,,,,,
ihatemoney,https://github.com/spiral-project/ihatemoney/tree/master/ihatemoney/forms.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ihatemoney/ihatemoney/forms.py,InviteForm,"def validate_emails(form, field):
    for email in [email.strip() for email in form.emails.data.split(',')]:
        try:
            email_validator.validate_email(email)
        except email_validator.EmailNotValidError:
            raise ValidationError(_('The email %(email)s is not valid', email=email))","for email in [email.strip() for email in form.emails.data.split(',')]:
    try:
        email_validator.validate_email(email)
    except email_validator.EmailNotValidError:
        raise ValidationError(_('The email %(email)s is not valid', email=email))","for i,email in enumerate([email.strip() for email in form.emails.data.split(',')]):
    try:
        email_validator.validate_email(email)
    except email_validator.EmailNotValidError:
        raise ValidationError(_('The email %(email)s is not valid', email=email))",,,,,,,,,,,
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for f in filenames:
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))","for i,f in enumerate(filenames):
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))",,,,,,,,,,,
rasa,https://github.com/RasaHQ/rasa/tree/master/.github/scripts/mr_publish_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/.github/scripts/mr_publish_results.py,,"def create_report_file() -> None:
    data = {}
    for (dirpath, dirnames, files) in os.walk(os.environ['RESULT_DIR']):
        for f in files:
            if f not in TASK_MAPPING.keys():
                continue
            data = generate_json(os.path.join(dirpath, f), TASK_MAPPING[f], data)
    with open(os.environ['SUMMARY_FILE'], 'w') as f:
        json.dump(data, f, sort_keys=True, indent=2)","for f in files:
    if f not in TASK_MAPPING.keys():
        continue
    data = generate_json(os.path.join(dirpath, f), TASK_MAPPING[f], data)","for i,f in enumerate(files):
    if f not in TASK_MAPPING.keys():
        continue
    data = generate_json(os.path.join(dirpath, f), TASK_MAPPING[f], data)",,,,,,,,,,,
attn2d,https://github.com/elbayadm/attn2d/tree/master/scripts/average_checkpoints.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attn2d/scripts/average_checkpoints.py,,"def last_n_checkpoints(paths, n, update_based, upper_bound=None):
    assert len(paths) == 1
    path = paths[0]
    if update_based:
        pt_regexp = re.compile('checkpoint_\\d+_(\\d+)\\.pt')
    else:
        pt_regexp = re.compile('checkpoint(\\d+)\\.pt')
    files = PathManager.ls(path)
    entries = []
    for f in files:
        m = pt_regexp.fullmatch(f)
        if m is not None:
            sort_key = int(m.group(1))
            if upper_bound is None or sort_key <= upper_bound:
                entries.append((sort_key, m.group(0)))
    if len(entries) < n:
        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)
    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]","for f in files:
    m = pt_regexp.fullmatch(f)
    if m is not None:
        sort_key = int(m.group(1))
        if upper_bound is None or sort_key <= upper_bound:
            entries.append((sort_key, m.group(0)))","for i,f in enumerate(files):
    m = pt_regexp.fullmatch(f)
    if m is not None:
        sort_key = int(m.group(1))
        if upper_bound is None or sort_key <= upper_bound:
            entries.append((sort_key, m.group(0)))",,,,,,,,,,,
zentral,https://github.com/zentralopensource/zentral/tree/master/tests/inventory/test_metrics_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zentral/tests/inventory/test_metrics_views.py,PrometheusViewsTestCase,"def test_prometheus_metrics_osx_apps_bundle_names(self):
    old_config = settings._collection['apps']['zentral.contrib.inventory'].pop('metrics_options', None)
    settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = ConfigDict({'osx_apps': {'sources': ['zentral tests'], 'bundle_names': ['Baller']}})
    response = self.client.get(reverse('inventory_metrics:all'), HTTP_AUTHORIZATION='Bearer CHANGE ME!!!')
    self.assertEqual(response.status_code, 200)
    seen = False
    for family in text_string_to_metric_families(response.content.decode('utf-8')):
        if family.name == 'zentral_inventory_active_machines_bucket':
            continue
        self.assertEqual(len(family.samples), 7)
        for sample in family.samples:
            self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
            le = sample.labels['le']
            self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
            if le == '1':
                self.assertEqual(sample.value, 0)
            else:
                self.assertEqual(sample.value, 1)
        self.assertFalse(seen)
        seen = True
    self.assertTrue(seen)
    if old_config:
        settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = old_config","for family in text_string_to_metric_families(response.content.decode('utf-8')):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True","for i,family in enumerate(text_string_to_metric_families(response.content.decode('utf-8'))):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True",,,,,,,,,,,
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]","for i,fextractor_type in enumerate(self.feature_dict):
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]",,,,,,,,,,,
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","for file in files:
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {file} has {questions} questions and {answers} answers.')","for i,file in enumerate(files):
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {i} has {questions} questions and {answers} answers.')",,,,,,,,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial","for file in vbox:
    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f.readlines():
            res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
            if res:
                serial.append(f'127.0.0.1:{res.group(1)}')","for i,file in enumerate(vbox):
    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f.readlines():
            res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
            if res:
                serial.append(f'127.0.0.1:{res.group(1)}')",,,,,,,,,,,
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})","for file_id in ids:
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})","for i,file_id in enumerate(ids):
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})",,,,,,,,,,,
luminoth,https://github.com/tryolabs/luminoth/tree/master/luminoth/utils/config.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luminoth/luminoth/utils/config.py,,"def load_config_files(filename_or_filenames, warn_overwrite=True):
    if isinstance(filename_or_filenames, list) or isinstance(filename_or_filenames, tuple):
        filenames = filename_or_filenames
    else:
        filenames = [filename_or_filenames]
    if len(filenames) <= 0:
        tf.logging.error('Tried to load 0 config files.')
    config = EasyDict({})
    for filename in filenames:
        with tf.gfile.GFile(filename) as f:
            new_config = EasyDict(yaml.load(f))
        config = merge_into(new_config, config, overwrite=True, warn_overwrite=warn_overwrite)
    return config","for filename in filenames:
    with tf.gfile.GFile(filename) as f:
        new_config = EasyDict(yaml.load(f))
    config = merge_into(new_config, config, overwrite=True, warn_overwrite=warn_overwrite)","for i,filename in enumerate(filenames):
    with tf.gfile.GFile(filename) as f:
        new_config = EasyDict(yaml.load(f))
    config = merge_into(new_config, config, overwrite=True, warn_overwrite=warn_overwrite)",,,,,,,,,,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","for filename in source_files:
    if options.verbose:
        print('Working on %s' % filename)
    fp = open(filename, encoding='utf-8')
    closep = 1
    try:
        eater.set_filename(filename)
        try:
            tokens = tokenize.generate_tokens(fp.readline)
            for _token in tokens:
                eater(*_token)
        except tokenize.TokenError as e:
            print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
    finally:
        if closep:
            fp.close()","for i,filename in enumerate(source_files):
    if options.verbose:
        print('Working on %s' % filename)
    fp = open(filename, encoding='utf-8')
    closep = 1
    try:
        eater.set_filename(filename)
        try:
            tokens = tokenize.generate_tokens(fp.readline)
            for _token in tokens:
                eater(*_token)
        except tokenize.TokenError as e:
            print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
    finally:
        if closep:
            fp.close()",,,,,,,,,,,
arcgis-python-api,https://github.com/Esri/arcgis-python-api/tree/master/misc/_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcgis-python-api/misc/_common.py,,"def delete_services_from_servers(gis, user=None):
    """"""delete services from federated servers""""""
    for server in gis.admin.servers.list():
        for folder_name in server.services.folders:
            if folder_name not in ignore_server_folders:
                'stop and delete services per folder (per user)'
                for service in server.services.list(folder=folder_name):
                    if service.type == 'FeatureServer' or service.type == 'SceneServer':
                        item_id = service.properties.portalProperties.get('portalItems')[0]['itemID']
                        item = gis.content.get(item_id)
                        if user is None:
                            if item.owner is None:
                                print('=== deleting service (admin)', service.type, service.serviceName, item.owner, service.url)
                                service.stop()
                                service.delete()
                        elif item.owner == user.username:
                            print('=== deleting service', service.type, service.serviceName, item.owner)
                            service.stop()
                            service.delete()
                        else:
                            pass
                    elif service.type == 'ImageServer' or service.type == 'MapServer':
                        service_owner = service.properties.properties.get('userName')
                        if user is None:
                            if service_owner is None and any((token in service.serviceName for token in image_service_prefixes)):
                                print('=== deleting service (admin)', service.type, service.serviceName, service_owner, service.url)
                                service.stop()
                                service.delete()
                        elif service_owner == user.username:
                            print('=== deleting service', service.type, service.serviceName, service_owner)
                            service.stop()
                            service.delete()
                        else:
                            pass
                    elif service.type in preserved_service_types:
                        pass
                    else:
                        print('=== Unrecognized Service Type', service.type, service.serviceName, service.properties)","for folder_name in server.services.folders:
    if folder_name not in ignore_server_folders:
        'stop and delete services per folder (per user)'
        for service in server.services.list(folder=folder_name):
            if service.type == 'FeatureServer' or service.type == 'SceneServer':
                item_id = service.properties.portalProperties.get('portalItems')[0]['itemID']
                item = gis.content.get(item_id)
                if user is None:
                    if item.owner is None:
                        print('=== deleting service (admin)', service.type, service.serviceName, item.owner, service.url)
                        service.stop()
                        service.delete()
                elif item.owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, item.owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type == 'ImageServer' or service.type == 'MapServer':
                service_owner = service.properties.properties.get('userName')
                if user is None:
                    if service_owner is None and any((token in service.serviceName for token in image_service_prefixes)):
                        print('=== deleting service (admin)', service.type, service.serviceName, service_owner, service.url)
                        service.stop()
                        service.delete()
                elif service_owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, service_owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type in preserved_service_types:
                pass
            else:
                print('=== Unrecognized Service Type', service.type, service.serviceName, service.properties)","for i,folder_name in enumerate(server.services.folders):
    if folder_name not in ignore_server_folders:
        'stop and delete services per folder (per user)'
        for service in server.services.list(folder=folder_name):
            if service.type == 'FeatureServer' or service.type == 'SceneServer':
                item_id = service.properties.portalProperties.get('portalItems')[0]['itemID']
                item = gis.content.get(item_id)
                if user is None:
                    if item.owner is None:
                        print('=== deleting service (admin)', service.type, service.serviceName, item.owner, service.url)
                        service.stop()
                        service.delete()
                elif item.owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, item.owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type == 'ImageServer' or service.type == 'MapServer':
                service_owner = service.properties.properties.get('userName')
                if user is None:
                    if service_owner is None and any((token in service.serviceName for token in image_service_prefixes)):
                        print('=== deleting service (admin)', service.type, service.serviceName, service_owner, service.url)
                        service.stop()
                        service.delete()
                elif service_owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, service_owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type in preserved_service_types:
                pass
            else:
                print('=== Unrecognized Service Type', service.type, service.serviceName, service.properties)",,,,,,,,,,,
Kunlun-M,https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kunlun-M/core/detection.py,Detection,"def framework(self):
    tree = self.rule()
    root = tree.getroot()
    (frame_data, language_data) = self.parse_xml(root, self.frame_data, self.language_data)
    projects_data = self.project_information(self.target_directory, False)
    frame_name = self.dependency_scan(root)
    if frame_name is not None:
        return frame_name
    frames_name = frame_data.keys()
    for frame_name in frames_name:
        for rule_name in frame_data[frame_name]:
            for project_data in projects_data:
                if rule_name in project_data:
                    logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                    return frame_name
    logger.info('[DETECTION] [FRAMEWORK] Unknown Framework')
    return 'Unknown Framework'","for frame_name in frames_name:
    for rule_name in frame_data[frame_name]:
        for project_data in projects_data:
            if rule_name in project_data:
                logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                return frame_name","for i,frame_name in enumerate(frames_name):
    for rule_name in frame_data[frame_name]:
        for project_data in projects_data:
            if rule_name in project_data:
                logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                return frame_name",,,,,,,,,,,
hyperpose,https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pifpaf/eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyperpose/hyperpose/Model/pifpaf/eval.py,,"def visualize(img, img_id, processed_img, pd_pif_maps, pd_paf_maps, humans, stride=8, save_dir='./save_dir'):
    print(f'{len(humans)} human found!')
    print('visualizing...')
    os.makedirs(save_dir, exist_ok=True)
    ori_img = np.clip(img * 255.0, 0.0, 255.0).astype(np.uint8)
    processed_img = np.clip(processed_img * 255.0, 0.0, 255.0).astype(np.uint8)
    vis_img = ori_img.copy()
    for human in humans:
        vis_img = human.draw_human(vis_img)
    (pd_pif_conf, pd_pif_vec, _, pd_pif_scale) = pd_pif_maps
    (pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, _, _, _, _) = pd_paf_maps
    pd_pif_conf_show = np.amax(pd_pif_conf, axis=0)
    pd_pif_hr_conf_show = np.amax(get_hr_conf(pd_pif_conf, pd_pif_vec, pd_pif_scale, stride=stride, thresh=0.1), axis=0)
    pd_paf_conf_show = np.amax(pd_paf_conf, axis=0)
    pd_paf_vec_show = np.zeros(shape=(pd_pif_hr_conf_show.shape[0], pd_pif_hr_conf_show.shape[1], 3)).astype(np.int8)
    pd_paf_vec_show = get_arrow_map(pd_paf_vec_show, pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, thresh=0.1)
    fig = plt.figure(figsize=(12, 12))
    a = fig.add_subplot(3, 3, 1)
    a.set_title('input image')
    plt.imshow(ori_img)
    a = fig.add_subplot(3, 3, 3)
    a.set_title('output result')
    plt.imshow(vis_img)
    a = fig.add_subplot(3, 3, 4)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 5)
    a.set_title('pif_conf_map')
    plt.imshow(pd_pif_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 6)
    a.set_title('pif_hr_conf_map')
    plt.imshow(pd_pif_hr_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 7)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 8)
    a.set_title('paf_conf_map')
    plt.imshow(pd_paf_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 9)
    a.set_title('paf_vec_map')
    plt.imshow(pd_paf_vec_show, alpha=0.8)
    plt.colorbar()
    plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))
    plt.close()","for human in humans:
    vis_img = human.draw_human(vis_img)","for i,human in enumerate(humans):
    vis_img = human.draw_human(vis_img)",,,,,,,,,,,
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/solo/utils/knn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/solo/utils/knn.py,WeightedKNNClassifier,"def compute(self) -> Tuple[float]:
    """"""Computes weighted k-NN accuracy @1 and @5. If cosine distance is selected,
        the weight is computed using the exponential of the temperature scaled cosine
        distance of the samples. If euclidean distance is selected, the weight corresponds
        to the inverse of the euclidean distance.

        Returns:
            Tuple[float]: k-NN accuracy @1 and @5.
        """"""
    train_features = torch.cat(self.train_features)
    train_targets = torch.cat(self.train_targets)
    test_features = torch.cat(self.test_features)
    test_targets = torch.cat(self.test_targets)
    if self.distance_fx == 'cosine':
        train_features = F.normalize(train_features)
        test_features = F.normalize(test_features)
    num_classes = torch.unique(test_targets).numel()
    num_train_images = train_targets.size(0)
    num_test_images = test_targets.size(0)
    num_train_images = train_targets.size(0)
    chunk_size = min(max(1, self.max_distance_matrix_size // num_train_images), num_test_images)
    k = min(self.k, num_train_images)
    (top1, top5, total) = (0.0, 0.0, 0)
    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)
    for idx in range(0, num_test_images, chunk_size):
        features = test_features[idx:min(idx + chunk_size, num_test_images), :]
        targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
        batch_size = targets.size(0)
        if self.distance_fx == 'cosine':
            similarities = torch.mm(features, train_features.t())
        elif self.distance_fx == 'euclidean':
            similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
        else:
            raise NotImplementedError
        (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
        candidates = train_targets.view(1, -1).expand(batch_size, -1)
        retrieved_neighbors = torch.gather(candidates, 1, indices)
        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
        if self.distance_fx == 'cosine':
            similarities = similarities.clone().div_(self.T).exp_()
        probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
        (_, predictions) = probs.sort(1, True)
        correct = predictions.eq(targets.data.view(-1, 1))
        top1 = top1 + correct.narrow(1, 0, 1).sum().item()
        top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
        total += targets.size(0)
    top1 = top1 * 100.0 / total
    top5 = top5 * 100.0 / total
    self.reset()
    return (top1, top5)","for idx in range(0, num_test_images, chunk_size):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)","for i,idx in enumerate(range(0, num_test_images, chunk_size)):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)",,,,,,,,,,,
checkov,https://github.com/bridgecrewio/checkov/tree/master/checkov/common/bridgecrew/integration_features/integration_feature_registry.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkov/checkov/common/bridgecrew/integration_features/integration_feature_registry.py,IntegrationFeatureRegistry,"def run_post_runner(self, scan_report: Report) -> None:
    for integration in self.features:
        if integration.is_valid():
            integration.post_runner(scan_report)","for integration in self.features:
    if integration.is_valid():
        integration.post_runner(scan_report)","for i,integration in enumerate(self.features):
    if integration.is_valid():
        integration.post_runner(scan_report)",,,,,,,,,,,
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/zoo/evaluation/metrics/data_extraction.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/zoo/evaluation/metrics/data_extraction.py,,"if __name__ == '__main__':
    parser = argparse.ArgumentParser('policy-evaluation')
    parser.add_argument('path', help='Directory path containing data files (.jsonl)', type=str)
    parser.add_argument('result_path', help='Directory path containing data files', type=str)
    parser.add_argument('agent_name', help='Agent name', type=str)
    parser.add_argument('--step-num', help='Number of steps', type=str, default=600)
    parser.add_argument('--timestep-sec', type=float, default=0.1, help='Timestep, can be seen as the pause duration between frames')
    args = parser.parse_args()
    agent_name = args.agent_name
    jsonl_paths = list(Path(args.path).glob('*.jsonl'))
    step_num = int(args.step_num.split(':')[-1])
    assert len(jsonl_paths) == 1
    for jsonl in jsonl_paths:
        data = extract_data(jsonl, step_num, args.timestep_sec)
        time_suffix = datetime.now().strftime('%Y%m%d-%H%M%S')
        scenario_path = Path(args.path).parent.parent
        result_path = Path(args.result_path)
        result_file = result_path / f'evaluation-data_{scenario_path.name}_{agent_name}_{time_suffix}.json'
        with open(result_file, 'w') as f:
            json.dump(data, f)","for jsonl in jsonl_paths:
    data = extract_data(jsonl, step_num, args.timestep_sec)
    time_suffix = datetime.now().strftime('%Y%m%d-%H%M%S')
    scenario_path = Path(args.path).parent.parent
    result_path = Path(args.result_path)
    result_file = result_path / f'evaluation-data_{scenario_path.name}_{agent_name}_{time_suffix}.json'
    with open(result_file, 'w') as f:
        json.dump(data, f)","for i,jsonl in enumerate(jsonl_paths):
    data = extract_data(jsonl, step_num, args.timestep_sec)
    time_suffix = datetime.now().strftime('%Y%m%d-%H%M%S')
    scenario_path = Path(args.path).parent.parent
    result_path = Path(args.result_path)
    result_file = result_path / f'evaluation-data_{scenario_path.name}_{agent_name}_{time_suffix}.json'
    with open(result_file, 'w') as f:
        json.dump(data, f)",,,,,,,,,,,
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/tests/utils/test_auto_resumer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/tests/utils/test_auto_resumer.py,,"def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    loaded_cfg = json.load(open(args_path))
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)","for k in cfg_dict:
    assert cfg_dict[k] == loaded_cfg[k]","for i,k in enumerate(cfg_dict):
    assert cfg_dict[k] == loaded_cfg[k]",,,,,,,,,,,
zvt,https://github.com/zvtvz/zvt/tree/master/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,ChinaETFDayKdataRecorder,"def on_finish_entity(self, entity):
    kdatas = get_kdata(entity_id=entity.id, level=IntervalLevel.LEVEL_1DAY.value, order=Etf1dKdata.timestamp.asc(), return_type='domain', session=self.session, filters=[Etf1dKdata.cumulative_net_value.is_(None)])
    if kdatas and len(kdatas) > 0:
        start = kdatas[0].timestamp
        end = kdatas[-1].timestamp
        df = self.fetch_cumulative_net_value(entity, start, end)
        if df is not None and (not df.empty):
            for kdata in kdatas:
                if kdata.timestamp in df.index:
                    kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
                    kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']
            self.session.commit()
            self.logger.info(f'{entity.code} - {entity.name}累计净值更新完成...')","for kdata in kdatas:
    if kdata.timestamp in df.index:
        kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
        kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']","for i,kdata in enumerate(kdatas):
    if kdata.timestamp in df.index:
        kdatas[i].cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
        kdatas[i].change_pct = df.loc[kdata.timestamp, 'JZZZL']",,,,,,,,,,,
nilearn,https://github.com/nilearn/nilearn/tree/master/nilearn/interfaces/fmriprep/load_confounds_strategy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nilearn/nilearn/interfaces/fmriprep/load_confounds_strategy.py,,"def _update_user_inputs(kwargs, default_parameters, check_parameters):
    """"""Update keyword parameters with user inputs if applicable.""""""
    parameters = default_parameters.copy()
    not_needed = []
    for key in check_parameters:
        value = kwargs.pop(key, None)
        if value is not None:
            parameters[key] = value
        if key == 'global_signal':
            if isinstance(value, str):
                parameters['strategy'] += ('global_signal',)
            else:
                parameters.pop('global_signal', None)
    not_needed = list(kwargs.keys())
    return (parameters, not_needed)","for key in check_parameters:
    value = kwargs.pop(key, None)
    if value is not None:
        parameters[key] = value
    if key == 'global_signal':
        if isinstance(value, str):
            parameters['strategy'] += ('global_signal',)
        else:
            parameters.pop('global_signal', None)","for i,key in enumerate(check_parameters):
    value = kwargs.pop(key, None)
    if value is not None:
        parameters[key] = value
    if key == 'global_signal':
        if isinstance(value, str):
            parameters['strategy'] += ('global_signal',)
        else:
            parameters.pop('global_signal', None)",,,,,,,,,,,
astropy,https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_groups.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/table/tests/test_groups.py,,"def test_group_mixins():
    """"""
    Test grouping a table with mixin columns
    """"""
    idx = np.arange(4)
    x = np.array([3.0, 1.0, 2.0, 1.0])
    q = x * u.m
    lon = coordinates.Longitude(x * u.deg)
    lat = coordinates.Latitude(x * u.deg)
    tm = time.Time(2000, format='jyear') + time.TimeDelta(x * 1e-10, format='sec')
    sc = coordinates.SkyCoord(ra=lon, dec=lat)
    aw = table_helpers.ArrayWrapper(x)
    nd = np.array([(3, 'c'), (1, 'a'), (2, 'b'), (1, 'a')], dtype='<i4,|S1').view(NdarrayMixin)
    qt = QTable([idx, x, q, lon, lat, tm, sc, aw, nd], names=['idx', 'x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd'])
    mixin_keys = ['x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd']
    for key in mixin_keys:
        qtg = qt.group_by(key)
        assert np.all(qtg['idx'] == [1, 3, 2, 0])
        for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
            assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
            assert np.all(qt[name][[2]] == qtg.groups[1][name])
            assert np.all(qt[name][[0]] == qtg.groups[2][name])
    uqt = unique(qt, keys=mixin_keys)
    assert len(uqt) == 3
    assert np.all(uqt['idx'] == [1, 2, 0])
    assert np.all(uqt['x'] == [1.0, 2.0, 3.0])
    idxg = qt['idx'].group_by(qt[mixin_keys])
    assert np.all(idxg == [1, 3, 2, 0])","for key in mixin_keys:
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])","for i,key in enumerate(mixin_keys):
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])",,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/tabular.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/datatypes/tabular.py,Eland,"def sniff_prefix(self, file_prefix: FilePrefix):
    """"""
        Determines whether the file is in ELAND export format

        A file in ELAND export format consists of lines of tab-separated data.
        There is no header.

        Rules for sniffing as True::

            - There must be 22 columns on each line
            - LANE, TILEm X, Y, INDEX, READ_NO, SEQ, QUAL, POSITION, *STRAND, FILT must be correct
            - We will only check that up to the first 5 alignments are correctly formatted.
        """"""
    count = 0
    for line in file_prefix.line_iterator():
        line = line.strip()
        if not line:
            break
        if line:
            line_pieces = line.split('\t')
            if len(line_pieces) != 22:
                return False
            if int(line_pieces[1]) < 0:
                raise Exception('Out of range')
            if int(line_pieces[2]) < 0:
                raise Exception('Out of range')
            if int(line_pieces[3]) < 0:
                raise Exception('Out of range')
            int(line_pieces[4])
            int(line_pieces[5])
            count += 1
            if count == 5:
                break
    if count > 0:
        return True","for line in file_prefix.line_iterator():
    line = line.strip()
    if not line:
        break
    if line:
        line_pieces = line.split('\t')
        if len(line_pieces) != 22:
            return False
        if int(line_pieces[1]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[2]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[3]) < 0:
            raise Exception('Out of range')
        int(line_pieces[4])
        int(line_pieces[5])
        count += 1
        if count == 5:
            break","for i,line in enumerate(file_prefix.line_iterator()):
    line = line.strip()
    if not line:
        break
    if line:
        line_pieces = line.split('\t')
        if len(line_pieces) != 22:
            return False
        if int(line_pieces[1]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[2]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[3]) < 0:
            raise Exception('Out of range')
        int(line_pieces[4])
        int(line_pieces[5])
        count += 1
        if count == 5:
            break",,,,,,,,,,,
sympy,https://github.com/sympy/sympy/tree/master/sympy/printing/fortran.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/fortran.py,FCodePrinter,"def _pad_leading_columns(self, lines):
    result = []
    for line in lines:
        if line.startswith('!'):
            result.append(self._lead['comment'] + line[1:].lstrip())
        else:
            result.append(self._lead['code'] + line)
    return result","for line in lines:
    if line.startswith('!'):
        result.append(self._lead['comment'] + line[1:].lstrip())
    else:
        result.append(self._lead['code'] + line)","for i,line in enumerate(lines):
    if line.startswith('!'):
        result.append(self._lead['comment'] + line[1:].lstrip())
    else:
        result.append(self._lead['code'] + line)",,,,,,,,,,,
SDV,https://github.com/sdv-dev/SDV/tree/master//tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV//tasks.py,,"def install_minimum(c):
    with open('setup.py', 'r') as setup_py:
        lines = setup_py.read().splitlines()
    versions = []
    started = False
    for line in lines:
        if started:
            if line == ']':
                started = False
                continue
            line = line.strip()
            if _validate_python_version(line):
                requirement = re.match('[^>]*', line).group(0)
                requirement = re.sub('[\'"",]', '', requirement)
                version = re.search('>=?[^(,|#)]*', line).group(0)
                if version:
                    version = re.sub('>=?', '==', version)
                    version = re.sub('[\'"",]', '', version)
                    requirement += version
                versions.append(requirement)
        elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
            started = True
    c.run(f""python -m pip install {' '.join(versions)}"")","for line in lines:
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True","for i,line in enumerate(lines):
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True",,,,,,,,,,,
DBNet.pytorch,https://github.com/WenmuZhou/DBNet.pytorch/tree/master/utils/cal_recall/rrc_evaluation_funcs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DBNet.pytorch/utils/cal_recall/rrc_evaluation_funcs.py,,"def get_tl_line_values_from_file_contents(content, CRLF=True, LTRB=True, withTranscription=False, withConfidence=False, imWidth=0, imHeight=0, sort_by_confidences=True):
    """"""
    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:
    xmin,ymin,xmax,ymax,[confidence],[transcription]
    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]
    """"""
    pointsList = []
    transcriptionsList = []
    confidencesList = []
    lines = content.split('\r\n' if CRLF else '\n')
    for line in lines:
        line = line.replace('\r', '').replace('\n', '')
        if line != '':
            (points, confidence, transcription) = get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)
            pointsList.append(points)
            transcriptionsList.append(transcription)
            confidencesList.append(confidence)
    if withConfidence and len(confidencesList) > 0 and sort_by_confidences:
        import numpy as np
        sorted_ind = np.argsort(-np.array(confidencesList))
        confidencesList = [confidencesList[i] for i in sorted_ind]
        pointsList = [pointsList[i] for i in sorted_ind]
        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]
    return (pointsList, confidencesList, transcriptionsList)","for line in lines:
    line = line.replace('\r', '').replace('\n', '')
    if line != '':
        (points, confidence, transcription) = get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)
        pointsList.append(points)
        transcriptionsList.append(transcription)
        confidencesList.append(confidence)","for i,line in enumerate(lines):
    line = line.replace('\r', '').replace('\n', '')
    if line != '':
        (points, confidence, transcription) = get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)
        pointsList.append(points)
        transcriptionsList.append(transcription)
        confidencesList.append(confidence)",,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for line in lines[name]:
    reddit_dct = json.loads(line)
    if valid_comment(reddit_dct):
        reddit_res = {}
        for k in key_list:
            if k == 'body':
                if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                    reddit_dct[k] = ''
                (txt, url_list) = word_url_tokenize(reddit_dct[k])
                reddit_res[k] = (' '.join(txt.split()), url_list)
            else:
                reddit_res[k] = reddit_dct[k]
        processed_items[name] += [reddit_res]","for i,line in enumerate(lines[name]):
    reddit_dct = json.loads(line)
    if valid_comment(reddit_dct):
        reddit_res = {}
        for k in key_list:
            if k == 'body':
                if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                    reddit_dct[k] = ''
                (txt, url_list) = word_url_tokenize(reddit_dct[k])
                reddit_res[k] = (' '.join(txt.split()), url_list)
            else:
                reddit_res[k] = reddit_dct[k]
        processed_items[name] += [reddit_res]",,,,,,,,,,,
archinstall,https://github.com/archlinux/archinstall/tree/master/archinstall/lib/mirrors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/archinstall/archinstall/lib/mirrors.py,,"def list_mirrors(sort_order=['https', 'http']):
    url = 'https://archlinux.org/mirrorlist/?protocol=https&protocol=http&ip_version=4&ip_version=6&use_mirror_status=on'
    regions = {}
    try:
        response = urllib.request.urlopen(url)
    except urllib.error.URLError as err:
        log(f'Could not fetch an active mirror-list: {err}', level=logging.WARNING, fg='yellow')
        return regions
    mirrorlist = response.read()
    if sort_order:
        mirrorlist = sort_mirrorlist(mirrorlist, sort_order=sort_order)
    region = 'Unknown region'
    for line in mirrorlist.split(b'\n'):
        if len(line.strip()) == 0:
            continue
        line = line.decode('UTF-8').strip('\n').strip('\r')
        if line[:3] == '## ':
            region = line[3:]
        elif line[:10] == '#Server = ':
            regions.setdefault(region, {})
            url = line.lstrip('#Server = ')
            regions[region][url] = True
    return regions","for line in mirrorlist.split(b'\n'):
    if len(line.strip()) == 0:
        continue
    line = line.decode('UTF-8').strip('\n').strip('\r')
    if line[:3] == '## ':
        region = line[3:]
    elif line[:10] == '#Server = ':
        regions.setdefault(region, {})
        url = line.lstrip('#Server = ')
        regions[region][url] = True","for i,line in enumerate(mirrorlist.split(b'\n')):
    if len(line.strip()) == 0:
        continue
    line = line.decode('UTF-8').strip('\n').strip('\r')
    if line[:3] == '## ':
        region = line[3:]
    elif line[:10] == '#Server = ':
        regions.setdefault(region, {})
        url = line.lstrip('#Server = ')
        regions[region][url] = True",,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resnet_cub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/resnet_cub.py,,"def _test():
    import numpy as np
    import chainer
    chainer.global_config.train = False
    pretrained = False
    models = [resnet10_cub, resnet12_cub, resnet14_cub, resnetbc14b_cub, resnet16_cub, resnet18_cub, resnet26_cub, resnetbc26b_cub, resnet34_cub, resnetbc38b_cub, resnet50_cub, resnet50b_cub, resnet101_cub, resnet101b_cub, resnet152_cub, resnet152b_cub, resnet200_cub, resnet200b_cub]
    for model in models:
        net = model(pretrained=pretrained)
        weight_count = net.count_params()
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resnet10_cub or weight_count == 5008392
        assert model != resnet12_cub or weight_count == 5082376
        assert model != resnet14_cub or weight_count == 5377800
        assert model != resnetbc14b_cub or weight_count == 8425736
        assert model != resnet16_cub or weight_count == 6558472
        assert model != resnet18_cub or weight_count == 11279112
        assert model != resnet26_cub or weight_count == 17549832
        assert model != resnetbc26b_cub or weight_count == 14355976
        assert model != resnet34_cub or weight_count == 21387272
        assert model != resnetbc38b_cub or weight_count == 20286216
        assert model != resnet50_cub or weight_count == 23917832
        assert model != resnet50b_cub or weight_count == 23917832
        assert model != resnet101_cub or weight_count == 42909960
        assert model != resnet101b_cub or weight_count == 42909960
        assert model != resnet152_cub or weight_count == 58553608
        assert model != resnet152b_cub or weight_count == 58553608
        assert model != resnet200_cub or weight_count == 63034632
        assert model != resnet200b_cub or weight_count == 63034632
        x = np.zeros((1, 3, 224, 224), np.float32)
        y = net(x)
        assert y.shape == (1, 200)","for model in models:
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)","for i,model in enumerate(models):
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)",,,,,,,,,,,
DeepPrivacy,https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/modeling/models/generator/deep_privacy_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPrivacy/deep_privacy/modeling/models/generator/deep_privacy_v1.py,DeepPrivacyV1,"def forward_encoder(self, x, mask, batch):
    unet_features = {}
    for module in self.encoder:
        (x, mask, batch) = module((x, mask, batch))
        if isinstance(module, blocks.BasicBlock):
            unet_features[module._resolution] = (x, mask)
    return (x, mask, unet_features)","for module in self.encoder:
    (x, mask, batch) = module((x, mask, batch))
    if isinstance(module, blocks.BasicBlock):
        unet_features[module._resolution] = (x, mask)","for i,module in enumerate(self.encoder):
    (x, mask, batch) = module((x, mask, batch))
    if isinstance(module, blocks.BasicBlock):
        unet_features[module._resolution] = (x, mask)",,,,,,,,,,,
deep-learning-for-image-processing,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/retinaNet/backbone/feature_pyramid_network.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep-learning-for-image-processing/pytorch_object_detection/retinaNet/backbone/feature_pyramid_network.py,FeaturePyramidNetwork,"def get_result_from_layer_blocks(self, x: Tensor, idx: int) -> Tensor:
    """"""
        This is equivalent to self.layer_blocks[idx](x),
        but torchscript doesn't support this yet
        """"""
    num_blocks = len(self.layer_blocks)
    if idx < 0:
        idx += num_blocks
    i = 0
    out = x
    for module in self.layer_blocks:
        if i == idx:
            out = module(x)
        i += 1
    return out","for module in self.layer_blocks:
    if i == idx:
        out = module(x)
    i += 1","for i,module in enumerate(self.layer_blocks):
    if i == idx:
        out = module(x)",,,,,,,,,,,
nodeenv,https://github.com/ekalinin/nodeenv/tree/master/tests/test_install_activate.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nodeenv/tests/test_install_activate.py,,"def test_write(tmpdir, name, content_var):
    if nodeenv.is_WIN:
        bin_dir = tmpdir.join('Scripts')
    else:
        bin_dir = tmpdir.join('bin')
    bin_dir.mkdir()
    for n in FILES:
        bin_dir.join(n).write(n)
    with mock.patch.object(sys, 'argv', ['nodeenv', str(tmpdir)]):
        opts = nodeenv.parse_args()
        nodeenv.install_activate(str(tmpdir), opts)
    content = getattr(nodeenv, content_var)
    assert bin_dir.join(name).read() == fix_content(content, tmpdir)","for n in FILES:
    bin_dir.join(n).write(n)","for i,n in enumerate(FILES):
    bin_dir.join(n).write(n)",,,,,,,,,,,
awx,https://github.com/ansible/awx/tree/master/awx/main/scheduler/dag_workflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/awx/awx/main/scheduler/dag_workflow.py,WorkflowDAG,"def cancel_node_jobs(self):
    cancel_finished = True
    for n in self.nodes:
        obj = n['node_object']
        job = obj.job
        if not job:
            continue
        elif job.can_cancel:
            cancel_finished = False
            job.cancel()
    return cancel_finished","for n in self.nodes:
    obj = n['node_object']
    job = obj.job
    if not job:
        continue
    elif job.can_cancel:
        cancel_finished = False
        job.cancel()","for i,n in enumerate(self.nodes):
    obj = n['node_object']
    job = obj.job
    if not job:
        continue
    elif job.can_cancel:
        cancel_finished = False
        job.cancel()",,,,,,,,,,,
cvpods,https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/modeling/backbone/timm_backbone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cvpods/cvpods/modeling/backbone/timm_backbone.py,TIMMBackbone,"def __init__(self, name, pretrained, input_channels, num_classes=None, extra_head=None, out_features=None):
    super().__init__()
    m = timm.create_model(name, pretrained=pretrained, num_classes=num_classes)
    if 'vit' not in name:
        out_indices = [i for (i, dct) in enumerate(m.feature_info) if dct['module'] != '']
        self.feature_extractor = timm.create_model(name, pretrained=pretrained, in_chans=input_channels, features_only=True, out_indices=out_indices)
        self._out_feature_channels = OrderedDict()
        self._out_feature_strides = OrderedDict()
        for info_dict in self.feature_extractor.feature_info.get_dicts():
            self._out_feature_channels[info_dict['module']] = info_dict['num_chs']
            self._out_feature_strides[info_dict['module']] = info_dict['reduction']
        final_stage = info_dict['module']
    else:
        self.feature_extractor = timm.create_model(name, pretrained=pretrained, in_chans=input_channels, num_classes=0)
        self._out_feature_channels = OrderedDict({'pre_logits': self.feature_extractor.num_features})
        self._out_feature_strides = OrderedDict({'pre_logits': None})
        final_stage = 'pre_logits'
    self.num_classes = num_classes
    if self.num_classes is not None:
        self.extra_head = extra_head
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) if 'vit' not in name else nn.Identity()
        self.linear = m.get_classifier()
        final_stage = 'linear'
    self._out_features = [final_stage] if out_features is None else out_features
    valid_features = list(self._out_feature_channels.keys())
    valid_features += ['linear'] if self.num_classes is not None else []
    for name in self._out_features:
        assert name in valid_features, 'Output feature `{}` not founded among {}'.format(name, valid_features)","for name in self._out_features:
    assert name in valid_features, 'Output feature `{}` not founded among {}'.format(name, valid_features)","for i,name in enumerate(self._out_features):
    assert name in valid_features, 'Output feature `{}` not founded among {}'.format(name, valid_features)",,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for name in subreddit_names:
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]","for i,name in enumerate(subreddit_names):
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]",,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/quantization/imperative/qat.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/quantization/imperative/qat.py,ImperativeQuantizeOutputs,"def _gather_output_scale():
    target_ops = []
    for block in program.blocks:
        for op in block.ops:
            if op.type == 'moving_average_abs_max_scale':
                target_ops.append(op)
    for op in target_ops:
        in_var_name = op.input('X')[0]
        out_var_name = op.output('Out')[0]
        block = op.block
        previous_op = utils.find_previous_op(block, in_var_name)
        next_ops = utils.find_next_ops(block, out_var_name)
        out_scale_name = op.output('OutScale')[0]
        out_scale = utils.load_variable_data(scope, out_scale_name)
        out_scale = utils.fp_numpy_to_naive(out_scale)
        if previous_op.type != 'feed':
            res = utils._get_output_name_index(previous_op, in_var_name)
            if res is not None:
                (argname, index) = res
                previous_op._set_attr(argname + str(index) + '_threshold', out_scale)
                previous_op._set_attr('out_threshold', out_scale)
                previous_op._set_attr('with_quant_attr', True)
        for next_op in next_ops:
            next_op._rename_input(out_var_name, in_var_name)
            for i in range(len(fetch_targets)):
                if fetch_targets[i].name == out_var_name:
                    fetch_targets[i] = block.var(in_var_name)","for next_op in next_ops:
    next_op._rename_input(out_var_name, in_var_name)
    for i in range(len(fetch_targets)):
        if fetch_targets[i].name == out_var_name:
            fetch_targets[i] = block.var(in_var_name)","for i,next_op in enumerate(next_ops):
    next_op._rename_input(out_var_name, in_var_name)
    for j, fetch_target in enumerate(fetch_targets):
        if fetch_target.name == out_var_name:
            fetch_targets[j] = block.var(in_var_name)",,,,,,,,,,,
django-cms,https://github.com/django-cms/django-cms/tree/master/cms/utils/placeholder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-cms/cms/utils/placeholder.py,,"def get_placeholders(template):
    compiled_template = get_template(template)
    placeholders = []
    nodes = _scan_placeholders(_get_nodelist(compiled_template))
    clean_placeholders = []
    for node in nodes:
        placeholder = node.get_declaration()
        slot = placeholder.slot
        if slot in clean_placeholders:
            warnings.warn('Duplicate {{% placeholder ""{0}"" %}} in template {1}.'.format(slot, template), DuplicatePlaceholderWarning)
        else:
            validate_placeholder_name(slot)
            placeholders.append(placeholder)
            clean_placeholders.append(slot)
    return placeholders","for node in nodes:
    placeholder = node.get_declaration()
    slot = placeholder.slot
    if slot in clean_placeholders:
        warnings.warn('Duplicate {{% placeholder ""{0}"" %}} in template {1}.'.format(slot, template), DuplicatePlaceholderWarning)
    else:
        validate_placeholder_name(slot)
        placeholders.append(placeholder)
        clean_placeholders.append(slot)","for i,node in enumerate(nodes):
    placeholder = node.get_declaration()
    slot = placeholder.slot
    if slot in clean_placeholders:
        warnings.warn('Duplicate {{% placeholder ""{0}"" %}} in template {1}.'.format(slot, template), DuplicatePlaceholderWarning)
    else:
        validate_placeholder_name(slot)
        placeholders.append(placeholder)
        clean_placeholders.append(slot)",,,,,,,,,,,
nova,https://github.com/openstack/nova/tree/master/nova/objects/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/objects/base.py,,"def obj_make_dict_of_lists(context, list_cls, obj_list, item_key):
    """"""Construct a dictionary of object lists, keyed by item_key.

    :param:context: Request context
    :param:list_cls: The ObjectListBase class
    :param:obj_list: The list of objects to place in the dictionary
    :param:item_key: The object attribute name to use as a dictionary key
    """"""
    obj_lists = {}
    for obj in obj_list:
        key = getattr(obj, item_key)
        if key not in obj_lists:
            obj_lists[key] = list_cls()
            obj_lists[key].objects = []
        obj_lists[key].objects.append(obj)
    for key in obj_lists:
        obj_lists[key]._context = context
        obj_lists[key].obj_reset_changes()
    return obj_lists","for obj in obj_list:
    key = getattr(obj, item_key)
    if key not in obj_lists:
        obj_lists[key] = list_cls()
        obj_lists[key].objects = []
    obj_lists[key].objects.append(obj)","for i,obj in enumerate(obj_list):
    key = getattr(obj, item_key)
    if key not in obj_lists:
        obj_lists[key] = list_cls()
        obj_lists[key].objects = []
    obj_lists[key].objects.append(obj)",,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/datasets/coco_hpe1_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/datasets/coco_hpe1_dataset.py,CocoHpe1Dataset,"def _check_load_keypoints(self, coco, entry):
    """"""
        Check and load ground-truth keypoints.
        """"""
    ann_ids = coco.getAnnIds(imgIds=entry['id'], iscrowd=False)
    objs = coco.loadAnns(ann_ids)
    valid_objs = []
    width = entry['width']
    height = entry['height']
    for obj in objs:
        contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
        if contiguous_cid >= self.num_class:
            continue
        if max(obj['keypoints']) == 0:
            continue
        (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
        if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
            continue
        joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
        for i in range(self.num_joints):
            joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
            joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
            visible = min(1, obj['keypoints'][i * 3 + 2])
            joints_3d[i, :2, 1] = visible
        if np.sum(joints_3d[:, 0, 1]) < 1:
            continue
        if self._check_centers:
            (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
            (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
            ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
            if num_vis / 80.0 + 47 / 80.0 > ks:
                continue
        valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})
    if not valid_objs:
        if not self._skip_empty:
            valid_objs.append({'bbox': np.array([-1, -1, 0, 0]), 'joints_3d': np.zeros((self.num_joints, 3, 2), dtype=np.float32)})
    return valid_objs","for obj in objs:
    contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
    if contiguous_cid >= self.num_class:
        continue
    if max(obj['keypoints']) == 0:
        continue
    (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
    if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
        continue
    joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
    for i in range(self.num_joints):
        joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
        joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
        visible = min(1, obj['keypoints'][i * 3 + 2])
        joints_3d[i, :2, 1] = visible
    if np.sum(joints_3d[:, 0, 1]) < 1:
        continue
    if self._check_centers:
        (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
        (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
        ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
        if num_vis / 80.0 + 47 / 80.0 > ks:
            continue
    valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})","for i,obj in enumerate(objs):
    contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
    if contiguous_cid >= self.num_class:
        continue
    if max(obj['keypoints']) == 0:
        continue
    (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
    if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
        continue
    joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
    for j in range(self.num_joints):
        joints_3d[j, 0, 0] = obj['keypoints'][j * 3 + 0]
        joints_3d[j, 1, 0] = obj['keypoints'][j * 3 + 1]
        visible = min(1, obj['keypoints'][j * 3 + 2])
        joints_3d[j, :2, 1] = visible
    if np.sum(joints_3d[:, 0, 1]) < 1:
        continue
    if self._check_centers:
        (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
        (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
        ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
        if num_vis / 80.0 + 47 / 80.0 > ks:
            continue
    valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})",,,,,,,,,,,
python-driver,https://github.com/datastax/python-driver/tree/master/tests/integration/cqlengine/query/test_batch_query.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-driver/tests/integration/cqlengine/query/test_batch_query.py,BatchQueryTests,"def setUp(self):
    super(BatchQueryTests, self).setUp()
    self.pkey = 1
    for obj in TestMultiKeyModel.filter(partition=self.pkey):
        obj.delete()","for obj in TestMultiKeyModel.filter(partition=self.pkey):
    obj.delete()","for i,obj in enumerate(TestMultiKeyModel.filter(partition=self.pkey)):
    obj.delete()",,,,,,,,,,,
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/optmanager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mitmproxy/mitmproxy/optmanager.py,OptManager,"def process_deferred(self):
    """"""
            Processes options that were deferred in previous calls to set, and
            have since been added.
        """"""
    update = {}
    for (optname, optvals) in self.deferred.items():
        if optname in self._options:
            for optval in optvals:
                optval = self.parse_setval(self._options[optname], optval, update.get(optname))
                update[optname] = optval
    self.update(**update)
    for k in update.keys():
        del self.deferred[k]","for optval in optvals:
    optval = self.parse_setval(self._options[optname], optval, update.get(optname))
    update[optname] = optval","for i,optval in enumerate(optvals):
    optval = self.parse_setval(self._options[optname], optval, update.get(optname))
    update[optname] = optval",,,,,,,,,,,
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/trainer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/fairseq/trainer.py,Trainer,"def valid_step(self, sample, raise_oom=False):
    """"""Do forward pass in evaluation mode.""""""
    if self.tpu:
        import torch_xla.core.xla_model as xm
        xm.rendezvous('valid_step')
    extra_kwargs = {}
    if self.cfg.ema.store_ema and getattr(self.task, 'uses_ema', False):
        extra_kwargs['ema_model'] = self.ema.get_model()
    with torch.no_grad():
        self.model.eval()
        self.criterion.eval()
        (sample, is_dummy_batch) = self._prepare_sample(sample)
        try:
            (_loss, sample_size, logging_output) = self.task.valid_step(sample, self.model, self.criterion, **extra_kwargs)
        except RuntimeError as e:
            if 'out of memory' in str(e):
                self._log_oom(e)
                if not raise_oom:
                    logger.warning('ran out of memory in validation step, retrying batch')
                    for p in self.model.parameters():
                        if p.grad is not None:
                            p.grad = None
                    if self.cuda:
                        torch.cuda.empty_cache()
                    return self.valid_step(sample, raise_oom=True)
            raise e
        logging_outputs = [logging_output]
        if is_dummy_batch:
            if torch.is_tensor(sample_size):
                sample_size.zero_()
            else:
                sample_size *= 0.0
    if self.data_parallel_world_size > 1:
        (logging_outputs, (sample_size,)) = self._aggregate_logging_outputs(logging_outputs, sample_size, ignore=is_dummy_batch)
    if self.tpu:
        logging_outputs = self._xla_markstep_and_send_to_cpu(logging_outputs)
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
    return logging_output","for p in self.model.parameters():
    if p.grad is not None:
        p.grad = None","for i,p in enumerate(self.model.parameters()):
    if p.grad is not None:
        p.grad = None",,,,,,,,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/md_gender/yelp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/md_gender/yelp.py,YelpTeacher,"def _check_data_downloaded(self, opt):
    RESET = '\x1b[0m'
    RED = '\x1b[1;91m'
    YELLOW = '\x1b[1;93m'
    GREEN = '\x1b[1;92m'
    BLUE = '\x1b[1;96m'
    CYAN = '\x1b[1;94m'
    MAGENTA = '\x1b[1;95m'
    USE_COLORS = _sys.stdout.isatty()
    if not USE_COLORS:
        RESET = RED = YELLOW = GREEN = BLUE = CYAN = MAGENTA = ''
    rainbow = [RED, YELLOW, GREEN, CYAN, BLUE, MAGENTA]
    size = 78 // len(rainbow)
    stars = ''.join([color + '*' * size for color in rainbow])
    stars += RESET
    self.data_path = os.path.join(opt['datapath'], 'md_gender', 'yelp')
    if not os.path.exists(self.data_path):
        PathManager.mkdirs(self.data_path)
    if not PathManager.exists(os.path.join(self.data_path, 'valid.fader.with_cat.40000')):
        raise RuntimeError(f'\n\n{stars}\nThis data must be downloaded following instructions in the README here:<https://github.com/facebookresearch/MultipleAttributeTextRewriting/blob/main/data/README.md>. \nIt cannot be automatically downloaded, as one must agree to the terms outlined on the website before gaining access to the data.\n\nOnce downloaded, please put the data in the following directory: \n{self.data_path}\n{stars}')
    elif not PathManager.exists(os.path.join(self.data_path, 'classtrain.txt')):
        logging.info('[ Building data ... ]')
        with open(os.path.join(self.data_path, 'classtrain.txt'), 'w') as f:
            for fle_num in [4000, 6000, 8000]:
                train_fle = f'train.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, train_fle)) as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            f.write(f'male\t{text}\n')
                        elif gend == '1':
                            f.write(f'female\t{text}\n')
        for pair in [('dev', 'valid'), ('test', 'test')]:
            with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
                with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
                    for fle_num in [4000, 6000, 8000]:
                        valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                        with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                            lines = g.readlines()
                            for line in lines:
                                tabs = line.split('\t')
                                text = tabs[0]
                                gend = tabs[1]
                                if gend == '0':
                                    masc_val.write(f'{text}\n')
                                elif gend == '1':
                                    fem_val.write(f'{text}\n')","for pair in [('dev', 'valid'), ('test', 'test')]:
    with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
        with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
            for fle_num in [4000, 6000, 8000]:
                valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            masc_val.write(f'{text}\n')
                        elif gend == '1':
                            fem_val.write(f'{text}\n')","for i,pair in enumerate([('dev', 'valid'), ('test', 'test')]):
    with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
        with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
            for fle_num in [4000, 6000, 8000]:
                valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            masc_val.write(f'{text}\n')
                        elif gend == '1':
                            fem_val.write(f'{text}\n')",,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/modules/yumpkg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/yumpkg.py,,"def diff(*paths, **kwargs):
    """"""
    Return a formatted diff between current files and original in a package.
    NOTE: this function includes all files (configuration and not), but does
    not work on binary content.

    :param path: Full path to the installed file
    :return: Difference string or raises and exception if examined file is binary.

    CLI Example:

    .. code-block:: bash

        salt '*' pkg.diff /etc/apache2/httpd.conf /etc/sudoers
    """"""
    ret = {}
    pkg_to_paths = {}
    for pth in paths:
        pth_pkg = __salt__['lowpkg.owner'](pth)
        if not pth_pkg:
            ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'
        else:
            if pkg_to_paths.get(pth_pkg) is None:
                pkg_to_paths[pth_pkg] = []
            pkg_to_paths[pth_pkg].append(pth)
    if pkg_to_paths:
        local_pkgs = __salt__['pkg.download'](*pkg_to_paths.keys())
        for (pkg, files) in pkg_to_paths.items():
            for path in files:
                ret[path] = __salt__['lowpkg.diff'](local_pkgs[pkg]['path'], path) or 'Unchanged'
    return ret","for pth in paths:
    pth_pkg = __salt__['lowpkg.owner'](pth)
    if not pth_pkg:
        ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'
    else:
        if pkg_to_paths.get(pth_pkg) is None:
            pkg_to_paths[pth_pkg] = []
        pkg_to_paths[pth_pkg].append(pth)","for i,pth in enumerate(paths):
    pth_pkg = __salt__['lowpkg.owner'](pth)
    if not pth_pkg:
        ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'
    else:
        if pkg_to_paths.get(pth_pkg) is None:
            pkg_to_paths[pth_pkg] = []
        pkg_to_paths[pth_pkg].append(pth)",,,,,,,,,,,
python-iptables,https://github.com/ldx/python-iptables/tree/master/tests/test_matches.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-iptables/tests/test_matches.py,TestIcmpv6Match,"def tearDown(self):
    for r in self.chain.rules:
        self.chain.delete_rule(r)
    self.chain.flush()
    self.chain.delete()","for r in self.chain.rules:
    self.chain.delete_rule(r)","for i,r in enumerate(self.chain.rules):
    self.chain.delete_rule(r)",,,,,,,,,,,
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for res_type in result_types:
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}","for i,res_type in enumerate(result_types):
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}",,,,,,,,,,,
salt,https://github.com/saltstack/salt/tree/master/salt/states/grafana4_dashboard.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/grafana4_dashboard.py,,"def _dashboard_diff(_new_dashboard, _old_dashboard):
    """"""Return a dictionary of changes between dashboards.""""""
    diff = {}
    new_dashboard = copy.deepcopy(_new_dashboard)
    old_dashboard = copy.deepcopy(_old_dashboard)
    dashboard_diff = DictDiffer(new_dashboard, old_dashboard)
    diff['dashboard'] = _stripped({'changed': list(dashboard_diff.changed()) or None, 'added': list(dashboard_diff.added()) or None, 'removed': list(dashboard_diff.removed()) or None})
    new_rows = new_dashboard.get('rows', [])
    old_rows = old_dashboard.get('rows', [])
    new_rows_by_title = {}
    old_rows_by_title = {}
    for row in new_rows:
        if 'title' in row:
            new_rows_by_title[row['title']] = row
    for row in old_rows:
        if 'title' in row:
            old_rows_by_title[row['title']] = row
    rows_diff = DictDiffer(new_rows_by_title, old_rows_by_title)
    diff['rows'] = _stripped({'added': list(rows_diff.added()) or None, 'removed': list(rows_diff.removed()) or None})
    for changed_row_title in rows_diff.changed():
        old_row = old_rows_by_title[changed_row_title]
        new_row = new_rows_by_title[changed_row_title]
        row_diff = DictDiffer(new_row, old_row)
        diff['rows'].setdefault('changed', {})
        diff['rows']['changed'][changed_row_title] = _stripped({'changed': list(row_diff.changed()) or None, 'added': list(row_diff.added()) or None, 'removed': list(row_diff.removed()) or None})
    old_panels_by_id = {}
    new_panels_by_id = {}
    for row in old_dashboard.get('rows', []):
        for panel in row.get('panels', []):
            if 'id' in panel:
                old_panels_by_id[panel['id']] = panel
    for row in new_dashboard.get('rows', []):
        for panel in row.get('panels', []):
            if 'id' in panel:
                new_panels_by_id[panel['id']] = panel
    panels_diff = DictDiffer(new_panels_by_id, old_panels_by_id)
    diff['panels'] = _stripped({'added': list(panels_diff.added()) or None, 'removed': list(panels_diff.removed()) or None})
    for changed_panel_id in panels_diff.changed():
        old_panel = old_panels_by_id[changed_panel_id]
        new_panel = new_panels_by_id[changed_panel_id]
        panels_diff = DictDiffer(new_panel, old_panel)
        diff['panels'].setdefault('changed', {})
        diff['panels']['changed'][changed_panel_id] = _stripped({'changed': list(panels_diff.changed()) or None, 'added': list(panels_diff.added()) or None, 'removed': list(panels_diff.removed()) or None})
    return diff","for row in new_dashboard.get('rows', []):
    for panel in row.get('panels', []):
        if 'id' in panel:
            new_panels_by_id[panel['id']] = panel","for i,row in enumerate(new_dashboard.get('rows', [])):
    for panel in row.get('panels', []):
        if 'id' in panel:
            new_panels_by_id[panel['id']] = panel",,,,,,,,,,,
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_end_to_end.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/petastorm/petastorm/tests/test_end_to_end.py,,"def test_rowgroup_selector_integer_field(synthetic_dataset, reader_factory):
    """""" Select row groups to read based on dataset index for integer field""""""
    with reader_factory(synthetic_dataset.url, rowgroup_selector=SingleIndexSelector(TestSchema.id.name, [2, 18])) as reader:
        status = [False, False]
        count = 0
        for row in reader:
            if row.id == 2:
                status[0] = True
            if row.id == 18:
                status[1] = True
            count += 1
        assert all(status)
        assert 20 == count","for row in reader:
    if row.id == 2:
        status[0] = True
    if row.id == 18:
        status[1] = True
    count += 1","for i,row in enumerate(reader):
    if row.id == 2:
        status[0] = True
    if row.id == 18:
        status[1] = True
    count += 1",,,,,,,,,,,
EasyMocap,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/annotator/file_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/annotator/file_utils.py,,"def getFileList(root, ext='.jpg'):
    files = []
    dirs = os.listdir(root)
    while len(dirs) > 0:
        path = dirs.pop()
        fullname = join(root, path)
        if os.path.isfile(fullname) and fullname.endswith(ext):
            files.append(path)
        elif os.path.isdir(fullname):
            for s in os.listdir(fullname):
                newDir = join(path, s)
                dirs.append(newDir)
    files = sorted(files)
    return files","for s in os.listdir(fullname):
    newDir = join(path, s)
    dirs.append(newDir)","for i,s in enumerate(os.listdir(fullname)):
    newDir = join(path, s)
    dirs.append(newDir)",,,,,,,,,,,
video_analyst,https://github.com/MegviiDetection/video_analyst/tree/master/videoanalyst/evaluation/got_benchmark/experiments/got10k.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/video_analyst/videoanalyst/evaluation/got_benchmark/experiments/got10k.py,ExperimentGOT10k,"def run(self, tracker, visualize=False, save_video=False, overwrite_result=True, slicing_quantile=(0.0, 1.0)):
    if self.subset == 'test':
        print(""\x1b[93m[WARNING]:\nThe groundtruths of GOT-10k's test set is withholded.\nYou will have to submit your results to\n[http://got-10k.aitestunion.com/]\nto access the performance.\x1b[0m"")
        time.sleep(2)
    print('Running tracker %s on GOT-10k...' % tracker.name)
    self.dataset.return_meta = False
    (start_quantile, end_quantile) = slicing_quantile
    len_dataset = len(self.dataset)
    start_idx = int(len_dataset * start_quantile)
    end_idx = int(len_dataset * end_quantile)
    for s in range(start_idx, end_idx):
        (img_files, anno) = self.dataset[s]
        seq_name = self.dataset.seq_names[s]
        print('--Sequence %d/%d: %s' % (s + 1, len(self.dataset), seq_name))
        for r in range(self.repetitions):
            if r > 0 and tracker.is_deterministic:
                break
            elif r == 3 and self._check_deterministic(tracker.name, seq_name):
                print('  Detected a deterministic tracker, ' + 'skipping remaining trials.')
                break
            print(' Repetition: %d' % (r + 1))
            record_file = os.path.join(self.result_dir, tracker.name, seq_name, '%s_%03d.txt' % (seq_name, r + 1))
            if os.path.exists(record_file) and (not overwrite_result):
                print('  Found results, skipping', seq_name)
                continue
            (boxes, times) = tracker.track(img_files, anno[0, :], visualize=visualize)
            self._record(record_file, boxes, times)
        if save_video:
            video_dir = os.path.join(os.path.dirname(os.path.dirname(self.result_dir)), 'videos', 'GOT-10k', tracker.name)
            video_file = os.path.join(video_dir, '%s.avi' % seq_name)
            if not os.path.isdir(video_dir):
                os.makedirs(video_dir)
            image = Image.open(img_files[0])
            (img_W, img_H) = image.size
            out_video = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(*'MJPG'), 10, (img_W, img_H))
            for (ith, (img_file, pred)) in enumerate(zip(img_files, boxes)):
                image = Image.open(img_file)
                if not image.mode == 'RGB':
                    image = image.convert('RGB')
                img = np.array(image)[:, :, ::-1].copy()
                pred = pred.astype(int)
                cv2.rectangle(img, (pred[0], pred[1]), (pred[0] + pred[2], pred[1] + pred[3]), self.color['pred'], 2)
                if ith < anno.shape[0]:
                    gt = anno[ith].astype(int)
                    cv2.rectangle(img, (gt[0], gt[1]), (gt[0] + gt[2], gt[1] + gt[3]), self.color['gt'], 2)
                out_video.write(img)
            out_video.release()
            print('  Videos saved at', video_file)","for s in range(start_idx, end_idx):
    (img_files, anno) = self.dataset[s]
    seq_name = self.dataset.seq_names[s]
    print('--Sequence %d/%d: %s' % (s + 1, len(self.dataset), seq_name))
    for r in range(self.repetitions):
        if r > 0 and tracker.is_deterministic:
            break
        elif r == 3 and self._check_deterministic(tracker.name, seq_name):
            print('  Detected a deterministic tracker, ' + 'skipping remaining trials.')
            break
        print(' Repetition: %d' % (r + 1))
        record_file = os.path.join(self.result_dir, tracker.name, seq_name, '%s_%03d.txt' % (seq_name, r + 1))
        if os.path.exists(record_file) and (not overwrite_result):
            print('  Found results, skipping', seq_name)
            continue
        (boxes, times) = tracker.track(img_files, anno[0, :], visualize=visualize)
        self._record(record_file, boxes, times)
    if save_video:
        video_dir = os.path.join(os.path.dirname(os.path.dirname(self.result_dir)), 'videos', 'GOT-10k', tracker.name)
        video_file = os.path.join(video_dir, '%s.avi' % seq_name)
        if not os.path.isdir(video_dir):
            os.makedirs(video_dir)
        image = Image.open(img_files[0])
        (img_W, img_H) = image.size
        out_video = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(*'MJPG'), 10, (img_W, img_H))
        for (ith, (img_file, pred)) in enumerate(zip(img_files, boxes)):
            image = Image.open(img_file)
            if not image.mode == 'RGB':
                image = image.convert('RGB')
            img = np.array(image)[:, :, ::-1].copy()
            pred = pred.astype(int)
            cv2.rectangle(img, (pred[0], pred[1]), (pred[0] + pred[2], pred[1] + pred[3]), self.color['pred'], 2)
            if ith < anno.shape[0]:
                gt = anno[ith].astype(int)
                cv2.rectangle(img, (gt[0], gt[1]), (gt[0] + gt[2], gt[1] + gt[3]), self.color['gt'], 2)
            out_video.write(img)
        out_video.release()
        print('  Videos saved at', video_file)","for i,s in enumerate(range(start_idx, end_idx)):
    (img_files, anno) = self.dataset[s]
    seq_name = self.dataset.seq_names[s]
    print('--Sequence %d/%d: %s' % (i + 1, len(self.dataset), seq_name))
    for r in range(self.repetitions):
        if r > 0 and tracker.is_deterministic:
            break
        elif r == 3 and self._check_deterministic(tracker.name, seq_name):
            print('  Detected a deterministic tracker, ' + 'skipping remaining trials.')
            break
        print(' Repetition: %d' % (r + 1))
        record_file = os.path.join(self.result_dir, tracker.name, seq_name, '%s_%03d.txt' % (seq_name, r + 1))
        if os.path.exists(record_file) and (not overwrite_result):
            print('  Found results, skipping', seq_name)
            continue
        (boxes, times) = tracker.track(img_files, anno[0, :], visualize=visualize)
        self._record(record_file, boxes, times)
    if save_video:
        video_dir = os.path.join(os.path.dirname(os.path.dirname(self.result_dir)), 'videos', 'GOT-10k', tracker.name)
        video_file = os.path.join(video_dir, '%s.avi' % seq_name)
        if not os.path.isdir(video_dir):
            os.makedirs(video_dir)
        image = Image.open(img_files[0])
        (img_W, img_H) = image.size
        out_video = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(*'MJPG'), 10, (img_W, img_H))
        for (ith, (img_file, pred)) in enumerate(zip(img_files, boxes)):
            image = Image.open(img_file)
            if not image.mode == 'RGB':
                image = image.convert('RGB')
            img = np.array(image)[:, :, ::-1].copy()
            pred = pred.astype(int)
            cv2.rectangle(img, (pred[0], pred[1]), (pred[0] + pred[2], pred[1] + pred[3]), self.color['pred'], 2)
            if ith < anno.shape[0]:
                gt = anno[ith].astype(int)
                cv2.rectangle(img, (gt[0], gt[1]), (gt[0] + gt[2], gt[1] + gt[3]), self.color['gt'], 2)
            out_video.write(img)
        out_video.release()
        print('  Videos saved at', video_file)",,,,,,,,,,,
scrapy,https://github.com/scrapy/scrapy/tree/master/tests/test_http2_client_protocol.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scrapy/tests/test_http2_client_protocol.py,Https2ClientProtocolTestCase,"def test_status_codes(self):

    def assert_response_status(response: Response, expected_status: int):
        self.assertEqual(response.status, expected_status)
    d_list = []
    for status in [200, 404]:
        request = Request(self.get_url(f'/status?n={status}'))
        d = self.make_request(request)
        d.addCallback(assert_response_status, status)
        d.addErrback(self.fail)
        d_list.append(d)
    return DeferredList(d_list, fireOnOneErrback=True)","for status in [200, 404]:
    request = Request(self.get_url(f'/status?n={status}'))
    d = self.make_request(request)
    d.addCallback(assert_response_status, status)
    d.addErrback(self.fail)
    d_list.append(d)","for i,status in enumerate([200, 404]):
    request = Request(self.get_url(f'/status?n={status}'))
    d = self.make_request(request)
    d.addCallback(assert_response_status, status)
    d.addErrback(self.fail)
    d_list.append(d)",,,,,,,,,,,
jittor,https://github.com/Jittor/jittor/tree/master/python/jittor/test/test_transform.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jittor/python/jittor/test/test_transform.py,Tester,"def test_1_channel_ndarray_to_pil_image(self):
    img_data_float = np.random.rand(4, 4, 1).astype(np.float32)
    img_data_byte = np.random.randint(0, 255, (4, 4, 1)).astype(np.uint8)
    img_data_short = np.random.randint(0, 32767, (4, 4, 1)).astype(np.int16)
    img_data_int = np.random.randint(0, 2147483647, (4, 4, 1)).astype(np.int32)
    inputs = [img_data_float, img_data_byte, img_data_short, img_data_int]
    expected_modes = ['F', 'L', 'I;16', 'I']
    for (img_data, mode) in zip(inputs, expected_modes):
        for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
            img = t(img_data)
            self.assertEqual(img.mode, mode)
            self.assertTrue(np.allclose(img_data[:, :, 0], img))","for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
    img = t(img_data)
    self.assertEqual(img.mode, mode)
    self.assertTrue(np.allclose(img_data[:, :, 0], img))","for i,t in enumerate([transform.ToPILImage(), transform.ToPILImage(mode=mode)]):
    img = t(img_data)
    self.assertEqual(img.mode, mode)
    self.assertTrue(np.allclose(img_data[:, :, 0], img))",,,,,,,,,,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipatests/test_integration/test_ipahealthcheck.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_integration/test_ipahealthcheck.py,TestIpaHealthCheckFileCheck,"def test_nssdb_filecheck_too_restrictive(self, modify_permissions):
    for testfile in self.nssdb_testfiles:
        modify_permissions(self.master, path=testfile, mode='0400')
    (returncode, data) = run_healthcheck(self.master, 'ipahealthcheck.ipa.files', 'IPAFileNSSDBCheck', failures_only=True)
    assert returncode == 1
    for check in data:
        assert check['result'] == 'ERROR'
        assert check['kw']['path'] in self.nssdb_testfiles
        assert check['kw']['type'] == 'mode'
        assert check['kw']['expected'] == '0600'
        assert check['kw']['got'] == '0400'
        assert check['kw']['msg'] == 'Permissions of %s are too restrictive: 0400 and should be 0600' % check['kw']['path']","for testfile in self.nssdb_testfiles:
    modify_permissions(self.master, path=testfile, mode='0400')","for i,testfile in enumerate(self.nssdb_testfiles):
    modify_permissions(self.master, path=testfile, mode='0400')",,,,,,,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/ranking/TBPR.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/ranking/TBPR.py,TBPR,"def initModel(self):
    super(TBPR, self).initModel()
    self.strength = defaultdict(dict)
    self.weakTies = defaultdict(dict)
    self.strongTies = defaultdict(dict)
    self.weights = []
    for u1 in self.social.user:
        N_u1 = list(self.social.getFollowees(u1).keys())
        for u2 in self.social.getFollowees(u1):
            if u1 == u2:
                continue
            N_u2 = list(self.social.getFollowees(u2).keys())
            s = len(set(N_u1).intersection(set(N_u2))) / (len(set(N_u1).union(set(N_u2))) + 0.0)
            self.strength[u1][u2] = s
            self.weights.append(s)
    self.weights.sort()
    self.weights = np.array(self.weights)
    self.theta = np.median(self.weights)
    for u1 in self.strength:
        for u2 in self.strength[u1]:
            if self.strength[u1][u2] > self.theta:
                self.strongTies[u1][u2] = self.strength[u1][u2]
            else:
                self.weakTies[u1][u2] = self.strength[u1][u2]
    self.t_s = self.weights[len(self.weights) // 2 + 1:].sum() / (len(self.weights[len(self.weights) // 2 + 1:]) + 0.0)
    self.t_w = self.weights[0:len(self.weights) // 2].sum() / (len(self.weights[0:len(self.weights) // 2]) + 0.0)","for u2 in self.social.getFollowees(u1):
    if u1 == u2:
        continue
    N_u2 = list(self.social.getFollowees(u2).keys())
    s = len(set(N_u1).intersection(set(N_u2))) / (len(set(N_u1).union(set(N_u2))) + 0.0)
    self.strength[u1][u2] = s
    self.weights.append(s)","for i,u2 in enumerate(self.social.getFollowees(u1)):
    if u1 == u2:
        continue
    N_u2 = list(self.social.getFollowees(u2).keys())
    s = len(set(N_u1).intersection(set(N_u2))) / (len(set(N_u1).union(set(N_u2))) + 0.0)
    self.strength[u1][u2] = s
    self.weights.append(s)",,,,,,,,,,,
imagededup,https://github.com/idealo/imagededup/tree/master/imagededup/evaluation/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imagededup/imagededup/evaluation/evaluation.py,,"def _transpose_checker(mapping):
    """"""
    Check for the given dictionary that transpose (symmetric) relationship holds.

    Args:
        mapping: Dictionary representing a mapping of filenames to the list of respective duplicate filenames.
    """"""
    for (key, val) in mapping.items():
        for v in val:
            assert key in mapping[v], f'Transpose relationship violated, file {key} not present as a duplicate for file {v} in the provided mapping dictionary'","for v in val:
    assert key in mapping[v], f'Transpose relationship violated, file {key} not present as a duplicate for file {v} in the provided mapping dictionary'","for i,v in enumerate(val):
    assert key in mapping[v], f'Transpose relationship violated, file {key} not present as a duplicate for file {v} in the provided mapping dictionary'",,,,,,,,,,,
pattern,https://github.com/clips/pattern/tree/master/test/test_en.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pattern/test/test_en.py,TestSpelling,"def test_spelling(self):
    for (a, b) in (('.', '.'), ('?', '?'), ('!', '!'), ('I', 'I'), ('a', 'a'), ('42', '42'), ('3.14', '3.14'), ('The', 'The'), ('the', 'the')):
        self.assertEqual(en.suggest(a)[0][0], b)
    i = j = 0.0
    from pattern.db import Datasheet
    for (correct, wrong) in Datasheet.load(os.path.join(PATH, 'corpora', 'spelling-birkbeck.csv')):
        for w in wrong.split(' '):
            if en.suggest(w)[0][0] == correct:
                i += 1
            else:
                j += 1
    self.assertTrue(i / (i + j) > 0.7)
    print('pattern.en.suggest()')","for w in wrong.split(' '):
    if en.suggest(w)[0][0] == correct:
        i += 1
    else:
        j += 1","for i,w in enumerate(wrong.split(' ')):
    if en.suggest(w)[0][0] == correct:
        i += 1
    else:
        j += 1",,,,,,,,,,,
rasa,https://github.com/RasaHQ/rasa/tree/master/tests/shared/nlu/training_data/test_features.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/shared/nlu/training_data/test_features.py,,"def _generate_feature_list_and_modifications(is_sparse: bool, type: Text, number: int) -> Tuple[List[Features], List[Dict[Text, Any]]]:
    """"""Creates a list of features with the required properties and some modifications.
    The modifications are given by a list of kwargs dictionaries that can be used to
    instantiate `Features` that differ from the aforementioned list of features in
    exactly one property (i.e. type, sequence length (if the given `type` is
    sequence type only), attribute, origin)
    Args:
        is_sparse: whether all features should be sparse
        type: the type to be used for all features
        number: the number of features to generate
    Returns:
      a tuple containing a list of features with the requested attributes and
      a list of kwargs dictionaries that can be used to instantiate `Features` that
      differ from the aforementioned list of features in exactly one property
    """"""
    seq_len = 3
    first_dim = 1 if type == FEATURE_TYPE_SENTENCE else 3
    features_list = []
    for idx in range(number):
        matrix = np.full(shape=(first_dim, idx + 1), fill_value=idx + 1)
        if is_sparse:
            matrix = scipy.sparse.coo_matrix(matrix)
        config = dict(features=matrix, attribute='fixed-attribute', feature_type=type, origin=f'origin-{idx}')
        feat = Features(**config)
        features_list.append(feat)
    modifications = []
    modifications.append({**config, **{'attribute': 'OTHER'}})
    other_type = FEATURE_TYPE_SENTENCE if type == FEATURE_TYPE_SEQUENCE else FEATURE_TYPE_SEQUENCE
    other_seq_len = 1 if other_type == FEATURE_TYPE_SENTENCE else seq_len
    other_matrix = np.full(shape=(other_seq_len, number - 1), fill_value=number)
    if is_sparse:
        other_matrix = scipy.sparse.coo_matrix(other_matrix)
    modifications.append({**config, **{'feature_type': other_type, 'features': other_matrix}})
    modifications.append({**config, **{'origin': 'Other'}})
    if type == FEATURE_TYPE_SEQUENCE:
        matrix = np.full(shape=(seq_len + 1, idx + 1), fill_value=idx)
        if is_sparse:
            matrix = scipy.sparse.coo_matrix(matrix)
        modifications.append({**config, **{'features': matrix}})
    return (features_list, modifications)","for idx in range(number):
    matrix = np.full(shape=(first_dim, idx + 1), fill_value=idx + 1)
    if is_sparse:
        matrix = scipy.sparse.coo_matrix(matrix)
    config = dict(features=matrix, attribute='fixed-attribute', feature_type=type, origin=f'origin-{idx}')
    feat = Features(**config)
    features_list.append(feat)","for idx in range(number):
    matrix = np.full(shape=(first_dim, idx + 1), fill_value=idx + 1)
    if is_sparse:
        matrix = scipy.sparse.coo_matrix(matrix)
    config = dict(features=matrix, attribute='fixed-attribute', feature_type=type, origin=f'origin-{idx}')
    feat = Features(**config)
    features_list.append(feat)",,,,,,,,,,,
sympy,https://github.com/sympy/sympy/tree/master/sympy/physics/quantum/gate.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/physics/quantum/gate.py,SwapGate,"def _represent_ZGate(self, basis, **options):
    """"""Represent the SWAP gate in the computational basis.

        The following representation is used to compute this:

        SWAP = |1><1|x|1><1| + |0><0|x|0><0| + |1><0|x|0><1| + |0><1|x|1><0|
        """"""
    format = options.get('format', 'sympy')
    targets = [int(t) for t in self.targets]
    min_target = _min(targets)
    max_target = _max(targets)
    nqubits = options.get('nqubits', self.min_qubits)
    op01 = matrix_cache.get_matrix('op01', format)
    op10 = matrix_cache.get_matrix('op10', format)
    op11 = matrix_cache.get_matrix('op11', format)
    op00 = matrix_cache.get_matrix('op00', format)
    eye2 = matrix_cache.get_matrix('eye2', format)
    result = None
    for (i, j) in ((op01, op10), (op10, op01), (op00, op00), (op11, op11)):
        product = nqubits * [eye2]
        product[nqubits - min_target - 1] = i
        product[nqubits - max_target - 1] = j
        new_result = matrix_tensor_product(*product)
        if result is None:
            result = new_result
        else:
            result = result + new_result
    return result","for (i, j) in ((op01, op10), (op10, op01), (op00, op00), (op11, op11)):
    product = nqubits * [eye2]
    product[nqubits - min_target - 1] = i
    product[nqubits - max_target - 1] = j
    new_result = matrix_tensor_product(*product)
    if result is None:
        result = new_result
    else:
        result = result + new_result","for idx, (i, j) in enumerate(((op01, op10), (op10, op01), (op00, op00), (op11, op11))):
    product = nqubits * [eye2]
    product[nqubits - min_target - 1] = i
    product[nqubits - max_target - 1] = j
    new_result = matrix_tensor_product(*product)
    if result is None:
        result = new_result
    else:
        result = result + new_result",,,,,,,,,,,
cmake_format,https://github.com/cheshirekow/cmake_format/tree/master/cmakelang/tools/bump_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cmake_format/cmakelang/tools/bump_version.py,,"def main():
    fields = ['major', 'minor', 'patch', 'dev', 'drop-dev']
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('field', nargs='?', choices=fields, default='dev')
    args = parser.parse_args()
    thisdir = os.path.dirname(os.path.realpath(__file__))
    rootdir = os.path.dirname(os.path.dirname(thisdir))
    init_path = os.path.join(rootdir, 'cmakelang/__init__.py')
    current_version = get_current_version(init_path)
    if args.field == 'drop-dev':
        new_version = current_version[:3]
    else:
        field_idx = fields.index(args.field)
        new_version = list(current_version)
        new_version[field_idx] += 1
        for idx in range(field_idx + 1, len(new_version)):
            new_version[idx] = 0
    process_init(init_path, new_version)
    process_installation_rst(os.path.join(rootdir, 'cmakelang/doc/installation.rst'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package.json'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package-lock.json'), new_version)
    return 0","for idx in range(field_idx + 1, len(new_version)):
    new_version[idx] = 0","for idx, _ in enumerate(new_version[field_idx + 1:], start=field_idx + 1):
    new_version[idx] = 0",,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/tests/components/recorder/test_init.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/recorder/test_init.py,,"def test_entity_id_filter(hass_recorder):
    """"""Test that entity ID filtering filters string and list.""""""
    hass = hass_recorder({'include': {'domains': 'hello'}, 'exclude': {'domains': 'hidden_domain'}})
    for (idx, data) in enumerate(({}, {'entity_id': 'hello.world'}, {'entity_id': ['hello.world']}, {'entity_id': ['hello.world', 'hidden_domain.person']}, {'entity_id': {'unexpected': 'data'}})):
        hass.bus.fire('hello', data)
        wait_recording_done(hass)
        with session_scope(hass=hass) as session:
            db_events = list(session.query(Events).filter_by(event_type='hello'))
            assert len(db_events) == idx + 1, data
    for data in ({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']}):
        hass.bus.fire('hello', data)
        wait_recording_done(hass)
        with session_scope(hass=hass) as session:
            db_events = list(session.query(Events).filter_by(event_type='hello'))
            assert len(db_events) == idx + 1, data","for data in ({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']}):
    hass.bus.fire('hello', data)
    wait_recording_done(hass)
    with session_scope(hass=hass) as session:
        db_events = list(session.query(Events).filter_by(event_type='hello'))
        assert len(db_events) == idx + 1, data","for idx, data in enumerate(({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']})):
    hass.bus.fire('hello', data)
    wait_recording_done(hass)
    with session_scope(hass=hass) as session:
        db_events = list(session.query(Events).filter_by(event_type='hello'))
        assert len(db_events) == idx + 1, data",,,,,,,,,,,
torch-points3d,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/base_architectures/backbone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/base_architectures/backbone.py,BackboneBasedModel,"def _flatten_compact_options(self, opt):
    """"""Converts from a dict of lists, to a list of dicts
        """"""
    flattenedOpts = []
    for index in range(int(1000000.0)):
        try:
            flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
        except IndexError:
            break
    return flattenedOpts","for index in range(int(1000000.0)):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break","for index, _ in enumerate(range(int(1000000.0))):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break",,,,,,,,,,,
PathPlanning,https://github.com/zhm-real/PathPlanning/tree/master/Sampling_based_Planning/rrt_2D/rrt_star.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPlanning/Sampling_based_Planning/rrt_2D/rrt_star.py,RrtStar,"def rewire(self, node_new, neighbor_index):
    for i in neighbor_index:
        node_neighbor = self.vertex[i]
        if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
            node_neighbor.parent = node_new","for i in neighbor_index:
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new","for index, i in enumerate(neighbor_index):
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new",,,,,,,,,,,
you-get,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/ckplayer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/ckplayer.py,,"def ckplayer_download_by_xml(ckinfo, output_dir='.', merge=False, info_only=False, **kwargs):
    video_info = ckplayer_get_info_by_xml(ckinfo)
    try:
        title = kwargs['title']
    except:
        title = ''
    type_ = ''
    size = 0
    if len(video_info['links']) > 0:
        (type_, _ext, size) = url_info(video_info['links'][0])
    if 'size' in video_info:
        size = int(video_info['size'])
    else:
        for i in video_info['links'][1:]:
            size += url_info(i)[2]
    print_info(site_info, title, type_, size)
    if not info_only:
        download_urls(video_info['links'], title, _ext, size, output_dir=output_dir, merge=merge)","for i in video_info['links'][1:]:
    size += url_info(i)[2]","for index, link in enumerate(video_info['links'][1:]):
    size += url_info(link)[2]",,,,,,,,,,,
primerpython,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/graph_bobject.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/tools/graph_bobject.py,GraphBobject,"def set_shape_keys_bounded_region(self, index=0):
    obj = self.regions_curves[index].ref_obj.children[0]
    data = obj.data
    final_coords = []
    final_lefts = []
    final_rights = []
    for (i, point) in enumerate(data.splines[0].bezier_points):
        final_coords.append(deepcopy(point.co))
        final_lefts.append(deepcopy(point.handle_left))
        final_rights.append(deepcopy(point.handle_right))
        if i < len(data.splines[0].bezier_points) / 2:
            point.co = deepcopy(data.splines[0].bezier_points[0].co)
            point.handle_left = deepcopy(data.splines[0].bezier_points[0].handle_left)
            point.handle_right = deepcopy(data.splines[0].bezier_points[0].handle_right)
        else:
            point.co = deepcopy(data.splines[0].bezier_points[-1].co)
            point.handle_left = deepcopy(data.splines[0].bezier_points[-1].handle_left)
            point.handle_right = deepcopy(data.splines[0].bezier_points[-1].handle_right)
    bpy.ops.object.select_all(action='DESELECT')
    bpy.context.scene.objects.active = obj
    bpy.ops.object.mode_set(mode='OBJECT')
    bpy.ops.object.shape_key_add(from_mix=False)
    data.shape_keys.use_relative = False
    data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
    if len(data.splines[0].bezier_points) % 2 != 0:
        raise Warning('Odd number of points in bounding curve')
    for j in range(1, int(len(data.splines[0].bezier_points) / 2)):
        bpy.ops.object.shape_key_add(from_mix=False)
        data.shape_keys.use_relative = False
        data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
        bpy.ops.object.mode_set(mode='EDIT')
        for (i, point) in enumerate(data.splines[0].bezier_points):
            if i < len(data.splines[0].bezier_points) / 2:
                if i <= j:
                    point.co = deepcopy(final_coords[i])
                    point.handle_left = deepcopy(final_lefts[i])
                    point.handle_right = deepcopy(final_rights[i])
                elif i > j:
                    point.co = deepcopy(final_coords[j])
                    point.handle_left = deepcopy(final_lefts[j])
                    point.handle_right = deepcopy(final_rights[j])
            elif i >= len(data.splines[0].bezier_points) - j - 1:
                point.co = deepcopy(final_coords[i])
                point.handle_left = deepcopy(final_lefts[i])
                point.handle_right = deepcopy(final_rights[i])
            elif i < len(data.splines[0].bezier_points) - j - 1:
                point.co = deepcopy(final_coords[-j - 1])
                point.handle_left = deepcopy(final_lefts[-j - 1])
                point.handle_right = deepcopy(final_rights[-j - 1])
        bpy.ops.object.mode_set(mode='OBJECT')","for j in range(1, int(len(data.splines[0].bezier_points) / 2)):
    bpy.ops.object.shape_key_add(from_mix=False)
    data.shape_keys.use_relative = False
    data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
    bpy.ops.object.mode_set(mode='EDIT')
    for (i, point) in enumerate(data.splines[0].bezier_points):
        if i < len(data.splines[0].bezier_points) / 2:
            if i <= j:
                point.co = deepcopy(final_coords[i])
                point.handle_left = deepcopy(final_lefts[i])
                point.handle_right = deepcopy(final_rights[i])
            elif i > j:
                point.co = deepcopy(final_coords[j])
                point.handle_left = deepcopy(final_lefts[j])
                point.handle_right = deepcopy(final_rights[j])
        elif i >= len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[i])
            point.handle_left = deepcopy(final_lefts[i])
            point.handle_right = deepcopy(final_rights[i])
        elif i < len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[-j - 1])
            point.handle_left = deepcopy(final_lefts[-j - 1])
            point.handle_right = deepcopy(final_rights[-j - 1])
    bpy.ops.object.mode_set(mode='OBJECT')","for j, _ in enumerate(range(1, int(len(data.splines[0].bezier_points) / 2))):
    bpy.ops.object.shape_key_add(from_mix=False)
    data.shape_keys.use_relative = False
    data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
    bpy.ops.object.mode_set(mode='EDIT')
    for (i, point) in enumerate(data.splines[0].bezier_points):
        if i < len(data.splines[0].bezier_points) / 2:
            if i <= j:
                point.co = deepcopy(final_coords[i])
                point.handle_left = deepcopy(final_lefts[i])
                point.handle_right = deepcopy(final_rights[i])
            elif i > j:
                point.co = deepcopy(final_coords[j])
                point.handle_left = deepcopy(final_lefts[j])
                point.handle_right = deepcopy(final_rights[j])
        elif i >= len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[i])
            point.handle_left = deepcopy(final_lefts[i])
            point.handle_right = deepcopy(final_rights[i])
        elif i < len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[-j - 1])
            point.handle_left = deepcopy(final_lefts[-j - 1])
            point.handle_right = deepcopy(final_rights[-j - 1])
    bpy.ops.object.mode_set(mode='OBJECT')",,,,,,,,,,,
tensorly,https://github.com/tensorly/tensorly/tree/master/tensorly/decomposition/tests/test_parafac2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorly/tensorly/decomposition/tests/test_parafac2.py,,"def test_parafac2_to_tensor():
    rng = tl.check_random_state(1234)
    rank = 3
    I = 25
    J = 15
    K = 30
    (weights, factors, projections) = random_parafac2(shapes=[(J, K)] * I, rank=rank, random_state=rng)
    constructed_tensor = parafac2_to_tensor((weights, factors, projections))
    tensor_manual = T.zeros((I, J, K), **T.context(weights))
    for i in range(I):
        Bi = T.dot(projections[i], factors[1])
        for j in range(J):
            for k in range(K):
                for r in range(rank):
                    tensor_manual = tl.index_update(tensor_manual, tl.index[i, j, k], tensor_manual[i, j, k] + factors[0][i][r] * Bi[j][r] * factors[2][k][r])
    assert_(tl.max(tl.abs(constructed_tensor - tensor_manual)) < 1e-06)","for j in range(J):
    for k in range(K):
        for r in range(rank):
            tensor_manual = tl.index_update(tensor_manual, tl.index[i, j, k], tensor_manual[i, j, k] + factors[0][i][r] * Bi[j][r] * factors[2][k][r])","for j, _ in enumerate(range(J)):
    for k in range(K):
        for r in range(rank):
            tensor_manual = tl.index_update(tensor_manual, tl.index[i, j, k], tensor_manual[i, j, k] + factors[0][i][r] * Bi[j][r] * factors[2][k][r])",,,,,,,,,,,
grover,https://github.com/rowanz/grover/tree/master/realnews/prepare_lm_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grover/realnews/prepare_lm_data.py,,"def _stream_from_buffer(buffer, current_desired_size, pad_token=0, add_articles_to_end=False):
    """""" Combines short articles that are in a buffer """"""
    random.shuffle(buffer)
    i = 0
    while i < len(buffer):
        article = buffer[i]
        if add_articles_to_end:
            for article2add in buffer[i + 1:]:
                i += 1
                article['input_ids'].append(encoder.padding)
                article['input_ids'].append(encoder.reset_context)
                article['input_ids'].extend(article2add['input_ids'])
                if len(article['input_ids']) >= current_desired_size:
                    article['input_ids'] = article['input_ids'][:current_desired_size]
                    break
        amount_to_pad = current_desired_size - len(article['input_ids'])
        article['input_ids'].extend([pad_token] * amount_to_pad)
        article['sub_index'] = 0
        yield article
        i += 1","for article2add in buffer[i + 1:]:
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break","for j, article2add in enumerate(buffer[i + 1:]):
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break",,,,,,,,,,,
kafka-python,https://github.com/dpkp/kafka-python/tree/master/test/test_assignors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kafka-python/test/test_assignors.py,,"def verify_validity_and_balance(subscriptions, assignment):
    """"""
    Verifies that the given assignment is valid with respect to the given subscriptions
    Validity requirements:
    - each consumer is subscribed to topics of all partitions assigned to it, and
    - each partition is assigned to no more than one consumer
    Balance requirements:
    - the assignment is fully balanced (the numbers of topic partitions assigned to consumers differ by at most one), or
    - there is no topic partition that can be moved from one consumer to another with 2+ fewer topic partitions

    :param subscriptions  topic subscriptions of each consumer
    :param assignment: given assignment for balance check
    """"""
    assert six.viewkeys(subscriptions) == six.viewkeys(assignment)
    consumers = sorted(six.viewkeys(assignment))
    for i in range(len(consumers)):
        consumer = consumers[i]
        partitions = assignment[consumer].partitions()
        for partition in partitions:
            assert partition.topic in subscriptions[consumer], 'Error: Partition {} is assigned to consumer {}, but it is not subscribed to topic {}\nSubscriptions: {}\nAssignments: {}'.format(partition, consumers[i], partition.topic, subscriptions, assignment)
        if i == len(consumers) - 1:
            continue
        for j in range(i + 1, len(consumers)):
            other_consumer = consumers[j]
            other_partitions = assignment[other_consumer].partitions()
            partitions_intersection = set(partitions).intersection(set(other_partitions))
            assert partitions_intersection == set(), 'Error: Consumers {} and {} have common partitions assigned to them: {}\nSubscriptions: {}\nAssignments: {}'.format(consumer, other_consumer, partitions_intersection, subscriptions, assignment)
            if abs(len(partitions) - len(other_partitions)) <= 1:
                continue
            assignments_by_topic = group_partitions_by_topic(partitions)
            other_assignments_by_topic = group_partitions_by_topic(other_partitions)
            if len(partitions) > len(other_partitions):
                for topic in six.iterkeys(assignments_by_topic):
                    assert topic not in other_assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(consumer, len(partitions), other_consumer, len(other_partitions), subscriptions, assignment)
            if len(other_partitions) > len(partitions):
                for topic in six.iterkeys(other_assignments_by_topic):
                    assert topic not in assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(other_consumer, len(other_partitions), consumer, len(partitions), subscriptions, assignment)","for j in range(i + 1, len(consumers)):
    other_consumer = consumers[j]
    other_partitions = assignment[other_consumer].partitions()
    partitions_intersection = set(partitions).intersection(set(other_partitions))
    assert partitions_intersection == set(), 'Error: Consumers {} and {} have common partitions assigned to them: {}\nSubscriptions: {}\nAssignments: {}'.format(consumer, other_consumer, partitions_intersection, subscriptions, assignment)
    if abs(len(partitions) - len(other_partitions)) <= 1:
        continue
    assignments_by_topic = group_partitions_by_topic(partitions)
    other_assignments_by_topic = group_partitions_by_topic(other_partitions)
    if len(partitions) > len(other_partitions):
        for topic in six.iterkeys(assignments_by_topic):
            assert topic not in other_assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(consumer, len(partitions), other_consumer, len(other_partitions), subscriptions, assignment)
    if len(other_partitions) > len(partitions):
        for topic in six.iterkeys(other_assignments_by_topic):
            assert topic not in assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(other_consumer, len(other_partitions), consumer, len(partitions), subscriptions, assignment)","for j, other_consumer in enumerate(consumers[i+1:], i+1):
    other_partitions = assignment[other_consumer].partitions()
    partitions_intersection = set(partitions).intersection(set(other_partitions))
    assert partitions_intersection == set(), 'Error: Consumers {} and {} have common partitions assigned to them: {}\nSubscriptions: {}\nAssignments: {}'.format(consumer, other_consumer, partitions_intersection, subscriptions, assignment)
    if abs(len(partitions) - len(other_partitions)) <= 1:
        continue
    assignments_by_topic = group_partitions_by_topic(partitions)
    other_assignments_by_topic = group_partitions_by_topic(other_partitions)
    if len(partitions) > len(other_partitions):
        for topic in six.iterkeys(assignments_by_topic):
            assert topic not in other_assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(consumer, len(partitions), other_consumer, len(other_partitions), subscriptions, assignment)
    if len(other_partitions) > len(partitions):
        for topic in six.iterkeys(other_assignments_by_topic):
            assert topic not in assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(other_consumer, len(other_partitions), consumer, len(partitions), subscriptions, assignment)",,,,,,,,,,,
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric/nn/models/dimenet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_geometric/torch_geometric/nn/models/dimenet_utils.py,,"def real_sph_harm(k, zero_m_only=True, spherical_coordinates=True):
    if not zero_m_only:
        S_m = [0]
        C_m = [1]
        for i in range(1, k):
            x = sym.symbols('x')
            y = sym.symbols('y')
            S_m += [x * S_m[i - 1] + y * C_m[i - 1]]
            C_m += [x * C_m[i - 1] - y * S_m[i - 1]]
    P_l_m = associated_legendre_polynomials(k, zero_m_only)
    if spherical_coordinates:
        theta = sym.symbols('theta')
        z = sym.symbols('z')
        for i in range(len(P_l_m)):
            for j in range(len(P_l_m[i])):
                if type(P_l_m[i][j]) != int:
                    P_l_m[i][j] = P_l_m[i][j].subs(z, sym.cos(theta))
        if not zero_m_only:
            phi = sym.symbols('phi')
            for i in range(len(S_m)):
                S_m[i] = S_m[i].subs(x, sym.sin(theta) * sym.cos(phi)).subs(y, sym.sin(theta) * sym.sin(phi))
            for i in range(len(C_m)):
                C_m[i] = C_m[i].subs(x, sym.sin(theta) * sym.cos(phi)).subs(y, sym.sin(theta) * sym.sin(phi))
    Y_func_l_m = [['0'] * (2 * j + 1) for j in range(k)]
    for i in range(k):
        Y_func_l_m[i][0] = sym.simplify(sph_harm_prefactor(i, 0) * P_l_m[i][0])
    if not zero_m_only:
        for i in range(1, k):
            for j in range(1, i + 1):
                Y_func_l_m[i][j] = sym.simplify(2 ** 0.5 * sph_harm_prefactor(i, j) * C_m[j] * P_l_m[i][j])
        for i in range(1, k):
            for j in range(1, i + 1):
                Y_func_l_m[i][-j] = sym.simplify(2 ** 0.5 * sph_harm_prefactor(i, -j) * S_m[j] * P_l_m[i][j])
    return Y_func_l_m","for j in range(len(P_l_m[i])):
    if type(P_l_m[i][j]) != int:
        P_l_m[i][j] = P_l_m[i][j].subs(z, sym.cos(theta))","for j, val in enumerate(P_l_m[i]):
    if type(val) != int:
        P_l_m[i][j] = val.subs(z, sym.cos(theta))",,,,,,,,,,,
sympy,https://github.com/sympy/sympy/tree/master/sympy/series/formal.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/series/formal.py,,"def simpleDE(f, x, g, order=4):
    """"""
    Generates simple DE.

    Explanation
    ===========

    DE is of the form

    .. math::
        f^k(x) + \\sum\\limits_{j=0}^{k-1} A_j f^j(x) = 0

    where :math:`A_j` should be rational function in x.

    Generates DE's upto order 4 (default). DE's can also have free parameters.

    By increasing order, higher order DE's can be found.

    Yields a tuple of (DE, order).
    """"""
    from sympy.solvers.solveset import linsolve
    a = symbols('a:%d' % order)

    def _makeDE(k):
        eq = f.diff(x, k) + Add(*[a[i] * f.diff(x, i) for i in range(0, k)])
        DE = g(x).diff(x, k) + Add(*[a[i] * g(x).diff(x, i) for i in range(0, k)])
        return (eq, DE)
    found = False
    for k in range(1, order + 1):
        (eq, DE) = _makeDE(k)
        eq = eq.expand()
        terms = eq.as_ordered_terms()
        ind = rational_independent(terms, x)
        if found or len(ind) == k:
            sol = dict(zip(a, (i for s in linsolve(ind, a[:k]) for i in s)))
            if sol:
                found = True
                DE = DE.subs(sol)
            DE = DE.as_numer_denom()[0]
            DE = DE.factor().as_coeff_mul(Derivative)[1][0]
            yield (DE.collect(Derivative(g(x))), k)","for k in range(1, order + 1):
    (eq, DE) = _makeDE(k)
    eq = eq.expand()
    terms = eq.as_ordered_terms()
    ind = rational_independent(terms, x)
    if found or len(ind) == k:
        sol = dict(zip(a, (i for s in linsolve(ind, a[:k]) for i in s)))
        if sol:
            found = True
            DE = DE.subs(sol)
        DE = DE.as_numer_denom()[0]
        DE = DE.factor().as_coeff_mul(Derivative)[1][0]
        yield (DE.collect(Derivative(g(x))), k)","for k, _ in enumerate(range(1, order + 1), start=1):
    (eq, DE) = _makeDE(k)
    eq = eq.expand()
    terms = eq.as_ordered_terms()
    ind = rational_independent(terms, x)
    if found or len(ind) == k:
        sol = dict(zip(a, (i for s in linsolve(ind, a[:k]) for i in s)))
        if sol:
            found = True
            DE = DE.subs(sol)
        DE = DE.as_numer_denom()[0]
        DE = DE.factor().as_coeff_mul(Derivative)[1][0]
        yield (DE.collect(Derivative(g(x))), k)",,,,,,,,,,,
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/distributions/gaussian_scale_mixture.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyro/pyro/distributions/gaussian_scale_mixture.py,GaussianScaleMixture,"def _compute_coeffs(self):
    """"""
        These coefficients are used internally in the backward call.
        """"""
    dimov2 = int(self.dim / 2)
    coeffs = torch.ones(dimov2)
    for k in range(dimov2 - 1):
        coeffs[k + 1:] *= self.dim - 2 * (k + 1)
    return coeffs","for k in range(dimov2 - 1):
    coeffs[k + 1:] *= self.dim - 2 * (k + 1)","for k, _ in enumerate(range(dimov2 - 1)):
    coeffs[k + 1:] *= self.dim - 2 * (k + 1)",,,,,,,,,,,
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","for line in f:
    infos = line.split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])","for line_num, line in enumerate(f):
    infos = line.split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])",,,,,,,,,,,
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for o in range(kernel.shape[1]):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for o, _ in enumerate(range(kernel.shape[1])):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)",,,,,,,,,,,
Python_and_the_Web,https://github.com/Python-World/Python_and_the_Web/tree/master/Scripts/Miscellaneous/Connect4Game/connect4game.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python_and_the_Web/Scripts/Miscellaneous/Connect4Game/connect4game.py,,"def draw_board(board):
    for c in range(COLUMN_COUNT):
        for r in range(ROW_COUNT):
            pygame.draw.rect(screen, BLACK, (c * SQUARESIZE, r * SQUARESIZE + SQUARESIZE, SQUARESIZE, SQUARESIZE))
            pygame.draw.circle(screen, GREY, (int(c * SQUARESIZE + SQUARESIZE / 2), int(r * SQUARESIZE + SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    for c in range(COLUMN_COUNT):
        for r in range(ROW_COUNT):
            if board[r][c] == 1:
                pygame.draw.circle(screen, RED, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
            elif board[r][c] == 2:
                pygame.draw.circle(screen, YELLOW, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    pygame.display.update()","for r in range(ROW_COUNT):
    if board[r][c] == 1:
        pygame.draw.circle(screen, RED, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    elif board[r][c] == 2:
        pygame.draw.circle(screen, YELLOW, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)","for r, row in enumerate(board):
    if row[c] == 1:
        pygame.draw.circle(screen, RED, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    elif row[c] == 2:
        pygame.draw.circle(screen, YELLOW, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)",,,,,,,,,,,
pytorch2keras,https://github.com/gmalivenko/pytorch2keras/tree/master/tests/layers/convolutions/convtranspose2d.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch2keras/tests/layers/convolutions/convtranspose2d.py,,"if __name__ == '__main__':
    max_error = 0
    for kernel_size in [1, 3, 5]:
        for padding in [0, 1, 3]:
            for stride in [1, 2]:
                for bias in [True, False]:
                    outs = np.random.choice([1, 3, 7])
                    model = LayerTest(3, outs, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)
                    model.eval()
                    input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
                    input_var = Variable(torch.FloatTensor(input_np))
                    output = model(input_var)
                    k_model = pytorch_to_keras(model, input_var, (3, 224, 224), verbose=True)
                    error = check_error(output, k_model, input_np)
                    if max_error < error:
                        max_error = error
    print('Max error: {0}'.format(max_error))","for stride in [1, 2]:
    for bias in [True, False]:
        outs = np.random.choice([1, 3, 7])
        model = LayerTest(3, outs, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)
        model.eval()
        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
        input_var = Variable(torch.FloatTensor(input_np))
        output = model(input_var)
        k_model = pytorch_to_keras(model, input_var, (3, 224, 224), verbose=True)
        error = check_error(output, k_model, input_np)
        if max_error < error:
            max_error = error","for stride_idx, stride in enumerate([1, 2]):
    for bias_idx, bias in enumerate([True, False]):
        outs = np.random.choice([1, 3, 7])
        model = LayerTest(3, outs, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)
        model.eval()
        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
        input_var = Variable(torch.FloatTensor(input_np))
        output = model(input_var)
        k_model = pytorch_to_keras(model, input_var, (3, 224, 224), verbose=True)
        error = check_error(output, k_model, input_np)
        if max_error < error:
            max_error = error",,,,,,,,,,,
repo_name,file_path,file_html,class_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/api/visualizations.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/galaxy/api/visualizations.py,VisualizationsController,"def index(self, trans: GalaxyWebTransaction, **kwargs):
    """"""
        GET /api/visualizations:
        """"""
    rval = []
    user = trans.user
    visualizations = self.get_visualizations_by_user(trans, user)
    visualizations += self.get_visualizations_shared_with_user(trans, user)
    visualizations += self.get_published_visualizations(trans, exclude_user=user)
    for visualization in visualizations:
        item = self.get_visualization_summary_dict(visualization)
        item = trans.security.encode_dict_ids(item)
        item['url'] = web.url_for('visualization', id=item['id'])
        rval.append(item)
    return rval","for visualization in visualizations:
    item = self.get_visualization_summary_dict(visualization)
    item = trans.security.encode_dict_ids(item)
    item['url'] = web.url_for('visualization', id=item['id'])
    rval.append(item)","rval = []
for i, visualization in enumerate(visualizations):
    item = self.get_visualization_summary_dict(visualization)
    item = trans.security.encode_dict_ids(item)
    item['url'] = web.url_for('visualization', id=item['id'])
    rval.append(item)",,,,,,,,,,,
python-sortedcontainers,https://github.com/grantjenks/python-sortedcontainers/tree/master/tests/test_coverage_sortedkeylist_modulo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sortedcontainers/tests/test_coverage_sortedkeylist_modulo.py,,"def test_contains():
    slt = SortedKeyList(key=modulo)
    slt._reset(7)
    assert 0 not in slt
    slt.update(range(100))
    for val in range(100):
        assert val in slt
    assert 100 not in slt
    slt._check()
    slt = SortedKeyList(range(100), key=modulo)
    slt._reset(4)
    assert all((val not in slt for val in range(100, 200)))","for val in range(100):
    assert val in slt",The code cannot be refactored with enumerate as it does not involve iterating over a list or sequence. It is simply iterating over a range of numbers and checking if each number is in a set.,,,,,,,,,,,
django-model-utils,https://github.com/jazzband/django-model-utils/tree/master/model_utils/fields.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-model-utils/model_utils/fields.py,AutoLastModifiedField,"def pre_save(self, model_instance, add):
    value = now()
    if add:
        current_value = getattr(model_instance, self.attname, self.get_default())
        if current_value != self.get_default():
            value = getattr(model_instance, self.attname)
        else:
            for field in model_instance._meta.get_fields():
                if isinstance(field, AutoCreatedField):
                    value = getattr(model_instance, field.name)
                    break
    setattr(model_instance, self.attname, value)
    return value","for field in model_instance._meta.get_fields():
    if isinstance(field, AutoCreatedField):
        value = getattr(model_instance, field.name)
        break",The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements. It is simply accessing the fields of a model instance and checking if it is an AutoCreatedField.,,,,,,,,,,,
VMZ,https://github.com/facebookresearch/VMZ/tree/master/c2/lib/models/audio_visual_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VMZ/c2/lib/models/audio_visual_model.py,,"def create_acoustic_resnet(model, data, num_labels, label=None, is_test=False, no_bias=0, final_spatial_kernel=2, final_temporal_kernel=4, model_depth=50, transformation_type='simple_block', spatial_bn_mom=0.9, audio_input_3d=False):
    if audio_input_3d:
        if audio_input_3d:
            data_swap = model.NCHW2NHWC(data, data + '_NHWC')
            data_greyscale = model.ReduceBackMean(data_swap, 'logmels', num_reduce_dim=1)
            data = data_greyscale
    conv1 = model.Conv(data, 'conv1', 1, 16, kernel=3, pad=1)
    builder = AudioModelBuilder(model, conv1, no_bias=no_bias, is_test=is_test, spatial_bn_mom=spatial_bn_mom)
    builder.add_simple_block(16, 32, down_sampling=True)
    builder.add_simple_block(32, 32)
    builder.add_simple_block(32, 32)
    (n1, n2, n3, n4) = BLOCK_CONFIG[model_depth]
    if transformation_type == 'simple_block':
        transformation = builder.add_simple_block
    elif transformation_type == 'bottleneck':
        transformation = builder.add_bottleneck
    else:
        print('Unknown transformation type...')
    if model_depth <= 34:
        filter_config = SHALLOW_FILTER_CONFIG
    else:
        filter_config = DEEP_FILTER_CONFIG
    transformation(32, filter_config[0][0], filter_config[0][1], down_sampling=True)
    for _ in range(n1 - 1):
        transformation(filter_config[0][0], filter_config[0][0], filter_config[0][1])
    transformation(filter_config[0][0], filter_config[1][0], filter_config[1][1], down_sampling=True)
    for _ in range(n2 - 1):
        transformation(filter_config[1][0], filter_config[1][0], filter_config[1][1])
    transformation(filter_config[1][0], filter_config[2][0], filter_config[2][1], down_sampling=True)
    for _ in range(n3 - 1):
        transformation(filter_config[2][0], filter_config[2][0], filter_config[2][1])
    transformation(filter_config[2][0], filter_config[3][0], filter_config[3][1], down_sampling=True)
    for _ in range(n4 - 1):
        transformation(filter_config[3][0], filter_config[3][0], filter_config[3][1])
    final_avg = model.MaxPool(builder.prev_blob, 'final_avg', kernels=[final_temporal_kernel, final_spatial_kernel], stride=1)
    last_out = brew.fc(model, final_avg, 'last_out_L{}'.format(num_labels), filter_config[3][0], num_labels)
    return last_out","for _ in range(n4 - 1):
    transformation(filter_config[3][0], filter_config[3][0], filter_config[3][1])",The code cannot be refactored with enumerate as it does not involve iterating over a sequence of elements. It simply executes a function call a certain number of times.,,,,,,,,,,,
glad,https://github.com/Dav1dde/glad/tree/master/glad/lang/c/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glad/glad/lang/c/generator.py,,"def replace_khr_types(output_str):
    replaced = output_str
    for (before, after) in _KHR_TYPE_REPLACEMENTS.items():
        replaced = replaced.replace(before, after)
    if replaced == output_str:
        return output_str
    return '#if defined(__khrplatform_h_)\n' + output_str + '#else\n' + replaced + '#endif\n'","for (before, after) in _KHR_TYPE_REPLACEMENTS.items():
    replaced = replaced.replace(before, after)",The code cannot be refactored with enumerate as it is already using the `.items()` method to iterate over the dictionary keys and values.,,,,,,,,,,,
p5,https://github.com/p5py/p5/tree/master/p5/visualTests/sanityTests/2DSanityTests/test32.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/p5/p5/visualTests/sanityTests/2DSanityTests/test32.py,,"def setup():
    global colors
    size(640, 360)
    no_stroke()
    color_mode('HSB', numChars)
    background(numChars / 2)
    background(0)
    for i in range(numChars):
        colors.append(Color(i, numChars, numChars))","for i in range(numChars):
    colors.append(Color(i, numChars, numChars))",The code cannot be refactored with enumerate as it is already using the index variable i to iterate over a range of values. Using enumerate would not provide any additional benefit in this case.,,,,,,,,,,,
taobao_seckill,https://github.com/jerry3747/taobao_seckill/tree/master/seckill/taobao_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taobao_seckill/seckill/taobao_api.py,,"def parse_submit_data(data):
    new_data = {}
    for (k, v) in data.items():
        if v.get('submit') == 'true' or v.get('submit'):
            new_data[k] = v
    return new_data","for (k, v) in data.items():
    if v.get('submit') == 'true' or v.get('submit'):
        new_data[k] = v",The code cannot be refactored with enumerate as it is already using the items() method to iterate over the dictionary and unpacking the key-value pairs into k and v.,,,,,,,,,,,
assistant-sdk-python,https://github.com/googlesamples/assistant-sdk-python/tree/master/google-assistant-sdk/googlesamples/assistant/grpc/pushtotalk.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/assistant-sdk-python/google-assistant-sdk/googlesamples/assistant/grpc/pushtotalk.py,,"def main(api_endpoint, credentials, project_id, device_model_id, device_id, device_config, lang, display, verbose, input_audio_file, output_audio_file, audio_sample_rate, audio_sample_width, audio_iter_size, audio_block_size, audio_flush_size, grpc_deadline, once, *args, **kwargs):
    """"""Samples for the Google Assistant API.

    Examples:
      Run the sample with microphone input and speaker output:

        $ python -m googlesamples.assistant

      Run the sample with file input and speaker output:

        $ python -m googlesamples.assistant -i <input file>

      Run the sample with file input and output:

        $ python -m googlesamples.assistant -i <input file> -o <output file>
    """"""
    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)
    try:
        with open(credentials, 'r') as f:
            credentials = google.oauth2.credentials.Credentials(token=None, **json.load(f))
            http_request = google.auth.transport.requests.Request()
            credentials.refresh(http_request)
    except Exception as e:
        logging.error('Error loading credentials: %s', e)
        logging.error('Run google-oauthlib-tool to initialize new OAuth 2.0 credentials.')
        sys.exit(-1)
    grpc_channel = google.auth.transport.grpc.secure_authorized_channel(credentials, http_request, api_endpoint)
    logging.info('Connecting to %s', api_endpoint)
    audio_device = None
    if input_audio_file:
        audio_source = audio_helpers.WaveSource(open(input_audio_file, 'rb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
    else:
        audio_source = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)
    if output_audio_file:
        audio_sink = audio_helpers.WaveSink(open(output_audio_file, 'wb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
    else:
        audio_sink = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)
    conversation_stream = audio_helpers.ConversationStream(source=audio_source, sink=audio_sink, iter_size=audio_iter_size, sample_width=audio_sample_width)
    if not device_id or not device_model_id:
        try:
            with open(device_config) as f:
                device = json.load(f)
                device_id = device['id']
                device_model_id = device['model_id']
                logging.info('Using device model %s and device id %s', device_model_id, device_id)
        except Exception as e:
            logging.warning('Device config not found: %s' % e)
            logging.info('Registering device')
            if not device_model_id:
                logging.error('Option --device-model-id required when registering a device instance.')
                sys.exit(-1)
            if not project_id:
                logging.error('Option --project-id required when registering a device instance.')
                sys.exit(-1)
            device_base_url = 'https://%s/v1alpha2/projects/%s/devices' % (api_endpoint, project_id)
            device_id = str(uuid.uuid1())
            payload = {'id': device_id, 'model_id': device_model_id, 'client_type': 'SDK_SERVICE'}
            session = google.auth.transport.requests.AuthorizedSession(credentials)
            r = session.post(device_base_url, data=json.dumps(payload))
            if r.status_code != 200:
                logging.error('Failed to register device: %s', r.text)
                sys.exit(-1)
            logging.info('Device registered: %s', device_id)
            pathlib.Path(os.path.dirname(device_config)).mkdir(exist_ok=True)
            with open(device_config, 'w') as f:
                json.dump(payload, f)
    device_handler = device_helpers.DeviceRequestHandler(device_id)

    @device_handler.command('action.devices.commands.OnOff')
    def onoff(on):
        if on:
            logging.info('Turning device on')
        else:
            logging.info('Turning device off')

    @device_handler.command('com.example.commands.BlinkLight')
    def blink(speed, number):
        logging.info('Blinking device %s times.' % number)
        delay = 1
        if speed == 'SLOWLY':
            delay = 2
        elif speed == 'QUICKLY':
            delay = 0.5
        for i in range(int(number)):
            logging.info('Device is blinking.')
            time.sleep(delay)
    with SampleAssistant(lang, device_model_id, device_id, conversation_stream, display, grpc_channel, grpc_deadline, device_handler) as assistant:
        if input_audio_file or output_audio_file:
            assistant.assist()
            return
        wait_for_user_trigger = not once
        while True:
            if wait_for_user_trigger:
                click.pause(info='Press Enter to send a new request...')
            continue_conversation = assistant.assist()
            wait_for_user_trigger = not continue_conversation
            if once and (not continue_conversation):
                break","for i in range(int(number)):
    logging.info('Device is blinking.')
    time.sleep(delay)",The code cannot be refactored with enumerate as it is iterating over a range of integers and not a collection of items.,,,,,,,,,,,
praw,https://github.com/praw-dev/praw/tree/master/tests/unit/models/reddit/test_wikipage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/praw/tests/unit/models/reddit/test_wikipage.py,TestWikiPage,"def test_pickle(self):
    page = WikiPage(self.reddit, subreddit=Subreddit(self.reddit, 'a'), name='x')
    for level in range(pickle.HIGHEST_PROTOCOL + 1):
        other = pickle.loads(pickle.dumps(page, protocol=level))
        assert page == other","for level in range(pickle.HIGHEST_PROTOCOL + 1):
    other = pickle.loads(pickle.dumps(page, protocol=level))
    assert page == other",The code cannot be refactored with enumerate as it is iterating over a range of values and not a list or iterable.,,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","for n1 in vertexSet[label['h']]:
    for n2 in vertexSet[label['t']]:
        fact_in_train.add((n1['name'], n2['name'], rel))",The code cannot be refactored with enumerate as it is iterating over the values of a dictionary using a key.,,,,,,,,,,,
pygorithm,https://github.com/OmkarPathak/pygorithm/tree/master/pygorithm/dynamic_programming/lcs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygorithm/pygorithm/dynamic_programming/lcs.py,,"def longest_common_subsequence(s1, s2):
    """"""
    :param s1: string
    :param s2: string
    :return: int
    """"""
    (m, n) = (len(s1), len(s2))
    dp = [[0] * (n + 1)] * (m + 1)
    '\n    dp[i][j] : contains length of LCS of s1[0..i-1] and s2[0..j-1]\n    '
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i - 1] == s2[j - 1]:
                dp[i][j] = dp[i - 1][j - 1] + 1
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
    return dp[m][n]","for i in range(1, m + 1):
    for j in range(1, n + 1):
        if s1[i - 1] == s2[j - 1]:
            dp[i][j] = dp[i - 1][j - 1] + 1
        else:
            dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])",The code cannot be refactored with enumerate as it is not iterating over a list or sequence. It is iterating over a range of numbers.,,,,,,,,,,,
swift,https://github.com/openstack/swift/tree/master/swift/cli/info.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/cli/info.py,,"def print_ring_locations(ring, datadir, account, container=None, obj=None, tpart=None, all_nodes=False, policy_index=None):
    """"""
    print out ring locations of specified type

    :param ring: ring instance
    :param datadir: name of directory where things are stored. Usually one of
                    ""accounts"", ""containers"", ""objects"", or ""objects-N"".
    :param account: account name
    :param container: container name
    :param obj: object name
    :param tpart: target partition in ring
    :param all_nodes: include all handoff nodes. If false, only the N primary
                      nodes and first N handoffs will be printed.
    :param policy_index: include policy_index in curl headers
    """"""
    if not ring:
        raise ValueError('No ring specified')
    if not datadir:
        raise ValueError('No datadir specified')
    if tpart is None and (not account):
        raise ValueError('No partition or account/container/object specified')
    if not account and (container or obj):
        raise ValueError('Container/object specified without account')
    if obj and (not container):
        raise ValueError('Object specified without container')
    if obj:
        target = '%s/%s/%s' % (account, container, obj)
    elif container:
        target = '%s/%s' % (account, container)
    else:
        target = '%s' % account
    if tpart:
        part = int(tpart)
    else:
        part = ring.get_part(account, container, obj)
    primary_nodes = ring.get_part_nodes(part)
    handoff_nodes = ring.get_more_nodes(part)
    if not all_nodes:
        handoff_nodes = itertools.islice(handoff_nodes, len(primary_nodes))
    handoff_nodes = list(handoff_nodes)
    if account and (not tpart):
        path_hash = hash_path(account, container, obj)
    else:
        path_hash = None
    print('Partition\t%s' % part)
    print('Hash     \t%s\n' % path_hash)
    for node in primary_nodes:
        print('Server:Port Device\t%s:%s %s' % (node['ip'], node['port'], node['device']))
    for node in handoff_nodes:
        print('Server:Port Device\t%s:%s %s\t [Handoff]' % (node['ip'], node['port'], node['device']))
    print('\n')
    for node in primary_nodes:
        cmd = curl_head_command(node['ip'], node['port'], node['device'], part, target, policy_index)
        print(cmd)
    for node in handoff_nodes:
        cmd = curl_head_command(node['ip'], node['port'], node['device'], part, target, policy_index)
        cmd += ' # [Handoff]'
        print(cmd)
    print('\n\nUse your own device location of servers:')
    print('such as ""export DEVICE=/srv/node""')
    if path_hash:
        for node in primary_nodes:
            print('ssh %s ""ls -lah ${DEVICE:-/srv/node*}/%s/%s""' % (node['ip'], node['device'], storage_directory(datadir, part, path_hash)))
        for node in handoff_nodes:
            print('ssh %s ""ls -lah ${DEVICE:-/srv/node*}/%s/%s"" # [Handoff]' % (node['ip'], node['device'], storage_directory(datadir, part, path_hash)))
    else:
        for node in primary_nodes:
            print('ssh %s ""ls -lah ${DEVICE:-/srv/node*}/%s/%s/%d""' % (node['ip'], node['device'], datadir, part))
        for node in handoff_nodes:
            print('ssh %s ""ls -lah ${DEVICE:-/srv/node*}/%s/%s/%d"" # [Handoff]' % (node['ip'], node['device'], datadir, part))
    print('\nnote: `/srv/node*` is used as default value of `devices`, the real value is set in the config file on each storage node.')","for node in primary_nodes:
    print('ssh %s ""ls -lah ${DEVICE:-/srv/node*}/%s/%s/%d""' % (node['ip'], node['device'], datadir, part))",The code cannot be refactored with enumerate as it is not iterating over a sequence of elements. It is simply iterating over a list of dictionaries and accessing their values.,,,,,,,,,,,
cats-blender-plugin,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/extern_tools/mmd_tools_local/core/model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/extern_tools/mmd_tools_local/core/model.py,Model,"def firstMesh(self):
    for i in self.meshes():
        return i
    return None","for i in self.meshes():
    return i",The code cannot be refactored with enumerate as it is only returning the first element of the iterable returned by `self.meshes()`. Using `enumerate` would not change the behavior of the code.,,,,,,,,,,,
UnityPack,https://github.com/HearthSim/UnityPack/tree/master/unitypack/type.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UnityPack/unitypack/type.py,TypeMetadata,"def load(self, buf, format=None):
    if format is None:
        format = self.asset.format
    self.generator_version = buf.read_string()
    self.target_platform = RuntimePlatform(buf.read_uint())
    if format >= 13:
        has_type_trees = buf.read_boolean()
        num_types = buf.read_int()
        for i in range(num_types):
            class_id = buf.read_int()
            if format >= 17:
                unk0 = buf.read_byte()
                script_id = buf.read_int16()
                if class_id == 114:
                    if script_id >= 0:
                        class_id = -2 - script_id
                    else:
                        class_id = -1
            self.class_ids.append(class_id)
            if class_id < 0:
                hash = buf.read(32)
            else:
                hash = buf.read(16)
            self.hashes[class_id] = hash
            if has_type_trees:
                tree = TypeTree(format)
                tree.load(buf)
                self.type_trees[class_id] = tree
            if format >= 21:
                unk1 = buf.read(4)
    else:
        num_fields = buf.read_int()
        for i in range(num_fields):
            class_id = buf.read_int()
            tree = TypeTree(format)
            tree.load(buf)
            self.type_trees[class_id] = tree","for i in range(num_fields):
    class_id = buf.read_int()
    tree = TypeTree(format)
    tree.load(buf)
    self.type_trees[class_id] = tree",The code cannot be refactored with enumerate as it is using a range of integers to iterate over and not a list or tuple of elements.,,,,,,,,,,,
django-cms,https://github.com/django-cms/django-cms/tree/master/cms/cache/page.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-cms/cms/cache/page.py,,"def set_page_cache(response):
    from django.core.cache import cache
    request = response._request
    toolbar = get_toolbar_from_request(request)
    is_authenticated = request.user.is_authenticated
    if is_authenticated or toolbar._cache_disabled or (not get_cms_setting('PAGE_CACHE')):
        add_never_cache_headers(response)
        return response
    timestamp = now()
    placeholders = toolbar.content_renderer.get_rendered_placeholders()
    ttl_list = []
    vary_cache_on_set = set()
    for ph in placeholders:
        ttl = ph.get_cache_expiration(request, timestamp)
        vary_cache_on = ph.get_vary_cache_on(request)
        ttl_list.append(ttl)
        if ttl and vary_cache_on:
            vary_cache_on_set |= set(vary_cache_on)
    if EXPIRE_NOW not in ttl_list:
        ttl_list.append(get_cms_setting('CACHE_DURATIONS')['content'])
        ttl_list.append(MAX_EXPIRATION_TTL)
        if hasattr(settings, 'CMS_LIMIT_TTL_CACHE_FUNCTION'):
            extension_point = settings.CMS_LIMIT_TTL_CACHE_FUNCTION
            (module, func_name) = extension_point.rsplit('.', 1)
            module = import_module(module)
            limit_ttl_cache_function = getattr(module, func_name)
            limit_ttl = limit_ttl_cache_function(response)
            if isinstance(limit_ttl, int):
                ttl_list.append(limit_ttl)
        ttl = min(ttl_list)
        if ttl > 0:
            patch_response_headers(response, cache_timeout=ttl)
            patch_vary_headers(response, sorted(vary_cache_on_set))
            version = _get_cache_version()
            expires_datetime = timestamp + timedelta(seconds=ttl)
            response_headers = get_response_headers(response)
            cache.set(_page_cache_key(request), (response.content, response_headers, expires_datetime), ttl, version=version)
            _set_cache_version(version)
    return response","for ph in placeholders:
    ttl = ph.get_cache_expiration(request, timestamp)
    vary_cache_on = ph.get_vary_cache_on(request)
    ttl_list.append(ttl)
    if ttl and vary_cache_on:
        vary_cache_on_set |= set(vary_cache_on)",The code cannot be refactored with enumerate as there is no need to access the index of the elements in the `placeholders` list.,,,,,,,,,,,
aws-cli,https://github.com/aws/aws-cli/tree/master/tests/functional/eks/test_update_kubeconfig.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/tests/functional/eks/test_update_kubeconfig.py,,"def sanitize_output(output):
    """"""
    Trims output and removes all lines after a line starting with warning.
    A line will only start with warning if it is the start of a
    ""not installed"" warning, which should be ignored when comparing output.
    """"""
    to_return = ''
    for line in output.splitlines():
        if bool(re.match('warning', line.strip(), re.I)):
            return to_return.strip()
        else:
            to_return += line
            to_return += '\n'
    return to_return.strip()","for line in output.splitlines():
    if bool(re.match('warning', line.strip(), re.I)):
        return to_return.strip()
    else:
        to_return += line
        to_return += '\n'","to_return = ''
for i, line in enumerate(output.splitlines()):
    if bool(re.match('warning', line.strip(), re.I)):
        return to_return.strip()
    else:
        to_return += line
        to_return += '\n'",,,,,,,,,,,
