repo_name,file_path,file_html,class_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/tests/ops/qubit/test_non_parametric_ops.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/tests/ops/qubit/test_non_parametric_ops.py,TestDecompositions,"def test_SISWAP_decomposition(self, siswap_op, tol):
    """"""Tests that the decomposition of the SISWAP gate and its SQISW alias gate is correct""""""
    op = siswap_op(wires=[0, 1])
    res = op.decomposition()
    assert len(res) == 12
    assert res[0].wires == Wires([0])
    assert res[1].wires == Wires([0])
    assert res[2].wires == Wires([0, 1])
    assert res[3].wires == Wires([0])
    assert res[4].wires == Wires([0])
    assert res[5].wires == Wires([0])
    assert res[6].wires == Wires([0])
    assert res[7].wires == Wires([1])
    assert res[8].wires == Wires([1])
    assert res[9].wires == Wires([0, 1])
    assert res[10].wires == Wires([0])
    assert res[11].wires == Wires([1])
    assert res[0].name == 'SX'
    assert res[1].name == 'RZ'
    assert res[2].name == 'CNOT'
    assert res[3].name == 'SX'
    assert res[4].name == 'RZ'
    assert res[5].name == 'SX'
    assert res[6].name == 'RZ'
    assert res[7].name == 'SX'
    assert res[8].name == 'RZ'
    assert res[9].name == 'CNOT'
    assert res[10].name == 'SX'
    assert res[11].name == 'SX'
    mats = []
    for i in reversed(res):
        if i.wires == Wires([1]):
            mats.append(np.kron(np.eye(2), i.matrix()))
        elif i.wires == Wires([0]):
            mats.append(np.kron(i.matrix(), np.eye(2)))
        elif i.wires == Wires([1, 0]) and i.name == 'CNOT':
            mats.append(np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]))
        else:
            mats.append(i.matrix())
    decomposed_matrix = np.linalg.multi_dot(mats)
    assert np.allclose(decomposed_matrix, op.matrix(), atol=tol, rtol=0)","for i in reversed(res):
    if i.wires == Wires([1]):
        mats.append(np.kron(np.eye(2), i.matrix()))
    elif i.wires == Wires([0]):
        mats.append(np.kron(i.matrix(), np.eye(2)))
    elif i.wires == Wires([1, 0]) and i.name == 'CNOT':
        mats.append(np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]))
    else:
        mats.append(i.matrix())","for i, val in enumerate(reversed(res)):
    if val.wires == Wires([1]):
        mats.append(np.kron(np.eye(2), val.matrix()))
    elif val.wires == Wires([0]):
        mats.append(np.kron(val.matrix(), np.eye(2)))
    elif val.wires == Wires([1, 0]) and val.name == 'CNOT':
        mats.append(np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]))
    else:
        mats.append(val.matrix())"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/tabular.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/datatypes/tabular.py,Eland,"def sniff_prefix(self, file_prefix: FilePrefix):
    """"""
        Determines whether the file is in ELAND export format

        A file in ELAND export format consists of lines of tab-separated data.
        There is no header.

        Rules for sniffing as True::

            - There must be 22 columns on each line
            - LANE, TILEm X, Y, INDEX, READ_NO, SEQ, QUAL, POSITION, *STRAND, FILT must be correct
            - We will only check that up to the first 5 alignments are correctly formatted.
        """"""
    count = 0
    for line in file_prefix.line_iterator():
        line = line.strip()
        if not line:
            break
        if line:
            line_pieces = line.split('\t')
            if len(line_pieces) != 22:
                return False
            if int(line_pieces[1]) < 0:
                raise Exception('Out of range')
            if int(line_pieces[2]) < 0:
                raise Exception('Out of range')
            if int(line_pieces[3]) < 0:
                raise Exception('Out of range')
            int(line_pieces[4])
            int(line_pieces[5])
            count += 1
            if count == 5:
                break
    if count > 0:
        return True","for line in file_prefix.line_iterator():
    line = line.strip()
    if not line:
        break
    if line:
        line_pieces = line.split('\t')
        if len(line_pieces) != 22:
            return False
        if int(line_pieces[1]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[2]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[3]) < 0:
            raise Exception('Out of range')
        int(line_pieces[4])
        int(line_pieces[5])
        count += 1
        if count == 5:
            break","for i, line in enumerate(file_prefix.line_iterator()):
    line = line.strip()
    if not line:
        break
    if line:
        line_pieces = line.split('\t')
        if len(line_pieces) != 22:
            return False
        if int(line_pieces[1]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[2]) < 0:
            raise Exception('Out of range')
        if int(line_pieces[3]) < 0:
            raise Exception('Out of range')
        int(line_pieces[4])
        int(line_pieces[5])
        count += 1
        if count == 5:
            break"
DeepPrivacy,https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/modeling/models/generator/deep_privacy_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPrivacy/deep_privacy/modeling/models/generator/deep_privacy_v1.py,DeepPrivacyV1,"def forward_encoder(self, x, mask, batch):
    unet_features = {}
    for module in self.encoder:
        (x, mask, batch) = module((x, mask, batch))
        if isinstance(module, blocks.BasicBlock):
            unet_features[module._resolution] = (x, mask)
    return (x, mask, unet_features)","for module in self.encoder:
    (x, mask, batch) = module((x, mask, batch))
    if isinstance(module, blocks.BasicBlock):
        unet_features[module._resolution] = (x, mask)","for i, module in enumerate(self.encoder):
    (x, mask, batch) = module((x, mask, batch))
    if isinstance(module, blocks.BasicBlock):
        unet_features[module._resolution] = (x, mask)"
PythonRobotics,https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/AStar/a_star_variants.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/PathPlanning/AStar/a_star_variants.py,,"def main():
    obs_dict = {}
    for i in range(51):
        for j in range(51):
            obs_dict[i, j] = False
    (o_x, o_y) = ([], [])
    s_x = 5.0
    s_y = 5.0
    g_x = 35.0
    g_y = 45.0
    draw_vertical_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_vertical_line(48, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 48, 50, o_x, o_y, obs_dict)
    all_x = [10, 10, 10, 15, 20, 20, 30, 30, 35, 30, 40, 45]
    all_y = [10, 30, 45, 20, 5, 40, 10, 40, 5, 40, 10, 25]
    all_len = [10, 10, 5, 10, 10, 5, 20, 10, 25, 10, 35, 15]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_vertical_line(x, y, l, o_x, o_y, obs_dict)
    (all_x[:], all_y[:], all_len[:]) = ([], [], [])
    all_x = [35, 40, 15, 10, 45, 20, 10, 15, 25, 45, 10, 30, 10, 40]
    all_y = [5, 10, 15, 20, 20, 25, 30, 35, 35, 35, 40, 40, 45, 45]
    all_len = [10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 5]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)
    if show_animation:
        plt.plot(o_x, o_y, '.k')
        plt.plot(s_x, s_y, 'og')
        plt.plot(g_x, g_y, 'xb')
        plt.grid(True)
    if use_jump_point:
        keypoint_list = key_points(obs_dict)
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101, keypoint_list)
        search_obj.jump_point()
    else:
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101)
        search_obj.a_star()","for (x, y, l) in zip(all_x, all_y, all_len):
    draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)","for i, (x, y, l) in enumerate(zip(all_x, all_y, all_len)):
    draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)"
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/tests/utils/test_auto_resumer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/tests/utils/test_auto_resumer.py,,"def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    loaded_cfg = json.load(open(args_path))
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)","for k in cfg_dict:
    assert cfg_dict[k] == loaded_cfg[k]","for i, k in enumerate(cfg_dict):
    assert cfg_dict[k] == loaded_cfg[k]"
wagtail,https://github.com/wagtail/wagtail/tree/master/wagtail/core/management/commands/replace_text.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wagtail/wagtail/core/management/commands/replace_text.py,,"def replace_in_model(model, from_text, to_text):
    text_field_names = [field.name for field in model._meta.fields if isinstance(field, models.TextField) or isinstance(field, models.CharField)]
    updated_fields = []
    for field in text_field_names:
        field_value = getattr(model, field)
        if field_value and from_text in field_value:
            updated_fields.append(field)
            setattr(model, field, field_value.replace(from_text, to_text))
    if updated_fields:
        model.save(update_fields=updated_fields)","for field in text_field_names:
    field_value = getattr(model, field)
    if field_value and from_text in field_value:
        updated_fields.append(field)
        setattr(model, field, field_value.replace(from_text, to_text))","for i, field in enumerate(text_field_names):
    field_value = getattr(model, field)
    if field_value and from_text in field_value:
        updated_fields.append(field)
        setattr(model, field, field_value.replace(from_text, to_text))"
archinstall,https://github.com/archlinux/archinstall/tree/master/archinstall/lib/mirrors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/archinstall/archinstall/lib/mirrors.py,,"def list_mirrors(sort_order=['https', 'http']):
    url = 'https://archlinux.org/mirrorlist/?protocol=https&protocol=http&ip_version=4&ip_version=6&use_mirror_status=on'
    regions = {}
    try:
        response = urllib.request.urlopen(url)
    except urllib.error.URLError as err:
        log(f'Could not fetch an active mirror-list: {err}', level=logging.WARNING, fg='yellow')
        return regions
    mirrorlist = response.read()
    if sort_order:
        mirrorlist = sort_mirrorlist(mirrorlist, sort_order=sort_order)
    region = 'Unknown region'
    for line in mirrorlist.split(b'\n'):
        if len(line.strip()) == 0:
            continue
        line = line.decode('UTF-8').strip('\n').strip('\r')
        if line[:3] == '## ':
            region = line[3:]
        elif line[:10] == '#Server = ':
            regions.setdefault(region, {})
            url = line.lstrip('#Server = ')
            regions[region][url] = True
    return regions","for line in mirrorlist.split(b'\n'):
    if len(line.strip()) == 0:
        continue
    line = line.decode('UTF-8').strip('\n').strip('\r')
    if line[:3] == '## ':
        region = line[3:]
    elif line[:10] == '#Server = ':
        regions.setdefault(region, {})
        url = line.lstrip('#Server = ')
        regions[region][url] = True","for i, line in enumerate(mirrorlist.split(b'\n')):
    if len(line.strip()) == 0:
        continue
    line = line.decode('UTF-8').strip('\n').strip('\r')
    if line[:3] == '## ':
        region = line[3:]
    elif line[:10] == '#Server = ':
        regions.setdefault(region, {})
        url = line.lstrip('#Server = ')
        regions[region][url] = True"
texar-pytorch,https://github.com/asyml/texar-pytorch/tree/master/texar/torch/data/data/scalar_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/texar/torch/data/data/scalar_data.py,ScalarData,"def process(self, raw_example: List[str]) -> Union[bool, int, float]:
    assert len(raw_example) == 1
    example_: Union[int, str]
    if self._data_type == np.bool_:
        example_ = strtobool(raw_example[0])
    else:
        example_ = raw_example[0]
    example = self._data_type(example_)
    for transform in self._other_transforms:
        example = transform(example)
    return example","for transform in self._other_transforms:
    example = transform(example)","for i, transform in enumerate(self._other_transforms):
    example = transform(example)"
weevely3,https://github.com/epinna/weevely3/tree/master/modules/net/scan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/weevely3/modules/net/scan.py,Scan,"def run(self):
    IPs = []
    for ip_or_network in self.args['addresses'].split(','):
        if ip_or_network.count('-') == 1:
            IPs += list(utils.iputil.ip_range(ip_or_network))
        elif ip_or_network.count('/') == 1:
            IPs += [str(utils.ipaddr.IPAddress(ip)) for ip in utils.ipaddr.IPNetwork(ip_or_network)]
        else:
            IPs.append(ip_or_network)
    prts = utils.iputil.port_range(self.args['ports'])
    results_string = ''
    for ips_chunk in list(utils.strings.chunks(IPs, self.args['addresses_per_request'])):
        for prts_chunk in list(utils.strings.chunks(prts, self.args['ports_per_request'])):
            results_string += self.vectors.get_result(name='fsockopen', format_args={'ips': ips_chunk, 'prts': prts_chunk, 'timeout': self.args['timeout']})
            log.warn('Scanning addresses %s-%s:%i-%i' % (ips_chunk[0], ips_chunk[-1], prts_chunk[0], prts_chunk[-1]))
    result = []
    for result_string in results_string.split('\n'):
        addr_string_splitted = result_string.split(' ')
        if addr_string_splitted[0] == 'OPN':
            address = addr_string_splitted[1]
            error = 'OPEN'
        elif addr_string_splitted[0] == 'ERR':
            address = addr_string_splitted[1]
            error = '%s (%s)' % (' '.join(addr_string_splitted[2:-1]), addr_string_splitted[-1])
        else:
            log.debug(messages.module_net_scan.unexpected_response)
            continue
        if self.args.get('print'):
            result.append((address, error))
        elif error == 'OPEN':
            result.append(address)
    return result","for ip_or_network in self.args['addresses'].split(','):
    if ip_or_network.count('-') == 1:
        IPs += list(utils.iputil.ip_range(ip_or_network))
    elif ip_or_network.count('/') == 1:
        IPs += [str(utils.ipaddr.IPAddress(ip)) for ip in utils.ipaddr.IPNetwork(ip_or_network)]
    else:
        IPs.append(ip_or_network)","for i, ip_or_network in enumerate(self.args['addresses'].split(',')):
    if ip_or_network.count('-') == 1:
        IPs += list(utils.iputil.ip_range(ip_or_network))
    elif ip_or_network.count('/') == 1:
        IPs += [str(utils.ipaddr.IPAddress(ip)) for ip in utils.ipaddr.IPNetwork(ip_or_network)]
    else:
        IPs.append(ip_or_network)"
advertools,https://github.com/eliasdabbas/advertools/tree/master/advertools/serp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/advertools/advertools/serp.py,,"def youtube_channel_details(key, channel_ids):
    """"""Return details of channels for which the ids are given.
    Assumes ``ids`` is a comma-separated list of channel ids with
    no spaces.""""""
    base_url = 'https://www.googleapis.com/youtube/v3/channels?part=snippet,contentDetails,statistics'
    channel_ids = _split_by_comma(channel_ids, length=50)
    final_df = pd.DataFrame()
    for channel_id in channel_ids:
        params = {'id': channel_id, 'key': key}
        logging.info(msg='Requesting: ' + 'channel details')
        channel_resp = requests.get(base_url, params=params)
        if channel_resp.status_code >= 400:
            raise Exception(channel_resp.json())
        items_df = pd.DataFrame(channel_resp.json()['items'])
        details = ['snippet', 'statistics', 'contentDetails']
        detail_df = pd.DataFrame()
        for detail in details:
            try:
                detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
            except KeyError:
                continue
        temp_df = pd.concat([items_df, detail_df], axis=1)
        final_df = final_df.append(temp_df, sort=False, ignore_index=True)
    return final_df","for detail in details:
    try:
        detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
    except KeyError:
        continue","for i, detail in enumerate(details):
    try:
        detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
    except KeyError:
        continue"
DBNet.pytorch,https://github.com/WenmuZhou/DBNet.pytorch/tree/master/utils/cal_recall/rrc_evaluation_funcs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DBNet.pytorch/utils/cal_recall/rrc_evaluation_funcs.py,,"def get_tl_line_values_from_file_contents(content, CRLF=True, LTRB=True, withTranscription=False, withConfidence=False, imWidth=0, imHeight=0, sort_by_confidences=True):
    """"""
    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:
    xmin,ymin,xmax,ymax,[confidence],[transcription]
    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]
    """"""
    pointsList = []
    transcriptionsList = []
    confidencesList = []
    lines = content.split('\r\n' if CRLF else '\n')
    for line in lines:
        line = line.replace('\r', '').replace('\n', '')
        if line != '':
            (points, confidence, transcription) = get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)
            pointsList.append(points)
            transcriptionsList.append(transcription)
            confidencesList.append(confidence)
    if withConfidence and len(confidencesList) > 0 and sort_by_confidences:
        import numpy as np
        sorted_ind = np.argsort(-np.array(confidencesList))
        confidencesList = [confidencesList[i] for i in sorted_ind]
        pointsList = [pointsList[i] for i in sorted_ind]
        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]
    return (pointsList, confidencesList, transcriptionsList)","for line in lines:
    line = line.replace('\r', '').replace('\n', '')
    if line != '':
        (points, confidence, transcription) = get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)
        pointsList.append(points)
        transcriptionsList.append(transcription)
        confidencesList.append(confidence)","for i, line in enumerate(lines):
    line = line.replace('\r', '').replace('\n', '')
    if line != '':
        (points, confidence, transcription) = get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)
        pointsList.append(points)
        transcriptionsList.append(transcription)
        confidencesList.append(confidence)"
TuChart,https://github.com/Seedarchangel/TuChart/tree/master/Tuchart/Graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TuChart/Tuchart/Graph.py,,"def graphpage(items, startdate, enddate, option, width1, height1):
    page = Page()
    for i in items:
        j = re.split('-', i)
        if len(j) == 3:
            a = generateline(j[1], j[2], startdate, enddate, option)
            if a is None:
                continue
            time = [d[0] for d in a]
            if j[2] != 'Kline':
                if len(a[0]) == 4 and a[0][2] == 'bar':
                    overlap = Overlap()
                    form = [e[1] for e in a]
                    bar = Bar(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    bar.add(j[0] + '-' + j[2], time, form, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider')
                    overlap.add(bar)
                    line = Line(j[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    price = [e[3] for e in a]
                    line.add(j[0] + 'price', time, price, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider', yaxis_type='value')
                    overlap.add(line, yaxis_index=1, is_add_yaxis=True)
                    page.add(overlap)
                if len(a[0]) == 5 and a[0][3] == 'pie':
                    overlap = Overlap()
                    timeline = Timeline(is_auto_play=False, timeline_bottom=0)
                    namearray = [c[0] for c in a]
                    valuearray = [d[1] for d in a]
                    quarter = [e[2] for e in a]
                    num = a[0][4]
                    for x in range(0, num / 10):
                        list1 = valuearray[x]
                        names = namearray[x]
                        quarters = quarter[x][0]
                        for (idx, val) in enumerate(list1):
                            list1[idx] = float(val)
                        pie = Pie(j[0] + '-' + '前十股东'.decode('utf-8'), width=width1 * 10 / 11, height=height1 * 10 / 11)
                        pie.add(j[0] + '-' + '前十股东'.decode('utf-8'), names, list1, radius=[30, 55], is_legend_show=False, is_label_show=True, label_formatter='{b}: {c}\n{d}%')
                        timeline.add(pie, quarters)
                    timeline.render()
                    return
                else:
                    form = [e[1] for e in a]
                    line = Line(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    line.add(j[0] + '-' + j[2], time, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                    page.add(line)
            else:
                overlap = Overlap()
                close = zip(*a)[2]
                candle = [[x[1], x[2], x[3], x[4]] for x in a]
                candlestick = Kline(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                candlestick.add(j[0], time, candle, is_datazoom_show=True, datazoom_type='slider', yaxis_interval=1)
                overlap.add(candlestick)
                if len(close) > 10:
                    ma10 = calculateMa(close, 10)
                    line1 = Line(title_color='#C0C0C0')
                    line1.add(j[0] + '-' + 'MA10', time, ma10)
                    overlap.add(line1)
                if len(close) > 20:
                    ma20 = calculateMa(close, 20)
                    line2 = Line(title_color='#C0C0C0')
                    line2.add(j[0] + '-' + 'MA20', time, ma20)
                    overlap.add(line2)
                if len(close) > 30:
                    ma30 = calculateMa(close, 30)
                    line3 = Line(title_color='#C0C0C0')
                    line3.add(j[0] + '-' + 'MA30', time, ma30)
                    overlap.add(line3)
                page.add(overlap)
        else:
            for k in range(1, len(j) / 3):
                j[3 * k - 1] = re.sub('\n&', '', j[3 * k - 1])
            sizearray = []
            layout = Overlap()
            for i in xrange(0, len(j), 3):
                array = j[i:i + 3]
                b = generateline(array[1], array[2], startdate, enddate, option)
                if b is None:
                    continue
                btime = [d[0] for d in b]
                if array[2] != 'Kline':
                    if len(b[0]) == 4 and b[0][2] == 'bar':
                        form = [e[1] for e in b]
                        bar = Bar(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                        bar.add(array[0] + '-' + array[2], btime, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                        layout.add(bar)
                        line = Line(array[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                        price = [e[3] for e in b]
                        line.add(array[0] + 'price', btime, price, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_type='value')
                        layout.add(line, yaxis_index=1, is_add_yaxis=True)
                    else:
                        line = Line(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                        line.add(array[0] + '-' + array[2], btime, b, is_datazoom_show=True, yaxis_max='dataMax', yaxis_min='dataMin', datazoom_type='slider')
                        layout.add(line)
                else:
                    candle = [[x[1], x[2], x[3], x[4]] for x in b]
                    candlestick = Kline(array[0] + '-' + array[1], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    candlestick.add(array[0], btime, candle, is_datazoom_show=True, datazoom_type=['slider'])
                    close = zip(*b)[2]
                    if len(close) > 10:
                        ma10 = calculateMa(close, 10)
                        line4 = Line(title_color='#C0C0C0')
                        line4.add(array[0] + '-' + 'MA10', btime, ma10)
                        layout.add(line4)
                    if len(close) > 20:
                        ma20 = calculateMa(close, 20)
                        line5 = Line(title_color='#C0C0C0')
                        line5.add(array[0] + '-' + 'MA20', btime, ma20)
                        layout.add(line5)
                    if len(close) > 30:
                        ma30 = calculateMa(close, 30)
                        line6 = Line(title_color='#C0C0C0')
                        line6.add(array[0] + '-' + 'MA30', btime, ma30)
                        layout.add(line6)
                    layout.add(candlestick)
            page.add(layout)
    page.render()","for i in items:
    j = re.split('-', i)
    if len(j) == 3:
        a = generateline(j[1], j[2], startdate, enddate, option)
        if a is None:
            continue
        time = [d[0] for d in a]
        if j[2] != 'Kline':
            if len(a[0]) == 4 and a[0][2] == 'bar':
                overlap = Overlap()
                form = [e[1] for e in a]
                bar = Bar(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                bar.add(j[0] + '-' + j[2], time, form, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider')
                overlap.add(bar)
                line = Line(j[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                price = [e[3] for e in a]
                line.add(j[0] + 'price', time, price, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider', yaxis_type='value')
                overlap.add(line, yaxis_index=1, is_add_yaxis=True)
                page.add(overlap)
            if len(a[0]) == 5 and a[0][3] == 'pie':
                overlap = Overlap()
                timeline = Timeline(is_auto_play=False, timeline_bottom=0)
                namearray = [c[0] for c in a]
                valuearray = [d[1] for d in a]
                quarter = [e[2] for e in a]
                num = a[0][4]
                for x in range(0, num / 10):
                    list1 = valuearray[x]
                    names = namearray[x]
                    quarters = quarter[x][0]
                    for (idx, val) in enumerate(list1):
                        list1[idx] = float(val)
                    pie = Pie(j[0] + '-' + '前十股东'.decode('utf-8'), width=width1 * 10 / 11, height=height1 * 10 / 11)
                    pie.add(j[0] + '-' + '前十股东'.decode('utf-8'), names, list1, radius=[30, 55], is_legend_show=False, is_label_show=True, label_formatter='{b}: {c}\n{d}%')
                    timeline.add(pie, quarters)
                timeline.render()
                return
            else:
                form = [e[1] for e in a]
                line = Line(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                line.add(j[0] + '-' + j[2], time, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                page.add(line)
        else:
            overlap = Overlap()
            close = zip(*a)[2]
            candle = [[x[1], x[2], x[3], x[4]] for x in a]
            candlestick = Kline(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
            candlestick.add(j[0], time, candle, is_datazoom_show=True, datazoom_type='slider', yaxis_interval=1)
            overlap.add(candlestick)
            if len(close) > 10:
                ma10 = calculateMa(close, 10)
                line1 = Line(title_color='#C0C0C0')
                line1.add(j[0] + '-' + 'MA10', time, ma10)
                overlap.add(line1)
            if len(close) > 20:
                ma20 = calculateMa(close, 20)
                line2 = Line(title_color='#C0C0C0')
                line2.add(j[0] + '-' + 'MA20', time, ma20)
                overlap.add(line2)
            if len(close) > 30:
                ma30 = calculateMa(close, 30)
                line3 = Line(title_color='#C0C0C0')
                line3.add(j[0] + '-' + 'MA30', time, ma30)
                overlap.add(line3)
            page.add(overlap)
    else:
        for k in range(1, len(j) / 3):
            j[3 * k - 1] = re.sub('\n&', '', j[3 * k - 1])
        sizearray = []
        layout = Overlap()
        for i in xrange(0, len(j), 3):
            array = j[i:i + 3]
            b = generateline(array[1], array[2], startdate, enddate, option)
            if b is None:
                continue
            btime = [d[0] for d in b]
            if array[2] != 'Kline':
                if len(b[0]) == 4 and b[0][2] == 'bar':
                    form = [e[1] for e in b]
                    bar = Bar(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    bar.add(array[0] + '-' + array[2], btime, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                    layout.add(bar)
                    line = Line(array[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    price = [e[3] for e in b]
                    line.add(array[0] + 'price', btime, price, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_type='value')
                    layout.add(line, yaxis_index=1, is_add_yaxis=True)
                else:
                    line = Line(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    line.add(array[0] + '-' + array[2], btime, b, is_datazoom_show=True, yaxis_max='dataMax', yaxis_min='dataMin', datazoom_type='slider')
                    layout.add(line)
            else:
                candle = [[x[1], x[2], x[3], x[4]] for x in b]
                candlestick = Kline(array[0] + '-' + array[1], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                candlestick.add(array[0], btime, candle, is_datazoom_show=True, datazoom_type=['slider'])
                close = zip(*b)[2]
                if len(close) > 10:
                    ma10 = calculateMa(close, 10)
                    line4 = Line(title_color='#C0C0C0')
                    line4.add(array[0] + '-' + 'MA10', btime, ma10)
                    layout.add(line4)
                if len(close) > 20:
                    ma20 = calculateMa(close, 20)
                    line5 = Line(title_color='#C0C0C0')
                    line5.add(array[0] + '-' + 'MA20', btime, ma20)
                    layout.add(line5)
                if len(close) > 30:
                    ma30 = calculateMa(close, 30)
                    line6 = Line(title_color='#C0C0C0')
                    line6.add(array[0] + '-' + 'MA30', btime, ma30)
                    layout.add(line6)
                layout.add(candlestick)
        page.add(layout)","for i, item in enumerate(items):
    j = re.split('-', item)
    if len(j) == 3:
        a = generateline(j[1], j[2], startdate, enddate, option)
        if a is None:
            continue
        time = [d[0] for d in a]
        if j[2] != 'Kline':
            if len(a[0]) == 4 and a[0][2] == 'bar':
                overlap = Overlap()
                form = [e[1] for e in a]
                bar = Bar(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                bar.add(j[0] + '-' + j[2], time, form, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider')
                overlap.add(bar)
                line = Line(j[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                price = [e[3] for e in a]
                line.add(j[0] + 'price', time, price, yaxis_min='dataMin', yaxis_max='dataMax', is_datazoom_show=True, datazoom_type='slider', yaxis_type='value')
                overlap.add(line, yaxis_index=1, is_add_yaxis=True)
                page.add(overlap)
            if len(a[0]) == 5 and a[0][3] == 'pie':
                overlap = Overlap()
                timeline = Timeline(is_auto_play=False, timeline_bottom=0)
                namearray = [c[0] for c in a]
                valuearray = [d[1] for d in a]
                quarter = [e[2] for e in a]
                num = a[0][4]
                for x in range(0, num / 10):
                    list1 = valuearray[x]
                    names = namearray[x]
                    quarters = quarter[x][0]
                    for (idx, val) in enumerate(list1):
                        list1[idx] = float(val)
                    pie = Pie(j[0] + '-' + '前十股东'.decode('utf-8'), width=width1 * 10 / 11, height=height1 * 10 / 11)
                    pie.add(j[0] + '-' + '前十股东'.decode('utf-8'), names, list1, radius=[30, 55], is_legend_show=False, is_label_show=True, label_formatter='{b}: {c}\n{d}%')
                    timeline.add(pie, quarters)
                timeline.render()
                return
            else:
                form = [e[1] for e in a]
                line = Line(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                line.add(j[0] + '-' + j[2], time, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                page.add(line)
        else:
            overlap = Overlap()
            close = zip(*a)[2]
            candle = [[x[1], x[2], x[3], x[4]] for x in a]
            candlestick = Kline(j[0] + '-' + j[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
            candlestick.add(j[0], time, candle, is_datazoom_show=True, datazoom_type='slider', yaxis_interval=1)
            overlap.add(candlestick)
            if len(close) > 10:
                ma10 = calculateMa(close, 10)
                line1 = Line(title_color='#C0C0C0')
                line1.add(j[0] + '-' + 'MA10', time, ma10)
                overlap.add(line1)
            if len(close) > 20:
                ma20 = calculateMa(close, 20)
                line2 = Line(title_color='#C0C0C0')
                line2.add(j[0] + '-' + 'MA20', time, ma20)
                overlap.add(line2)
            if len(close) > 30:
                ma30 = calculateMa(close, 30)
                line3 = Line(title_color='#C0C0C0')
                line3.add(j[0] + '-' + 'MA30', time, ma30)
                overlap.add(line3)
            page.add(overlap)
    else:
        for k in range(1, len(j) / 3):
            j[3 * k - 1] = re.sub('\n&', '', j[3 * k - 1])
        sizearray = []
        layout = Overlap()
        for idx in xrange(0, len(j), 3):
            array = j[idx:idx + 3]
            b = generateline(array[1], array[2], startdate, enddate, option)
            if b is None:
                continue
            btime = [d[0] for d in b]
            if array[2] != 'Kline':
                if len(b[0]) == 4 and b[0][2] == 'bar':
                    form = [e[1] for e in b]
                    bar = Bar(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    bar.add(array[0] + '-' + array[2], btime, form, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_max='dataMax')
                    layout.add(bar)
                    line = Line(array[0] + 'price', width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    price = [e[3] for e in b]
                    line.add(array[0] + 'price', btime, price, is_datazoom_show=True, datazoom_type='slider', yaxis_min='dataMin', yaxis_type='value')
                    layout.add(line, yaxis_index=1, is_add_yaxis=True)
                else:
                    line = Line(array[0] + '-' + array[2], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                    line.add(array[0] + '-' + array[2], btime, b, is_datazoom_show=True, yaxis_max='dataMax', yaxis_min='dataMin', datazoom_type='slider')
                    layout.add(line)
            else:
                candle = [[x[1], x[2], x[3], x[4]] for x in b]
                candlestick = Kline(array[0] + '-' + array[1], width=width1 * 10 / 11, height=height1 * 10 / 11 / len(items))
                candlestick.add(array[0], btime, candle, is_datazoom_show=True, datazoom_type=['slider'])
                close = zip(*b)[2]
                if len(close) > 10:
                    ma10 = calculateMa(close, 10)
                    line4 = Line(title_color='#C0C0C0')
                    line4.add(array[0] + '-' + 'MA10', btime, ma10)
                    layout.add(line4)
                if len(close) > 20:
                    ma20 = calculateMa(close, 20)
                    line5 = Line(title_color='#C0C0C0')
                    line5.add(array[0] + '-' + 'MA20', btime, ma20)
                    layout.add(line5)
                if len(close) > 30:
                    ma30 = calculateMa(close, 30)
                    line6 = Line(title_color='#C0C0C0')
                    line6.add(array[0] + '-' + 'MA30', btime, ma30)
                    layout.add(line6)
                layout.add(candlestick)
        page.add(layout)"
astropy,https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_groups.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/table/tests/test_groups.py,,"def test_group_mixins():
    """"""
    Test grouping a table with mixin columns
    """"""
    idx = np.arange(4)
    x = np.array([3.0, 1.0, 2.0, 1.0])
    q = x * u.m
    lon = coordinates.Longitude(x * u.deg)
    lat = coordinates.Latitude(x * u.deg)
    tm = time.Time(2000, format='jyear') + time.TimeDelta(x * 1e-10, format='sec')
    sc = coordinates.SkyCoord(ra=lon, dec=lat)
    aw = table_helpers.ArrayWrapper(x)
    nd = np.array([(3, 'c'), (1, 'a'), (2, 'b'), (1, 'a')], dtype='<i4,|S1').view(NdarrayMixin)
    qt = QTable([idx, x, q, lon, lat, tm, sc, aw, nd], names=['idx', 'x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd'])
    mixin_keys = ['x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd']
    for key in mixin_keys:
        qtg = qt.group_by(key)
        assert np.all(qtg['idx'] == [1, 3, 2, 0])
        for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
            assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
            assert np.all(qt[name][[2]] == qtg.groups[1][name])
            assert np.all(qt[name][[0]] == qtg.groups[2][name])
    uqt = unique(qt, keys=mixin_keys)
    assert len(uqt) == 3
    assert np.all(uqt['idx'] == [1, 2, 0])
    assert np.all(uqt['x'] == [1.0, 2.0, 3.0])
    idxg = qt['idx'].group_by(qt[mixin_keys])
    assert np.all(idxg == [1, 3, 2, 0])","for key in mixin_keys:
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])","for i, key in enumerate(mixin_keys):
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])"
tvm,https://github.com/apache/tvm/tree/master/python/tvm/micro/model_library_format.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/micro/model_library_format.py,,"def _build_sid_map(graph_json):
    """"""Build a simpler storage id info map from graph JSON.

    Parameters
    ----------
    graph_json : str
        String representation of the graph_json created from tvm.relay.build().

    Returns
    -------
    list :
        A list with one entry per storage id describing that memory.
    """"""
    graph = json.loads(graph_json)
    seen_storage_ids = set()
    memory_map = []
    for (node_id, storage_id) in enumerate(graph['attrs']['storage_id'][1]):
        if storage_id in seen_storage_ids:
            continue
        seen_storage_ids.add(storage_id)
        num_elements = 1
        for dim in graph['attrs']['shape'][1][storage_id]:
            num_elements *= dim
        dltype = graph['attrs']['dltype'][1][storage_id]
        m = re.match('^[a-zA-Z]+([0-9]+)$', dltype)
        assert m, f'Exported graph contains unknown dltype {dltype}'
        elem_bits = int(m.group(1))
        map_entry = {'storage_id': storage_id, 'size_bytes': (num_elements * elem_bits + 7) // 8}
        if node_id in graph['arg_nodes']:
            map_entry['input_binding'] = graph['nodes'][node_id]['name']
        memory_map.append(map_entry)
    return memory_map","for dim in graph['attrs']['shape'][1][storage_id]:
    num_elements *= dim","for i, dim in enumerate(graph['attrs']['shape'][1][storage_id]):
    num_elements *= dim"
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/dns_data_management.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/dns_data_management.py,IPASystemRecords,"def _get_location_dns_records_for_server(self, zone_obj, hostname, locations, roles=None, include_master_role=True, include_kerberos_realm=True):
    server = self.servers_data[hostname]
    if roles:
        eff_roles = server['roles'] & roles
    else:
        eff_roles = server['roles']
    hostname_abs = DNSName(hostname).make_absolute()
    for location in locations:
        if location == self.servers_data[hostname]['location']:
            priority = self.PRIORITY_HIGH
        else:
            priority = self.PRIORITY_LOW
        if include_kerberos_realm:
            self.__add_kerberos_txt_rec(zone_obj, location)
        if include_master_role:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
            self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
        if 'AD trust controller' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
        if 'NTP server' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)
    return zone_obj","for location in locations:
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)","for i, location in enumerate(locations):
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for post_id in post_ids:
    for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if i % 1000000 == 0:
            print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)","for (i, post_id) in enumerate(post_ids):
    for (j, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if j % 1000000 == 0:
            print('read %d lines, found %d' % (j, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)"
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)","for cell_block in mesh.cells:
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))","for i, cell_block in enumerate(mesh.cells):
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for j, cell in enumerate(cell_block.data):
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))"
nevergrad,https://github.com/facebookresearch/nevergrad/tree/master/nevergrad/benchmark/experiments.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nevergrad/nevergrad/benchmark/experiments.py,,"def constrained_illconditioned_parallel(seed: tp.Optional[int]=None) -> tp.Iterator[Experiment]:
    """"""Many optimizers on ill cond problems with constraints.""""""
    seedg = create_seed_generator(seed)
    functions = [ArtificialFunction(name, block_dimension=50, rotation=rotation) for name in ['cigar', 'ellipsoid'] for rotation in [True, False]]
    for func in functions:
        func.parametrization.register_cheap_constraint(_Constraint('sum', as_bool=False))
    for function in functions:
        for budget in [400, 4000, 40000]:
            optims: tp.List[str] = get_optimizers('large', seed=next(seedg))
            for optim in optims:
                yield Experiment(function, optim, budget=budget, num_workers=1, seed=next(seedg))","for optim in optims:
    yield Experiment(function, optim, budget=budget, num_workers=1, seed=next(seedg))","for i, optim in enumerate(optims):
    yield Experiment(function, optim, budget=budget, num_workers=1, seed=next(seedg))"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for item in self.data.trainSet_i:
    if len(self.data.trainSet_i[item]) > 1:
        self.itemNet[item] = self.data.trainSet_i[item]","for i, item in enumerate(self.data.trainSet_i):
    if len(self.data.trainSet_i[item]) > 1:
        self.itemNet[item] = self.data.trainSet_i[item]"
deep-learning-for-image-processing,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/retinaNet/backbone/feature_pyramid_network.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep-learning-for-image-processing/pytorch_object_detection/retinaNet/backbone/feature_pyramid_network.py,FeaturePyramidNetwork,"def get_result_from_layer_blocks(self, x: Tensor, idx: int) -> Tensor:
    """"""
        This is equivalent to self.layer_blocks[idx](x),
        but torchscript doesn't support this yet
        """"""
    num_blocks = len(self.layer_blocks)
    if idx < 0:
        idx += num_blocks
    i = 0
    out = x
    for module in self.layer_blocks:
        if i == idx:
            out = module(x)
        i += 1
    return out","for module in self.layer_blocks:
    if i == idx:
        out = module(x)
    i += 1","for i, module in enumerate(self.layer_blocks):
    if i == idx:
        out = module(x)"
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for entry in vOriginTable:
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)","for i, entry in enumerate(vOriginTable):
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)"
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for sel in view.sel():
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))","for i, sel in enumerate(view.sel()):
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))"
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for o in range(kernel.shape[1]):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for o, _ in enumerate(kernel.shape[1]):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)"
jc,https://github.com/kellyjonbrazil/jc/tree/master/jc/parsers/hciconfig.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jc/jc/parsers/hciconfig.py,,"def _process(proc_data):
    """"""
    Final processing to conform to the schema.

    Parameters:

        proc_data:   (List of Dictionaries) raw structured data to process

    Returns:

        List of Dictionaries. Structured data to conform to the schema.
    """"""
    for entry in proc_data:
        int_list = ['acl_mtu', 'acl_mtu_packets', 'sco_mtu', 'sco_mtu_packets', 'rx_bytes', 'rx_acl', 'rx_sco', 'rx_events', 'rx_errors', 'tx_bytes', 'tx_acl', 'tx_sco', 'tx_commands', 'tx_errors']
        for key in entry:
            if key in int_list:
                entry[key] = jc.utils.convert_to_int(entry[key])
        if 'service_classes' in entry and len(entry['service_classes']) == 1 and ('Unspecified' in entry['service_classes']):
            entry['service_classes'] = None
    return proc_data","for entry in proc_data:
    int_list = ['acl_mtu', 'acl_mtu_packets', 'sco_mtu', 'sco_mtu_packets', 'rx_bytes', 'rx_acl', 'rx_sco', 'rx_events', 'rx_errors', 'tx_bytes', 'tx_acl', 'tx_sco', 'tx_commands', 'tx_errors']
    for key in entry:
        if key in int_list:
            entry[key] = jc.utils.convert_to_int(entry[key])
    if 'service_classes' in entry and len(entry['service_classes']) == 1 and ('Unspecified' in entry['service_classes']):
        entry['service_classes'] = None","for i, entry in enumerate(proc_data):
    int_list = ['acl_mtu', 'acl_mtu_packets', 'sco_mtu', 'sco_mtu_packets', 'rx_bytes', 'rx_acl', 'rx_sco', 'rx_events', 'rx_errors', 'tx_bytes', 'tx_acl', 'tx_sco', 'tx_commands', 'tx_errors']
    for key in entry:
        if key in int_list:
            entry[key] = jc.utils.convert_to_int(entry[key])
    if 'service_classes' in entry and len(entry['service_classes']) == 1 and ('Unspecified' in entry['service_classes']):
        entry['service_classes'] = None"
advertools,https://github.com/eliasdabbas/advertools/tree/master/advertools/serp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/advertools/advertools/serp.py,,"def youtube_channel_details(key, channel_ids):
    """"""Return details of channels for which the ids are given.
    Assumes ``ids`` is a comma-separated list of channel ids with
    no spaces.""""""
    base_url = 'https://www.googleapis.com/youtube/v3/channels?part=snippet,contentDetails,statistics'
    channel_ids = _split_by_comma(channel_ids, length=50)
    final_df = pd.DataFrame()
    for channel_id in channel_ids:
        params = {'id': channel_id, 'key': key}
        logging.info(msg='Requesting: ' + 'channel details')
        channel_resp = requests.get(base_url, params=params)
        if channel_resp.status_code >= 400:
            raise Exception(channel_resp.json())
        items_df = pd.DataFrame(channel_resp.json()['items'])
        details = ['snippet', 'statistics', 'contentDetails']
        detail_df = pd.DataFrame()
        for detail in details:
            try:
                detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
            except KeyError:
                continue
        temp_df = pd.concat([items_df, detail_df], axis=1)
        final_df = final_df.append(temp_df, sort=False, ignore_index=True)
    return final_df","for channel_id in channel_ids:
    params = {'id': channel_id, 'key': key}
    logging.info(msg='Requesting: ' + 'channel details')
    channel_resp = requests.get(base_url, params=params)
    if channel_resp.status_code >= 400:
        raise Exception(channel_resp.json())
    items_df = pd.DataFrame(channel_resp.json()['items'])
    details = ['snippet', 'statistics', 'contentDetails']
    detail_df = pd.DataFrame()
    for detail in details:
        try:
            detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
        except KeyError:
            continue
    temp_df = pd.concat([items_df, detail_df], axis=1)
    final_df = final_df.append(temp_df, sort=False, ignore_index=True)","for i, channel_id in enumerate(channel_ids):
    params = {'id': channel_id, 'key': key}
    logging.info(msg='Requesting: ' + 'channel details')
    channel_resp = requests.get(base_url, params=params)
    if channel_resp.status_code >= 400:
        raise Exception(channel_resp.json())
    items_df = pd.DataFrame(channel_resp.json()['items'])
    details = ['snippet', 'statistics', 'contentDetails']
    detail_df = pd.DataFrame()
    for detail in details:
        try:
            detail_df = pd.concat([detail_df, pd.DataFrame([x[detail] for x in channel_resp.json()['items']])], axis=1)
        except KeyError:
            continue
    temp_df = pd.concat([items_df, detail_df], axis=1)
    final_df = final_df.append(temp_df, sort=False, ignore_index=True)"
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/pennylane/qaoa/cycle.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pennylane/pennylane/qaoa/cycle.py,,"def _inner_out_flow_constraint_hamiltonian(graph: Union[nx.DiGraph, rx.PyDiGraph], node: int) -> Hamiltonian:
    """"""Calculates the inner portion of the Hamiltonian in :func:`out_flow_constraint`.
    For a given :math:`i`, this function returns:

    .. math::

        d_{i}^{out}(d_{i}^{out} - 2)\\mathbb{I}
        - 2(d_{i}^{out}-1)\\sum_{j,(i,j)\\in E}\\hat{Z}_{ij} +
        ( \\sum_{j,(i,j)\\in E}\\hat{Z}_{ij} )^{2}

    Args:
        graph (nx.DiGraph or rx.PyDiGraph): the directed graph specifying possible edges
        node: a fixed node

    Returns:
        qml.Hamiltonian: The inner part of the out-flow constraint Hamiltonian.
    """"""
    if not isinstance(graph, (nx.DiGraph, rx.PyDiGraph)):
        raise ValueError(f'Input graph must be a nx.DiGraph or rx.PyDiGraph, got {type(graph).__name__}')
    coeffs = []
    ops = []
    is_rx = isinstance(graph, rx.PyDiGraph)
    get_nvalues = lambda T: (graph.nodes().index(T[0]), graph.nodes().index(T[1])) if is_rx else T
    edges_to_qubits = edges_to_wires(graph)
    out_edges = graph.out_edges(node)
    d = len(out_edges)
    if is_rx:
        out_edges = sorted(out_edges)
    for edge in out_edges:
        if len(edge) > 2:
            edge = tuple(edge[:2])
        wire = (edges_to_qubits[get_nvalues(edge)],)
        coeffs.append(1)
        ops.append(qml.PauliZ(wire))
    (coeffs, ops) = _square_hamiltonian_terms(coeffs, ops)
    for edge in out_edges:
        if len(edge) > 2:
            edge = tuple(edge[:2])
        wire = (edges_to_qubits[get_nvalues(edge)],)
        coeffs.append(-2 * (d - 1))
        ops.append(qml.PauliZ(wire))
    coeffs.append(d * (d - 2))
    ops.append(qml.Identity(0))
    H = Hamiltonian(coeffs, ops)
    H.simplify()
    H.grouping_indices = [list(range(len(H.ops)))]
    return H","for edge in out_edges:
    if len(edge) > 2:
        edge = tuple(edge[:2])
    wire = (edges_to_qubits[get_nvalues(edge)],)
    coeffs.append(-2 * (d - 1))
    ops.append(qml.PauliZ(wire))","for i, edge in enumerate(out_edges):
    if len(edge) > 2:
        edge = tuple(edge[:2])
    wire = (edges_to_qubits[get_nvalues(edge)],)
    coeffs.append(-2 * (d - 1))
    ops.append(qml.PauliZ(wire))"
weblate,https://github.com/WeblateOrg/weblate/tree/master/weblate/utils/classloader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/weblate/weblate/utils/classloader.py,ClassLoader,"def load_data(self):
    result = {}
    value = self.get_settings()
    for path in value:
        obj = load_class(path, self.name)
        if self.construct:
            obj = obj()
        result[obj.get_identifier()] = obj
    return result","for path in value:
    obj = load_class(path, self.name)
    if self.construct:
        obj = obj()
    result[obj.get_identifier()] = obj","for i, path in enumerate(value):
    obj = load_class(path, self.name)
    if self.construct:
        obj = obj()
    result[obj.get_identifier()] = obj"
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for j in range(4):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)","for j, num in enumerate(range(4)):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)"
oppia,https://github.com/oppia/oppia/tree/master/core/controllers/learner_goals_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/controllers/learner_goals_test.py,LearnerGoalsHandlerTests,"def test_add_topic_to_learner_goal(self) -> None:
    self.login(self.VIEWER_EMAIL)
    csrf_token = self.get_new_csrf_token()
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, self.TOPIC_ID_1), {}, csrf_token=csrf_token)
    self.assertEqual(learner_goals_services.get_all_topic_ids_to_learn(self.viewer_id), [self.TOPIC_ID_1])
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, self.TOPIC_ID_2), {}, csrf_token=csrf_token)
    self.assertEqual(learner_goals_services.get_all_topic_ids_to_learn(self.viewer_id), [self.TOPIC_ID_1, self.TOPIC_ID_2])
    learner_progress_services.mark_topic_as_learnt(self.viewer_id, self.TOPIC_ID_3)
    response = self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, self.TOPIC_ID_3), {}, csrf_token=csrf_token)
    self.assertEqual(response['belongs_to_learnt_list'], True)
    self.assertEqual(learner_goals_services.get_all_topic_ids_to_learn(self.viewer_id), [self.TOPIC_ID_1, self.TOPIC_ID_2])
    response = self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, 'InvalidActivityType', self.TOPIC_ID_1), {}, csrf_token=csrf_token, expected_status_int=400)
    self.assertIn(""Received InvalidActivityType which is not in the allowed range of choices: ['learntopic']"", response['error'])
    for topic_id in range(2, feconf.MAX_CURRENT_GOALS_COUNT + 1):
        self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % topic_id), {}, csrf_token=csrf_token)
    response = self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % str(feconf.MAX_CURRENT_GOALS_COUNT + 3)), {}, csrf_token=csrf_token)
    self.assertEqual(response['goals_limit_exceeded'], True)
    self.logout()","for topic_id in range(2, feconf.MAX_CURRENT_GOALS_COUNT + 1):
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % topic_id), {}, csrf_token=csrf_token)","for i, topic_id in enumerate(range(2, feconf.MAX_CURRENT_GOALS_COUNT + 1)):
    self.post_json('%s/%s/%s' % (feconf.LEARNER_GOALS_DATA_URL, constants.ACTIVITY_TYPE_LEARN_TOPIC, 'topic_id_%s' % topic_id), {}, csrf_token=csrf_token)"
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)","for _ in batches:
    if self._should_stop(supervisor):
        return
    results = session.run(train_fetches, train_feed_dict)
    train_loss += results['loss']
    if self.compute_summaries:
        supervisor.summary_computed(session, results['summaries'], results['step'])","for i, batch in enumerate(batches):
    if self._should_stop(supervisor):
        return
    results = session.run(train_fetches, train_feed_dict)
    train_loss += results['loss']
    if self.compute_summaries:
        supervisor.summary_computed(session, results['summaries'], results['step'])"
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for span in spans:
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))","for i, span in enumerate(spans):
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))"
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)","for credential in server_object.credentials.all():
    if credential.username == loginuser:
        port = credential.port
        method = credential.method
        if method == 'password':
            password = credential.password
        else:
            password = credential.password
            key = credential.key","for i, credential in enumerate(server_object.credentials.all()):
    if credential.username == loginuser:
        port = credential.port
        method = credential.method
        if method == 'password':
            password = credential.password
        else:
            password = credential.password
            key = credential.key"
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/doctype/payment_entry/payment_entry.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/doctype/payment_entry/payment_entry.py,PaymentEntry,"def validate_paid_invoices(self):
    no_oustanding_refs = {}
    for d in self.get('references'):
        if not d.allocated_amount:
            continue
        if d.reference_doctype in ('Sales Invoice', 'Purchase Invoice'):
            (outstanding_amount, is_return) = frappe.get_cached_value(d.reference_doctype, d.reference_name, ['outstanding_amount', 'is_return'])
            if outstanding_amount <= 0 and (not is_return):
                no_oustanding_refs.setdefault(d.reference_doctype, []).append(d)
    for (k, v) in no_oustanding_refs.items():
        frappe.msgprint(_('{} - {} now have {} as they had no outstanding amount left before submitting the Payment Entry.').format(_(k), frappe.bold(', '.join((d.reference_name for d in v))), frappe.bold(_('negative outstanding amount'))) + '<br><br>' + _('If this is undesirable please cancel the corresponding Payment Entry.'), title=_('Warning'), indicator='orange')","for (k, v) in no_oustanding_refs.items():
    frappe.msgprint(_('{} - {} now have {} as they had no outstanding amount left before submitting the Payment Entry.').format(_(k), frappe.bold(', '.join((d.reference_name for d in v))), frappe.bold(_('negative outstanding amount'))) + '<br><br>' + _('If this is undesirable please cancel the corresponding Payment Entry.'), title=_('Warning'), indicator='orange')","for i, (k, v) in enumerate(no_oustanding_refs.items()):
    frappe.msgprint(_('{} - {} now have {} as they had no outstanding amount left before submitting the Payment Entry.').format(_(k), frappe.bold(', '.join((d.reference_name for d in v))), frappe.bold(_('negative outstanding amount'))) + '<br><br>' + _('If this is undesirable please cancel the corresponding Payment Entry.'), title=_('Warning'), indicator='orange')"
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","for overwrite in remaining_overwrites:
    if overwrite.is_role() and roles.has(overwrite.id):
        denies |= overwrite.deny
        allows |= overwrite.allow","for i, overwrite in enumerate(remaining_overwrites):
    if overwrite.is_role() and roles.has(overwrite.id):
        denies |= overwrite.deny
        allows |= overwrite.allow"
torchdiffeq,https://github.com/rtqichen/torchdiffeq/tree/master/tests/norm_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchdiffeq/tests/norm_tests.py,TestNorms,"def test_adjoint_norm(self):

    def f(t, x):
        return x
    t = torch.tensor([0.0, 1.0])
    adjoint_params = (torch.rand(7, requires_grad=True), torch.rand((), requires_grad=True))

    def make_spy_on_adjoint_norm(adjoint_norm, actual_norm):
        is_spy_called = [False]

        def spy_on_adjoint_norm(tensor):
            nonlocal is_spy_called
            is_spy_called[0] = True
            norm_result = adjoint_norm(tensor)
            true_norm_result = actual_norm(tensor)
            self.assertIsInstance(norm_result, torch.Tensor)
            self.assertEqual(norm_result.shape, true_norm_result.shape)
            self.assertLess((norm_result - true_norm_result).abs().max(), 1e-06)
            return norm_result
        return (spy_on_adjoint_norm, is_spy_called)
    for shape in ((), (1,), (2, 2)):
        for (use_adjoint_options, seminorm) in ((False, False), (True, False), (True, True)):
            with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
                x0 = torch.full(shape, 1.0)
                if use_adjoint_options:
                    if seminorm:
                        kwargs = dict(adjoint_options=dict(norm='seminorm'))
                    else:
                        kwargs = dict(adjoint_options={})
                else:
                    kwargs = {}
                xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
                _adjoint_norm = xs.grad_fn.adjoint_options['norm']
                is_called = False

                def actual_norm(tensor_tuple):
                    nonlocal is_called
                    is_called = True
                    self.assertIsInstance(tensor_tuple, tuple)
                    (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
                    self.assertEqual(t.shape, ())
                    self.assertEqual(y.shape, shape)
                    self.assertEqual(adj_y.shape, shape)
                    self.assertEqual(adj_param1.shape, (7,))
                    self.assertEqual(adj_param2.shape, ())
                    out = max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt())
                    if not seminorm:
                        out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
                    return out
                (xs.grad_fn.adjoint_options['norm'], is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
                xs.sum().backward()
                self.assertTrue(is_called)
                self.assertTrue(is_spy_called[0])
    for (use_adjoint_options, seminorm) in ((False, False), (True, False), (True, True)):
        with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
            x0 = (torch.tensor(1.0), torch.tensor([[0.5, 0.5], [0.1, 0.1]]))
            if use_adjoint_options:
                if seminorm:
                    kwargs = dict(adjoint_options=dict(norm='seminorm'))
                else:
                    kwargs = dict(adjoint_options={})
            else:
                kwargs = {}
            xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
            adjoint_options_dict = xs[0].grad_fn.next_functions[0][0].next_functions[0][0].adjoint_options
            _adjoint_norm = adjoint_options_dict['norm']
            is_called = False

            def actual_norm(tensor_tuple):
                nonlocal is_called
                is_called = True
                self.assertIsInstance(tensor_tuple, tuple)
                (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
                self.assertEqual(t.shape, ())
                self.assertEqual(y.shape, (5,))
                self.assertEqual(adj_y.shape, (5,))
                self.assertEqual(adj_param1.shape, (7,))
                self.assertEqual(adj_param2.shape, ())
                ya = y[0]
                yb = y[1:]
                adj_ya = adj_y[0]
                adj_yb = adj_y[1:4]
                out = max(t.abs(), ya.abs(), yb.pow(2).mean().sqrt(), adj_ya.abs(), adj_yb.pow(2).mean().sqrt())
                if not seminorm:
                    out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
                return out
            (spy_on_adjoint_norm, is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
            adjoint_options_dict['norm'] = spy_on_adjoint_norm
            xs[0].sum().backward()
            self.assertTrue(is_called)
            self.assertTrue(is_spy_called[0])
    is_called = False

    def adjoint_norm(tensor_tuple):
        nonlocal is_called
        is_called = True
        self.assertIsInstance(tensor_tuple, tuple)
        (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
        self.assertEqual(t.shape, ())
        self.assertEqual(y.shape, ())
        self.assertEqual(adj_y.shape, ())
        self.assertEqual(adj_param1.shape, (7,))
        self.assertEqual(adj_param2.shape, ())
        return max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt(), adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
    x0 = torch.tensor(1.0)
    xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, adjoint_options=dict(norm=adjoint_norm))
    xs.sum().backward()
    self.assertTrue(is_called)
    is_called = False

    def adjoint_norm(tensor_tuple):
        nonlocal is_called
        is_called = True
        self.assertIsInstance(tensor_tuple, tuple)
        (t, ya, yb, adj_ya, adj_yb, adj_param1, adj_param2) = tensor_tuple
        self.assertEqual(t.shape, ())
        self.assertEqual(ya.shape, ())
        self.assertEqual(yb.shape, (2, 2))
        self.assertEqual(adj_ya.shape, ())
        self.assertEqual(adj_yb.shape, (2, 2))
        self.assertEqual(adj_param1.shape, (7,))
        self.assertEqual(adj_param2.shape, ())
        return max(t.abs(), ya.abs(), yb.pow(2).mean().sqrt(), adj_ya.abs(), adj_yb.pow(2).mean().sqrt(), adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
    x0 = (torch.tensor(1.0), torch.tensor([[0.5, 0.5], [0.1, 0.1]]))
    xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, adjoint_options=dict(norm=adjoint_norm))
    xs[0].sum().backward()
    self.assertTrue(is_called)","for (use_adjoint_options, seminorm) in ((False, False), (True, False), (True, True)):
    with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
        x0 = torch.full(shape, 1.0)
        if use_adjoint_options:
            if seminorm:
                kwargs = dict(adjoint_options=dict(norm='seminorm'))
            else:
                kwargs = dict(adjoint_options={})
        else:
            kwargs = {}
        xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
        _adjoint_norm = xs.grad_fn.adjoint_options['norm']
        is_called = False

        def actual_norm(tensor_tuple):
            nonlocal is_called
            is_called = True
            self.assertIsInstance(tensor_tuple, tuple)
            (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
            self.assertEqual(t.shape, ())
            self.assertEqual(y.shape, shape)
            self.assertEqual(adj_y.shape, shape)
            self.assertEqual(adj_param1.shape, (7,))
            self.assertEqual(adj_param2.shape, ())
            out = max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt())
            if not seminorm:
                out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
            return out
        (xs.grad_fn.adjoint_options['norm'], is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
        xs.sum().backward()
        self.assertTrue(is_called)
        self.assertTrue(is_spy_called[0])","for i, (use_adjoint_options, seminorm) in enumerate(((False, False), (True, False), (True, True))):
    with self.subTest(shape=shape, use_adjoint_options=use_adjoint_options, seminorm=seminorm):
        x0 = torch.full(shape, 1.0)
        if use_adjoint_options:
            if seminorm:
                kwargs = dict(adjoint_options=dict(norm='seminorm'))
            else:
                kwargs = dict(adjoint_options={})
        else:
            kwargs = {}
        xs = torchdiffeq.odeint_adjoint(f, x0, t, adjoint_params=adjoint_params, **kwargs)
        _adjoint_norm = xs.grad_fn.adjoint_options['norm']
        is_called = False

        def actual_norm(tensor_tuple):
            nonlocal is_called
            is_called = True
            self.assertIsInstance(tensor_tuple, tuple)
            (t, y, adj_y, adj_param1, adj_param2) = tensor_tuple
            self.assertEqual(t.shape, ())
            self.assertEqual(y.shape, shape)
            self.assertEqual(adj_y.shape, shape)
            self.assertEqual(adj_param1.shape, (7,))
            self.assertEqual(adj_param2.shape, ())
            out = max(t.abs(), y.pow(2).mean().sqrt(), adj_y.pow(2).mean().sqrt())
            if not seminorm:
                out = max(out, adj_param1.pow(2).mean().sqrt(), adj_param2.abs())
            return out
        (xs.grad_fn.adjoint_options['norm'], is_spy_called) = make_spy_on_adjoint_norm(_adjoint_norm, actual_norm)
        xs.sum().backward()
        self.assertTrue(is_called)
        self.assertTrue(is_spy_called[0])"
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})","for file_id in ids:
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})","for i, file_id in enumerate(ids):
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/md_gender/yelp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/md_gender/yelp.py,YelpTeacher,"def _check_data_downloaded(self, opt):
    RESET = '\x1b[0m'
    RED = '\x1b[1;91m'
    YELLOW = '\x1b[1;93m'
    GREEN = '\x1b[1;92m'
    BLUE = '\x1b[1;96m'
    CYAN = '\x1b[1;94m'
    MAGENTA = '\x1b[1;95m'
    USE_COLORS = _sys.stdout.isatty()
    if not USE_COLORS:
        RESET = RED = YELLOW = GREEN = BLUE = CYAN = MAGENTA = ''
    rainbow = [RED, YELLOW, GREEN, CYAN, BLUE, MAGENTA]
    size = 78 // len(rainbow)
    stars = ''.join([color + '*' * size for color in rainbow])
    stars += RESET
    self.data_path = os.path.join(opt['datapath'], 'md_gender', 'yelp')
    if not os.path.exists(self.data_path):
        PathManager.mkdirs(self.data_path)
    if not PathManager.exists(os.path.join(self.data_path, 'valid.fader.with_cat.40000')):
        raise RuntimeError(f'\n\n{stars}\nThis data must be downloaded following instructions in the README here:<https://github.com/facebookresearch/MultipleAttributeTextRewriting/blob/main/data/README.md>. \nIt cannot be automatically downloaded, as one must agree to the terms outlined on the website before gaining access to the data.\n\nOnce downloaded, please put the data in the following directory: \n{self.data_path}\n{stars}')
    elif not PathManager.exists(os.path.join(self.data_path, 'classtrain.txt')):
        logging.info('[ Building data ... ]')
        with open(os.path.join(self.data_path, 'classtrain.txt'), 'w') as f:
            for fle_num in [4000, 6000, 8000]:
                train_fle = f'train.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, train_fle)) as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            f.write(f'male\t{text}\n')
                        elif gend == '1':
                            f.write(f'female\t{text}\n')
        for pair in [('dev', 'valid'), ('test', 'test')]:
            with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
                with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
                    for fle_num in [4000, 6000, 8000]:
                        valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                        with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                            lines = g.readlines()
                            for line in lines:
                                tabs = line.split('\t')
                                text = tabs[0]
                                gend = tabs[1]
                                if gend == '0':
                                    masc_val.write(f'{text}\n')
                                elif gend == '1':
                                    fem_val.write(f'{text}\n')","for pair in [('dev', 'valid'), ('test', 'test')]:
    with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
        with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
            for fle_num in [4000, 6000, 8000]:
                valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            masc_val.write(f'{text}\n')
                        elif gend == '1':
                            fem_val.write(f'{text}\n')","for i, pair in enumerate([('dev', 'valid'), ('test', 'test')]):
    with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
        with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
            for fle_num in [4000, 6000, 8000]:
                valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            masc_val.write(f'{text}\n')
                        elif gend == '1':
                            fem_val.write(f'{text}\n')"
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/datasets/coco_hpe1_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/datasets/coco_hpe1_dataset.py,CocoHpe1Dataset,"def _check_load_keypoints(self, coco, entry):
    """"""
        Check and load ground-truth keypoints.
        """"""
    ann_ids = coco.getAnnIds(imgIds=entry['id'], iscrowd=False)
    objs = coco.loadAnns(ann_ids)
    valid_objs = []
    width = entry['width']
    height = entry['height']
    for obj in objs:
        contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
        if contiguous_cid >= self.num_class:
            continue
        if max(obj['keypoints']) == 0:
            continue
        (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
        if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
            continue
        joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
        for i in range(self.num_joints):
            joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
            joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
            visible = min(1, obj['keypoints'][i * 3 + 2])
            joints_3d[i, :2, 1] = visible
        if np.sum(joints_3d[:, 0, 1]) < 1:
            continue
        if self._check_centers:
            (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
            (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
            ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
            if num_vis / 80.0 + 47 / 80.0 > ks:
                continue
        valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})
    if not valid_objs:
        if not self._skip_empty:
            valid_objs.append({'bbox': np.array([-1, -1, 0, 0]), 'joints_3d': np.zeros((self.num_joints, 3, 2), dtype=np.float32)})
    return valid_objs","for i in range(self.num_joints):
    joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
    joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
    visible = min(1, obj['keypoints'][i * 3 + 2])
    joints_3d[i, :2, 1] = visible","for i, _ in enumerate(range(self.num_joints)):
    joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
    joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
    visible = min(1, obj['keypoints'][i * 3 + 2])
    joints_3d[i, :2, 1] = visible"
ChatLearner,https://github.com/bshao001/ChatLearner/tree/master/chatbot/patternutils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ChatLearner/chatbot/patternutils.py,,"def _text2int(text):
    if text.isdigit():
        return int(text)
    num_words = {}
    units = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen']
    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']
    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']
    num_words['and'] = (1, 0)
    for (idx, word) in enumerate(units):
        num_words[word] = (1, idx)
    for (idx, word) in enumerate(tens):
        num_words[word] = (1, idx * 10)
    for (idx, word) in enumerate(scales):
        num_words[word] = (10 ** (idx * 3 or 2), 0)
    current = result = 0
    for word in text.replace('-', ' ').lower().split():
        if word not in num_words:
            return -1
        (scale, increment) = num_words[word]
        current = current * scale + increment
        if scale > 100:
            result += current
            current = 0
    return result + current","for word in text.replace('-', ' ').lower().split():
    if word not in num_words:
        return -1
    (scale, increment) = num_words[word]
    current = current * scale + increment
    if scale > 100:
        result += current
        current = 0","for i, word in enumerate(text.replace('-', ' ').lower().split()):
    if word not in num_words:
        return -1
    (scale, increment) = num_words[word]
    current = current * scale + increment
    if scale > 100:
        result += current
        current = 0"
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_saving_in_empty_folder(self):
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs:
            if isinstance(temp_fs, TempFS):
                self.eopatch.save(temp_fs.root_path)
            else:
                self.eopatch.save('/', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
            subfolder = 'new-subfolder'
            self.eopatch.save('new-subfolder', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for fs_loader in self.filesystem_loaders:
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for i, fs_loader in enumerate(self.filesystem_loaders):
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))"
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/models/v1beta1_priority_level_configuration_condition.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python/kubernetes/client/models/v1beta1_priority_level_configuration_condition.py,V1beta1PriorityLevelConfigurationCondition,"def to_dict(self):
    """"""Returns the model properties as a dict""""""
    result = {}
    for (attr, _) in six.iteritems(self.openapi_types):
        value = getattr(self, attr)
        if isinstance(value, list):
            result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
        elif hasattr(value, 'to_dict'):
            result[attr] = value.to_dict()
        elif isinstance(value, dict):
            result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
        else:
            result[attr] = value
    return result","for (attr, _) in six.iteritems(self.openapi_types):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value","for i, (attr, _) in enumerate(six.iteritems(self.openapi_types)):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for line in lines[name]:
    reddit_dct = json.loads(line)
    if valid_comment(reddit_dct):
        reddit_res = {}
        for k in key_list:
            if k == 'body':
                if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                    reddit_dct[k] = ''
                (txt, url_list) = word_url_tokenize(reddit_dct[k])
                reddit_res[k] = (' '.join(txt.split()), url_list)
            else:
                reddit_res[k] = reddit_dct[k]
        processed_items[name] += [reddit_res]","for i, line in enumerate(lines[name]):
    reddit_dct = json.loads(line)
    if valid_comment(reddit_dct):
        reddit_res = {}
        for k in key_list:
            if k == 'body':
                if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                    reddit_dct[k] = ''
                (txt, url_list) = word_url_tokenize(reddit_dct[k])
                reddit_res[k] = (' '.join(txt.split()), url_list)
            else:
                reddit_res[k] = reddit_dct[k]
        processed_items[name] += [reddit_res]"
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_dragonbones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_dragonbones.py,,"def bone_is_deform_bone(self, bone, sprites):
    for sprite in sprites:
        if sprite.type == 'MESH':
            init_mesh = sprite.data
            meshes = []
            if sprite.coa_type == 'MESH':
                meshes.append(sprite.data)
            elif sprite.coa_type == 'SLOT':
                for slot in sprite.coa_slot:
                    meshes.append(slot.mesh)
            for mesh in meshes:
                sprite.data = mesh
                if sprite.parent_bone == bone.name:
                    sprite.data = init_mesh
                    return True
                if not bone.name in sprite.vertex_groups:
                    break
                else:
                    v_group = sprite.vertex_groups[bone.name]
                    for vert in sprite.data.vertices:
                        try:
                            weight = v_group.weight(vert.index)
                            if weight > 0:
                                sprite.data = init_mesh
                                return True
                        except:
                            pass
            sprite.data = init_mesh
    return False","for slot in sprite.coa_slot:
    meshes.append(slot.mesh)","for i, slot in enumerate(sprite.coa_slot):
    meshes.append(slot.mesh)"
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/quantization/imperative/qat.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/contrib/slim/quantization/imperative/qat.py,ImperativeQuantizeOutputs,"def _gather_output_scale():
    target_ops = []
    for block in program.blocks:
        for op in block.ops:
            if op.type == 'moving_average_abs_max_scale':
                target_ops.append(op)
    for op in target_ops:
        in_var_name = op.input('X')[0]
        out_var_name = op.output('Out')[0]
        block = op.block
        previous_op = utils.find_previous_op(block, in_var_name)
        next_ops = utils.find_next_ops(block, out_var_name)
        out_scale_name = op.output('OutScale')[0]
        out_scale = utils.load_variable_data(scope, out_scale_name)
        out_scale = utils.fp_numpy_to_naive(out_scale)
        if previous_op.type != 'feed':
            res = utils._get_output_name_index(previous_op, in_var_name)
            if res is not None:
                (argname, index) = res
                previous_op._set_attr(argname + str(index) + '_threshold', out_scale)
                previous_op._set_attr('out_threshold', out_scale)
                previous_op._set_attr('with_quant_attr', True)
        for next_op in next_ops:
            next_op._rename_input(out_var_name, in_var_name)
            for i in range(len(fetch_targets)):
                if fetch_targets[i].name == out_var_name:
                    fetch_targets[i] = block.var(in_var_name)","for next_op in next_ops:
    next_op._rename_input(out_var_name, in_var_name)
    for i in range(len(fetch_targets)):
        if fetch_targets[i].name == out_var_name:
            fetch_targets[i] = block.var(in_var_name)","for i, next_op in enumerate(next_ops):
    next_op._rename_input(out_var_name, in_var_name)
    for j, fetch_target in enumerate(fetch_targets):
        if fetch_target.name == out_var_name:
            fetch_targets[j] = block.var(in_var_name)"
kafka-python,https://github.com/dpkp/kafka-python/tree/master/test/test_assignors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kafka-python/test/test_assignors.py,,"def verify_validity_and_balance(subscriptions, assignment):
    """"""
    Verifies that the given assignment is valid with respect to the given subscriptions
    Validity requirements:
    - each consumer is subscribed to topics of all partitions assigned to it, and
    - each partition is assigned to no more than one consumer
    Balance requirements:
    - the assignment is fully balanced (the numbers of topic partitions assigned to consumers differ by at most one), or
    - there is no topic partition that can be moved from one consumer to another with 2+ fewer topic partitions

    :param subscriptions  topic subscriptions of each consumer
    :param assignment: given assignment for balance check
    """"""
    assert six.viewkeys(subscriptions) == six.viewkeys(assignment)
    consumers = sorted(six.viewkeys(assignment))
    for i in range(len(consumers)):
        consumer = consumers[i]
        partitions = assignment[consumer].partitions()
        for partition in partitions:
            assert partition.topic in subscriptions[consumer], 'Error: Partition {} is assigned to consumer {}, but it is not subscribed to topic {}\nSubscriptions: {}\nAssignments: {}'.format(partition, consumers[i], partition.topic, subscriptions, assignment)
        if i == len(consumers) - 1:
            continue
        for j in range(i + 1, len(consumers)):
            other_consumer = consumers[j]
            other_partitions = assignment[other_consumer].partitions()
            partitions_intersection = set(partitions).intersection(set(other_partitions))
            assert partitions_intersection == set(), 'Error: Consumers {} and {} have common partitions assigned to them: {}\nSubscriptions: {}\nAssignments: {}'.format(consumer, other_consumer, partitions_intersection, subscriptions, assignment)
            if abs(len(partitions) - len(other_partitions)) <= 1:
                continue
            assignments_by_topic = group_partitions_by_topic(partitions)
            other_assignments_by_topic = group_partitions_by_topic(other_partitions)
            if len(partitions) > len(other_partitions):
                for topic in six.iterkeys(assignments_by_topic):
                    assert topic not in other_assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(consumer, len(partitions), other_consumer, len(other_partitions), subscriptions, assignment)
            if len(other_partitions) > len(partitions):
                for topic in six.iterkeys(other_assignments_by_topic):
                    assert topic not in assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(other_consumer, len(other_partitions), consumer, len(partitions), subscriptions, assignment)","for j in range(i + 1, len(consumers)):
    other_consumer = consumers[j]
    other_partitions = assignment[other_consumer].partitions()
    partitions_intersection = set(partitions).intersection(set(other_partitions))
    assert partitions_intersection == set(), 'Error: Consumers {} and {} have common partitions assigned to them: {}\nSubscriptions: {}\nAssignments: {}'.format(consumer, other_consumer, partitions_intersection, subscriptions, assignment)
    if abs(len(partitions) - len(other_partitions)) <= 1:
        continue
    assignments_by_topic = group_partitions_by_topic(partitions)
    other_assignments_by_topic = group_partitions_by_topic(other_partitions)
    if len(partitions) > len(other_partitions):
        for topic in six.iterkeys(assignments_by_topic):
            assert topic not in other_assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(consumer, len(partitions), other_consumer, len(other_partitions), subscriptions, assignment)
    if len(other_partitions) > len(partitions):
        for topic in six.iterkeys(other_assignments_by_topic):
            assert topic not in assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(other_consumer, len(other_partitions), consumer, len(partitions), subscriptions, assignment)","for j, consumer in enumerate(consumers[i+1:], i+1):
    other_consumer = consumer
    other_partitions = assignment[other_consumer].partitions()
    partitions_intersection = set(partitions).intersection(set(other_partitions))
    assert partitions_intersection == set(), 'Error: Consumers {} and {} have common partitions assigned to them: {}\nSubscriptions: {}\nAssignments: {}'.format(consumer, other_consumer, partitions_intersection, subscriptions, assignment)
    if abs(len(partitions) - len(other_partitions)) <= 1:
        continue
    assignments_by_topic = group_partitions_by_topic(partitions)
    other_assignments_by_topic = group_partitions_by_topic(other_partitions)
    if len(partitions) > len(other_partitions):
        for topic in six.iterkeys(assignments_by_topic):
            assert topic not in other_assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(consumer, len(partitions), other_consumer, len(other_partitions), subscriptions, assignment)
    if len(other_partitions) > len(partitions):
        for topic in six.iterkeys(other_assignments_by_topic):
            assert topic not in assignments_by_topic, 'Error: Some partitions can be moved from {} ({} partitions) to {} ({} partitions) to achieve a better balance\nSubscriptions: {}\nAssignments: {}'.format(other_consumer, len(other_partitions), consumer, len(partitions), subscriptions, assignment)"
sympy,https://github.com/sympy/sympy/tree/master/sympy/series/formal.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/series/formal.py,,"def simpleDE(f, x, g, order=4):
    """"""
    Generates simple DE.

    Explanation
    ===========

    DE is of the form

    .. math::
        f^k(x) + \\sum\\limits_{j=0}^{k-1} A_j f^j(x) = 0

    where :math:`A_j` should be rational function in x.

    Generates DE's upto order 4 (default). DE's can also have free parameters.

    By increasing order, higher order DE's can be found.

    Yields a tuple of (DE, order).
    """"""
    from sympy.solvers.solveset import linsolve
    a = symbols('a:%d' % order)

    def _makeDE(k):
        eq = f.diff(x, k) + Add(*[a[i] * f.diff(x, i) for i in range(0, k)])
        DE = g(x).diff(x, k) + Add(*[a[i] * g(x).diff(x, i) for i in range(0, k)])
        return (eq, DE)
    found = False
    for k in range(1, order + 1):
        (eq, DE) = _makeDE(k)
        eq = eq.expand()
        terms = eq.as_ordered_terms()
        ind = rational_independent(terms, x)
        if found or len(ind) == k:
            sol = dict(zip(a, (i for s in linsolve(ind, a[:k]) for i in s)))
            if sol:
                found = True
                DE = DE.subs(sol)
            DE = DE.as_numer_denom()[0]
            DE = DE.factor().as_coeff_mul(Derivative)[1][0]
            yield (DE.collect(Derivative(g(x))), k)","for k in range(1, order + 1):
    (eq, DE) = _makeDE(k)
    eq = eq.expand()
    terms = eq.as_ordered_terms()
    ind = rational_independent(terms, x)
    if found or len(ind) == k:
        sol = dict(zip(a, (i for s in linsolve(ind, a[:k]) for i in s)))
        if sol:
            found = True
            DE = DE.subs(sol)
        DE = DE.as_numer_denom()[0]
        DE = DE.factor().as_coeff_mul(Derivative)[1][0]
        yield (DE.collect(Derivative(g(x))), k)","for k, num in enumerate(range(1, order + 1), start=1):
    (eq, DE) = _makeDE(k)
    eq = eq.expand()
    terms = eq.as_ordered_terms()
    ind = rational_independent(terms, x)
    if found or len(ind) == k:
        sol = dict(zip(a, (i for s in linsolve(ind, a[:k]) for i in s)))
        if sol:
            found = True
            DE = DE.subs(sol)
        DE = DE.as_numer_denom()[0]
        DE = DE.factor().as_coeff_mul(Derivative)[1][0]
        yield (DE.collect(Derivative(g(x))), k)"
cmake_format,https://github.com/cheshirekow/cmake_format/tree/master/cmakelang/tools/bump_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cmake_format/cmakelang/tools/bump_version.py,,"def main():
    fields = ['major', 'minor', 'patch', 'dev', 'drop-dev']
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('field', nargs='?', choices=fields, default='dev')
    args = parser.parse_args()
    thisdir = os.path.dirname(os.path.realpath(__file__))
    rootdir = os.path.dirname(os.path.dirname(thisdir))
    init_path = os.path.join(rootdir, 'cmakelang/__init__.py')
    current_version = get_current_version(init_path)
    if args.field == 'drop-dev':
        new_version = current_version[:3]
    else:
        field_idx = fields.index(args.field)
        new_version = list(current_version)
        new_version[field_idx] += 1
        for idx in range(field_idx + 1, len(new_version)):
            new_version[idx] = 0
    process_init(init_path, new_version)
    process_installation_rst(os.path.join(rootdir, 'cmakelang/doc/installation.rst'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package.json'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package-lock.json'), new_version)
    return 0","for idx in range(field_idx + 1, len(new_version)):
    new_version[idx] = 0","for idx, version in enumerate(new_version[field_idx + 1:], start=field_idx + 1):
    new_version[idx] = 0"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/api/visualizations.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/galaxy/api/visualizations.py,VisualizationsController,"def index(self, trans: GalaxyWebTransaction, **kwargs):
    """"""
        GET /api/visualizations:
        """"""
    rval = []
    user = trans.user
    visualizations = self.get_visualizations_by_user(trans, user)
    visualizations += self.get_visualizations_shared_with_user(trans, user)
    visualizations += self.get_published_visualizations(trans, exclude_user=user)
    for visualization in visualizations:
        item = self.get_visualization_summary_dict(visualization)
        item = trans.security.encode_dict_ids(item)
        item['url'] = web.url_for('visualization', id=item['id'])
        rval.append(item)
    return rval","for visualization in visualizations:
    item = self.get_visualization_summary_dict(visualization)
    item = trans.security.encode_dict_ids(item)
    item['url'] = web.url_for('visualization', id=item['id'])
    rval.append(item)","for i, visualization in enumerate(visualizations):
    item = self.get_visualization_summary_dict(visualization)
    item = trans.security.encode_dict_ids(item)
    item['url'] = web.url_for('visualization', id=item['id'])
    rval.append(item)"
UNetPlusPlus,https://github.com/MrGiovanni/UNetPlusPlus/tree/master/pytorch/nnunet/preprocessing/sanity_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UNetPlusPlus/pytorch/nnunet/preprocessing/sanity_checks.py,,"def verify_dataset_integrity(folder):
    """"""
    folder needs the imagesTr, imagesTs and labelsTr subfolders. There also needs to be a dataset.json
    checks if all training cases and labels are present
    checks if all test cases (if any) are present
    for each case, checks whether all modalities apre present
    for each case, checks whether the pixel grids are aligned
    checks whether the labels really only contain values they should
    :param folder:
    :return:
    """"""
    assert isfile(join(folder, 'dataset.json')), 'There needs to be a dataset.json file in folder, folder=%s' % folder
    assert isdir(join(folder, 'imagesTr')), 'There needs to be a imagesTr subfolder in folder, folder=%s' % folder
    assert isdir(join(folder, 'labelsTr')), 'There needs to be a labelsTr subfolder in folder, folder=%s' % folder
    dataset = load_json(join(folder, 'dataset.json'))
    training_cases = dataset['training']
    num_modalities = len(dataset['modality'].keys())
    test_cases = dataset['test']
    expected_train_identifiers = [i['image'].split('/')[-1][:-7] for i in training_cases]
    expected_test_identifiers = [i.split('/')[-1][:-7] for i in test_cases]
    nii_files_in_imagesTr = subfiles(join(folder, 'imagesTr'), suffix='.nii.gz', join=False)
    nii_files_in_labelsTr = subfiles(join(folder, 'labelsTr'), suffix='.nii.gz', join=False)
    label_files = []
    geometries_OK = True
    has_nan = False
    if len(expected_train_identifiers) != len(np.unique(expected_train_identifiers)):
        raise RuntimeError('found duplicate training cases in dataset.json')
    print('Verifying training set')
    for c in expected_train_identifiers:
        print('checking case', c)
        expected_label_file = join(folder, 'labelsTr', c + '.nii.gz')
        label_files.append(expected_label_file)
        expected_image_files = [join(folder, 'imagesTr', c + '_%04.0d.nii.gz' % i) for i in range(num_modalities)]
        assert isfile(expected_label_file), 'could not find label file for case %s. Expected file: \n%s' % (c, expected_label_file)
        assert all([isfile(i) for i in expected_image_files]), 'some image files are missing for case %s. Expected files:\n %s' % (c, expected_image_files)
        label_itk = sitk.ReadImage(expected_label_file)
        nans_in_seg = np.any(np.isnan(sitk.GetArrayFromImage(label_itk)))
        has_nan = has_nan | nans_in_seg
        if nans_in_seg:
            print('There are NAN values in segmentation %s' % expected_label_file)
        images_itk = [sitk.ReadImage(i) for i in expected_image_files]
        for (i, img) in enumerate(images_itk):
            nans_in_image = np.any(np.isnan(sitk.GetArrayFromImage(img)))
            has_nan = has_nan | nans_in_image
            same_geometry = verify_same_geometry(img, label_itk)
            if not same_geometry:
                geometries_OK = False
                print('The geometry of the image %s does not match the geometry of the label file. The pixel arrays will not be aligned and nnU-Net cannot use this data. Please make sure your image modalities are coregistered and have the same geometry as the label' % expected_image_files[0][:-12])
            if nans_in_image:
                print('There are NAN values in image %s' % expected_image_files[i])
        for i in expected_image_files:
            nii_files_in_imagesTr.remove(os.path.basename(i))
        nii_files_in_labelsTr.remove(os.path.basename(expected_label_file))
    assert len(nii_files_in_imagesTr) == 0, 'there are training cases in imagesTr that are not listed in dataset.json: %s' % nii_files_in_imagesTr
    assert len(nii_files_in_labelsTr) == 0, 'there are training cases in labelsTr that are not listed in dataset.json: %s' % nii_files_in_labelsTr
    print('Verifying label values')
    expected_labels = list((int(i) for i in dataset['labels'].keys()))
    p = Pool(default_num_threads)
    results = p.starmap(verify_contains_only_expected_labels, zip(label_files, [expected_labels] * len(label_files)))
    p.close()
    p.join()
    fail = False
    print('Expected label values are', expected_labels)
    for (i, r) in enumerate(results):
        if not r[0]:
            print('Unexpected labels found in file %s. Found these unexpected values (they should not be there) %s' % (label_files[i], r[1]))
            fail = True
    if fail:
        raise AssertionError('Found unexpected labels in the training dataset. Please correct that or adjust your dataset.json accordingly')
    else:
        print('Labels OK')
    if len(expected_test_identifiers) > 0:
        print('Verifying test set')
        nii_files_in_imagesTs = subfiles(join(folder, 'imagesTs'), suffix='.nii.gz', join=False)
        for c in expected_test_identifiers:
            expected_image_files = [join(folder, 'imagesTs', c + '_%04.0d.nii.gz' % i) for i in range(num_modalities)]
            assert all([isfile(i) for i in expected_image_files]), 'some image files are missing for case %s. Expected files:\n %s' % (c, expected_image_files)
            if num_modalities > 1:
                images_itk = [sitk.ReadImage(i) for i in expected_image_files]
                reference_img = images_itk[0]
                for (i, img) in enumerate(images_itk[1:]):
                    assert verify_same_geometry(img, reference_img), 'The modalities of the image %s do not seem to be registered. Please coregister your modalities.' % expected_image_files[i]
            for i in expected_image_files:
                nii_files_in_imagesTs.remove(os.path.basename(i))
        assert len(nii_files_in_imagesTs) == 0, 'there are training cases in imagesTs that are not listed in dataset.json: %s' % nii_files_in_imagesTr
    (all_same, unique_orientations) = verify_all_same_orientation(join(folder, 'imagesTr'))
    if not all_same:
        print('WARNING: Not all images in the dataset have the same axis ordering. We very strongly recommend you correct that by reorienting the data. fslreorient2std should do the trick')
    if not geometries_OK:
        raise Warning('GEOMETRY MISMATCH FOUND! CHECK THE TEXT OUTPUT! This does not cause an error at this point  but you should definitely check whether your geometries are alright!')
    else:
        print('Dataset OK')
    if has_nan:
        raise RuntimeError('Some images have nan values in them. This will break the training. See text output above to see which ones')","for i in expected_image_files:
    nii_files_in_imagesTs.remove(os.path.basename(i))","for i, file in enumerate(expected_image_files):
    nii_files_in_imagesTs.remove(os.path.basename(file))"
python-shortcuts,https://github.com/alexander-akhmetov/python-shortcuts/tree/master/shortcuts/loader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-shortcuts/shortcuts/loader.py,WFVariableStringField,"def deserialized_data(self) -> str:
    """"""
        Raises:
            shortcuts.exceptions.UnknownVariableError: if variable's type is not supported
        """"""
    value = self._data['Value']
    value_string = value['string']
    positions = {}
    supported_types = list(SYSTEM_VARIABLES_TYPE_TO_VAR.keys()) + ['Variable']
    for (variable_range, variable_data) in value['attachmentsByRange'].items():
        if variable_data['Type'] not in supported_types:
            raise exceptions.UnknownVariableError(f""Unknown variable type: {variable_data['Type']} (possibly it is a magic variable)"")
        variable_type = variable_data['Type']
        if variable_type == 'Variable':
            variable_name = variable_data['VariableName']
        elif variable_type in SYSTEM_VARIABLES_TYPE_TO_VAR:
            variable_name = SYSTEM_VARIABLES_TYPE_TO_VAR[variable_type]
        position = self._get_position(variable_range)
        positions[position] = '{{%s}}' % variable_name
    offset = 0
    for (pos, variable) in collections.OrderedDict(sorted(positions.items())).items():
        value_string = value_string[:pos + offset] + variable + value_string[pos + offset:]
        offset += len(variable)
    return value_string","for (variable_range, variable_data) in value['attachmentsByRange'].items():
    if variable_data['Type'] not in supported_types:
        raise exceptions.UnknownVariableError(f""Unknown variable type: {variable_data['Type']} (possibly it is a magic variable)"")
    variable_type = variable_data['Type']
    if variable_type == 'Variable':
        variable_name = variable_data['VariableName']
    elif variable_type in SYSTEM_VARIABLES_TYPE_TO_VAR:
        variable_name = SYSTEM_VARIABLES_TYPE_TO_VAR[variable_type]
    position = self._get_position(variable_range)
    positions[position] = '{{%s}}' % variable_name","for i, (variable_range, variable_data) in enumerate(value['attachmentsByRange'].items()):
    if variable_data['Type'] not in supported_types:
        raise exceptions.UnknownVariableError(f""Unknown variable type: {variable_data['Type']} (possibly it is a magic variable)"")
    variable_type = variable_data['Type']
    if variable_type == 'Variable':
        variable_name = variable_data['VariableName']
    elif variable_type in SYSTEM_VARIABLES_TYPE_TO_VAR:
        variable_name = SYSTEM_VARIABLES_TYPE_TO_VAR[variable_type]
    position = self._get_position(variable_range)
    positions[position] = '{{%s}}' % variable_name"
hyperpose,https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pifpaf/eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyperpose/hyperpose/Model/pifpaf/eval.py,,"def visualize(img, img_id, processed_img, pd_pif_maps, pd_paf_maps, humans, stride=8, save_dir='./save_dir'):
    print(f'{len(humans)} human found!')
    print('visualizing...')
    os.makedirs(save_dir, exist_ok=True)
    ori_img = np.clip(img * 255.0, 0.0, 255.0).astype(np.uint8)
    processed_img = np.clip(processed_img * 255.0, 0.0, 255.0).astype(np.uint8)
    vis_img = ori_img.copy()
    for human in humans:
        vis_img = human.draw_human(vis_img)
    (pd_pif_conf, pd_pif_vec, _, pd_pif_scale) = pd_pif_maps
    (pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, _, _, _, _) = pd_paf_maps
    pd_pif_conf_show = np.amax(pd_pif_conf, axis=0)
    pd_pif_hr_conf_show = np.amax(get_hr_conf(pd_pif_conf, pd_pif_vec, pd_pif_scale, stride=stride, thresh=0.1), axis=0)
    pd_paf_conf_show = np.amax(pd_paf_conf, axis=0)
    pd_paf_vec_show = np.zeros(shape=(pd_pif_hr_conf_show.shape[0], pd_pif_hr_conf_show.shape[1], 3)).astype(np.int8)
    pd_paf_vec_show = get_arrow_map(pd_paf_vec_show, pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, thresh=0.1)
    fig = plt.figure(figsize=(12, 12))
    a = fig.add_subplot(3, 3, 1)
    a.set_title('input image')
    plt.imshow(ori_img)
    a = fig.add_subplot(3, 3, 3)
    a.set_title('output result')
    plt.imshow(vis_img)
    a = fig.add_subplot(3, 3, 4)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 5)
    a.set_title('pif_conf_map')
    plt.imshow(pd_pif_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 6)
    a.set_title('pif_hr_conf_map')
    plt.imshow(pd_pif_hr_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 7)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 8)
    a.set_title('paf_conf_map')
    plt.imshow(pd_paf_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 9)
    a.set_title('paf_vec_map')
    plt.imshow(pd_paf_vec_show, alpha=0.8)
    plt.colorbar()
    plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))
    plt.close()","for human in humans:
    vis_img = human.draw_human(vis_img)","for i, human in enumerate(humans):
    vis_img = human.draw_human(vis_img)"
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resnet_cub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/resnet_cub.py,,"def _test():
    import numpy as np
    import chainer
    chainer.global_config.train = False
    pretrained = False
    models = [resnet10_cub, resnet12_cub, resnet14_cub, resnetbc14b_cub, resnet16_cub, resnet18_cub, resnet26_cub, resnetbc26b_cub, resnet34_cub, resnetbc38b_cub, resnet50_cub, resnet50b_cub, resnet101_cub, resnet101b_cub, resnet152_cub, resnet152b_cub, resnet200_cub, resnet200b_cub]
    for model in models:
        net = model(pretrained=pretrained)
        weight_count = net.count_params()
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resnet10_cub or weight_count == 5008392
        assert model != resnet12_cub or weight_count == 5082376
        assert model != resnet14_cub or weight_count == 5377800
        assert model != resnetbc14b_cub or weight_count == 8425736
        assert model != resnet16_cub or weight_count == 6558472
        assert model != resnet18_cub or weight_count == 11279112
        assert model != resnet26_cub or weight_count == 17549832
        assert model != resnetbc26b_cub or weight_count == 14355976
        assert model != resnet34_cub or weight_count == 21387272
        assert model != resnetbc38b_cub or weight_count == 20286216
        assert model != resnet50_cub or weight_count == 23917832
        assert model != resnet50b_cub or weight_count == 23917832
        assert model != resnet101_cub or weight_count == 42909960
        assert model != resnet101b_cub or weight_count == 42909960
        assert model != resnet152_cub or weight_count == 58553608
        assert model != resnet152b_cub or weight_count == 58553608
        assert model != resnet200_cub or weight_count == 63034632
        assert model != resnet200b_cub or weight_count == 63034632
        x = np.zeros((1, 3, 224, 224), np.float32)
        y = net(x)
        assert y.shape == (1, 200)","for model in models:
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)","for i, model in enumerate(models):
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)"
nilearn,https://github.com/nilearn/nilearn/tree/master/nilearn/interfaces/fmriprep/load_confounds_strategy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nilearn/nilearn/interfaces/fmriprep/load_confounds_strategy.py,,"def _update_user_inputs(kwargs, default_parameters, check_parameters):
    """"""Update keyword parameters with user inputs if applicable.""""""
    parameters = default_parameters.copy()
    not_needed = []
    for key in check_parameters:
        value = kwargs.pop(key, None)
        if value is not None:
            parameters[key] = value
        if key == 'global_signal':
            if isinstance(value, str):
                parameters['strategy'] += ('global_signal',)
            else:
                parameters.pop('global_signal', None)
    not_needed = list(kwargs.keys())
    return (parameters, not_needed)","for key in check_parameters:
    value = kwargs.pop(key, None)
    if value is not None:
        parameters[key] = value
    if key == 'global_signal':
        if isinstance(value, str):
            parameters['strategy'] += ('global_signal',)
        else:
            parameters.pop('global_signal', None)","for i, key in enumerate(check_parameters):
    value = kwargs.pop(key, None)
    if value is not None:
        parameters[key] = value
    if key == 'global_signal':
        if isinstance(value, str):
            parameters['strategy'] += ('global_signal',)
        else:
            parameters.pop('global_signal', None)"
p5,https://github.com/p5py/p5/tree/master/p5/visualTests/sanityTests/2DSanityTests/test32.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/p5/p5/visualTests/sanityTests/2DSanityTests/test32.py,,"def setup():
    global colors
    size(640, 360)
    no_stroke()
    color_mode('HSB', numChars)
    background(numChars / 2)
    background(0)
    for i in range(numChars):
        colors.append(Color(i, numChars, numChars))","for i in range(numChars):
    colors.append(Color(i, numChars, numChars))","for i, char in enumerate(range(numChars)):
    colors.append(Color(i, numChars, numChars))"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/repository_dependencies/repository_dependency_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/repository_dependencies/repository_dependency_manager.py,RepositoryDependencyInstallManager,"def create_repository_dependency_objects(self, tool_path, tool_shed_url, repo_info_dicts, install_repository_dependencies=False, no_changes_checked=False, tool_panel_section_id=None, new_tool_panel_section_label=None):
    """"""
        Discover all repository dependencies and make sure all tool_shed_repository and
        associated repository_dependency records exist as well as the dependency relationships
        between installed repositories.  This method is called when uninstalled repositories
        are being reinstalled.  If the user elected to install repository dependencies, all
        items in the all_repo_info_dicts list will be processed.  However, if repository
        dependencies are not to be installed, only those items contained in the received
        repo_info_dicts list will be processed.
        """"""
    install_model = self.app.install_model
    log.debug('Creating repository dependency objects...')
    all_created_or_updated_tool_shed_repositories = []
    created_or_updated_tool_shed_repositories = []
    tool_panel_section_keys = []
    filtered_repo_info_dicts = []
    all_required_repo_info_dict = self.get_required_repo_info_dicts(tool_shed_url, repo_info_dicts)
    all_repo_info_dicts = all_required_repo_info_dict.get('all_repo_info_dicts', [])
    if not all_repo_info_dicts:
        all_repo_info_dicts = [rid for rid in repo_info_dicts]
    for repo_info_dict in all_repo_info_dicts:
        if self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts) or install_repository_dependencies:
            for (name, repo_info_tuple) in repo_info_dict.items():
                can_update_db_record = False
                clear_metadata = True
                (description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
                (repository_db_record, installed_changeset_revision) = repository_util.repository_was_previously_installed(self.app, tool_shed_url, name, repo_info_tuple, from_tip=False)
                if repository_db_record:
                    if installed_changeset_revision != changeset_revision and repository_db_record.status == install_model.ToolShedRepository.installation_status.INSTALLED:
                        log.info(""Repository '%s' already present at revision %s, will be updated to revision %s"", repository_db_record.name, installed_changeset_revision, changeset_revision)
                        can_update_db_record = True
                        clear_metadata = False
                    elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.INSTALLED, install_model.ToolShedRepository.installation_status.CLONING, install_model.ToolShedRepository.installation_status.SETTING_TOOL_VERSIONS, install_model.ToolShedRepository.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES, install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES, install_model.ToolShedRepository.installation_status.LOADING_PROPRIETARY_DATATYPES]:
                        info_msg = ""Skipping installation of revision %s of repository '%s' because it was installed "" % (changeset_revision, repository_db_record.name)
                        info_msg += ""with the (possibly updated) revision %s and its current installation status is '%s'."" % (installed_changeset_revision, repository_db_record.status)
                        log.info(info_msg)
                        can_update_db_record = False
                    elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.ERROR, install_model.ToolShedRepository.installation_status.NEW, install_model.ToolShedRepository.installation_status.UNINSTALLED]:
                        name = repository_db_record.name
                        installed_changeset_revision = repository_db_record.installed_changeset_revision
                        can_update_db_record = True
                    elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.DEACTIVATED]:
                        log.info(f""Reactivating deactivated tool_shed_repository '{str(repository_db_record.name)}'."")
                        self.app.installed_repository_manager.activate_repository(repository_db_record)
                        can_update_db_record = False
                    elif repository_db_record.status not in [install_model.ToolShedRepository.installation_status.NEW]:
                        changeset_revision = repository_db_record.installed_changeset_revision
                        self.reset_previously_installed_repository(repository_db_record)
                        can_update_db_record = True
                else:
                    installed_changeset_revision = changeset_revision
                    can_update_db_record = True
                if can_update_db_record:
                    tpm = tool_panel_manager.ToolPanelManager(self.app)
                    if repository_db_record and repository_db_record.metadata_:
                        (_, tool_panel_section_key) = tpm.handle_tool_panel_selection(toolbox=self.app.toolbox, metadata=repository_db_record.metadata_, no_changes_checked=no_changes_checked, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
                    else:
                        (tool_panel_section_key, _) = tpm.handle_tool_panel_section(self.app.toolbox, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
                    metadata_dict = {} if clear_metadata else None
                    current_changeset_revision = changeset_revision if clear_metadata else None
                    tool_shed_repository = repository_util.create_or_update_tool_shed_repository(app=self.app, name=name, description=description, installed_changeset_revision=installed_changeset_revision, ctx_rev=ctx_rev, repository_clone_url=repository_clone_url, status=install_model.ToolShedRepository.installation_status.NEW, metadata_dict=metadata_dict, current_changeset_revision=current_changeset_revision, owner=repository_owner, dist_to_shed=False)
                    if tool_shed_repository not in all_created_or_updated_tool_shed_repositories:
                        all_created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                    if install_repository_dependencies or self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts):
                        if tool_shed_repository not in created_or_updated_tool_shed_repositories:
                            created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                            tool_panel_section_keys.append(tool_panel_section_key)
                            filtered_repo_info_dicts.append(repo_info_dict)
    self.build_repository_dependency_relationships(all_repo_info_dicts, all_created_or_updated_tool_shed_repositories)
    return (created_or_updated_tool_shed_repositories, tool_panel_section_keys, all_repo_info_dicts, filtered_repo_info_dicts)","for (name, repo_info_tuple) in repo_info_dict.items():
    can_update_db_record = False
    clear_metadata = True
    (description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
    (repository_db_record, installed_changeset_revision) = repository_util.repository_was_previously_installed(self.app, tool_shed_url, name, repo_info_tuple, from_tip=False)
    if repository_db_record:
        if installed_changeset_revision != changeset_revision and repository_db_record.status == install_model.ToolShedRepository.installation_status.INSTALLED:
            log.info(""Repository '%s' already present at revision %s, will be updated to revision %s"", repository_db_record.name, installed_changeset_revision, changeset_revision)
            can_update_db_record = True
            clear_metadata = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.INSTALLED, install_model.ToolShedRepository.installation_status.CLONING, install_model.ToolShedRepository.installation_status.SETTING_TOOL_VERSIONS, install_model.ToolShedRepository.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES, install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES, install_model.ToolShedRepository.installation_status.LOADING_PROPRIETARY_DATATYPES]:
            info_msg = ""Skipping installation of revision %s of repository '%s' because it was installed "" % (changeset_revision, repository_db_record.name)
            info_msg += ""with the (possibly updated) revision %s and its current installation status is '%s'."" % (installed_changeset_revision, repository_db_record.status)
            log.info(info_msg)
            can_update_db_record = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.ERROR, install_model.ToolShedRepository.installation_status.NEW, install_model.ToolShedRepository.installation_status.UNINSTALLED]:
            name = repository_db_record.name
            installed_changeset_revision = repository_db_record.installed_changeset_revision
            can_update_db_record = True
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.DEACTIVATED]:
            log.info(f""Reactivating deactivated tool_shed_repository '{str(repository_db_record.name)}'."")
            self.app.installed_repository_manager.activate_repository(repository_db_record)
            can_update_db_record = False
        elif repository_db_record.status not in [install_model.ToolShedRepository.installation_status.NEW]:
            changeset_revision = repository_db_record.installed_changeset_revision
            self.reset_previously_installed_repository(repository_db_record)
            can_update_db_record = True
    else:
        installed_changeset_revision = changeset_revision
        can_update_db_record = True
    if can_update_db_record:
        tpm = tool_panel_manager.ToolPanelManager(self.app)
        if repository_db_record and repository_db_record.metadata_:
            (_, tool_panel_section_key) = tpm.handle_tool_panel_selection(toolbox=self.app.toolbox, metadata=repository_db_record.metadata_, no_changes_checked=no_changes_checked, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        else:
            (tool_panel_section_key, _) = tpm.handle_tool_panel_section(self.app.toolbox, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        metadata_dict = {} if clear_metadata else None
        current_changeset_revision = changeset_revision if clear_metadata else None
        tool_shed_repository = repository_util.create_or_update_tool_shed_repository(app=self.app, name=name, description=description, installed_changeset_revision=installed_changeset_revision, ctx_rev=ctx_rev, repository_clone_url=repository_clone_url, status=install_model.ToolShedRepository.installation_status.NEW, metadata_dict=metadata_dict, current_changeset_revision=current_changeset_revision, owner=repository_owner, dist_to_shed=False)
        if tool_shed_repository not in all_created_or_updated_tool_shed_repositories:
            all_created_or_updated_tool_shed_repositories.append(tool_shed_repository)
        if install_repository_dependencies or self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts):
            if tool_shed_repository not in created_or_updated_tool_shed_repositories:
                created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                tool_panel_section_keys.append(tool_panel_section_key)
                filtered_repo_info_dicts.append(repo_info_dict)","for i, (name, repo_info_tuple) in enumerate(repo_info_dict.items()):
    can_update_db_record = False
    clear_metadata = True
    (description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
    (repository_db_record, installed_changeset_revision) = repository_util.repository_was_previously_installed(self.app, tool_shed_url, name, repo_info_tuple, from_tip=False)
    if repository_db_record:
        if installed_changeset_revision != changeset_revision and repository_db_record.status == install_model.ToolShedRepository.installation_status.INSTALLED:
            log.info(""Repository '%s' already present at revision %s, will be updated to revision %s"", repository_db_record.name, installed_changeset_revision, changeset_revision)
            can_update_db_record = True
            clear_metadata = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.INSTALLED, install_model.ToolShedRepository.installation_status.CLONING, install_model.ToolShedRepository.installation_status.SETTING_TOOL_VERSIONS, install_model.ToolShedRepository.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES, install_model.ToolShedRepository.installation_status.INSTALLING_TOOL_DEPENDENCIES, install_model.ToolShedRepository.installation_status.LOADING_PROPRIETARY_DATATYPES]:
            info_msg = ""Skipping installation of revision %s of repository '%s' because it was installed "" % (changeset_revision, repository_db_record.name)
            info_msg += ""with the (possibly updated) revision %s and its current installation status is '%s'."" % (installed_changeset_revision, repository_db_record.status)
            log.info(info_msg)
            can_update_db_record = False
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.ERROR, install_model.ToolShedRepository.installation_status.NEW, install_model.ToolShedRepository.installation_status.UNINSTALLED]:
            name = repository_db_record.name
            installed_changeset_revision = repository_db_record.installed_changeset_revision
            can_update_db_record = True
        elif repository_db_record.status in [install_model.ToolShedRepository.installation_status.DEACTIVATED]:
            log.info(f""Reactivating deactivated tool_shed_repository '{str(repository_db_record.name)}'."")
            self.app.installed_repository_manager.activate_repository(repository_db_record)
            can_update_db_record = False
        elif repository_db_record.status not in [install_model.ToolShedRepository.installation_status.NEW]:
            changeset_revision = repository_db_record.installed_changeset_revision
            self.reset_previously_installed_repository(repository_db_record)
            can_update_db_record = True
    else:
        installed_changeset_revision = changeset_revision
        can_update_db_record = True
    if can_update_db_record:
        tpm = tool_panel_manager.ToolPanelManager(self.app)
        if repository_db_record and repository_db_record.metadata_:
            (_, tool_panel_section_key) = tpm.handle_tool_panel_selection(toolbox=self.app.toolbox, metadata=repository_db_record.metadata_, no_changes_checked=no_changes_checked, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        else:
            (tool_panel_section_key, _) = tpm.handle_tool_panel_section(self.app.toolbox, tool_panel_section_id=tool_panel_section_id, new_tool_panel_section_label=new_tool_panel_section_label)
        metadata_dict = {} if clear_metadata else None
        current_changeset_revision = changeset_revision if clear_metadata else None
        tool_shed_repository = repository_util.create_or_update_tool_shed_repository(app=self.app, name=name, description=description, installed_changeset_revision=installed_changeset_revision, ctx_rev=ctx_rev, repository_clone_url=repository_clone_url, status=install_model.ToolShedRepository.installation_status.NEW, metadata_dict=metadata_dict, current_changeset_revision=current_changeset_revision, owner=repository_owner, dist_to_shed=False)
        if tool_shed_repository not in all_created_or_updated_tool_shed_repositories:
            all_created_or_updated_tool_shed_repositories.append(tool_shed_repository)
        if install_repository_dependencies or self.is_in_repo_info_dicts(repo_info_dict, repo_info_dicts):
            if tool_shed_repository not in created_or_updated_tool_shed_repositories:
                created_or_updated_tool_shed_repositories.append(tool_shed_repository)
                tool_panel_section_keys.append(tool_panel_section_key)
                filtered_repo_info_dicts.append(repo_info_dict)"
Cura,https://github.com/Ultimaker/Cura/tree/master/plugins/VersionUpgrade/VersionUpgrade44to45/VersionUpgrade44to45.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/plugins/VersionUpgrade/VersionUpgrade44to45/VersionUpgrade44to45.py,VersionUpgrade44to45,"def upgradeInstanceContainer(self, serialized: str, filename: str) -> Tuple[List[str], List[str]]:
    """"""Upgrades instance containers to have the new version number.

        This renames the renamed settings in the containers.
        """"""
    parser = configparser.ConfigParser(interpolation=None, comment_prefixes=())
    parser.read_string(serialized)
    parser['metadata']['setting_version'] = '11'
    if 'values' in parser:
        for (preferred, removed) in _merged_settings.items():
            if removed in parser['values']:
                if preferred not in parser['values']:
                    parser['values'][preferred] = parser['values'][removed]
                del parser['values'][removed]
        for removed in _removed_settings:
            if removed in parser['values']:
                del parser['values'][removed]
    result = io.StringIO()
    parser.write(result)
    return ([filename], [result.getvalue()])","for removed in _removed_settings:
    if removed in parser['values']:
        del parser['values'][removed]","for i, removed in enumerate(_removed_settings):
    if removed in parser['values']:
        del parser['values'][removed]"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/cmu_dog/agents.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/cmu_dog/agents.py,,"def _all_split_datafiles(opt: Opt) -> List[str]:
    datafiles = []
    split_type = SplitType(opt.get('cmu_dog_split_type'))
    if split_type in {SplitType.SEEN, SplitType.UNSEEN}:
        for split in ['train', 'valid', 'test']:
            datafiles.append(_datafile(split, SplitType.SEEN))
        datafiles.append(_datafile('test', SplitType.UNSEEN))
    else:
        for split in ['train', 'valid', 'test']:
            datafiles.append(_datafile(split, split_type))
    return datafiles","for split in ['train', 'valid', 'test']:
    datafiles.append(_datafile(split, SplitType.SEEN))","for i, split in enumerate(['train', 'valid', 'test']):
    datafiles.append(_datafile(split, SplitType.SEEN))"
texar,https://github.com/asyml/texar/tree/master/texar/tf/core/layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar/texar/tf/core/layers.py,MergeLayer,"def call(self, inputs):
    if self._layers is None:
        layer_outputs = inputs
        if not isinstance(layer_outputs, (list, tuple)):
            layer_outputs = [layer_outputs]
    else:
        layer_outputs = []
        for layer in self._layers:
            layer_output = layer(inputs)
            layer_outputs.append(layer_output)
    if self._mode == 'concat':
        outputs = tf.concat(values=layer_outputs, axis=self._axis)
    elif self._mode == 'elemwise_sum':
        outputs = layer_outputs[0]
        for i in range(1, len(layer_outputs)):
            outputs = tf.add(outputs, layer_outputs[i])
    elif self._mode == 'elemwise_mul':
        outputs = layer_outputs[0]
        for i in range(1, len(layer_outputs)):
            outputs = tf.multiply(outputs, layer_outputs[i])
    elif self._mode == 'sum':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_sum(_concat, axis=self._axis)
    elif self._mode == 'mean':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_mean(_concat, axis=self._axis)
    elif self._mode == 'prod':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_prod(_concat, axis=self._axis)
    elif self._mode == 'max':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_max(_concat, axis=self._axis)
    elif self._mode == 'min':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_min(_concat, axis=self._axis)
    elif self._mode == 'and':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_all(_concat, axis=self._axis)
    elif self._mode == 'or':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_any(_concat, axis=self._axis)
    elif self._mode == 'logsumexp':
        _concat = tf.concat(values=layer_outputs, axis=self._axis)
        outputs = tf.reduce_logsumexp(_concat, axis=self._axis)
    else:
        raise ValueError(""Unknown merge mode: '%s'"" % self._mode)
    if not self.built or not self._vars_built:
        self._collect_weights()
        self._vars_built = True
    return outputs","for i in range(1, len(layer_outputs)):
    outputs = tf.multiply(outputs, layer_outputs[i])","for i, layer in enumerate(layer_outputs[1:], start=1):
    outputs = tf.multiply(outputs, layer)"
pytracking,https://github.com/visionml/pytracking/tree/master/pytracking/evaluation/lasotdataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytracking/pytracking/evaluation/lasotdataset.py,LaSOTDataset,"def clean_seq_list(self):
    clean_lst = []
    for i in range(len(self.sequence_list)):
        (cls, _) = self.sequence_list[i].split('-')
        clean_lst.append(cls)
    return clean_lst","for i in range(len(self.sequence_list)):
    (cls, _) = self.sequence_list[i].split('-')
    clean_lst.append(cls)","for i, sequence in enumerate(self.sequence_list):
    (cls, _) = sequence.split('-')
    clean_lst.append(cls)"
integrations-core,https://github.com/DataDog/integrations-core/tree/master/presto/tests/test_presto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/presto/tests/test_presto.py,,"def test(dd_agent_check):
    instance = {}
    aggregator = dd_agent_check(instance, rate=True)
    for metric in METRICS + JVM_E2E_METRICS_NEW:
        aggregator.assert_metric(metric)
    aggregator.assert_all_metrics_covered()
    aggregator.assert_metrics_using_metadata(get_metadata_metrics(), exclude=JVM_E2E_METRICS_NEW)","for metric in METRICS + JVM_E2E_METRICS_NEW:
    aggregator.assert_metric(metric)","for i, metric in enumerate(METRICS + JVM_E2E_METRICS_NEW):
    aggregator.assert_metric(metric)"
PythonRobotics,https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/AStar/a_star_variants.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/PathPlanning/AStar/a_star_variants.py,,"def main():
    obs_dict = {}
    for i in range(51):
        for j in range(51):
            obs_dict[i, j] = False
    (o_x, o_y) = ([], [])
    s_x = 5.0
    s_y = 5.0
    g_x = 35.0
    g_y = 45.0
    draw_vertical_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_vertical_line(48, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 48, 50, o_x, o_y, obs_dict)
    all_x = [10, 10, 10, 15, 20, 20, 30, 30, 35, 30, 40, 45]
    all_y = [10, 30, 45, 20, 5, 40, 10, 40, 5, 40, 10, 25]
    all_len = [10, 10, 5, 10, 10, 5, 20, 10, 25, 10, 35, 15]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_vertical_line(x, y, l, o_x, o_y, obs_dict)
    (all_x[:], all_y[:], all_len[:]) = ([], [], [])
    all_x = [35, 40, 15, 10, 45, 20, 10, 15, 25, 45, 10, 30, 10, 40]
    all_y = [5, 10, 15, 20, 20, 25, 30, 35, 35, 35, 40, 40, 45, 45]
    all_len = [10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 5]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)
    if show_animation:
        plt.plot(o_x, o_y, '.k')
        plt.plot(s_x, s_y, 'og')
        plt.plot(g_x, g_y, 'xb')
        plt.grid(True)
    if use_jump_point:
        keypoint_list = key_points(obs_dict)
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101, keypoint_list)
        search_obj.jump_point()
    else:
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101)
        search_obj.a_star()","for i in range(51):
    for j in range(51):
        obs_dict[i, j] = False","for i, num in enumerate(range(51)):
    for j, num in enumerate(range(51)):
        obs_dict[i, j] = False"
subfinder,https://github.com/ausaki/subfinder/tree/master/subfinder/subfinder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/subfinder/subfinder/subfinder.py,SubFinder,"def _download(self, videofile):
    """"""调用 SubSearcher 搜索并下载字幕""""""
    basename = os.path.basename(videofile)
    subinfos = []
    for subsearcher_cls in self.subsearcher:
        subsearcher = subsearcher_cls(self, api_urls=self.api_urls)
        self.logger.info('{0}：开始使用 {1} 搜索字幕'.format(basename, subsearcher))
        try:
            subinfos = subsearcher.search_subs(videofile, self.languages, self.exts, self.keyword)
        except Exception as e:
            err = str(e)
            if self.debug:
                err = traceback.format_exc()
            self.logger.error('{}：搜索字幕发生错误： {}'.format(basename, err))
            continue
        if subinfos:
            break
    self.logger.info('{1}：找到 {0} 个字幕, 准备下载'.format(len(subinfos), basename))
    for subinfo in subinfos:
        if isinstance(subinfo['subname'], (list, tuple)):
            self._history[videofile].extend(subinfo['subname'])
        else:
            self._history[videofile].append(subinfo['subname'])","for subinfo in subinfos:
    if isinstance(subinfo['subname'], (list, tuple)):
        self._history[videofile].extend(subinfo['subname'])
    else:
        self._history[videofile].append(subinfo['subname'])","for i, subinfo in enumerate(subinfos):
    if isinstance(subinfo['subname'], (list, tuple)):
        self._history[videofile].extend(subinfo['subname'])
    else:
        self._history[videofile].append(subinfo['subname'])"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/quantization_model_evaluator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/quantization_model_evaluator.py,ModelEvaluator,"def _initialize_input_shape_with_data_range(self):
    input_shapes = []
    for (_, _, batch_input, _) in self.dataset:
        input_shapes.extend(self.preprocessor.query_data_batch_shapes(batch_input))
    shapes_statistic = np.swapaxes(np.array(input_shapes), 1, 0)
    per_input_tamplates = []
    for stat_shape in shapes_statistic:
        shape_template = [-1] * len(stat_shape[0])
        undefined_shapes = np.squeeze(np.sum(shapes_statistic == -1, axis=1), 0).astype(int)
        for (i, ds) in enumerate(undefined_shapes):
            if ds > 0:
                continue
            axis_sizes = stat_shape[:, i]
            min_size = np.min(axis_sizes)
            max_size = np.max(axis_sizes)
            shape_template[i] = min_size if min_size == max_size else (min_size, max_size)
        per_input_tamplates.append(shape_template)
    self._initialize_input_shape(dynamic_shape_helper=per_input_tamplates)","for stat_shape in shapes_statistic:
    shape_template = [-1] * len(stat_shape[0])
    undefined_shapes = np.squeeze(np.sum(shapes_statistic == -1, axis=1), 0).astype(int)
    for (i, ds) in enumerate(undefined_shapes):
        if ds > 0:
            continue
        axis_sizes = stat_shape[:, i]
        min_size = np.min(axis_sizes)
        max_size = np.max(axis_sizes)
        shape_template[i] = min_size if min_size == max_size else (min_size, max_size)
    per_input_tamplates.append(shape_template)","for (j, stat_shape) in enumerate(shapes_statistic):
    shape_template = [-1] * len(stat_shape[0])
    undefined_shapes = np.squeeze(np.sum(shapes_statistic == -1, axis=1), 0).astype(int)
    for (i, ds) in enumerate(undefined_shapes):
        if ds > 0:
            continue
        axis_sizes = stat_shape[:, i]
        min_size = np.min(axis_sizes)
        max_size = np.max(axis_sizes)
        shape_template[i] = min_size if min_size == max_size else (min_size, max_size)
    per_input_tamplates.append(shape_template)"
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for ip in myip_addr or remote_addr:
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()","for i, ip in enumerate(myip_addr or remote_addr):
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()"
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/modnas/estim/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/modnas/estim/base.py,,"def build_criterions_all(crit_configs, device_ids=None):
    """"""Build Criterions from configs.""""""
    crits_all = []
    crits_train = []
    crits_eval = []
    crits_valid = []
    for crit_conf in streamline_spec(crit_configs):
        crit = backend.get_criterion(crit_conf, device_ids=device_ids)
        crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
        if not isinstance(crit_mode, list):
            crit_mode = [crit_mode]
        if 'all' in crit_mode:
            crits_all.append(crit)
        if 'train' in crit_mode:
            crits_train.append(crit)
        if 'eval' in crit_mode:
            crits_eval.append(crit)
        if 'valid' in crit_mode:
            crits_valid.append(crit)
    return (crits_all, crits_train, crits_eval, crits_valid)","for crit_conf in streamline_spec(crit_configs):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)","for i, crit_conf in enumerate(streamline_spec(crit_configs)):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)"
jqdatasdk,https://github.com/JoinQuant/jqdatasdk/tree/master/jqdatasdk/client.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jqdatasdk/jqdatasdk/client.py,JQDataClient,"def ensure_auth(self):
    if self.inited:
        return
    if not self.username and (not self.token):
        raise RuntimeError('not inited')
    if self.username:
        (error, response) = (None, None)
        for _ in range(self.request_attempt_count):
            try:
                self._create_client()
                response = self.client.auth(self.username, self.password, self.compress, get_mac_address(), current_version)
                break
            except socket_error as ex:
                error = ex
                time.sleep(0.5)
                if self.client:
                    self.client.close()
                    self.client = None
                continue
        else:
            if error and (not response):
                raise error
        if response and response.error:
            self.data_api_url = response.error
        else:
            self.data_api_url = AUTH_API_URL
    else:
        response = self.client.auth_by_token(self.token)
    auth_message = response.msg
    if not isatty():
        auth_message = ''
    if not response.status:
        self._threading_local._instance = None
        raise self.get_error(response)
    elif self.not_auth:
        print('auth success %s' % auth_message)
        self.not_auth = False
    self.inited = True","for _ in range(self.request_attempt_count):
    try:
        self._create_client()
        response = self.client.auth(self.username, self.password, self.compress, get_mac_address(), current_version)
        break
    except socket_error as ex:
        error = ex
        time.sleep(0.5)
        if self.client:
            self.client.close()
            self.client = None
        continue
else:
    if error and (not response):
        raise error","for i, _ in enumerate(range(self.request_attempt_count)):
    try:
        self._create_client()
        response = self.client.auth(self.username, self.password, self.compress, get_mac_address(), current_version)
        break
    except socket_error as ex:
        error = ex
        time.sleep(0.5)
        if self.client:
            self.client.close()
            self.client = None
        continue
else:
    if error and (not response):
        raise error"
sentinelsat,https://github.com/sentinelsat/sentinelsat/tree/master/tests/test_docs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentinelsat/tests/test_docs.py,,"def test_rst(rst_file):
    with open(rst_file) as input_file:
        contents = input_file.read()
    all_errors = []
    errors = rstcheck.check(contents, report_level=2, ignore={'languages': ['python', 'bash']})
    for (line_number, error) in errors:
        if 'Title underline too short' in error:
            continue
        m = re.search('Unknown interpreted text role ""([^""]+)""', error)
        if m and m.group(1) in ['program', 'paramref']:
            continue
        m = re.search('Unknown directive type ""([^""]+)""', error)
        if m and m.group(1) in ['automodule']:
            continue
        all_errors.append((line_number, error))
    assert len(all_errors) == 0","for (line_number, error) in errors:
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))","for i, (line_number, error) in enumerate(errors):
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))"
arcgis-python-api,https://github.com/Esri/arcgis-python-api/tree/master/misc/_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcgis-python-api/misc/_common.py,,"def delete_services_from_servers(gis, user=None):
    """"""delete services from federated servers""""""
    for server in gis.admin.servers.list():
        for folder_name in server.services.folders:
            if folder_name not in ignore_server_folders:
                'stop and delete services per folder (per user)'
                for service in server.services.list(folder=folder_name):
                    if service.type == 'FeatureServer' or service.type == 'SceneServer':
                        item_id = service.properties.portalProperties.get('portalItems')[0]['itemID']
                        item = gis.content.get(item_id)
                        if user is None:
                            if item.owner is None:
                                print('=== deleting service (admin)', service.type, service.serviceName, item.owner, service.url)
                                service.stop()
                                service.delete()
                        elif item.owner == user.username:
                            print('=== deleting service', service.type, service.serviceName, item.owner)
                            service.stop()
                            service.delete()
                        else:
                            pass
                    elif service.type == 'ImageServer' or service.type == 'MapServer':
                        service_owner = service.properties.properties.get('userName')
                        if user is None:
                            if service_owner is None and any((token in service.serviceName for token in image_service_prefixes)):
                                print('=== deleting service (admin)', service.type, service.serviceName, service_owner, service.url)
                                service.stop()
                                service.delete()
                        elif service_owner == user.username:
                            print('=== deleting service', service.type, service.serviceName, service_owner)
                            service.stop()
                            service.delete()
                        else:
                            pass
                    elif service.type in preserved_service_types:
                        pass
                    else:
                        print('=== Unrecognized Service Type', service.type, service.serviceName, service.properties)","for folder_name in server.services.folders:
    if folder_name not in ignore_server_folders:
        'stop and delete services per folder (per user)'
        for service in server.services.list(folder=folder_name):
            if service.type == 'FeatureServer' or service.type == 'SceneServer':
                item_id = service.properties.portalProperties.get('portalItems')[0]['itemID']
                item = gis.content.get(item_id)
                if user is None:
                    if item.owner is None:
                        print('=== deleting service (admin)', service.type, service.serviceName, item.owner, service.url)
                        service.stop()
                        service.delete()
                elif item.owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, item.owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type == 'ImageServer' or service.type == 'MapServer':
                service_owner = service.properties.properties.get('userName')
                if user is None:
                    if service_owner is None and any((token in service.serviceName for token in image_service_prefixes)):
                        print('=== deleting service (admin)', service.type, service.serviceName, service_owner, service.url)
                        service.stop()
                        service.delete()
                elif service_owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, service_owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type in preserved_service_types:
                pass
            else:
                print('=== Unrecognized Service Type', service.type, service.serviceName, service.properties)","for i, folder_name in enumerate(server.services.folders):
    if folder_name not in ignore_server_folders:
        'stop and delete services per folder (per user)'
        for service in server.services.list(folder=folder_name):
            if service.type == 'FeatureServer' or service.type == 'SceneServer':
                item_id = service.properties.portalProperties.get('portalItems')[0]['itemID']
                item = gis.content.get(item_id)
                if user is None:
                    if item.owner is None:
                        print('=== deleting service (admin)', service.type, service.serviceName, item.owner, service.url)
                        service.stop()
                        service.delete()
                elif item.owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, item.owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type == 'ImageServer' or service.type == 'MapServer':
                service_owner = service.properties.properties.get('userName')
                if user is None:
                    if service_owner is None and any((token in service.serviceName for token in image_service_prefixes)):
                        print('=== deleting service (admin)', service.type, service.serviceName, service_owner, service.url)
                        service.stop()
                        service.delete()
                elif service_owner == user.username:
                    print('=== deleting service', service.type, service.serviceName, service_owner)
                    service.stop()
                    service.delete()
                else:
                    pass
            elif service.type in preserved_service_types:
                pass
            else:
                print('=== Unrecognized Service Type', service.type, service.serviceName, service.properties)"
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)","for item in builder.widgets.items():
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)","for i, item in enumerate(builder.widgets.items()):
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect((i, item), sig, handler_names, callback_handler_dict, builder.connections)"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/md_gender/yelp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/md_gender/yelp.py,YelpTeacher,"def _check_data_downloaded(self, opt):
    RESET = '\x1b[0m'
    RED = '\x1b[1;91m'
    YELLOW = '\x1b[1;93m'
    GREEN = '\x1b[1;92m'
    BLUE = '\x1b[1;96m'
    CYAN = '\x1b[1;94m'
    MAGENTA = '\x1b[1;95m'
    USE_COLORS = _sys.stdout.isatty()
    if not USE_COLORS:
        RESET = RED = YELLOW = GREEN = BLUE = CYAN = MAGENTA = ''
    rainbow = [RED, YELLOW, GREEN, CYAN, BLUE, MAGENTA]
    size = 78 // len(rainbow)
    stars = ''.join([color + '*' * size for color in rainbow])
    stars += RESET
    self.data_path = os.path.join(opt['datapath'], 'md_gender', 'yelp')
    if not os.path.exists(self.data_path):
        PathManager.mkdirs(self.data_path)
    if not PathManager.exists(os.path.join(self.data_path, 'valid.fader.with_cat.40000')):
        raise RuntimeError(f'\n\n{stars}\nThis data must be downloaded following instructions in the README here:<https://github.com/facebookresearch/MultipleAttributeTextRewriting/blob/main/data/README.md>. \nIt cannot be automatically downloaded, as one must agree to the terms outlined on the website before gaining access to the data.\n\nOnce downloaded, please put the data in the following directory: \n{self.data_path}\n{stars}')
    elif not PathManager.exists(os.path.join(self.data_path, 'classtrain.txt')):
        logging.info('[ Building data ... ]')
        with open(os.path.join(self.data_path, 'classtrain.txt'), 'w') as f:
            for fle_num in [4000, 6000, 8000]:
                train_fle = f'train.fader.with_cat.{fle_num}'
                with open(os.path.join(self.data_path, train_fle)) as g:
                    lines = g.readlines()
                    for line in lines:
                        tabs = line.split('\t')
                        text = tabs[0]
                        gend = tabs[1]
                        if gend == '0':
                            f.write(f'male\t{text}\n')
                        elif gend == '1':
                            f.write(f'female\t{text}\n')
        for pair in [('dev', 'valid'), ('test', 'test')]:
            with open(os.path.join(self.data_path, f'female_only.{pair[0]}.en'), 'w') as fem_val:
                with open(os.path.join(self.data_path, f'male_only.{pair[0]}.en'), 'w') as masc_val:
                    for fle_num in [4000, 6000, 8000]:
                        valid_fle = f'{pair[1]}.fader.with_cat.{fle_num}'
                        with open(os.path.join(self.data_path, valid_fle), 'r') as g:
                            lines = g.readlines()
                            for line in lines:
                                tabs = line.split('\t')
                                text = tabs[0]
                                gend = tabs[1]
                                if gend == '0':
                                    masc_val.write(f'{text}\n')
                                elif gend == '1':
                                    fem_val.write(f'{text}\n')","for fle_num in [4000, 6000, 8000]:
    train_fle = f'train.fader.with_cat.{fle_num}'
    with open(os.path.join(self.data_path, train_fle)) as g:
        lines = g.readlines()
        for line in lines:
            tabs = line.split('\t')
            text = tabs[0]
            gend = tabs[1]
            if gend == '0':
                f.write(f'male\t{text}\n')
            elif gend == '1':
                f.write(f'female\t{text}\n')","for i, fle_num in enumerate([4000, 6000, 8000]):
    train_fle = f'train.fader.with_cat.{fle_num}'
    with open(os.path.join(self.data_path, train_fle)) as g:
        lines = g.readlines()
        for line in lines:
            tabs = line.split('\t')
            text = tabs[0]
            gend = tabs[1]
            if gend == '0':
                f.write(f'male\t{text}\n')
            elif gend == '1':
                f.write(f'female\t{text}\n')"
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for f in filenames:
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))","for i, f in enumerate(filenames):
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))"
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for (component_name, component_specs) in rte_components.items():
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for i, (component_name, component_specs) in enumerate(rte_components.items()):
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)"
Unsupervised-Classification,https://github.com/wvangansbeke/Unsupervised-Classification/tree/master/data/stl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Unsupervised-Classification/data/stl.py,STL10,"def _check_integrity(self):
    root = self.root
    for fentry in self.train_list + self.test_list:
        (filename, md5) = (fentry[0], fentry[1])
        fpath = os.path.join(root, self.base_folder, filename)
        if not check_integrity(fpath, md5):
            return False
    return True","for fentry in self.train_list + self.test_list:
    (filename, md5) = (fentry[0], fentry[1])
    fpath = os.path.join(root, self.base_folder, filename)
    if not check_integrity(fpath, md5):
        return False","for i, fentry in enumerate(self.train_list + self.test_list):
    (filename, md5) = (fentry[0], fentry[1])
    fpath = os.path.join(root, self.base_folder, filename)
    if not check_integrity(fpath, md5):
        return False"
zentral,https://github.com/zentralopensource/zentral/tree/master/tests/inventory/test_metrics_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zentral/tests/inventory/test_metrics_views.py,PrometheusViewsTestCase,"def test_prometheus_metrics_osx_apps_bundle_names(self):
    old_config = settings._collection['apps']['zentral.contrib.inventory'].pop('metrics_options', None)
    settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = ConfigDict({'osx_apps': {'sources': ['zentral tests'], 'bundle_names': ['Baller']}})
    response = self.client.get(reverse('inventory_metrics:all'), HTTP_AUTHORIZATION='Bearer CHANGE ME!!!')
    self.assertEqual(response.status_code, 200)
    seen = False
    for family in text_string_to_metric_families(response.content.decode('utf-8')):
        if family.name == 'zentral_inventory_active_machines_bucket':
            continue
        self.assertEqual(len(family.samples), 7)
        for sample in family.samples:
            self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
            le = sample.labels['le']
            self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
            if le == '1':
                self.assertEqual(sample.value, 0)
            else:
                self.assertEqual(sample.value, 1)
        self.assertFalse(seen)
        seen = True
    self.assertTrue(seen)
    if old_config:
        settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = old_config","for family in text_string_to_metric_families(response.content.decode('utf-8')):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True","seen = False
for i, family in enumerate(text_string_to_metric_families(response.content.decode('utf-8'))):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    if not seen:
        seen = True
    else:
        self.fail(""More than one family found"")"
aws-cli,https://github.com/aws/aws-cli/tree/master/tests/functional/eks/test_update_kubeconfig.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/tests/functional/eks/test_update_kubeconfig.py,,"def sanitize_output(output):
    """"""
    Trims output and removes all lines after a line starting with warning.
    A line will only start with warning if it is the start of a
    ""not installed"" warning, which should be ignored when comparing output.
    """"""
    to_return = ''
    for line in output.splitlines():
        if bool(re.match('warning', line.strip(), re.I)):
            return to_return.strip()
        else:
            to_return += line
            to_return += '\n'
    return to_return.strip()","for line in output.splitlines():
    if bool(re.match('warning', line.strip(), re.I)):
        return to_return.strip()
    else:
        to_return += line
        to_return += '\n'","for i, line in enumerate(output.splitlines()):
    if bool(re.match('warning', line.strip(), re.I)):
        return to_return.strip()
    else:
        to_return += line
        to_return += '\n'"
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for (marker_type, active_markers) in active_markers_by_type.items():
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, (marker_type, active_markers) in enumerate(active_markers_by_type.items()):
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/bpath.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/bpath.py,Path,"def intersectPath(self, path, setinside=None):
    points = []

    def addPoint(i, P):
        if eq(P, self[i].A, EPS):
            return
        if eq(P, self[i].B, EPS):
            return
        oi = self[i].order(P)
        points.append((i, oi, P))
    for (i, si) in enumerate(self):
        for cut in path:
            (P1, P2) = si.intersect(cut)
            if P1 is not None and P2 is not None and eq(P1, P2, EPS):
                P2 = None
            if P1:
                addPoint(i, P1)
            if P2:
                addPoint(i, P2)
    points.sort(key=itemgetter(0, 1))
    for (i, o, P) in reversed(points):
        split = self[i].split(P)
        if not isinstance(split, int):
            self.insert(i + 1, split)
            self[i]._cross = True
    if setinside is not None:
        self.markInside(path, setinside)
    return points","for cut in path:
    (P1, P2) = si.intersect(cut)
    if P1 is not None and P2 is not None and eq(P1, P2, EPS):
        P2 = None
    if P1:
        addPoint(i, P1)
    if P2:
        addPoint(i, P2)","for i, cut in enumerate(path):
    (P1, P2) = si.intersect(cut)
    if P1 is not None and P2 is not None and eq(P1, P2, EPS):
        P2 = None
    if P1:
        addPoint(i, P1)
    if P2:
        addPoint(i, P2)"
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","for line in f:
    infos = line.split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])","for idx, line in enumerate(f):
    infos = line.split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])"
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/models/v1alpha1_volume_attachment_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python/kubernetes/client/models/v1alpha1_volume_attachment_status.py,V1alpha1VolumeAttachmentStatus,"def to_dict(self):
    """"""Returns the model properties as a dict""""""
    result = {}
    for (attr, _) in six.iteritems(self.openapi_types):
        value = getattr(self, attr)
        if isinstance(value, list):
            result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
        elif hasattr(value, 'to_dict'):
            result[attr] = value.to_dict()
        elif isinstance(value, dict):
            result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
        else:
            result[attr] = value
    return result","for (attr, _) in six.iteritems(self.openapi_types):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value","for i, (attr, _) in enumerate(six.iteritems(self.openapi_types)):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value"
2s-AGCN,https://github.com/lshiwjx/2s-AGCN/tree/master/model/aagcn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/2s-AGCN/model/aagcn.py,unit_gcn,"def __init__(self, in_channels, out_channels, A, coff_embedding=4, num_subset=3, adaptive=True, attention=True):
    super(unit_gcn, self).__init__()
    inter_channels = out_channels // coff_embedding
    self.inter_c = inter_channels
    self.out_c = out_channels
    self.in_c = in_channels
    self.num_subset = num_subset
    num_jpts = A.shape[-1]
    self.conv_d = nn.ModuleList()
    for i in range(self.num_subset):
        self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))
    if adaptive:
        self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)))
        self.alpha = nn.Parameter(torch.zeros(1))
        self.conv_a = nn.ModuleList()
        self.conv_b = nn.ModuleList()
        for i in range(self.num_subset):
            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))
            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))
    else:
        self.A = Variable(torch.from_numpy(A.astype(np.float32)), requires_grad=False)
    self.adaptive = adaptive
    if attention:
        self.conv_ta = nn.Conv1d(out_channels, 1, 9, padding=4)
        nn.init.constant_(self.conv_ta.weight, 0)
        nn.init.constant_(self.conv_ta.bias, 0)
        ker_jpt = num_jpts - 1 if not num_jpts % 2 else num_jpts
        pad = (ker_jpt - 1) // 2
        self.conv_sa = nn.Conv1d(out_channels, 1, ker_jpt, padding=pad)
        nn.init.xavier_normal_(self.conv_sa.weight)
        nn.init.constant_(self.conv_sa.bias, 0)
        rr = 2
        self.fc1c = nn.Linear(out_channels, out_channels // rr)
        self.fc2c = nn.Linear(out_channels // rr, out_channels)
        nn.init.kaiming_normal_(self.fc1c.weight)
        nn.init.constant_(self.fc1c.bias, 0)
        nn.init.constant_(self.fc2c.weight, 0)
        nn.init.constant_(self.fc2c.bias, 0)
    self.attention = attention
    if in_channels != out_channels:
        self.down = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1), nn.BatchNorm2d(out_channels))
    else:
        self.down = lambda x: x
    self.bn = nn.BatchNorm2d(out_channels)
    self.soft = nn.Softmax(-2)
    self.tan = nn.Tanh()
    self.sigmoid = nn.Sigmoid()
    self.relu = nn.ReLU(inplace=True)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            conv_init(m)
        elif isinstance(m, nn.BatchNorm2d):
            bn_init(m, 1)
    bn_init(self.bn, 1e-06)
    for i in range(self.num_subset):
        conv_branch_init(self.conv_d[i], self.num_subset)","for i in range(self.num_subset):
    self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))
    self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))","for i, subset in enumerate(self.num_subset):
    self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))
    self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))"
ezdxf,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/math/clipping.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/math/clipping.py,GHPolygon,"def clip(self, clip: GHPolygon, s_entry, c_entry) -> list[list[Vec2]]:
    """"""Clip this polygon using another one as a clipper.

        This is where the algorithm is executed. It allows you to make
        a UNION, INTERSECT or DIFFERENCE operation between two polygons.

        Given two polygons A, B the following operations may be performed:

        A|B ... A OR B  (Union of A and B)
        A&B ... A AND B (Intersection of A and B)
        A\\B ... A - B
        B\\A ... B - A

        The entry records store the direction the algorithm should take when
        it arrives at that entry point in an intersection. Depending on the
        operation requested, the direction is set as follows for entry points
        (f=forward, b=backward; exit points are always set to the opposite):

              Entry
              A   B
              -----
        A|B   b   b
        A&B   f   f
        A\\B  b   f
        B\\A  f   b

        f = True, b = False when stored in the entry record
        """"""
    for subject_vertex in self:
        if not subject_vertex.intersect:
            for clipper_vertex in clip:
                if not clipper_vertex.intersect:
                    (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
                    if ip is None:
                        continue
                    subject_node = _Node(ip, us, intersect=True, entry=False)
                    clipper_node = _Node(ip, uc, intersect=True, entry=False)
                    subject_node.neighbour = clipper_node
                    clipper_node.neighbour = subject_node
                    self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
                    clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))
    s_entry ^= is_inside_polygon(self.first.vtx, clip)
    for subject_vertex in self:
        if subject_vertex.intersect:
            subject_vertex.entry = s_entry
            s_entry = not s_entry
    c_entry ^= is_inside_polygon(clip.first.vtx, self)
    for clipper_vertex in clip:
        if clipper_vertex.intersect:
            clipper_vertex.entry = c_entry
            c_entry = not c_entry
    clipped_polygons: list[list[Vec2]] = []
    while self.unprocessed():
        current: _Node = self.first_intersect
        clipped = GHPolygon()
        clipped.add(_Node(current))
        while True:
            current.set_checked()
            if current.entry:
                while True:
                    current = current.next
                    clipped.add(_Node(current))
                    if current.intersect:
                        break
            else:
                while True:
                    current = current.prev
                    clipped.add(_Node(current))
                    if current.intersect:
                        break
            current = current.neighbour
            if current.checked:
                break
        clipped_polygons.append(clipped.points)
    return clipped_polygons","for clipper_vertex in clip:
    if not clipper_vertex.intersect:
        (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
        if ip is None:
            continue
        subject_node = _Node(ip, us, intersect=True, entry=False)
        clipper_node = _Node(ip, uc, intersect=True, entry=False)
        subject_node.neighbour = clipper_node
        clipper_node.neighbour = subject_node
        self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
        clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))","for i, clipper_vertex in enumerate(clip):
    if not clipper_vertex.intersect:
        (ip, us, uc) = line_intersection(subject_vertex.vtx, next_vertex_node(subject_vertex.next).vtx, clipper_vertex.vtx, next_vertex_node(clipper_vertex.next).vtx)
        if ip is None:
            continue
        subject_node = _Node(ip, us, intersect=True, entry=False)
        clipper_node = _Node(ip, uc, intersect=True, entry=False)
        subject_node.neighbour = clipper_node
        clipper_node.neighbour = subject_node
        self.insert(subject_node, subject_vertex, next_vertex_node(subject_vertex.next))
        clip.insert(clipper_node, clipper_vertex, next_vertex_node(clipper_vertex.next))"
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])","for (ind, (s, walk_id)) in enumerate(zip(succ, cur_walk_ids)):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])"
pororo,https://github.com/kakaobrain/pororo/tree/master/pororo/tasks/image_captioning.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pororo/pororo/tasks/image_captioning.py,PororoCaptionBrainCaption,"def _generate(self, features, boxes, caption, caption_mask):
    """"""
        Generate caption using decoding steps

        Args:
            features (torch.tensor): image feature tensor
            boxes (torch.tensor): bounding box features
            caption (torch.tensor): dummy caption template
            caption_mask (torch.tensor): mask template

        Returns:
            torch.tensor : generate token tensor

        """"""
    for i in range(self._max_len - 1):
        pred = self._generator(features, boxes, caption, caption_mask)
        pred = pred[:, i, :]
        pred_id = torch.argmax(pred, axis=-1)
        if pred_id[0] == self._end_token:
            return caption
        caption[:, i + 1] = pred_id[0]
        caption_mask[:, i + 1] = False
    return caption","for i in range(self._max_len - 1):
    pred = self._generator(features, boxes, caption, caption_mask)
    pred = pred[:, i, :]
    pred_id = torch.argmax(pred, axis=-1)
    if pred_id[0] == self._end_token:
        return caption
    caption[:, i + 1] = pred_id[0]
    caption_mask[:, i + 1] = False","for i, num in enumerate(range(self._max_len - 1)):
    pred = self._generator(features, boxes, caption, caption_mask)
    pred = pred[:, i, :]
    pred_id = torch.argmax(pred, axis=-1)
    if pred_id[0] == self._end_token:
        return caption
    caption[:, i + 1] = pred_id[0]
    caption_mask[:, i + 1] = False"
boto3,https://github.com/boto/boto3/tree/master/boto3/resources/model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/boto3/boto3/resources/model.py,ResourceModel,"def _get_related_resources(self, subresources):
    """"""
        Get a list of sub-resources or references.

        :type subresources: bool
        :param subresources: ``True`` to get sub-resources, ``False`` to
                             get references.
        :rtype: list(:py:class:`Action`)
        """"""
    resources = []
    for (name, definition) in self._get_has_definition().items():
        if subresources:
            name = self._get_name('subresource', name, snake_case=False)
        else:
            name = self._get_name('reference', name)
        action = Action(name, definition, self._resource_defs)
        data_required = False
        for identifier in action.resource.identifiers:
            if identifier.source == 'data':
                data_required = True
                break
        if subresources and (not data_required):
            resources.append(action)
        elif not subresources and data_required:
            resources.append(action)
    return resources","for (name, definition) in self._get_has_definition().items():
    if subresources:
        name = self._get_name('subresource', name, snake_case=False)
    else:
        name = self._get_name('reference', name)
    action = Action(name, definition, self._resource_defs)
    data_required = False
    for identifier in action.resource.identifiers:
        if identifier.source == 'data':
            data_required = True
            break
    if subresources and (not data_required):
        resources.append(action)
    elif not subresources and data_required:
        resources.append(action)","for i, (name, definition) in enumerate(self._get_has_definition().items()):
    if subresources:
        name = self._get_name('subresource', name, snake_case=False)
    else:
        name = self._get_name('reference', name)
    action = Action(name, definition, self._resource_defs)
    data_required = False
    for identifier in action.resource.identifiers:
        if identifier.source == 'data':
            data_required = True
            break
    if subresources and (not data_required):
        resources.append(action)
    elif not subresources and data_required:
        resources.append(action)"
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/magnum-plugins/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/magnum-plugins/all/conanfile.py,MagnumConan,"def package(self):
    cm = self._configure_cmake()
    cm.install()
    if not self.options.shared_plugins:
        build_modules_folder = os.path.join(self.package_folder, 'lib', 'cmake')
        os.makedirs(build_modules_folder)
        for (component, target, library, folder, deps) in self._plugins:
            build_module_path = os.path.join(build_modules_folder, 'conan-magnum-plugins-{}.cmake'.format(component))
            with open(build_module_path, 'w+') as f:
                f.write(textwrap.dedent('                        if(NOT ${{CMAKE_VERSION}} VERSION_LESS ""3.0"")\n                            if(TARGET MagnumPlugins::{target})\n                                set_target_properties(MagnumPlugins::{target} PROPERTIES INTERFACE_SOURCES \n                                                    ""${{CMAKE_CURRENT_LIST_DIR}}/../../include/MagnumPlugins/{library}/importStaticPlugin.cpp"")\n                            endif()\n                        endif()\n                    '.format(target=target, library=library)))
    tools.rmdir(os.path.join(self.package_folder, 'share'))
    self.copy('*.cmake', src=os.path.join(self.source_folder, 'cmake'), dst=os.path.join('lib', 'cmake'))
    self.copy('COPYING', src=self._source_subfolder, dst='licenses')","for (component, target, library, folder, deps) in self._plugins:
    build_module_path = os.path.join(build_modules_folder, 'conan-magnum-plugins-{}.cmake'.format(component))
    with open(build_module_path, 'w+') as f:
        f.write(textwrap.dedent('                        if(NOT ${{CMAKE_VERSION}} VERSION_LESS ""3.0"")\n                            if(TARGET MagnumPlugins::{target})\n                                set_target_properties(MagnumPlugins::{target} PROPERTIES INTERFACE_SOURCES \n                                                    ""${{CMAKE_CURRENT_LIST_DIR}}/../../include/MagnumPlugins/{library}/importStaticPlugin.cpp"")\n                            endif()\n                        endif()\n                    '.format(target=target, library=library)))","for i, (component, target, library, folder, deps) in enumerate(self._plugins):
    build_module_path = os.path.join(build_modules_folder, 'conan-magnum-plugins-{}.cmake'.format(component))
    with open(build_module_path, 'w+') as f:
        f.write(textwrap.dedent('                        if(NOT ${{CMAKE_VERSION}} VERSION_LESS ""3.0"")\n                            if(TARGET MagnumPlugins::{target})\n                                set_target_properties(MagnumPlugins::{target} PROPERTIES INTERFACE_SOURCES \n                                                    ""${{CMAKE_CURRENT_LIST_DIR}}/../../include/MagnumPlugins/{library}/importStaticPlugin.cpp"")\n                            endif()\n                        endif()\n                    '.format(target=target, library=library)))"
Bert-Chinese-Text-Classification-Pytorch,https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch/tree/master/pytorch_pretrained/tokenization.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Bert-Chinese-Text-Classification-Pytorch/pytorch_pretrained/tokenization.py,WordpieceTokenizer,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""
    output_tokens = []
    for token in whitespace_tokenize(text):
        chars = list(token)
        if len(chars) > self.max_input_chars_per_word:
            output_tokens.append(self.unk_token)
            continue
        is_bad = False
        start = 0
        sub_tokens = []
        while start < len(chars):
            end = len(chars)
            cur_substr = None
            while start < end:
                substr = ''.join(chars[start:end])
                if start > 0:
                    substr = '##' + substr
                if substr in self.vocab:
                    cur_substr = substr
                    break
                end -= 1
            if cur_substr is None:
                is_bad = True
                break
            sub_tokens.append(cur_substr)
            start = end
        if is_bad:
            output_tokens.append(self.unk_token)
        else:
            output_tokens.extend(sub_tokens)
    return output_tokens","for token in whitespace_tokenize(text):
    chars = list(token)
    if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue
    is_bad = False
    start = 0
    sub_tokens = []
    while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
            substr = ''.join(chars[start:end])
            if start > 0:
                substr = '##' + substr
            if substr in self.vocab:
                cur_substr = substr
                break
            end -= 1
        if cur_substr is None:
            is_bad = True
            break
        sub_tokens.append(cur_substr)
        start = end
    if is_bad:
        output_tokens.append(self.unk_token)
    else:
        output_tokens.extend(sub_tokens)","for i, token in enumerate(whitespace_tokenize(text)):
    chars = list(token)
    if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue
    is_bad = False
    start = 0
    sub_tokens = []
    while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
            substr = ''.join(chars[start:end])
            if start > 0:
                substr = '##' + substr
            if substr in self.vocab:
                cur_substr = substr
                break
            end -= 1
        if cur_substr is None:
            is_bad = True
            break
        sub_tokens.append(cur_substr)
        start = end
    if is_bad:
        output_tokens.append(self.unk_token)
    else:
        output_tokens.extend(sub_tokens)"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/svgcode.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/svgcode.py,SVGcode,"def path2gcode(self, path, samples_per_unit=100, d=4):
    gcode = []
    if isinstance(path, str):
        path = Path(path)

    def rv(v):
        return f'{round(v, d):{d}}'.rstrip('0').rstrip('.')
    for segment in path:
        subdiv = max(1, round(segment.length(error=1e-05) * samples_per_unit))
        if isinstance(segment, Move):
            gcode.append(f'G0 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
        elif isinstance(segment, (Line, Close)):
            gcode.append(f'G1 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
        elif isinstance(segment, Arc) and abs(segment.rx - segment.ry) < 1e-09:
            garc = 'G02' if segment.sweep > 0 else 'G03'
            gcode.append(' '.join([f'{garc}', f'X{rv(segment.end.x)}', f'Y{rv(-segment.end.y)}', f'R{rv(segment.rx)}']))
        else:
            subdiv_points = numpy.linspace(0, 1, subdiv, endpoint=True)[1:]
            points = segment.npoint(subdiv_points)
            gcode.extend([f'G1 X{rv(sp[0])} Y{rv(-sp[1])}' for sp in points])
    return '\n'.join(gcode)","for segment in path:
    subdiv = max(1, round(segment.length(error=1e-05) * samples_per_unit))
    if isinstance(segment, Move):
        gcode.append(f'G0 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, (Line, Close)):
        gcode.append(f'G1 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, Arc) and abs(segment.rx - segment.ry) < 1e-09:
        garc = 'G02' if segment.sweep > 0 else 'G03'
        gcode.append(' '.join([f'{garc}', f'X{rv(segment.end.x)}', f'Y{rv(-segment.end.y)}', f'R{rv(segment.rx)}']))
    else:
        subdiv_points = numpy.linspace(0, 1, subdiv, endpoint=True)[1:]
        points = segment.npoint(subdiv_points)
        gcode.extend([f'G1 X{rv(sp[0])} Y{rv(-sp[1])}' for sp in points])","for i, segment in enumerate(path):
    subdiv = max(1, round(segment.length(error=1e-05) * samples_per_unit))
    if isinstance(segment, Move):
        gcode.append(f'G0 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, (Line, Close)):
        gcode.append(f'G1 X{rv(segment.end.x)} Y{rv(-segment.end.y)}')
    elif isinstance(segment, Arc) and abs(segment.rx - segment.ry) < 1e-09:
        garc = 'G02' if segment.sweep > 0 else 'G03'
        gcode.append(' '.join([f'{garc}', f'X{rv(segment.end.x)}', f'Y{rv(-segment.end.y)}', f'R{rv(segment.rx)}']))
    else:
        subdiv_points = numpy.linspace(0, 1, subdiv, endpoint=True)[1:]
        points = segment.npoint(subdiv_points)
        gcode.extend([f'G1 X{rv(sp[0])} Y{rv(-sp[1])}' for sp in points])"
grover,https://github.com/rowanz/grover/tree/master/realnews/prepare_lm_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grover/realnews/prepare_lm_data.py,,"def _stream_from_buffer(buffer, current_desired_size, pad_token=0, add_articles_to_end=False):
    """""" Combines short articles that are in a buffer """"""
    random.shuffle(buffer)
    i = 0
    while i < len(buffer):
        article = buffer[i]
        if add_articles_to_end:
            for article2add in buffer[i + 1:]:
                i += 1
                article['input_ids'].append(encoder.padding)
                article['input_ids'].append(encoder.reset_context)
                article['input_ids'].extend(article2add['input_ids'])
                if len(article['input_ids']) >= current_desired_size:
                    article['input_ids'] = article['input_ids'][:current_desired_size]
                    break
        amount_to_pad = current_desired_size - len(article['input_ids'])
        article['input_ids'].extend([pad_token] * amount_to_pad)
        article['sub_index'] = 0
        yield article
        i += 1","for article2add in buffer[i + 1:]:
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break","for j, article2add in enumerate(buffer[i + 1:]):
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break"
easytrader,https://github.com/shidenggui/easytrader/tree/master/easytrader/xq_follower.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/easytrader/easytrader/xq_follower.py,XueQiuFollower,"def project_transactions(self, transactions, assets):
    for transaction in transactions:
        weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
        initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
        transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
        transaction['stock_code'] = transaction['stock_symbol'].lower()
        transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
        transaction['amount'] = int(round(initial_amount, -2))
        if transaction['action'] == 'sell' and self._adjust_sell:
            transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])","for transaction in transactions:
    weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
    initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
    transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
    transaction['stock_code'] = transaction['stock_symbol'].lower()
    transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
    transaction['amount'] = int(round(initial_amount, -2))
    if transaction['action'] == 'sell' and self._adjust_sell:
        transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])","for i, transaction in enumerate(transactions):
    weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
    initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
    transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
    transaction['stock_code'] = transaction['stock_symbol'].lower()
    transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
    transaction['amount'] = int(round(initial_amount, -2))
    if transaction['action'] == 'sell' and self._adjust_sell:
        transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])"
gated-graph-neural-network-samples,https://github.com/microsoft/gated-graph-neural-network-samples/tree/master//chem_tensorflow_async.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gated-graph-neural-network-samples//chem_tensorflow_async.py,AsyncGGNNChemModel,"def __tensorise_edge_sequence(self, edges) -> Tuple[np.ndarray, List[List[np.ndarray]], List[List[np.ndarray]], List[np.ndarray]]:
    sending_nodes = []
    msg_targets = []
    receiving_nodes = []
    all_nodes = set()
    for step_edges in edges:
        msg_targets_uniq = set((w for (_, __, w) in step_edges))
        recv_nodes = list(sorted(msg_targets_uniq))
        recv_nodes_to_uniq_id = {v: i for (i, v) in enumerate(recv_nodes)}
        sending_nodes_in_step = []
        msg_targets_in_step = []
        for target_e_typ in range(self.num_edge_types):
            sending_nodes_in_step.append(np.array([v for (v, e_typ, _) in step_edges if e_typ == target_e_typ], dtype=np.int32))
            msg_targets_in_step.append(np.array([recv_nodes_to_uniq_id[w] for (_, e_typ, w) in step_edges if e_typ == target_e_typ], dtype=np.int32))
        msg_targets.append(msg_targets_in_step)
        sending_nodes.append(sending_nodes_in_step)
        receiving_nodes.append(np.array(recv_nodes, dtype=np.int32))
        all_nodes.update((v for (v, _, __) in step_edges))
        all_nodes.update((w for (_, __, w) in step_edges))
    all_updated_nodes = set()
    all_updated_nodes.update((v for step_receiving_nodes in receiving_nodes for v in step_receiving_nodes))
    initial_nodes = list(sorted(all_nodes - all_updated_nodes))
    return (np.array(initial_nodes, dtype=np.int32), sending_nodes, msg_targets, receiving_nodes)","for target_e_typ in range(self.num_edge_types):
    sending_nodes_in_step.append(np.array([v for (v, e_typ, _) in step_edges if e_typ == target_e_typ], dtype=np.int32))
    msg_targets_in_step.append(np.array([recv_nodes_to_uniq_id[w] for (_, e_typ, w) in step_edges if e_typ == target_e_typ], dtype=np.int32))","for i, target_e_typ in enumerate(range(self.num_edge_types)):
    sending_nodes_in_step.append(np.array([v for (v, e_typ, _) in step_edges if e_typ == target_e_typ], dtype=np.int32))
    msg_targets_in_step.append(np.array([recv_nodes_to_uniq_id[w] for (_, e_typ, w) in step_edges if e_typ == target_e_typ], dtype=np.int32))"
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric/nn/models/dimenet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_geometric/torch_geometric/nn/models/dimenet_utils.py,,"def real_sph_harm(k, zero_m_only=True, spherical_coordinates=True):
    if not zero_m_only:
        S_m = [0]
        C_m = [1]
        for i in range(1, k):
            x = sym.symbols('x')
            y = sym.symbols('y')
            S_m += [x * S_m[i - 1] + y * C_m[i - 1]]
            C_m += [x * C_m[i - 1] - y * S_m[i - 1]]
    P_l_m = associated_legendre_polynomials(k, zero_m_only)
    if spherical_coordinates:
        theta = sym.symbols('theta')
        z = sym.symbols('z')
        for i in range(len(P_l_m)):
            for j in range(len(P_l_m[i])):
                if type(P_l_m[i][j]) != int:
                    P_l_m[i][j] = P_l_m[i][j].subs(z, sym.cos(theta))
        if not zero_m_only:
            phi = sym.symbols('phi')
            for i in range(len(S_m)):
                S_m[i] = S_m[i].subs(x, sym.sin(theta) * sym.cos(phi)).subs(y, sym.sin(theta) * sym.sin(phi))
            for i in range(len(C_m)):
                C_m[i] = C_m[i].subs(x, sym.sin(theta) * sym.cos(phi)).subs(y, sym.sin(theta) * sym.sin(phi))
    Y_func_l_m = [['0'] * (2 * j + 1) for j in range(k)]
    for i in range(k):
        Y_func_l_m[i][0] = sym.simplify(sph_harm_prefactor(i, 0) * P_l_m[i][0])
    if not zero_m_only:
        for i in range(1, k):
            for j in range(1, i + 1):
                Y_func_l_m[i][j] = sym.simplify(2 ** 0.5 * sph_harm_prefactor(i, j) * C_m[j] * P_l_m[i][j])
        for i in range(1, k):
            for j in range(1, i + 1):
                Y_func_l_m[i][-j] = sym.simplify(2 ** 0.5 * sph_harm_prefactor(i, -j) * S_m[j] * P_l_m[i][j])
    return Y_func_l_m","for j in range(len(P_l_m[i])):
    if type(P_l_m[i][j]) != int:
        P_l_m[i][j] = P_l_m[i][j].subs(z, sym.cos(theta))","for j, val in enumerate(P_l_m[i]):
    if type(val) != int:
        P_l_m[i][j] = val.subs(z, sym.cos(theta))"
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/solo/utils/knn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/solo/utils/knn.py,WeightedKNNClassifier,"def compute(self) -> Tuple[float]:
    """"""Computes weighted k-NN accuracy @1 and @5. If cosine distance is selected,
        the weight is computed using the exponential of the temperature scaled cosine
        distance of the samples. If euclidean distance is selected, the weight corresponds
        to the inverse of the euclidean distance.

        Returns:
            Tuple[float]: k-NN accuracy @1 and @5.
        """"""
    train_features = torch.cat(self.train_features)
    train_targets = torch.cat(self.train_targets)
    test_features = torch.cat(self.test_features)
    test_targets = torch.cat(self.test_targets)
    if self.distance_fx == 'cosine':
        train_features = F.normalize(train_features)
        test_features = F.normalize(test_features)
    num_classes = torch.unique(test_targets).numel()
    num_train_images = train_targets.size(0)
    num_test_images = test_targets.size(0)
    num_train_images = train_targets.size(0)
    chunk_size = min(max(1, self.max_distance_matrix_size // num_train_images), num_test_images)
    k = min(self.k, num_train_images)
    (top1, top5, total) = (0.0, 0.0, 0)
    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)
    for idx in range(0, num_test_images, chunk_size):
        features = test_features[idx:min(idx + chunk_size, num_test_images), :]
        targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
        batch_size = targets.size(0)
        if self.distance_fx == 'cosine':
            similarities = torch.mm(features, train_features.t())
        elif self.distance_fx == 'euclidean':
            similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
        else:
            raise NotImplementedError
        (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
        candidates = train_targets.view(1, -1).expand(batch_size, -1)
        retrieved_neighbors = torch.gather(candidates, 1, indices)
        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
        if self.distance_fx == 'cosine':
            similarities = similarities.clone().div_(self.T).exp_()
        probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
        (_, predictions) = probs.sort(1, True)
        correct = predictions.eq(targets.data.view(-1, 1))
        top1 = top1 + correct.narrow(1, 0, 1).sum().item()
        top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
        total += targets.size(0)
    top1 = top1 * 100.0 / total
    top5 = top5 * 100.0 / total
    self.reset()
    return (top1, top5)","for idx in range(0, num_test_images, chunk_size):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)","for i, idx in enumerate(range(0, num_test_images, chunk_size)):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_or_warn (the default; fix file if we can, warn otherwise)')
    postproc.add_option('--prefer-avconv', action='store_false', dest='prefer_ffmpeg', help='Prefer avconv over ffmpeg for running the postprocessors')
    postproc.add_option('--prefer-ffmpeg', action='store_true', dest='prefer_ffmpeg', help='Prefer ffmpeg over avconv for running the postprocessors (default)')
    postproc.add_option('--ffmpeg-location', '--avconv-location', metavar='PATH', dest='ffmpeg_location', help='Location of the ffmpeg/avconv binary; either the path to the binary or its containing directory.')
    postproc.add_option('--exec', metavar='CMD', dest='exec_cmd', help=""Execute a command on the file after downloading and post-processing, similar to find's -exec syntax. Example: --exec 'adb push {} /sdcard/Music/ && rm {}'"")
    postproc.add_option('--convert-subs', '--convert-subtitles', metavar='FORMAT', dest='convertsubtitles', default=None, help='Convert the subtitles to other format (currently supported: srt|ass|vtt|lrc)')
    parser.add_option_group(general)
    parser.add_option_group(network)
    parser.add_option_group(geo)
    parser.add_option_group(selection)
    parser.add_option_group(downloader)
    parser.add_option_group(filesystem)
    parser.add_option_group(thumbnail)
    parser.add_option_group(verbosity)
    parser.add_option_group(workarounds)
    parser.add_option_group(video_format)
    parser.add_option_group(subtitles)
    parser.add_option_group(authentication)
    parser.add_option_group(adobe_pass)
    parser.add_option_group(postproc)
    if overrideArguments is not None:
        (opts, args) = parser.parse_args(overrideArguments)
        if opts.verbose:
            write_string('[debug] Override config: ' + repr(overrideArguments) + '\n')
    else:

        def compat_conf(conf):
            if sys.version_info < (3,):
                return [a.decode(preferredencoding(), 'replace') for a in conf]
            return conf
        command_line_conf = compat_conf(sys.argv[1:])
        (opts, args) = parser.parse_args(command_line_conf)
        system_conf = user_conf = custom_conf = []
        if '--config-location' in command_line_conf:
            location = compat_expanduser(opts.config_location)
            if os.path.isdir(location):
                location = os.path.join(location, 'youtube-dl.conf')
            if not os.path.exists(location):
                parser.error('config-location %s does not exist.' % location)
            custom_conf = _readOptions(location)
        elif '--ignore-config' in command_line_conf:
            pass
        else:
            system_conf = _readOptions('/etc/youtube-dl.conf')
            if '--ignore-config' not in system_conf:
                user_conf = _readUserConf()
        argv = system_conf + user_conf + custom_conf + command_line_conf
        (opts, args) = parser.parse_args(argv)
        if opts.verbose:
            for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
                write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))
    return (parser, opts, args)","for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))","for i, (conf_label, conf) in enumerate((('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf))):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))"
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/test/training/test_synthetic_reward_training.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/test/training/test_synthetic_reward_training.py,,"def create_sequence_data(state_dim, action_dim, seq_len, batch_size, num_batches):
    SCALE = 2
    weight = SCALE * torch.randn(state_dim + action_dim)
    data = [None for _ in range(num_batches)]
    for i in range(num_batches):
        state = SCALE * torch.randn(seq_len, batch_size, state_dim)
        action = SCALE * torch.randn(seq_len, batch_size, action_dim)
        valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
        feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
        feature_mask = (feature_mask >= seq_len - valid_step).float()
        assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
        feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
        assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
        feature = torch.cat((state, action), dim=2)
        masked_feature = feature * feature_mask
        left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
        right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
        reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
        mask = torch.arange(seq_len).repeat(batch_size, 1)
        mask = (mask >= seq_len - valid_step).float()
        reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
        data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))
    return (weight, data)","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))","for i, num in enumerate(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))"
videos,https://github.com/3b1b/videos/tree/master/_2017/eoc/chapter7.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/eoc/chapter7.py,Goals,"def construct(self):
    goals = [TexText('Goal %d:' % d, s) for (d, s) in [(1, 'Formal definition of derivatives'), (2, '$(\\epsilon, \\delta)$ definition of a limit'), (3, ""L'Hôpital's rule"")]]
    for goal in goals:
        goal.scale(1.3)
        goal.shift(3 * DOWN).to_edge(LEFT)
    curr_goal = goals[0]
    self.play(FadeIn(curr_goal))
    self.wait(2)
    for goal in goals[1:]:
        self.play(Transform(curr_goal, goal))
        self.wait(2)","for goal in goals[1:]:
    self.play(Transform(curr_goal, goal))
    self.wait(2)","for i, goal in enumerate(goals[1:]):
    self.play(Transform(curr_goal, goal))
    self.wait(2)"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/yumpkg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/yumpkg.py,,"def diff(*paths, **kwargs):
    """"""
    Return a formatted diff between current files and original in a package.
    NOTE: this function includes all files (configuration and not), but does
    not work on binary content.

    :param path: Full path to the installed file
    :return: Difference string or raises and exception if examined file is binary.

    CLI Example:

    .. code-block:: bash

        salt '*' pkg.diff /etc/apache2/httpd.conf /etc/sudoers
    """"""
    ret = {}
    pkg_to_paths = {}
    for pth in paths:
        pth_pkg = __salt__['lowpkg.owner'](pth)
        if not pth_pkg:
            ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'
        else:
            if pkg_to_paths.get(pth_pkg) is None:
                pkg_to_paths[pth_pkg] = []
            pkg_to_paths[pth_pkg].append(pth)
    if pkg_to_paths:
        local_pkgs = __salt__['pkg.download'](*pkg_to_paths.keys())
        for (pkg, files) in pkg_to_paths.items():
            for path in files:
                ret[path] = __salt__['lowpkg.diff'](local_pkgs[pkg]['path'], path) or 'Unchanged'
    return ret","for pth in paths:
    pth_pkg = __salt__['lowpkg.owner'](pth)
    if not pth_pkg:
        ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'
    else:
        if pkg_to_paths.get(pth_pkg) is None:
            pkg_to_paths[pth_pkg] = []
        pkg_to_paths[pth_pkg].append(pth)","for i, pth in enumerate(paths):
    pth_pkg = __salt__['lowpkg.owner'](pth)
    if not pth_pkg:
        ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'
    else:
        if pkg_to_paths.get(pth_pkg) is None:
            pkg_to_paths[pth_pkg] = []
        pkg_to_paths[pth_pkg].append(pth)"
fake2db,https://github.com/emirozer/fake2db/tree/master/fake2db/couchdb_handler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fake2db/fake2db/couchdb_handler.py,Fake2dbCouchdbHandler,"def data_filler_simple_registration(self, number_of_rows, db):
    """"""creates and fills the table with simple regis. information
        """"""
    try:
        simple_registration = db
        for i in range(0, number_of_rows):
            post_simple_reg = {'id': rnd_id_generator(self), 'email': self.faker.safe_email(), 'password': self.faker.md5(raw_output=False)}
            simple_registration.save(post_simple_reg)
        logger.warning('simple_registration Commits are successful after write job!', extra=d)
    except Exception as e:
        logger.error(e, extra=d)","for i in range(0, number_of_rows):
    post_simple_reg = {'id': rnd_id_generator(self), 'email': self.faker.safe_email(), 'password': self.faker.md5(raw_output=False)}
    simple_registration.save(post_simple_reg)","for i, row in enumerate(range(0, number_of_rows)):
    post_simple_reg = {'id': rnd_id_generator(self), 'email': self.faker.safe_email(), 'password': self.faker.md5(raw_output=False)}
    simple_registration.save(post_simple_reg)"
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/core/layers_with_gpipe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/core/layers_with_gpipe.py,GPipeBatchMajorTransformerStack,"def __init__(self, params):
    p = params.Copy()
    num_layers = p.num_encoder_layers + p.num_decoder_layers
    if isinstance(p.splits, (list, tuple)):
        assert p.splits[-1] == num_layers
        for (i, j) in zip(p.splits[:-1], p.splits[1:]):
            assert i <= j, 'Splits must be in increasing order.'
    else:
        num_splits = p.splits
        layers_per_split = (num_layers - 1) // num_splits + 1
        p.splits = []
        for i in range(num_splits):
            p.splits.append((i + 1) * layers_per_split)
        p.splits[-1] = num_layers
    p.state_dtype = p.dtype
    if p.fprop_dtype:
        p.state_dtype = p.fprop_dtype
    transformers = []
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        p.emb_tpl.dropout_tpl = layers.DeterministicDropoutLayer.Params()
    p.emb_tpl.packed_input = p.packed_input
    p.emb_tpl.add_tgt_embedding_layer = p.num_decoder_layers > 0
    p.emb_tpl.name = 'emb'
    transformers.append(p.emb_tpl)
    if p.softmax_tpl:
        p.softmax_tpl.name = 'softmax'
        p.softmax_tpl.inputs_from_decoder = p.num_decoder_layers > 0
    for i in range(p.num_encoder_layers):
        params = p.encoder_tpl.Copy()
        params.name = 'encoder_%d' % i
        if i == p.num_encoder_layers - 1:
            params.output_layer_norm = True
        params.packed_input = p.packed_input
        if len(p.splits) > 1 or p.num_micro_batches > 1:
            params = params.cls.SetupDeterministicDropout(params)
        assert not params.has_aux_atten
        transformers.append(params)
    for i in range(p.num_decoder_layers):
        params = p.decoder_tpl.Copy()
        params.name = 'decoder_%d' % i
        params.mask_self_atten = True
        if i == p.num_decoder_layers - 1:
            params.output_layer_norm = True
        params.packed_input = p.packed_input
        if len(p.splits) > 1 or p.num_micro_batches > 1:
            params = params.cls.SetupDeterministicDropout(params)
        assert params.has_aux_atten
        transformers.append(params)
    cells = []
    cell_start = 0
    offset = 1
    for (split, cell_end) in enumerate(p.splits):
        sub = transformers[cell_start:cell_end + offset]
        if split == len(p.splits) - 1 and p.softmax_tpl:
            sub.append(p.softmax_tpl)
        cell = FeatureExtractionLayer.Params().Set(name='cell_{}'.format(split), sub=sub)
        cells.append(cell)
        cell_start = cell_end + offset
    p.cell_tpl = cells
    super().__init__(p)
    if p.label_smoothing:
        self.CreateChild('smoother', p.label_smoothing)","for i in range(p.num_encoder_layers):
    params = p.encoder_tpl.Copy()
    params.name = 'encoder_%d' % i
    if i == p.num_encoder_layers - 1:
        params.output_layer_norm = True
    params.packed_input = p.packed_input
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        params = params.cls.SetupDeterministicDropout(params)
    assert not params.has_aux_atten
    transformers.append(params)","for i, num in enumerate(p.num_encoder_layers):
    params = p.encoder_tpl.Copy()
    params.name = 'encoder_%d' % i
    if i == p.num_encoder_layers - 1:
        params.output_layer_norm = True
    params.packed_input = p.packed_input
    if len(p.splits) > 1 or p.num_micro_batches > 1:
        params = params.cls.SetupDeterministicDropout(params)
    assert not params.has_aux_atten
    transformers.append(params)"
pony,https://github.com/ponyorm/pony/tree/master/pony/orm/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pony/pony/orm/core.py,,"def has_perm(user, perm, x):
    if isinstance(x, EntityMeta):
        entity = x
    elif isinstance(x, Entity):
        entity = x.__class__
    elif isinstance(x, Attribute):
        if x.hidden:
            return False
        entity = x.entity
    else:
        throw(TypeError, ""The third parameter of 'has_perm' function should be entity class, entity instance or attribute. Got: %r"" % x)
    access_rules = entity._access_rules_.get(perm)
    if not access_rules:
        return False
    cache = entity._database_._get_cache()
    perm_cache = cache.perm_cache[user][perm]
    result = perm_cache.get(x)
    if result is not None:
        return result
    user_groups = get_user_groups(user)
    result = False
    if isinstance(x, EntityMeta):
        for rule in access_rules:
            if user_groups.issuperset(rule.groups) and entity not in rule.entities_to_exclude:
                result = True
                break
    elif isinstance(x, Attribute):
        attr = x
        for rule in access_rules:
            if user_groups.issuperset(rule.groups) and entity not in rule.entities_to_exclude and (attr not in rule.attrs_to_exclude):
                result = True
                break
            reverse = attr.reverse
            if reverse:
                reverse_rules = reverse.entity._access_rules_.get(perm)
                if not reverse_rules:
                    return False
                for reverse_rule in access_rules:
                    if user_groups.issuperset(reverse_rule.groups) and reverse.entity not in reverse_rule.entities_to_exclude and (reverse not in reverse_rule.attrs_to_exclude):
                        result = True
                        break
                if result:
                    break
    else:
        obj = x
        user_roles = get_user_roles(user, obj)
        obj_labels = get_object_labels(obj)
        for rule in access_rules:
            if x in rule.entities_to_exclude:
                continue
            elif not user_groups.issuperset(rule.groups):
                pass
            elif not user_roles.issuperset(rule.roles):
                pass
            elif not obj_labels.issuperset(rule.labels):
                pass
            else:
                result = True
                break
    perm_cache[perm] = result
    return result","for reverse_rule in access_rules:
    if user_groups.issuperset(reverse_rule.groups) and reverse.entity not in reverse_rule.entities_to_exclude and (reverse not in reverse_rule.attrs_to_exclude):
        result = True
        break","for i, reverse_rule in enumerate(access_rules):
    if user_groups.issuperset(reverse_rule.groups) and reverse.entity not in reverse_rule.entities_to_exclude and (reverse not in reverse_rule.attrs_to_exclude):
        result = True
        break"
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/data/transform/operators.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/ppdet/data/transform/operators.py,CropWithSampling,"def apply(self, sample, context):
    """"""
        Crop the image and modify bounding box.
        Operators:
            1. Scale the image width and height.
            2. Crop the image according to a radom sample.
            3. Rescale the bounding box.
            4. Determine if the new bbox is satisfied in the new image.
        Returns:
            sample: the image, bounding box are replaced.
        """"""
    assert 'image' in sample, 'image data not found'
    im = sample['image']
    gt_bbox = sample['gt_bbox']
    gt_class = sample['gt_class']
    (im_height, im_width) = im.shape[:2]
    gt_score = None
    if 'gt_score' in sample:
        gt_score = sample['gt_score']
    sampled_bbox = []
    gt_bbox = gt_bbox.tolist()
    for sampler in self.batch_sampler:
        found = 0
        for i in range(sampler[1]):
            if found >= sampler[0]:
                break
            sample_bbox = generate_sample_bbox(sampler)
            if satisfy_sample_constraint(sampler, sample_bbox, gt_bbox, self.satisfy_all):
                sampled_bbox.append(sample_bbox)
                found = found + 1
    im = np.array(im)
    while sampled_bbox:
        idx = int(np.random.uniform(0, len(sampled_bbox)))
        sample_bbox = sampled_bbox.pop(idx)
        sample_bbox = clip_bbox(sample_bbox)
        (crop_bbox, crop_class, crop_score) = filter_and_process(sample_bbox, gt_bbox, gt_class, scores=gt_score)
        if self.avoid_no_bbox:
            if len(crop_bbox) < 1:
                continue
        xmin = int(sample_bbox[0] * im_width)
        xmax = int(sample_bbox[2] * im_width)
        ymin = int(sample_bbox[1] * im_height)
        ymax = int(sample_bbox[3] * im_height)
        im = im[ymin:ymax, xmin:xmax]
        sample['image'] = im
        sample['gt_bbox'] = crop_bbox
        sample['gt_class'] = crop_class
        sample['gt_score'] = crop_score
        return sample
    return sample","for i in range(sampler[1]):
    if found >= sampler[0]:
        break
    sample_bbox = generate_sample_bbox(sampler)
    if satisfy_sample_constraint(sampler, sample_bbox, gt_bbox, self.satisfy_all):
        sampled_bbox.append(sample_bbox)
        found = found + 1","for i, num in enumerate(range(sampler[1])):
    if found >= sampler[0]:
        break
    sample_bbox = generate_sample_bbox(sampler)
    if satisfy_sample_constraint(sampler, sample_bbox, gt_bbox, self.satisfy_all):
        sampled_bbox.append(sample_bbox)
        found = found + 1"
tensorly,https://github.com/tensorly/tensorly/tree/master/tensorly/decomposition/tests/test_parafac2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorly/tensorly/decomposition/tests/test_parafac2.py,,"def test_parafac2_to_tensor():
    rng = tl.check_random_state(1234)
    rank = 3
    I = 25
    J = 15
    K = 30
    (weights, factors, projections) = random_parafac2(shapes=[(J, K)] * I, rank=rank, random_state=rng)
    constructed_tensor = parafac2_to_tensor((weights, factors, projections))
    tensor_manual = T.zeros((I, J, K), **T.context(weights))
    for i in range(I):
        Bi = T.dot(projections[i], factors[1])
        for j in range(J):
            for k in range(K):
                for r in range(rank):
                    tensor_manual = tl.index_update(tensor_manual, tl.index[i, j, k], tensor_manual[i, j, k] + factors[0][i][r] * Bi[j][r] * factors[2][k][r])
    assert_(tl.max(tl.abs(constructed_tensor - tensor_manual)) < 1e-06)","for j in range(J):
    for k in range(K):
        for r in range(rank):
            tensor_manual = tl.index_update(tensor_manual, tl.index[i, j, k], tensor_manual[i, j, k] + factors[0][i][r] * Bi[j][r] * factors[2][k][r])","for j, _ in enumerate(range(J)):
    for k, _ in enumerate(range(K)):
        for r, _ in enumerate(range(rank)):
            tensor_manual = tl.index_update(tensor_manual, tl.index[i, j, k], tensor_manual[i, j, k] + factors[0][i][r] * Bi[j][r] * factors[2][k][r])"
Kunlun-M,https://github.com/LoRexxar/Kunlun-M/tree/master/core/detection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kunlun-M/core/detection.py,Detection,"def framework(self):
    tree = self.rule()
    root = tree.getroot()
    (frame_data, language_data) = self.parse_xml(root, self.frame_data, self.language_data)
    projects_data = self.project_information(self.target_directory, False)
    frame_name = self.dependency_scan(root)
    if frame_name is not None:
        return frame_name
    frames_name = frame_data.keys()
    for frame_name in frames_name:
        for rule_name in frame_data[frame_name]:
            for project_data in projects_data:
                if rule_name in project_data:
                    logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                    return frame_name
    logger.info('[DETECTION] [FRAMEWORK] Unknown Framework')
    return 'Unknown Framework'","for frame_name in frames_name:
    for rule_name in frame_data[frame_name]:
        for project_data in projects_data:
            if rule_name in project_data:
                logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                return frame_name","for i, frame_name in enumerate(frames_name):
    for rule_name in frame_data[frame_name]:
        for project_data in projects_data:
            if rule_name in project_data:
                logger.debug(""[DETECTION] [FRAMEWORK] Find the project's framework may be:"" + frame_name)
                return frame_name"
primerpython,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/graph_bobject.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/tools/graph_bobject.py,GraphBobject,"def set_shape_keys_bounded_region(self, index=0):
    obj = self.regions_curves[index].ref_obj.children[0]
    data = obj.data
    final_coords = []
    final_lefts = []
    final_rights = []
    for (i, point) in enumerate(data.splines[0].bezier_points):
        final_coords.append(deepcopy(point.co))
        final_lefts.append(deepcopy(point.handle_left))
        final_rights.append(deepcopy(point.handle_right))
        if i < len(data.splines[0].bezier_points) / 2:
            point.co = deepcopy(data.splines[0].bezier_points[0].co)
            point.handle_left = deepcopy(data.splines[0].bezier_points[0].handle_left)
            point.handle_right = deepcopy(data.splines[0].bezier_points[0].handle_right)
        else:
            point.co = deepcopy(data.splines[0].bezier_points[-1].co)
            point.handle_left = deepcopy(data.splines[0].bezier_points[-1].handle_left)
            point.handle_right = deepcopy(data.splines[0].bezier_points[-1].handle_right)
    bpy.ops.object.select_all(action='DESELECT')
    bpy.context.scene.objects.active = obj
    bpy.ops.object.mode_set(mode='OBJECT')
    bpy.ops.object.shape_key_add(from_mix=False)
    data.shape_keys.use_relative = False
    data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
    if len(data.splines[0].bezier_points) % 2 != 0:
        raise Warning('Odd number of points in bounding curve')
    for j in range(1, int(len(data.splines[0].bezier_points) / 2)):
        bpy.ops.object.shape_key_add(from_mix=False)
        data.shape_keys.use_relative = False
        data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
        bpy.ops.object.mode_set(mode='EDIT')
        for (i, point) in enumerate(data.splines[0].bezier_points):
            if i < len(data.splines[0].bezier_points) / 2:
                if i <= j:
                    point.co = deepcopy(final_coords[i])
                    point.handle_left = deepcopy(final_lefts[i])
                    point.handle_right = deepcopy(final_rights[i])
                elif i > j:
                    point.co = deepcopy(final_coords[j])
                    point.handle_left = deepcopy(final_lefts[j])
                    point.handle_right = deepcopy(final_rights[j])
            elif i >= len(data.splines[0].bezier_points) - j - 1:
                point.co = deepcopy(final_coords[i])
                point.handle_left = deepcopy(final_lefts[i])
                point.handle_right = deepcopy(final_rights[i])
            elif i < len(data.splines[0].bezier_points) - j - 1:
                point.co = deepcopy(final_coords[-j - 1])
                point.handle_left = deepcopy(final_lefts[-j - 1])
                point.handle_right = deepcopy(final_rights[-j - 1])
        bpy.ops.object.mode_set(mode='OBJECT')","for j in range(1, int(len(data.splines[0].bezier_points) / 2)):
    bpy.ops.object.shape_key_add(from_mix=False)
    data.shape_keys.use_relative = False
    data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
    bpy.ops.object.mode_set(mode='EDIT')
    for (i, point) in enumerate(data.splines[0].bezier_points):
        if i < len(data.splines[0].bezier_points) / 2:
            if i <= j:
                point.co = deepcopy(final_coords[i])
                point.handle_left = deepcopy(final_lefts[i])
                point.handle_right = deepcopy(final_rights[i])
            elif i > j:
                point.co = deepcopy(final_coords[j])
                point.handle_left = deepcopy(final_lefts[j])
                point.handle_right = deepcopy(final_rights[j])
        elif i >= len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[i])
            point.handle_left = deepcopy(final_lefts[i])
            point.handle_right = deepcopy(final_rights[i])
        elif i < len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[-j - 1])
            point.handle_left = deepcopy(final_lefts[-j - 1])
            point.handle_right = deepcopy(final_rights[-j - 1])
    bpy.ops.object.mode_set(mode='OBJECT')","for j, num in enumerate(range(1, int(len(data.splines[0].bezier_points) / 2))):
    bpy.ops.object.shape_key_add(from_mix=False)
    data.shape_keys.use_relative = False
    data.shape_keys.key_blocks[-1].interpolation = 'KEY_LINEAR'
    bpy.ops.object.mode_set(mode='EDIT')
    for (i, point) in enumerate(data.splines[0].bezier_points):
        if i < len(data.splines[0].bezier_points) / 2:
            if i <= j:
                point.co = deepcopy(final_coords[i])
                point.handle_left = deepcopy(final_lefts[i])
                point.handle_right = deepcopy(final_rights[i])
            elif i > j:
                point.co = deepcopy(final_coords[j])
                point.handle_left = deepcopy(final_lefts[j])
                point.handle_right = deepcopy(final_rights[j])
        elif i >= len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[i])
            point.handle_left = deepcopy(final_lefts[i])
            point.handle_right = deepcopy(final_rights[i])
        elif i < len(data.splines[0].bezier_points) - j - 1:
            point.co = deepcopy(final_coords[-j - 1])
            point.handle_left = deepcopy(final_lefts[-j - 1])
            point.handle_right = deepcopy(final_rights[-j - 1])
    bpy.ops.object.mode_set(mode='OBJECT')"
PathPlanning,https://github.com/zhm-real/PathPlanning/tree/master/Sampling_based_Planning/rrt_2D/rrt_star.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPlanning/Sampling_based_Planning/rrt_2D/rrt_star.py,RrtStar,"def rewire(self, node_new, neighbor_index):
    for i in neighbor_index:
        node_neighbor = self.vertex[i]
        if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
            node_neighbor.parent = node_new","for i in neighbor_index:
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new","for index, i in enumerate(neighbor_index):
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new"
enum4linux-ng,https://github.com/cddmp/enum4linux-ng/tree/master//enum4linux-ng.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enum4linux-ng//enum4linux-ng.py,EnumGroupsRpc,"def run(self):
    """"""
        Run module enum groups.
        """"""
    module_name = ENUM_GROUPS_RPC
    print_heading(f'Groups via RPC on {self.target.host}')
    output = {}
    groups = None
    for grouptype in ['local', 'builtin', 'domain']:
        print_info(f'Enumerating {grouptype} groups')
        enum = self.enum(grouptype)
        if enum.retval is None:
            output = process_error(enum.retmsg, ['groups'], module_name, output)
        else:
            if groups is None:
                groups = {}
            print_success(enum.retmsg)
            groups.update(enum.retval)
    if groups:
        if self.with_members:
            print_info('Enumerating group members')
            for rid in groups.keys():
                groupname = groups[rid]['groupname']
                grouptype = groups[rid]['type']
                group_members = self.get_members_from_name(groupname, grouptype, rid)
                if group_members.retval or group_members.retval == '':
                    print_success(group_members.retmsg)
                else:
                    output = process_error(group_members.retmsg, ['groups'], module_name, output)
                groups[rid]['members'] = group_members.retval
        if self.detailed:
            print_info('Enumerating group details')
            for rid in groups.keys():
                groupname = groups[rid]['groupname']
                grouptype = groups[rid]['type']
                details = self.get_details_from_rid(rid, groupname, grouptype)
                if details.retval:
                    print_success(details.retmsg)
                else:
                    output = process_error(details.retmsg, ['groups'], module_name, output)
                groups[rid]['details'] = details.retval
        print_success(f'After merging groups results we have {len(groups.keys())} group(s) total:\n{yamlize(groups, sort=True)}')
    output['groups'] = groups
    return output","for rid in groups.keys():
    groupname = groups[rid]['groupname']
    grouptype = groups[rid]['type']
    details = self.get_details_from_rid(rid, groupname, grouptype)
    if details.retval:
        print_success(details.retmsg)
    else:
        output = process_error(details.retmsg, ['groups'], module_name, output)
    groups[rid]['details'] = details.retval","for i, rid in enumerate(groups.keys()):
    groupname = groups[rid]['groupname']
    grouptype = groups[rid]['type']
    details = self.get_details_from_rid(rid, groupname, grouptype)
    if details.retval:
        print_success(details.retmsg)
    else:
        output = process_error(details.retmsg, ['groups'], module_name, output)
    groups[rid]['details'] = details.retval"
UnityPack,https://github.com/HearthSim/UnityPack/tree/master/unitypack/type.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UnityPack/unitypack/type.py,TypeMetadata,"def load(self, buf, format=None):
    if format is None:
        format = self.asset.format
    self.generator_version = buf.read_string()
    self.target_platform = RuntimePlatform(buf.read_uint())
    if format >= 13:
        has_type_trees = buf.read_boolean()
        num_types = buf.read_int()
        for i in range(num_types):
            class_id = buf.read_int()
            if format >= 17:
                unk0 = buf.read_byte()
                script_id = buf.read_int16()
                if class_id == 114:
                    if script_id >= 0:
                        class_id = -2 - script_id
                    else:
                        class_id = -1
            self.class_ids.append(class_id)
            if class_id < 0:
                hash = buf.read(32)
            else:
                hash = buf.read(16)
            self.hashes[class_id] = hash
            if has_type_trees:
                tree = TypeTree(format)
                tree.load(buf)
                self.type_trees[class_id] = tree
            if format >= 21:
                unk1 = buf.read(4)
    else:
        num_fields = buf.read_int()
        for i in range(num_fields):
            class_id = buf.read_int()
            tree = TypeTree(format)
            tree.load(buf)
            self.type_trees[class_id] = tree","for i in range(num_types):
    class_id = buf.read_int()
    if format >= 17:
        unk0 = buf.read_byte()
        script_id = buf.read_int16()
        if class_id == 114:
            if script_id >= 0:
                class_id = -2 - script_id
            else:
                class_id = -1
    self.class_ids.append(class_id)
    if class_id < 0:
        hash = buf.read(32)
    else:
        hash = buf.read(16)
    self.hashes[class_id] = hash
    if has_type_trees:
        tree = TypeTree(format)
        tree.load(buf)
        self.type_trees[class_id] = tree
    if format >= 21:
        unk1 = buf.read(4)","for i, num in enumerate(num_types):
    class_id = buf.read_int()
    if format >= 17:
        unk0 = buf.read_byte()
        script_id = buf.read_int16()
        if class_id == 114:
            if script_id >= 0:
                class_id = -2 - script_id
            else:
                class_id = -1
    self.class_ids.append(class_id)
    if class_id < 0:
        hash = buf.read(32)
    else:
        hash = buf.read(16)
    self.hashes[class_id] = hash
    if has_type_trees:
        tree = TypeTree(format)
        tree.load(buf)
        self.type_trees[class_id] = tree
    if format >= 21:
        unk1 = buf.read(4)"
django,https://github.com/django/django/tree/master/django/core/paginator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/django/core/paginator.py,Paginator,"def __iter__(self):
    for page_number in self.page_range:
        yield self.page(page_number)","for page_number in self.page_range:
    yield self.page(page_number)","for i, page_number in enumerate(self.page_range):
    yield self.page(page_number)"
pattern,https://github.com/clips/pattern/tree/master/test/test_en.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pattern/test/test_en.py,TestSpelling,"def test_spelling(self):
    for (a, b) in (('.', '.'), ('?', '?'), ('!', '!'), ('I', 'I'), ('a', 'a'), ('42', '42'), ('3.14', '3.14'), ('The', 'The'), ('the', 'the')):
        self.assertEqual(en.suggest(a)[0][0], b)
    i = j = 0.0
    from pattern.db import Datasheet
    for (correct, wrong) in Datasheet.load(os.path.join(PATH, 'corpora', 'spelling-birkbeck.csv')):
        for w in wrong.split(' '):
            if en.suggest(w)[0][0] == correct:
                i += 1
            else:
                j += 1
    self.assertTrue(i / (i + j) > 0.7)
    print('pattern.en.suggest()')","for w in wrong.split(' '):
    if en.suggest(w)[0][0] == correct:
        i += 1
    else:
        j += 1","for i, w in enumerate(wrong.split(' ')):
    if en.suggest(w)[0][0] == correct:
        i += 1
    else:
        j += 1"
drf-spectacular,https://github.com/tfranzel/drf-spectacular/tree/master/docs/blueprints/rollup.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/drf-spectacular/docs/blueprints/rollup.py,,"def rollup_properties(component, resolved_sub_serializers):
    if any(('allOf' in r.schema for r in resolved_sub_serializers)):
        return
    all_field_sets = [set(list(r.schema['properties'])) for r in resolved_sub_serializers]
    common_fields = all_field_sets[0].intersection(*all_field_sets[1:])
    common_schema = {'properties': {}, 'required': set()}
    for r in resolved_sub_serializers:
        for cf in sorted(common_fields):
            if cf in r.schema['properties']:
                common_schema['properties'][cf] = r.schema['properties'][cf]
                del r.schema['properties'][cf]
                if cf in r.schema.get('required', []):
                    common_schema['required'].add(cf)
        r.schema = {'allOf': [component.ref, r.schema]}
    del component.schema['oneOf']
    component.schema['properties'] = common_schema['properties']
    if common_schema['required']:
        component.schema['required'] = sorted(common_schema['required'])","for cf in sorted(common_fields):
    if cf in r.schema['properties']:
        common_schema['properties'][cf] = r.schema['properties'][cf]
        del r.schema['properties'][cf]
        if cf in r.schema.get('required', []):
            common_schema['required'].add(cf)","for i, cf in enumerate(sorted(common_fields)):
    if cf in r.schema['properties']:
        common_schema['properties'][cf] = r.schema['properties'][cf]
        del r.schema['properties'][cf]
        if cf in r.schema.get('required', []):
            common_schema['required'].add(cf)"
swift,https://github.com/openstack/swift/tree/master/swift/common/ring/composite_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/common/ring/composite_builder.py,CompositeRingBuilder,"def rebalance(self):
    """"""
        Cooperatively rebalances all component ring builders.

        This method does not change the state of the composite ring; a
        subsequent call to :meth:`compose` is required to generate updated
        composite :class:`RingData`.

        :return: A list of dicts, one per component builder, each having the
            following keys:

            * 'builder_file' maps to the component builder file;
            * 'builder' maps to the corresponding instance of
              :class:`swift.common.ring.builder.RingBuilder`;
            * 'result' maps to the results of the rebalance of that component
              i.e. a tuple of: `(number_of_partitions_altered,
              resulting_balance, number_of_removed_devices)`

            The list has the same order as components in the composite ring.
        :raises RingBuilderError: if there is an error while rebalancing any
            component builder.
        """"""
    self._load_components()
    self.update_last_part_moves()
    component_builders = list(zip(self._builder_files, self._builders))
    shuffle(component_builders)
    results = {}
    for (builder_file, builder) in component_builders:
        try:
            results[builder] = {'builder': builder, 'builder_file': builder_file, 'result': builder.rebalance()}
            builder.validate()
        except RingBuilderError as err:
            self._builders = None
            raise RingBuilderError('An error occurred while rebalancing component %s: %s' % (builder_file, err))
    for (builder_file, builder) in component_builders:
        builder.save(builder_file)
    return [results[builder] for builder in self._builders]","for (builder_file, builder) in component_builders:
    try:
        results[builder] = {'builder': builder, 'builder_file': builder_file, 'result': builder.rebalance()}
        builder.validate()
    except RingBuilderError as err:
        self._builders = None
        raise RingBuilderError('An error occurred while rebalancing component %s: %s' % (builder_file, err))","for i, (builder_file, builder) in enumerate(component_builders):
    try:
        results[builder] = {'builder': builder, 'builder_file': builder_file, 'result': builder.rebalance()}
        builder.validate()
    except RingBuilderError as err:
        self._builders = None
        raise RingBuilderError('An error occurred while rebalancing component %s: %s' % (builder_file, err))"
stm32-rs,https://github.com/stm32-rs/stm32-rs/tree/master/scripts/group.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stm32-rs/scripts/group.py,,"def main(devices, output):
    print('Stage 1: Enumerating all fields in all devices')
    peripherals = {}
    for device_path in tqdm(glob.glob(os.path.join(devices, '*.yaml'))):
        device_name = os.path.splitext(os.path.basename(device_path))[0]
        with open(device_path, encoding='utf-8') as f:
            device = yaml.safe_load(f)
            device['_path'] = device_path
        if '_svd' not in device:
            raise RuntimeError('You must have an _svd key in the YAML file')
        svdpath = patch.abspath(device_path, device['_svd'])
        svd = ET.parse(svdpath)
        patch.process_device(svd, device)
        for ptag in svd.iter('peripheral'):
            if 'derivedFrom' in ptag.attrib:
                continue
            pname = ptag.find('name').text
            device_members = []
            for rtag in ptag.iter('register'):
                rname = rtag.find('name').text
                roffset = rtag.find('addressOffset').text
                for ftag in rtag.iter('field'):
                    fname = ftag.find('name').text
                    foffset = ftag.find('bitOffset').text
                    fwidth = ftag.find('bitWidth').text
                    device_members.append('{}__{}_{}__{}_{}_{}'.format(pname, roffset, rname, foffset, fwidth, fname))
            if pname not in peripherals:
                peripherals[pname] = {}
            peripherals[pname][device_name] = device_members
    print('Stage 2: Inverting to find all devices for each field and merging')
    fields = {}
    merged_fields = {}
    for (pname, devices) in tqdm(peripherals.items()):
        fields[pname] = {}
        for (device, device_fields) in devices.items():
            for fname in device_fields:
                if fname not in fields[pname]:
                    fields[pname][fname] = set()
                fields[pname][fname].add(device)
        merged_fields[pname] = {}
        skip_fields = set()
        for (field, devices) in fields[pname].items():
            if field in skip_fields:
                continue
            merged_key = [field]
            for (field2, devices2) in fields[pname].items():
                if field != field2 and devices == devices2:
                    merged_key.append(field2)
                    skip_fields.add(field2)
            merged_fields[pname][','.join(sorted(merged_key))] = devices
    print('Stage 3: Building tree of subsets')
    field_tree = {}
    for pname in tqdm(merged_fields):
        field_tree[pname] = {}
        for (fieldset, devices) in merged_fields[pname].items():
            field_tree[pname][fieldset] = (devices, {})

        def treeify(fieldset, devices, children, siblings):
            moved_siblings = []
            for (fieldset2, (devices2, children2)) in siblings.items():
                if fieldset2 != fieldset and devices2 < devices:
                    children[fieldset2] = (devices2, children2)
                    moved_siblings.append(fieldset2)
            for fieldset2 in moved_siblings:
                del siblings[fieldset2]
            moved_children = []
            for fieldset2 in moved_siblings:
                if fieldset2 in moved_children:
                    continue
                (devices2, children2) = children[fieldset2]
                moved_children += treeify(fieldset2, devices2, children2, children)
            return moved_siblings
        fieldsets = list(field_tree[pname].keys())
        moved_fieldsets = []
        for fieldset in fieldsets:
            if fieldset in moved_fieldsets:
                continue
            (devices, children) = field_tree[pname][fieldset]
            moved_fieldsets += treeify(fieldset, devices, children, field_tree[pname])

        def strip_devices(siblings):
            fieldsets = list(siblings.keys())
            for fieldset in fieldsets:
                (devices, children) = siblings[fieldset]
                if children:
                    strip_devices(children)
                    siblings[fieldset] = children
                else:
                    siblings[fieldset] = list(devices)
        strip_devices(field_tree[pname])
    print('Stage 4: Writing results JSON')
    with open(output, 'w') as f:
        json.dump(field_tree, f, indent=2, sort_keys=True)","for fieldset2 in moved_siblings:
    del siblings[fieldset2]","for i, fieldset2 in enumerate(moved_siblings):
    del siblings[fieldset2]"
onnx-tensorflow,https://github.com/onnx/onnx-tensorflow/tree/master/test/backend/test_node.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnx-tensorflow/test/backend/test_node.py,TestNode,"def test_scan_v8(self):
    if legacy_opset_pre_ver(8) or not legacy_opset_pre_ver(9):
        raise unittest.SkipTest('ONNX version {} not supported.'.format(defs.onnx_opset_version()))
    initial = self._get_rnd_int(0, 100, shape=[5, 1]).astype(np.float32)
    x1 = self._get_rnd_float32(0, 1000, shape=[5, 20, 6, 2])
    x2 = self._get_rnd_float32(0, 1000, shape=[5, 20, 6, 2])
    directions = [0, 1]
    sequence_lens = np.array([15, 20, 14, 18, 20]).astype(np.int32)
    Y = initial + (np.shape(x1)[1] if sequence_lens is str else np.reshape(sequence_lens, [-1, 1]))
    x1_out = x1 + 1
    x2_out = x2[:, ::-1] + 1
    Z = np.concatenate([x1_out, x2_out], 2)
    if sequence_lens is not str:
        for batch in range(len(sequence_lens)):
            shape = list(np.shape(Z[batch]))
            seq_len = sequence_lens[batch]
            zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])
            Z[batch] = np.concatenate([Z[batch][:seq_len], zero_pad])
    output = self._run_scan_node(initial, x1, x2, [6, 4], [3, 2], sequence_lens=sequence_lens, directions=directions)
    output_z = np.concatenate([output['z1'], output['z2'], output['z3'], output['z4']], 2)
    np.testing.assert_almost_equal(output['y'], Y)
    np.testing.assert_almost_equal(output_z, Z)","for batch in range(len(sequence_lens)):
    shape = list(np.shape(Z[batch]))
    seq_len = sequence_lens[batch]
    zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])
    Z[batch] = np.concatenate([Z[batch][:seq_len], zero_pad])","for batch, sequence_len in enumerate(sequence_lens):
    shape = list(np.shape(Z[batch]))
    seq_len = sequence_len
    zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])
    Z[batch] = np.concatenate([Z[batch][:seq_len], zero_pad])"
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","for file in files:
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {file} has {questions} questions and {answers} answers.')","for i, file in enumerate(files):
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {file} has {questions} questions and {answers} answers.')"
Kats,https://github.com/facebookresearch/Kats/tree/master/kats/tests/test_consts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kats/kats/tests/test_consts.py,TimeSeriesDataOpsTest,"def test_get_item(self) -> None:
    self.assertEqual(self.ts_date_transform_concat_univ[:len(self.ts_univ_1)], self.ts_univ_1)
    self.assertEqual(self.ts_date_transform_concat_multi[:len(self.ts_multi_1)], self.ts_multi_1)
    for col in self.ts_date_transform_concat_multi.value.columns:
        ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
        self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)
    self.assertEqual(self.ts_date_transform_concat_multi[MULTIVAR_VALUE_DF_COLS], self.ts_date_transform_concat_multi)
    self.assertEqual(self.ts_univ_1[:], self.ts_univ_1)
    self.assertEqual(self.ts_univ_1[0:0], TimeSeriesData(time=pd.Series(name=TIME_COL_NAME), value=pd.Series(name=VALUE_COL_NAME), time_col_name=TIME_COL_NAME))","for col in self.ts_date_transform_concat_multi.value.columns:
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)","for i, col in enumerate(self.ts_date_transform_concat_multi.value.columns):
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)"
pytorch2keras,https://github.com/gmalivenko/pytorch2keras/tree/master/tests/layers/convolutions/convtranspose2d.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch2keras/tests/layers/convolutions/convtranspose2d.py,,"if __name__ == '__main__':
    max_error = 0
    for kernel_size in [1, 3, 5]:
        for padding in [0, 1, 3]:
            for stride in [1, 2]:
                for bias in [True, False]:
                    outs = np.random.choice([1, 3, 7])
                    model = LayerTest(3, outs, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)
                    model.eval()
                    input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
                    input_var = Variable(torch.FloatTensor(input_np))
                    output = model(input_var)
                    k_model = pytorch_to_keras(model, input_var, (3, 224, 224), verbose=True)
                    error = check_error(output, k_model, input_np)
                    if max_error < error:
                        max_error = error
    print('Max error: {0}'.format(max_error))","for stride in [1, 2]:
    for bias in [True, False]:
        outs = np.random.choice([1, 3, 7])
        model = LayerTest(3, outs, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)
        model.eval()
        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
        input_var = Variable(torch.FloatTensor(input_np))
        output = model(input_var)
        k_model = pytorch_to_keras(model, input_var, (3, 224, 224), verbose=True)
        error = check_error(output, k_model, input_np)
        if max_error < error:
            max_error = error","for i, stride in enumerate([1, 2]):
    for j, bias in enumerate([True, False]):
        outs = np.random.choice([1, 3, 7])
        model = LayerTest(3, outs, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)
        model.eval()
        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
        input_var = Variable(torch.FloatTensor(input_np))
        output = model(input_var)
        k_model = pytorch_to_keras(model, input_var, (3, 224, 224), verbose=True)
        error = check_error(output, k_model, input_np)
        if max_error < error:
            max_error = error"
Hypernets,https://github.com/DataCanvasIO/Hypernets/tree/master/hypernets/tabular/dask_ex/_toolbox.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Hypernets/hypernets/tabular/dask_ex/_toolbox.py,DaskToolBox,"def exist_dask_dataframe(*args):
    for a in args:
        if isinstance(a, dd.DataFrame):
            return True
        if isinstance(a, (tuple, list, set)):
            return DaskToolBox.exist_dask_dataframe(*a)
    return False","for a in args:
    if isinstance(a, dd.DataFrame):
        return True
    if isinstance(a, (tuple, list, set)):
        return DaskToolBox.exist_dask_dataframe(*a)","for i, a in enumerate(args):
    if isinstance(a, dd.DataFrame):
        return True
    if isinstance(a, (tuple, list, set)):
        return DaskToolBox.exist_dask_dataframe(*a)"
FakeNewsNet,https://github.com/KaiDMML/FakeNewsNet/tree/master/code/tweet_collection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FakeNewsNet/code/tweet_collection.py,TweetCollector,"def collect_data(self, choices):
    for choice in choices:
        news_list = self.load_news_file(choice)
        collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for choice in choices:
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for i, choice in enumerate(choices):
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)"
pytorch-pose-hg-3d,https://github.com/xingyizhou/pytorch-pose-hg-3d/tree/master/src/lib/datasets/mpii.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-pose-hg-3d/src/lib/datasets/mpii.py,MPII,"def convert_eval_format(self, pred, conf, meta):
    ret = np.zeros((pred.shape[0], pred.shape[1], 2))
    for i in range(pred.shape[0]):
        ret[i] = transform_preds(pred[i], meta['center'][i].numpy(), meta['scale'][i].numpy(), [self.opt.output_h, self.opt.output_w])
    return ret","for i in range(pred.shape[0]):
    ret[i] = transform_preds(pred[i], meta['center'][i].numpy(), meta['scale'][i].numpy(), [self.opt.output_h, self.opt.output_w])","for i, shape in enumerate(pred.shape[0]):
    ret[i] = transform_preds(pred[i], meta['center'][i].numpy(), meta['scale'][i].numpy(), [self.opt.output_h, self.opt.output_w])"
ansible,https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_internal/containers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/lib/ansible_test/_internal/containers.py,,"def create_support_container_context(args, ssh, containers):
    """"""Context manager that provides SSH port forwards. Returns updated container metadata.""""""
    host_type = HostType.control
    revised = ContainerDatabase(containers.data.copy())
    source = revised.data.pop(HostType.origin, None)
    container_map = {}
    if host_type not in revised.data:
        if not source:
            raise Exception('Missing origin container details.')
        for (context_name, context) in source.items():
            for (container_name, container) in context.items():
                if '-controller-' in container_name:
                    continue
                for (port, access_port) in container.port_map():
                    container_map[container.host_ip, access_port] = (context_name, container_name, port)
    if not container_map:
        return SupportContainerContext(revised, None)
    if not ssh:
        raise Exception('The %s host was not pre-configured for container access and SSH forwarding is not available.' % host_type)
    forwards = list(container_map.keys())
    process = create_ssh_port_forwards(args, ssh, forwards)
    result = SupportContainerContext(revised, process)
    try:
        port_forwards = process.collect_port_forwards()
        contexts = {}
        for (forward, forwarded_port) in port_forwards.items():
            (access_host, access_port) = forward
            (context_name, container_name, container_port) = container_map[access_host, access_port]
            container = source[context_name][container_name]
            context = contexts.setdefault(context_name, {})
            forwarded_container = context.setdefault(container_name, ContainerAccess('127.0.0.1', container.names, None, {}))
            forwarded_container.forwards[container_port] = forwarded_port
            display.info('Container ""%s"" port %d available at %s:%d is forwarded over SSH as port %d.' % (container_name, container_port, access_host, access_port, forwarded_port), verbosity=1)
        revised.data[host_type] = contexts
        return result
    except Exception:
        result.close()
        raise","for (context_name, context) in source.items():
    for (container_name, container) in context.items():
        if '-controller-' in container_name:
            continue
        for (port, access_port) in container.port_map():
            container_map[container.host_ip, access_port] = (context_name, container_name, port)","for (i, (context_name, context)) in enumerate(source.items()):
    for (j, (container_name, container)) in enumerate(context.items()):
        if '-controller-' in container_name:
            continue
        for (k, (port, access_port)) in enumerate(container.port_map()):
            container_map[container.host_ip, access_port] = (context_name, container_name, port)"
TSD,https://github.com/Sense-X/TSD/tree/master/mmdet/models/detectors/reppoints_detector.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/detectors/reppoints_detector.py,RepPointsDetector,"def merge_aug_results(self, aug_bboxes, aug_scores, img_metas):
    """"""Merge augmented detection bboxes and scores.

        Args:
            aug_bboxes (list[Tensor]): shape (n, 4*#class)
            aug_scores (list[Tensor] or None): shape (n, #class)
            img_shapes (list[Tensor]): shape (3, ).

        Returns:
            tuple: (bboxes, scores)
        """"""
    recovered_bboxes = []
    for (bboxes, img_info) in zip(aug_bboxes, img_metas):
        img_shape = img_info[0]['img_shape']
        scale_factor = img_info[0]['scale_factor']
        flip = img_info[0]['flip']
        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
        recovered_bboxes.append(bboxes)
    bboxes = torch.cat(recovered_bboxes, dim=0)
    if aug_scores is None:
        return bboxes
    else:
        scores = torch.cat(aug_scores, dim=0)
        return (bboxes, scores)","for (bboxes, img_info) in zip(aug_bboxes, img_metas):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)","for i, (bboxes, img_info) in enumerate(zip(aug_bboxes, img_metas)):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)"
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/youtube.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/youtube.py,YoutubeIE,"def _get_automatic_captions(self, video_id, webpage):
    """"""We need the webpage for getting the captions url, pass it as an
           argument to speed up the process.""""""
    self.to_screen('%s: Looking for automatic captions' % video_id)
    player_config = self._get_ytplayer_config(video_id, webpage)
    err_msg = ""Couldn't find automatic captions for %s"" % video_id
    if not player_config:
        self._downloader.report_warning(err_msg)
        return {}
    try:
        args = player_config['args']
        caption_url = args.get('ttsurl')
        if caption_url:
            timestamp = args['timestamp']
            list_params = compat_urllib_parse_urlencode({'type': 'list', 'tlangs': 1, 'asrs': 1})
            list_url = caption_url + '&' + list_params
            caption_list = self._download_xml(list_url, video_id)
            original_lang_node = caption_list.find('track')
            if original_lang_node is None:
                self._downloader.report_warning(""Video doesn't have automatic captions"")
                return {}
            original_lang = original_lang_node.attrib['lang_code']
            caption_kind = original_lang_node.attrib.get('kind', '')
            sub_lang_list = {}
            for lang_node in caption_list.findall('target'):
                sub_lang = lang_node.attrib['lang_code']
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
                    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})
                sub_lang_list[sub_lang] = sub_formats
            return sub_lang_list

        def make_captions(sub_url, sub_langs):
            parsed_sub_url = compat_urllib_parse_urlparse(sub_url)
            caption_qs = compat_parse_qs(parsed_sub_url.query)
            captions = {}
            for sub_lang in sub_langs:
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    caption_qs.update({'tlang': [sub_lang], 'fmt': [ext]})
                    sub_url = compat_urlparse.urlunparse(parsed_sub_url._replace(query=compat_urllib_parse_urlencode(caption_qs, True)))
                    sub_formats.append({'url': sub_url, 'ext': ext})
                captions[sub_lang] = sub_formats
            return captions
        player_response = args.get('player_response')
        if player_response and isinstance(player_response, compat_str):
            player_response = self._parse_json(player_response, video_id, fatal=False)
            if player_response:
                renderer = player_response['captions']['playerCaptionsTracklistRenderer']
                base_url = renderer['captionTracks'][0]['baseUrl']
                sub_lang_list = []
                for lang in renderer['translationLanguages']:
                    lang_code = lang.get('languageCode')
                    if lang_code:
                        sub_lang_list.append(lang_code)
                return make_captions(base_url, sub_lang_list)
        caption_tracks = args['caption_tracks']
        caption_translation_languages = args['caption_translation_languages']
        caption_url = compat_parse_qs(caption_tracks.split(',')[0])['u'][0]
        sub_lang_list = []
        for lang in caption_translation_languages.split(','):
            lang_qs = compat_parse_qs(compat_urllib_parse_unquote_plus(lang))
            sub_lang = lang_qs.get('lc', [None])[0]
            if sub_lang:
                sub_lang_list.append(sub_lang)
        return make_captions(caption_url, sub_lang_list)
    except (KeyError, IndexError, ExtractorError):
        self._downloader.report_warning(err_msg)
        return {}","for ext in self._SUBTITLE_FORMATS:
    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})","for i, ext in enumerate(self._SUBTITLE_FORMATS):
    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})"
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/tests/convert_gl2tf_gconv2d.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/tests/convert_gl2tf_gconv2d.py,,"def conv2d(x, in_channels, out_channels, kernel_size, strides=1, padding=0, groups=1, use_bias=True, name='conv2d'):
    """"""
    Convolution 2D layer wrapper.

    Parameters:
    ----------
    x : Tensor
        Input tensor.
    in_channels : int
        Number of input channels.
    out_channels : int
        Number of output channels.
    kernel_size : int or tuple/list of 2 int
        Convolution window size.
    strides : int or tuple/list of 2 int
        Strides of the convolution.
    padding : int or tuple/list of 2 int
        Padding value for convolution layer.
    groups : int, default 1
        Number of groups.
    use_bias : bool, default False
        Whether the layer uses a bias vector.
    name : str, default 'conv2d'
        Layer name.

    Returns:
    -------
    Tensor
        Resulted tensor.
    """"""
    if isinstance(kernel_size, int):
        kernel_size = (kernel_size, kernel_size)
    if isinstance(strides, int):
        strides = (strides, strides)
    if isinstance(padding, int):
        padding = (padding, padding)
    if padding[0] > 0 or padding[1] > 0:
        x = tf.pad(x, [[0, 0], [0, 0], list(padding), list(padding)])
    if groups == 1:
        x = tf.layers.conv2d(inputs=x, filters=out_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name)
    elif groups == out_channels and out_channels == in_channels:
        kernel = tf.get_variable(name=name + '/dw_kernel', shape=kernel_size + (in_channels, 1), initializer=tf.variance_scaling_initializer(2.0))
        x = tf.nn.depthwise_conv2d(input=x, filter=kernel, strides=(1, 1) + strides, padding='VALID', rate=(1, 1), name=name, data_format='NCHW')
        if use_bias:
            raise NotImplementedError
    else:
        assert in_channels % groups == 0
        assert out_channels % groups == 0
        in_group_channels = in_channels // groups
        out_group_channels = out_channels // groups
        group_list = []
        for gi in range(groups):
            xi = x[:, gi * in_group_channels:(gi + 1) * in_group_channels, :, :]
            xi = tf.layers.conv2d(inputs=xi, filters=out_group_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name + '/convgroup{}'.format(gi + 1))
            group_list.append(xi)
        x = tf.concat(group_list, axis=1, name=name + '/concat')
    return x","for gi in range(groups):
    xi = x[:, gi * in_group_channels:(gi + 1) * in_group_channels, :, :]
    xi = tf.layers.conv2d(inputs=xi, filters=out_group_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name + '/convgroup{}'.format(gi + 1))
    group_list.append(xi)","for gi, group in enumerate(range(groups)):
    xi = x[:, gi * in_group_channels:(gi + 1) * in_group_channels, :, :]
    xi = tf.layers.conv2d(inputs=xi, filters=out_group_channels, kernel_size=kernel_size, strides=strides, padding='valid', data_format='channels_first', use_bias=use_bias, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0), name=name + '/convgroup{}'.format(gi + 1))
    group_list.append(xi)"
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]","for i, fextractor_type in enumerate(self.feature_dict):
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]"
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])","for i in range(RUN_STEP):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)","for i, _ in enumerate(range(RUN_STEP)):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)"
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/datasets/coco_hpe1_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/datasets/coco_hpe1_dataset.py,CocoHpe1Dataset,"def _check_load_keypoints(self, coco, entry):
    """"""
        Check and load ground-truth keypoints.
        """"""
    ann_ids = coco.getAnnIds(imgIds=entry['id'], iscrowd=False)
    objs = coco.loadAnns(ann_ids)
    valid_objs = []
    width = entry['width']
    height = entry['height']
    for obj in objs:
        contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
        if contiguous_cid >= self.num_class:
            continue
        if max(obj['keypoints']) == 0:
            continue
        (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
        if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
            continue
        joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
        for i in range(self.num_joints):
            joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
            joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
            visible = min(1, obj['keypoints'][i * 3 + 2])
            joints_3d[i, :2, 1] = visible
        if np.sum(joints_3d[:, 0, 1]) < 1:
            continue
        if self._check_centers:
            (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
            (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
            ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
            if num_vis / 80.0 + 47 / 80.0 > ks:
                continue
        valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})
    if not valid_objs:
        if not self._skip_empty:
            valid_objs.append({'bbox': np.array([-1, -1, 0, 0]), 'joints_3d': np.zeros((self.num_joints, 3, 2), dtype=np.float32)})
    return valid_objs","for obj in objs:
    contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
    if contiguous_cid >= self.num_class:
        continue
    if max(obj['keypoints']) == 0:
        continue
    (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
    if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
        continue
    joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
    for i in range(self.num_joints):
        joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
        joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
        visible = min(1, obj['keypoints'][i * 3 + 2])
        joints_3d[i, :2, 1] = visible
    if np.sum(joints_3d[:, 0, 1]) < 1:
        continue
    if self._check_centers:
        (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
        (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
        ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
        if num_vis / 80.0 + 47 / 80.0 > ks:
            continue
    valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})","for idx, obj in enumerate(objs):
    contiguous_cid = self.json_id_to_contiguous[obj['category_id']]
    if contiguous_cid >= self.num_class:
        continue
    if max(obj['keypoints']) == 0:
        continue
    (xmin, ymin, xmax, ymax) = self.bbox_clip_xyxy(self.bbox_xywh_to_xyxy(obj['bbox']), width, height)
    if obj['area'] <= 0 or xmax <= xmin or ymax <= ymin:
        continue
    joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)
    for i in range(self.num_joints):
        joints_3d[i, 0, 0] = obj['keypoints'][i * 3 + 0]
        joints_3d[i, 1, 0] = obj['keypoints'][i * 3 + 1]
        visible = min(1, obj['keypoints'][i * 3 + 2])
        joints_3d[i, :2, 1] = visible
    if np.sum(joints_3d[:, 0, 1]) < 1:
        continue
    if self._check_centers:
        (bbox_center, bbox_area) = self._get_box_center_area((xmin, ymin, xmax, ymax))
        (kp_center, num_vis) = self._get_keypoints_center_count(joints_3d)
        ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)
        if num_vis / 80.0 + 47 / 80.0 > ks:
            continue
    valid_objs.append({'bbox': (xmin, ymin, xmax, ymax), 'joints_3d': joints_3d})"
skll,https://github.com/EducationalTestingService/skll/tree/master/tests/test_input.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/skll/tests/test_input.py,,"def test_cv_folds_and_grid_search_folds():
    for ((task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search), (chosen_cv_folds, chosen_grid_search_folds)) in zip(product(['train', 'evaluate', 'predict', 'cross_validate'], [None, 5, join(train_dir, 'folds_file_test.csv')], [None, 7], [None, True, False]), [(None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (10, 5), (10, 5), (10, 5), (10, 7), (10, 7), (10, 7), (5, 5), (5, 5), (5, 5), (5, 7), (5, 7), (5, 7), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 5), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 7)]):
        yield (check_cv_folds_and_grid_search_folds, task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search, chosen_cv_folds, chosen_grid_search_folds)","for ((task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search), (chosen_cv_folds, chosen_grid_search_folds)) in zip(product(['train', 'evaluate', 'predict', 'cross_validate'], [None, 5, join(train_dir, 'folds_file_test.csv')], [None, 7], [None, True, False]), [(None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (10, 5), (10, 5), (10, 5), (10, 7), (10, 7), (10, 7), (5, 5), (5, 5), (5, 5), (5, 7), (5, 7), (5, 7), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 5), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 7)]):
    yield (check_cv_folds_and_grid_search_folds, task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search, chosen_cv_folds, chosen_grid_search_folds)","for idx, ((task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search), (chosen_cv_folds, chosen_grid_search_folds)) in enumerate(zip(product(['train', 'evaluate', 'predict', 'cross_validate'], [None, 5, join(train_dir, 'folds_file_test.csv')], [None, 7], [None, True, False]), [(None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 5), (None, 5), (None, 5), (None, 7), (None, 7), (None, 7), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (None, 'fold_mapping'), (10, 5), (10, 5), (10, 5), (10, 7), (10, 7), (10, 7), (5, 5), (5, 5), (5, 5), (5, 7), (5, 7), (5, 7), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 5), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 'fold_mapping'), ('fold_mapping', 7)])):
    yield (check_cv_folds_and_grid_search_folds, task, cv_folds_or_file, grid_search_folds, use_folds_file_for_grid_search, chosen_cv_folds, chosen_grid_search_folds)"
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)","for (img_path, annotation_path) in tqdm(filename_pairs):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)","for i, (img_path, annotation_path) in enumerate(tqdm(filename_pairs)):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)"
Tuxemon,https://github.com/Tuxemon/Tuxemon/tree/master/tuxemon/states/combat/combat.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tuxemon/tuxemon/states/combat/combat.py,CombatState,"def remove_monster_actions_from_queue(self, monster: Monster) -> None:
    """"""
        Remove all queued actions for a particular monster.

        This is used mainly for removing actions after monster is fainted.

        Parameters:
            monster: Monster whose actions will be removed.

        """"""
    to_remove = set()
    for action in self._action_queue:
        if action.user is monster or action.target is monster:
            to_remove.add(action)
    for action in to_remove:
        self._action_queue.remove(action)","for action in self._action_queue:
    if action.user is monster or action.target is monster:
        to_remove.add(action)","for i, action in enumerate(self._action_queue):
    if action.user is monster or action.target is monster:
        to_remove.add(action)"
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/youtube.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/youtube.py,YoutubeIE,"def _get_automatic_captions(self, video_id, webpage):
    """"""We need the webpage for getting the captions url, pass it as an
           argument to speed up the process.""""""
    self.to_screen('%s: Looking for automatic captions' % video_id)
    player_config = self._get_ytplayer_config(video_id, webpage)
    err_msg = ""Couldn't find automatic captions for %s"" % video_id
    if not player_config:
        self._downloader.report_warning(err_msg)
        return {}
    try:
        args = player_config['args']
        caption_url = args.get('ttsurl')
        if caption_url:
            timestamp = args['timestamp']
            list_params = compat_urllib_parse_urlencode({'type': 'list', 'tlangs': 1, 'asrs': 1})
            list_url = caption_url + '&' + list_params
            caption_list = self._download_xml(list_url, video_id)
            original_lang_node = caption_list.find('track')
            if original_lang_node is None:
                self._downloader.report_warning(""Video doesn't have automatic captions"")
                return {}
            original_lang = original_lang_node.attrib['lang_code']
            caption_kind = original_lang_node.attrib.get('kind', '')
            sub_lang_list = {}
            for lang_node in caption_list.findall('target'):
                sub_lang = lang_node.attrib['lang_code']
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    params = compat_urllib_parse_urlencode({'lang': original_lang, 'tlang': sub_lang, 'fmt': ext, 'ts': timestamp, 'kind': caption_kind})
                    sub_formats.append({'url': caption_url + '&' + params, 'ext': ext})
                sub_lang_list[sub_lang] = sub_formats
            return sub_lang_list

        def make_captions(sub_url, sub_langs):
            parsed_sub_url = compat_urllib_parse_urlparse(sub_url)
            caption_qs = compat_parse_qs(parsed_sub_url.query)
            captions = {}
            for sub_lang in sub_langs:
                sub_formats = []
                for ext in self._SUBTITLE_FORMATS:
                    caption_qs.update({'tlang': [sub_lang], 'fmt': [ext]})
                    sub_url = compat_urlparse.urlunparse(parsed_sub_url._replace(query=compat_urllib_parse_urlencode(caption_qs, True)))
                    sub_formats.append({'url': sub_url, 'ext': ext})
                captions[sub_lang] = sub_formats
            return captions
        player_response = args.get('player_response')
        if player_response and isinstance(player_response, compat_str):
            player_response = self._parse_json(player_response, video_id, fatal=False)
            if player_response:
                renderer = player_response['captions']['playerCaptionsTracklistRenderer']
                base_url = renderer['captionTracks'][0]['baseUrl']
                sub_lang_list = []
                for lang in renderer['translationLanguages']:
                    lang_code = lang.get('languageCode')
                    if lang_code:
                        sub_lang_list.append(lang_code)
                return make_captions(base_url, sub_lang_list)
        caption_tracks = args['caption_tracks']
        caption_translation_languages = args['caption_translation_languages']
        caption_url = compat_parse_qs(caption_tracks.split(',')[0])['u'][0]
        sub_lang_list = []
        for lang in caption_translation_languages.split(','):
            lang_qs = compat_parse_qs(compat_urllib_parse_unquote_plus(lang))
            sub_lang = lang_qs.get('lc', [None])[0]
            if sub_lang:
                sub_lang_list.append(sub_lang)
        return make_captions(caption_url, sub_lang_list)
    except (KeyError, IndexError, ExtractorError):
        self._downloader.report_warning(err_msg)
        return {}","for lang in renderer['translationLanguages']:
    lang_code = lang.get('languageCode')
    if lang_code:
        sub_lang_list.append(lang_code)","for i, lang in enumerate(renderer['translationLanguages']):
    lang_code = lang.get('languageCode')
    if lang_code:
        sub_lang_list.append(lang_code)"
django-cms,https://github.com/django-cms/django-cms/tree/master/cms/utils/placeholder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-cms/cms/utils/placeholder.py,,"def get_placeholders(template):
    compiled_template = get_template(template)
    placeholders = []
    nodes = _scan_placeholders(_get_nodelist(compiled_template))
    clean_placeholders = []
    for node in nodes:
        placeholder = node.get_declaration()
        slot = placeholder.slot
        if slot in clean_placeholders:
            warnings.warn('Duplicate {{% placeholder ""{0}"" %}} in template {1}.'.format(slot, template), DuplicatePlaceholderWarning)
        else:
            validate_placeholder_name(slot)
            placeholders.append(placeholder)
            clean_placeholders.append(slot)
    return placeholders","for node in nodes:
    placeholder = node.get_declaration()
    slot = placeholder.slot
    if slot in clean_placeholders:
        warnings.warn('Duplicate {{% placeholder ""{0}"" %}} in template {1}.'.format(slot, template), DuplicatePlaceholderWarning)
    else:
        validate_placeholder_name(slot)
        placeholders.append(placeholder)
        clean_placeholders.append(slot)","for i, node in enumerate(nodes):
    placeholder = node.get_declaration()
    slot = placeholder.slot
    if slot in clean_placeholders:
        warnings.warn('Duplicate {{% placeholder ""{0}"" %}} in template {1}'.format(slot, template), DuplicatePlaceholderWarning)
    else:
        validate_placeholder_name(slot)
        placeholders.append(placeholder)
        clean_placeholders.append(slot)"
Python_and_the_Web,https://github.com/Python-World/Python_and_the_Web/tree/master/Scripts/Miscellaneous/Connect4Game/connect4game.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python_and_the_Web/Scripts/Miscellaneous/Connect4Game/connect4game.py,,"def draw_board(board):
    for c in range(COLUMN_COUNT):
        for r in range(ROW_COUNT):
            pygame.draw.rect(screen, BLACK, (c * SQUARESIZE, r * SQUARESIZE + SQUARESIZE, SQUARESIZE, SQUARESIZE))
            pygame.draw.circle(screen, GREY, (int(c * SQUARESIZE + SQUARESIZE / 2), int(r * SQUARESIZE + SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    for c in range(COLUMN_COUNT):
        for r in range(ROW_COUNT):
            if board[r][c] == 1:
                pygame.draw.circle(screen, RED, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
            elif board[r][c] == 2:
                pygame.draw.circle(screen, YELLOW, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    pygame.display.update()","for r in range(ROW_COUNT):
    if board[r][c] == 1:
        pygame.draw.circle(screen, RED, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    elif board[r][c] == 2:
        pygame.draw.circle(screen, YELLOW, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)","for r, row in enumerate(board):
    if row[c] == 1:
        pygame.draw.circle(screen, RED, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)
    elif row[c] == 2:
        pygame.draw.circle(screen, YELLOW, (int(c * SQUARESIZE + SQUARESIZE / 2), height - int(r * SQUARESIZE + SQUARESIZE / 2)), RADIUS)"
rasa,https://github.com/RasaHQ/rasa/tree/master/tests/shared/nlu/training_data/test_features.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/tests/shared/nlu/training_data/test_features.py,,"def _generate_feature_list_and_modifications(is_sparse: bool, type: Text, number: int) -> Tuple[List[Features], List[Dict[Text, Any]]]:
    """"""Creates a list of features with the required properties and some modifications.
    The modifications are given by a list of kwargs dictionaries that can be used to
    instantiate `Features` that differ from the aforementioned list of features in
    exactly one property (i.e. type, sequence length (if the given `type` is
    sequence type only), attribute, origin)
    Args:
        is_sparse: whether all features should be sparse
        type: the type to be used for all features
        number: the number of features to generate
    Returns:
      a tuple containing a list of features with the requested attributes and
      a list of kwargs dictionaries that can be used to instantiate `Features` that
      differ from the aforementioned list of features in exactly one property
    """"""
    seq_len = 3
    first_dim = 1 if type == FEATURE_TYPE_SENTENCE else 3
    features_list = []
    for idx in range(number):
        matrix = np.full(shape=(first_dim, idx + 1), fill_value=idx + 1)
        if is_sparse:
            matrix = scipy.sparse.coo_matrix(matrix)
        config = dict(features=matrix, attribute='fixed-attribute', feature_type=type, origin=f'origin-{idx}')
        feat = Features(**config)
        features_list.append(feat)
    modifications = []
    modifications.append({**config, **{'attribute': 'OTHER'}})
    other_type = FEATURE_TYPE_SENTENCE if type == FEATURE_TYPE_SEQUENCE else FEATURE_TYPE_SEQUENCE
    other_seq_len = 1 if other_type == FEATURE_TYPE_SENTENCE else seq_len
    other_matrix = np.full(shape=(other_seq_len, number - 1), fill_value=number)
    if is_sparse:
        other_matrix = scipy.sparse.coo_matrix(other_matrix)
    modifications.append({**config, **{'feature_type': other_type, 'features': other_matrix}})
    modifications.append({**config, **{'origin': 'Other'}})
    if type == FEATURE_TYPE_SEQUENCE:
        matrix = np.full(shape=(seq_len + 1, idx + 1), fill_value=idx)
        if is_sparse:
            matrix = scipy.sparse.coo_matrix(matrix)
        modifications.append({**config, **{'features': matrix}})
    return (features_list, modifications)","for idx in range(number):
    matrix = np.full(shape=(first_dim, idx + 1), fill_value=idx + 1)
    if is_sparse:
        matrix = scipy.sparse.coo_matrix(matrix)
    config = dict(features=matrix, attribute='fixed-attribute', feature_type=type, origin=f'origin-{idx}')
    feat = Features(**config)
    features_list.append(feat)","for idx, num in enumerate(range(number)):
    matrix = np.full(shape=(first_dim, idx + 1), fill_value=idx + 1)
    if is_sparse:
        matrix = scipy.sparse.coo_matrix(matrix)
    config = dict(features=matrix, attribute='fixed-attribute', feature_type=type, origin=f'origin-{idx}')
    feat = Features(**config)
    features_list.append(feat)"
iou-tracker,https://github.com/bochinski/iou-tracker/tree/master//viou_tracker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/iou-tracker//viou_tracker.py,,"def track_viou_matlab_wrapper(frames_path, detections, sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio=1.0):
    """"""
    Matlab wrapper of the v-iou tracker for the detrac evaluation toolkit.

    Args:
         detections (numpy.array): numpy array of detections, usually supplied by run_tracker.m
         sigma_l (float): low detection threshold.
         sigma_h (float): high detection threshold.
         sigma_iou (float): IOU threshold.
         t_min (float): minimum track length in frames.

    Returns:
        float: speed in frames per second.
        list: list of tracks.
    """"""
    detections = detections.reshape((7, -1)).transpose()
    dets = load_mot(detections, with_classes=False)
    start = time()
    tracks = track_viou(frames_path + 'img{:05d}.jpg', dets, sigma_l, sigma_h, sigma_iou, int(t_min), int(ttl), tracker_type, keep_upper_height_ratio)
    end = time()
    id_ = 1
    out = []
    for track in tracks:
        for (i, bbox) in enumerate(track['bboxes']):
            out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
        id_ += 1
    num_frames = len(dets)
    speed = num_frames / (end - start)
    return (speed, out)","for track in tracks:
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
    id_ += 1","for id_, track in enumerate(tracks):
    for i, bbox in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]"
sympy,https://github.com/sympy/sympy/tree/master/sympy/printing/fortran.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/fortran.py,FCodePrinter,"def _pad_leading_columns(self, lines):
    result = []
    for line in lines:
        if line.startswith('!'):
            result.append(self._lead['comment'] + line[1:].lstrip())
        else:
            result.append(self._lead['code'] + line)
    return result","for line in lines:
    if line.startswith('!'):
        result.append(self._lead['comment'] + line[1:].lstrip())
    else:
        result.append(self._lead['code'] + line)","for i, line in enumerate(lines):
    if line.startswith('!'):
        result.append(self._lead['comment'] + line[1:].lstrip())
    else:
        result.append(self._lead['code'] + line)"
spaCy,https://github.com/explosion/spaCy/tree/master/spacy/training/batchers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/training/batchers.py,,"def minibatch_by_padded_size(seqs: Iterable[ItemT], size: Sizing, buffer: int=256, discard_oversize: bool=False, get_length: Callable=len) -> Iterable[List[ItemT]]:
    """"""Minibatch a sequence by the size of padded batches that would result,
    with sequences binned by length within a window.

    The padded size is defined as the maximum length of sequences within the
    batch multiplied by the number of sequences in the batch.

    size (int or Sequence[int]): The largest padded size to batch sequences into.
    buffer (int): The number of sequences to accumulate before sorting by length.
        A larger buffer will result in more even sizing, but if the buffer is
        very large, the iteration order will be less random, which can result
        in suboptimal training.
    discard_oversize (bool): Whether to discard sequences that are by themselves
        longer than the largest padded batch size.
    get_length (Callable or None): Function to get the length of a sequence item.
        The `len` function is used by default.
    """"""
    if isinstance(size, int):
        size_ = itertools.repeat(size)
    else:
        size_ = iter(size)
    for outer_batch in minibatch(seqs, size=buffer):
        outer_batch = list(outer_batch)
        target_size = next(size_)
        for indices in _batch_by_length(outer_batch, target_size, get_length):
            subbatch = [outer_batch[i] for i in indices]
            padded_size = max((len(seq) for seq in subbatch)) * len(subbatch)
            if discard_oversize and padded_size >= target_size:
                pass
            else:
                yield subbatch","for indices in _batch_by_length(outer_batch, target_size, get_length):
    subbatch = [outer_batch[i] for i in indices]
    padded_size = max((len(seq) for seq in subbatch)) * len(subbatch)
    if discard_oversize and padded_size >= target_size:
        pass
    else:
        yield subbatch","for i, indices in enumerate(_batch_by_length(outer_batch, target_size, get_length)):
    subbatch = [outer_batch[i] for i in indices]
    padded_size = max((len(seq) for seq in subbatch)) * len(subbatch)
    if discard_oversize and padded_size >= target_size:
        pass
    else:
        yield subbatch"
salt,https://github.com/saltstack/salt/tree/master/salt/utils/data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/data.py,,"def traverse_dict_and_list(data, key, default=None, delimiter=DEFAULT_TARGET_DELIM):
    """"""
    Traverse a dict or list using a colon-delimited (or otherwise delimited,
    using the 'delimiter' param) target string. The target 'foo:bar:0' will
    return data['foo']['bar'][0] if this value exists, and will otherwise
    return the dict in the default argument.
    Function will automatically determine the target type.
    The target 'foo:bar:0' will return data['foo']['bar'][0] if data like
    {'foo':{'bar':['baz']}} , if data like {'foo':{'bar':{'0':'baz'}}}
    then return data['foo']['bar']['0']
    """"""
    ptr = data
    if isinstance(key, str):
        key = key.split(delimiter)
    if isinstance(key, int):
        key = [key]
    for each in key:
        if isinstance(ptr, list):
            try:
                idx = int(each)
            except ValueError:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[each]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    return default
            else:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[idx]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    try:
                        ptr = ptr[idx]
                    except IndexError:
                        return default
        else:
            try:
                ptr = ptr[each]
            except KeyError:
                import salt.utils.args
                try:
                    loaded_key = salt.utils.args.yamlify_arg(each)
                except Exception:
                    return default
                if loaded_key == each:
                    return default
                else:
                    try:
                        ptr = ptr[loaded_key]
                    except (KeyError, TypeError):
                        return default
            except TypeError:
                return default
    return ptr","for embedded in (x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass","for i, embedded in enumerate(x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass"
numpy,https://github.com/numpy/numpy/tree/master/numpy/f2py/f2py2e.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/f2py/f2py2e.py,,"def callcrackfortran(files, options):
    rules.options = options
    crackfortran.debug = options['debug']
    crackfortran.verbose = options['verbose']
    if 'module' in options:
        crackfortran.f77modulename = options['module']
    if 'skipfuncs' in options:
        crackfortran.skipfuncs = options['skipfuncs']
    if 'onlyfuncs' in options:
        crackfortran.onlyfuncs = options['onlyfuncs']
    crackfortran.include_paths[:] = options['include_paths']
    crackfortran.dolowercase = options['do-lower']
    postlist = crackfortran.crackfortran(files)
    if 'signsfile' in options:
        outmess('Saving signatures to file ""%s""\n' % options['signsfile'])
        pyf = crackfortran.crack2fortran(postlist)
        if options['signsfile'][-6:] == 'stdout':
            sys.stdout.write(pyf)
        else:
            with open(options['signsfile'], 'w') as f:
                f.write(pyf)
    if options['coutput'] is None:
        for mod in postlist:
            mod['coutput'] = '%smodule.c' % mod['name']
    else:
        for mod in postlist:
            mod['coutput'] = options['coutput']
    if options['f2py_wrapper_output'] is None:
        for mod in postlist:
            mod['f2py_wrapper_output'] = '%s-f2pywrappers.f' % mod['name']
    else:
        for mod in postlist:
            mod['f2py_wrapper_output'] = options['f2py_wrapper_output']
    return postlist","for mod in postlist:
    mod['coutput'] = options['coutput']","for i, mod in enumerate(postlist):
    mod['coutput'] = options['coutput']"
elasticdl,https://github.com/sql-machine-learning/elasticdl/tree/master/elasticdl/python/common/evaluation_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elasticdl/elasticdl/python/common/evaluation_utils.py,EvaluationMetrics,"def reset_metric_states(self):
    """"""Resets all of the metric state variables.""""""
    for metrics in self._metrics_dict.values():
        for metric_inst in metrics.values():
            metric_inst.reset_states()","for metrics in self._metrics_dict.values():
    for metric_inst in metrics.values():
        metric_inst.reset_states()","for i, metrics in enumerate(self._metrics_dict.values()):
    for j, metric_inst in enumerate(metrics.values()):
        metric_inst.reset_states()"
goatools,https://github.com/tanghaibao/goatools/tree/master/goatools/grouper/grprobj.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/goatools/goatools/grouper/grprobj.py,Grouper,"def get_usrgos_g_hdrgos(self, hdrgos):
    """"""Return usrgos under provided hdrgos.""""""
    usrgos_all = set()
    if isinstance(hdrgos, str):
        hdrgos = [hdrgos]
    for hdrgo in hdrgos:
        usrgos_cur = self.hdrgo2usrgos.get(hdrgo, None)
        if usrgos_cur is not None:
            usrgos_all |= usrgos_cur
        if hdrgo in self.hdrgo_is_usrgo:
            usrgos_all.add(hdrgo)
    return usrgos_all","for hdrgo in hdrgos:
    usrgos_cur = self.hdrgo2usrgos.get(hdrgo, None)
    if usrgos_cur is not None:
        usrgos_all |= usrgos_cur
    if hdrgo in self.hdrgo_is_usrgo:
        usrgos_all.add(hdrgo)","for i, hdrgo in enumerate(hdrgos):
    usrgos_cur = self.hdrgo2usrgos.get(hdrgo, None)
    if usrgos_cur is not None:
        usrgos_all |= usrgos_cur
    if hdrgo in self.hdrgo_is_usrgo:
        usrgos_all.add(hdrgo)"
scrapy,https://github.com/scrapy/scrapy/tree/master/tests/test_http2_client_protocol.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scrapy/tests/test_http2_client_protocol.py,Https2ClientProtocolTestCase,"def test_status_codes(self):

    def assert_response_status(response: Response, expected_status: int):
        self.assertEqual(response.status, expected_status)
    d_list = []
    for status in [200, 404]:
        request = Request(self.get_url(f'/status?n={status}'))
        d = self.make_request(request)
        d.addCallback(assert_response_status, status)
        d.addErrback(self.fail)
        d_list.append(d)
    return DeferredList(d_list, fireOnOneErrback=True)","for status in [200, 404]:
    request = Request(self.get_url(f'/status?n={status}'))
    d = self.make_request(request)
    d.addCallback(assert_response_status, status)
    d.addErrback(self.fail)
    d_list.append(d)","for i, status in enumerate([200, 404]):
    request = Request(self.get_url(f'/status?n={status}'))
    d = self.make_request(request)
    d.addCallback(assert_response_status, status)
    d.addErrback(self.fail)
    d_list.append(d)"
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/zoo/evaluation/metrics/data_extraction.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/zoo/evaluation/metrics/data_extraction.py,,"if __name__ == '__main__':
    parser = argparse.ArgumentParser('policy-evaluation')
    parser.add_argument('path', help='Directory path containing data files (.jsonl)', type=str)
    parser.add_argument('result_path', help='Directory path containing data files', type=str)
    parser.add_argument('agent_name', help='Agent name', type=str)
    parser.add_argument('--step-num', help='Number of steps', type=str, default=600)
    parser.add_argument('--timestep-sec', type=float, default=0.1, help='Timestep, can be seen as the pause duration between frames')
    args = parser.parse_args()
    agent_name = args.agent_name
    jsonl_paths = list(Path(args.path).glob('*.jsonl'))
    step_num = int(args.step_num.split(':')[-1])
    assert len(jsonl_paths) == 1
    for jsonl in jsonl_paths:
        data = extract_data(jsonl, step_num, args.timestep_sec)
        time_suffix = datetime.now().strftime('%Y%m%d-%H%M%S')
        scenario_path = Path(args.path).parent.parent
        result_path = Path(args.result_path)
        result_file = result_path / f'evaluation-data_{scenario_path.name}_{agent_name}_{time_suffix}.json'
        with open(result_file, 'w') as f:
            json.dump(data, f)","for jsonl in jsonl_paths:
    data = extract_data(jsonl, step_num, args.timestep_sec)
    time_suffix = datetime.now().strftime('%Y%m%d-%H%M%S')
    scenario_path = Path(args.path).parent.parent
    result_path = Path(args.result_path)
    result_file = result_path / f'evaluation-data_{scenario_path.name}_{agent_name}_{time_suffix}.json'
    with open(result_file, 'w') as f:
        json.dump(data, f)","for i, jsonl in enumerate(jsonl_paths):
    data = extract_data(jsonl, step_num, args.timestep_sec)
    time_suffix = datetime.now().strftime('%Y%m%d-%H%M%S')
    scenario_path = Path(args.path).parent.parent
    result_path = Path(args.result_path)
    result_file = result_path / f'evaluation-data_{scenario_path.name}_{agent_name}_{time_suffix}.json'
    with open(result_file, 'w') as f:
        json.dump(data, f)"
video_analyst,https://github.com/MegviiDetection/video_analyst/tree/master/videoanalyst/evaluation/got_benchmark/experiments/got10k.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/video_analyst/videoanalyst/evaluation/got_benchmark/experiments/got10k.py,ExperimentGOT10k,"def run(self, tracker, visualize=False, save_video=False, overwrite_result=True, slicing_quantile=(0.0, 1.0)):
    if self.subset == 'test':
        print(""\x1b[93m[WARNING]:\nThe groundtruths of GOT-10k's test set is withholded.\nYou will have to submit your results to\n[http://got-10k.aitestunion.com/]\nto access the performance.\x1b[0m"")
        time.sleep(2)
    print('Running tracker %s on GOT-10k...' % tracker.name)
    self.dataset.return_meta = False
    (start_quantile, end_quantile) = slicing_quantile
    len_dataset = len(self.dataset)
    start_idx = int(len_dataset * start_quantile)
    end_idx = int(len_dataset * end_quantile)
    for s in range(start_idx, end_idx):
        (img_files, anno) = self.dataset[s]
        seq_name = self.dataset.seq_names[s]
        print('--Sequence %d/%d: %s' % (s + 1, len(self.dataset), seq_name))
        for r in range(self.repetitions):
            if r > 0 and tracker.is_deterministic:
                break
            elif r == 3 and self._check_deterministic(tracker.name, seq_name):
                print('  Detected a deterministic tracker, ' + 'skipping remaining trials.')
                break
            print(' Repetition: %d' % (r + 1))
            record_file = os.path.join(self.result_dir, tracker.name, seq_name, '%s_%03d.txt' % (seq_name, r + 1))
            if os.path.exists(record_file) and (not overwrite_result):
                print('  Found results, skipping', seq_name)
                continue
            (boxes, times) = tracker.track(img_files, anno[0, :], visualize=visualize)
            self._record(record_file, boxes, times)
        if save_video:
            video_dir = os.path.join(os.path.dirname(os.path.dirname(self.result_dir)), 'videos', 'GOT-10k', tracker.name)
            video_file = os.path.join(video_dir, '%s.avi' % seq_name)
            if not os.path.isdir(video_dir):
                os.makedirs(video_dir)
            image = Image.open(img_files[0])
            (img_W, img_H) = image.size
            out_video = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(*'MJPG'), 10, (img_W, img_H))
            for (ith, (img_file, pred)) in enumerate(zip(img_files, boxes)):
                image = Image.open(img_file)
                if not image.mode == 'RGB':
                    image = image.convert('RGB')
                img = np.array(image)[:, :, ::-1].copy()
                pred = pred.astype(int)
                cv2.rectangle(img, (pred[0], pred[1]), (pred[0] + pred[2], pred[1] + pred[3]), self.color['pred'], 2)
                if ith < anno.shape[0]:
                    gt = anno[ith].astype(int)
                    cv2.rectangle(img, (gt[0], gt[1]), (gt[0] + gt[2], gt[1] + gt[3]), self.color['gt'], 2)
                out_video.write(img)
            out_video.release()
            print('  Videos saved at', video_file)","for s in range(start_idx, end_idx):
    (img_files, anno) = self.dataset[s]
    seq_name = self.dataset.seq_names[s]
    print('--Sequence %d/%d: %s' % (s + 1, len(self.dataset), seq_name))
    for r in range(self.repetitions):
        if r > 0 and tracker.is_deterministic:
            break
        elif r == 3 and self._check_deterministic(tracker.name, seq_name):
            print('  Detected a deterministic tracker, ' + 'skipping remaining trials.')
            break
        print(' Repetition: %d' % (r + 1))
        record_file = os.path.join(self.result_dir, tracker.name, seq_name, '%s_%03d.txt' % (seq_name, r + 1))
        if os.path.exists(record_file) and (not overwrite_result):
            print('  Found results, skipping', seq_name)
            continue
        (boxes, times) = tracker.track(img_files, anno[0, :], visualize=visualize)
        self._record(record_file, boxes, times)
    if save_video:
        video_dir = os.path.join(os.path.dirname(os.path.dirname(self.result_dir)), 'videos', 'GOT-10k', tracker.name)
        video_file = os.path.join(video_dir, '%s.avi' % seq_name)
        if not os.path.isdir(video_dir):
            os.makedirs(video_dir)
        image = Image.open(img_files[0])
        (img_W, img_H) = image.size
        out_video = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(*'MJPG'), 10, (img_W, img_H))
        for (ith, (img_file, pred)) in enumerate(zip(img_files, boxes)):
            image = Image.open(img_file)
            if not image.mode == 'RGB':
                image = image.convert('RGB')
            img = np.array(image)[:, :, ::-1].copy()
            pred = pred.astype(int)
            cv2.rectangle(img, (pred[0], pred[1]), (pred[0] + pred[2], pred[1] + pred[3]), self.color['pred'], 2)
            if ith < anno.shape[0]:
                gt = anno[ith].astype(int)
                cv2.rectangle(img, (gt[0], gt[1]), (gt[0] + gt[2], gt[1] + gt[3]), self.color['gt'], 2)
            out_video.write(img)
        out_video.release()
        print('  Videos saved at', video_file)","for i, s in enumerate(range(start_idx, end_idx)):
    (img_files, anno) = self.dataset[s]
    seq_name = self.dataset.seq_names[s]
    print('--Sequence %d/%d: %s' % (i + 1, len(self.dataset), seq_name))
    for r in range(self.repetitions):
        if r > 0 and tracker.is_deterministic:
            break
        elif r == 3 and self._check_deterministic(tracker.name, seq_name):
            print('  Detected a deterministic tracker, ' + 'skipping remaining trials.')
            break
        print(' Repetition: %d' % (r + 1))
        record_file = os.path.join(self.result_dir, tracker.name, seq_name, '%s_%03d.txt' % (seq_name, r + 1))
        if os.path.exists(record_file) and (not overwrite_result):
            print('  Found results, skipping', seq_name)
            continue
        (boxes, times) = tracker.track(img_files, anno[0, :], visualize=visualize)
        self._record(record_file, boxes, times)
    if save_video:
        video_dir = os.path.join(os.path.dirname(os.path.dirname(self.result_dir)), 'videos', 'GOT-10k', tracker.name)
        video_file = os.path.join(video_dir, '%s.avi' % seq_name)
        if not os.path.isdir(video_dir):
            os.makedirs(video_dir)
        image = Image.open(img_files[0])
        (img_W, img_H) = image.size
        out_video = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(*'MJPG'), 10, (img_W, img_H))
        for (ith, (img_file, pred)) in enumerate(zip(img_files, boxes)):
            image = Image.open(img_file)
            if not image.mode == 'RGB':
                image = image.convert('RGB')
            img = np.array(image)[:, :, ::-1].copy()
            pred = pred.astype(int)
            cv2.rectangle(img, (pred[0], pred[1]), (pred[0] + pred[2], pred[1] + pred[3]), self.color['pred'], 2)
            if ith < anno.shape[0]:
                gt = anno[ith].astype(int)
                cv2.rectangle(img, (gt[0], gt[1]), (gt[0] + gt[2], gt[1] + gt[3]), self.color['gt'], 2)
            out_video.write(img)
        out_video.release()
        print('  Videos saved at', video_file)"
neural_prophet,https://github.com/ourownstory/neural_prophet/tree/master/neuralprophet/time_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_prophet/neuralprophet/time_dataset.py,TimeDataset,"def __getitem__(self, index):
    """"""Overrides parent class method to get an item at index.

        Args:
            index (int): sample location in dataset

        Returns:
            sample (OrderedDict): model inputs
                time (torch tensor, float), dims: (1)
                seasonalities (OrderedDict), named seasonalities, each with features
                    (torch tensor, float) of dims: (n_features[name])
                lags (torch tensor, float), dims: (n_lags)
                covariates (OrderedDict), named covariates, each with features
                    (np.array, float) of dims: (n_lags)
                events (OrderedDict), all events both additive and multiplicative,
                    each with features (np.array, float) of dims: (n_lags)
                regressors (OrderedDict), all regressors both additive and multiplicative,
                    each with features (np.array, float) of dims: (n_lags)
            targets (torch tensor, float): targets to be predicted, dims: (n_forecasts)
        """"""
    sample = OrderedDict({})
    for (key, data) in self.inputs.items():
        if key in self.two_level_inputs:
            sample[key] = OrderedDict({})
            for (name, period_features) in self.inputs[key].items():
                sample[key][name] = period_features[index]
        elif key == 'events' or key == 'regressors':
            sample[key] = OrderedDict({})
            for (mode, features) in self.inputs[key].items():
                sample[key][mode] = features[index, :, :]
        else:
            sample[key] = data[index]
    targets = self.targets[index]
    return (sample, targets)","for (key, data) in self.inputs.items():
    if key in self.two_level_inputs:
        sample[key] = OrderedDict({})
        for (name, period_features) in self.inputs[key].items():
            sample[key][name] = period_features[index]
    elif key == 'events' or key == 'regressors':
        sample[key] = OrderedDict({})
        for (mode, features) in self.inputs[key].items():
            sample[key][mode] = features[index, :, :]
    else:
        sample[key] = data[index]","for (index, (key, data)) in enumerate(self.inputs.items()):
    if key in self.two_level_inputs:
        sample[key] = OrderedDict({})
        for (name, period_features) in self.inputs[key].items():
            sample[key][name] = period_features[index]
    elif key == 'events' or key == 'regressors':
        sample[key] = OrderedDict({})
        for (mode, features) in self.inputs[key].items():
            sample[key][mode] = features[index, :, :]
    else:
        sample[key] = data[index]"
imbalanced-semi-self,https://github.com/YyzHarry/imbalanced-semi-self/tree/master/dataset/imbalance_svhn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imbalanced-semi-self/dataset/imbalance_svhn.py,ImbalanceSVHN,"def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):
    img_max = 1000
    img_num_per_cls = []
    if imb_type == 'exp':
        for cls_idx in range(cls_num):
            num = img_max * imb_factor ** (cls_idx / (cls_num - 1.0))
            img_num_per_cls.append(int(num))
    elif imb_type == 'step':
        for cls_idx in range(cls_num // 2):
            img_num_per_cls.append(int(img_max))
        for cls_idx in range(cls_num // 2):
            img_num_per_cls.append(int(img_max * imb_factor))
    else:
        img_num_per_cls.extend([int(img_max)] * cls_num)
    return img_num_per_cls","for cls_idx in range(cls_num // 2):
    img_num_per_cls.append(int(img_max))","for cls_idx, num in enumerate(range(cls_num // 2)):
    img_num_per_cls.append(int(img_max))"
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for line in input_file:
    filenames.append(line.rstrip())","for i, line in enumerate(input_file):
    filenames.append(line.rstrip())"
hachoir,https://github.com/vstinner/hachoir/tree/master/hachoir/parser/program/nds.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hachoir/hachoir/parser/program/nds.py,FATContent,"def createFields(self):
    num_entries = self.parent['header']['fat_size'].value // 8
    for i in range(0, num_entries):
        yield FATFileEntry(self, 'entry[]')","for i in range(0, num_entries):
    yield FATFileEntry(self, 'entry[]')","for i, _ in enumerate(range(0, num_entries)):
    yield FATFileEntry(self, 'entry[]')"
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)","for collider in collrates:
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table","for i, collider in enumerate(collrates):
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table"
AttGAN-Tensorflow,https://github.com/LynnHo/AttGAN-Tensorflow/tree/master//test_multi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AttGAN-Tensorflow//test_multi.py,,"def sample_graph():
    test_next = test_iter.get_next()
    if not os.path.exists(py.join(output_dir, 'generator.pb')):
        (Genc, Gdec, _) = module.get_model(args.model, n_atts, weight_decay=args.weight_decay)
        xa = tf.placeholder(tf.float32, shape=[None, args.crop_size, args.crop_size, 3])
        b_ = tf.placeholder(tf.float32, shape=[None, n_atts])
        x = Gdec(Genc(xa, training=False), b_, training=False)
    else:
        with tf.gfile.GFile(py.join(output_dir, 'generator.pb'), 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
            tf.import_graph_def(graph_def, name='generator')
        xa = sess.graph.get_tensor_by_name('generator/xa:0')
        b_ = sess.graph.get_tensor_by_name('generator/b_:0')
        x = sess.graph.get_tensor_by_name('generator/xb:0')
    save_dir = './output/%s/samples_testing_multi' % args.experiment_name
    tmp = ''
    for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
        tmp += '_%s_%s' % (test_att_name, '{:g}'.format(test_int))
    save_dir = py.join(save_dir, tmp[1:])
    py.mkdir(save_dir)

    def run():
        cnt = 0
        for _ in tqdm.trange(len_test_dataset):
            (xa_ipt, a_ipt) = sess.run(test_next)
            b_ipt = np.copy(a_ipt)
            for test_att_name in args.test_att_names:
                i = args.att_names.index(test_att_name)
                b_ipt[..., i] = 1 - b_ipt[..., i]
                b_ipt = data.check_attribute_conflict(b_ipt, test_att_name, args.att_names)
            b__ipt = (b_ipt * 2 - 1).astype(np.float32)
            for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
                i = args.att_names.index(test_att_name)
                b__ipt[..., i] = b__ipt[..., i] * test_int
            x_opt_list = [xa_ipt]
            x_opt = sess.run(x, feed_dict={xa: xa_ipt, b_: b__ipt})
            x_opt_list.append(x_opt)
            sample = np.transpose(x_opt_list, (1, 2, 0, 3, 4))
            sample = np.reshape(sample, (sample.shape[0], -1, sample.shape[2] * sample.shape[3], sample.shape[4]))
            for s in sample:
                cnt += 1
                im.imwrite(s, '%s/%d.jpg' % (save_dir, cnt))
    return run","for _ in tqdm.trange(len_test_dataset):
    (xa_ipt, a_ipt) = sess.run(test_next)
    b_ipt = np.copy(a_ipt)
    for test_att_name in args.test_att_names:
        i = args.att_names.index(test_att_name)
        b_ipt[..., i] = 1 - b_ipt[..., i]
        b_ipt = data.check_attribute_conflict(b_ipt, test_att_name, args.att_names)
    b__ipt = (b_ipt * 2 - 1).astype(np.float32)
    for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
        i = args.att_names.index(test_att_name)
        b__ipt[..., i] = b__ipt[..., i] * test_int
    x_opt_list = [xa_ipt]
    x_opt = sess.run(x, feed_dict={xa: xa_ipt, b_: b__ipt})
    x_opt_list.append(x_opt)
    sample = np.transpose(x_opt_list, (1, 2, 0, 3, 4))
    sample = np.reshape(sample, (sample.shape[0], -1, sample.shape[2] * sample.shape[3], sample.shape[4]))
    for s in sample:
        cnt += 1
        im.imwrite(s, '%s/%d.jpg' % (save_dir, cnt))","for i, _ in enumerate(range(len_test_dataset)):
    (xa_ipt, a_ipt) = sess.run(test_next)
    b_ipt = np.copy(a_ipt)
    for test_att_name in args.test_att_names:
        j = args.att_names.index(test_att_name)
        b_ipt[..., j] = 1 - b_ipt[..., j]
        b_ipt = data.check_attribute_conflict(b_ipt, test_att_name, args.att_names)
    b__ipt = (b_ipt * 2 - 1).astype(np.float32)
    for (test_att_name, test_int) in zip(args.test_att_names, args.test_ints):
        j = args.att_names.index(test_att_name)
        b__ipt[..., j] = b__ipt[..., j] * test_int
    x_opt_list = [xa_ipt]
    x_opt = sess.run(x, feed_dict={xa: xa_ipt, b_: b__ipt})
    x_opt_list.append(x_opt)
    sample = np.transpose(x_opt_list, (1, 2, 0, 3, 4))
    sample = np.reshape(sample, (sample.shape[0], -1, sample.shape[2] * sample.shape[3], sample.shape[4]))
    for s in sample:
        cnt += 1
        im.imwrite(s, '%s/%d.jpg' % (save_dir, cnt))"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/policy.py,LambdaMode,"def run_resource_set(self, event, resources):
    from c7n.actions import EventAction
    with self.policy.ctx:
        self.policy.ctx.metrics.put_metric('ResourceCount', len(resources), 'Count', Scope='Policy', buffer=False)
        if 'debug' in event:
            self.policy.log.info('Invoking actions %s', self.policy.resource_manager.actions)
        self.policy._write_file('resources.json', utils.dumps(resources, indent=2))
        for action in self.policy.resource_manager.actions:
            self.policy.log.info('policy:%s invoking action:%s resources:%d', self.policy.name, action.name, len(resources))
            if isinstance(action, EventAction):
                results = action.process(resources, event)
            else:
                results = action.process(resources)
            self.policy._write_file('action-%s' % action.name, utils.dumps(results))
    return resources","for action in self.policy.resource_manager.actions:
    self.policy.log.info('policy:%s invoking action:%s resources:%d', self.policy.name, action.name, len(resources))
    if isinstance(action, EventAction):
        results = action.process(resources, event)
    else:
        results = action.process(resources)
    self.policy._write_file('action-%s' % action.name, utils.dumps(results))","for i, action in enumerate(self.policy.resource_manager.actions):
    self.policy.log.info('policy:%s invoking action:%s resources:%d', self.policy.name, action.name, len(resources))
    if isinstance(action, EventAction):
        results = action.process(resources, event)
    else:
        results = action.process(resources)
    self.policy._write_file('action-%s' % action.name, utils.dumps(results))"
pyglet,https://github.com/pyglet/pyglet/tree/master/doc/ext/docstrings.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/doc/ext/docstrings.py,,"def convert(converter, start):
    (affected, result) = process_block(converter, lines, start)
    for x in range(start, start + affected + 1):
        del lines[start]
    for (i, line) in enumerate(result):
        lines.insert(start + i, line)","for x in range(start, start + affected + 1):
    del lines[start]","for i, x in enumerate(range(start, start + affected + 1)):
    del lines[start]"
mayavi,https://github.com/enthought/mayavi/tree/master/tvtk/indenter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/tvtk/indenter.py,VTKDocMassager,"def _rename_methods(self, doc):
    lines = doc.split('\n')
    nl = []
    for line in lines:
        words = line.split(' ')
        nw = []
        for word in words:
            if word[:3] == 'vtk':
                nw.append(word)
            else:
                nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
        nl.append(' '.join(nw))
    return '\n'.join(nl)","for word in words:
    if word[:3] == 'vtk':
        nw.append(word)
    else:
        nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))","for i, word in enumerate(words):
    if word[:3] == 'vtk':
        nw.append(word)
    else:
        nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))"
SDV,https://github.com/sdv-dev/SDV/tree/master//tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV//tasks.py,,"def install_minimum(c):
    with open('setup.py', 'r') as setup_py:
        lines = setup_py.read().splitlines()
    versions = []
    started = False
    for line in lines:
        if started:
            if line == ']':
                started = False
                continue
            line = line.strip()
            if _validate_python_version(line):
                requirement = re.match('[^>]*', line).group(0)
                requirement = re.sub('[\'"",]', '', requirement)
                version = re.search('>=?[^(,|#)]*', line).group(0)
                if version:
                    version = re.sub('>=?', '==', version)
                    version = re.sub('[\'"",]', '', version)
                    requirement += version
                versions.append(requirement)
        elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
            started = True
    c.run(f""python -m pip install {' '.join(versions)}"")","for line in lines:
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True","for i, line in enumerate(lines):
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True"
torch-points3d,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/base_architectures/backbone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/base_architectures/backbone.py,BackboneBasedModel,"def _flatten_compact_options(self, opt):
    """"""Converts from a dict of lists, to a list of dicts
        """"""
    flattenedOpts = []
    for index in range(int(1000000.0)):
        try:
            flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
        except IndexError:
            break
    return flattenedOpts","for index in range(int(1000000.0)):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break","for index, _ in enumerate(range(int(1000000.0))):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break"
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for res_type in result_types:
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}","for i, res_type in enumerate(result_types):
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}"
neon,https://github.com/NervanaSystems/neon/tree/master/neon/backends/autodiff.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neon/neon/backends/autodiff.py,Autodiff,"def get_grad_tensor(self, tensors):
    """"""
        Get gradient values in type Tensor w.r.t the list of `tensors`. If a
        tensor is not used, its gradient will be set to zero.

        Arguments:
            Tensors (list): List of Tensors to compute gradients on.

        Returns
            list: A list of Tensors, each of them is the gradent of the input
                  tensor.
        """"""
    grad_op_trees = self.get_grad_op_tree(tensors)
    grad_vals = []
    for grad_op_tree in grad_op_trees:
        grad_val = self.be.empty(grad_op_tree.shape)
        grad_val[:] = grad_op_tree
        grad_vals.append(grad_val)
    return grad_vals","for grad_op_tree in grad_op_trees:
    grad_val = self.be.empty(grad_op_tree.shape)
    grad_val[:] = grad_op_tree
    grad_vals.append(grad_val)","for i, grad_op_tree in enumerate(grad_op_trees):
    grad_val = self.be.empty(grad_op_tree.shape)
    grad_val[:] = grad_op_tree
    grad_vals.append(grad_val)"
fairscale,https://github.com/facebookresearch/fairscale/tree/master/tests/optim/test_single_node_adascale.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairscale/tests/optim/test_single_node_adascale.py,,"def run_a_bit(replay_data=None):
    data = []
    replay_data_idx = 0
    for _ in range(6):
        for i in range(num_grads_to_accum):
            if replay_data is None:
                in_data = torch.rand(in_dim).cuda()
                data.append(in_data)
            else:
                in_data = replay_data[replay_data_idx]
                replay_data_idx += 1
            out = model(in_data)
            out.sum().backward()
            if i == num_grads_to_accum - 1:
                optim.step()
                optim.zero_grad()
    return (out, data)","for i in range(num_grads_to_accum):
    if replay_data is None:
        in_data = torch.rand(in_dim).cuda()
        data.append(in_data)
    else:
        in_data = replay_data[replay_data_idx]
        replay_data_idx += 1
    out = model(in_data)
    out.sum().backward()
    if i == num_grads_to_accum - 1:
        optim.step()
        optim.zero_grad()","for i, num in enumerate(range(num_grads_to_accum)):
    if replay_data is None:
        in_data = torch.rand(in_dim).cuda()
        data.append(in_data)
    else:
        in_data = replay_data[replay_data_idx]
        replay_data_idx += 1
    out = model(in_data)
    out.sum().backward()
    if i == num_grads_to_accum - 1:
        optim.step()
        optim.zero_grad()"
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for recorder in self.recorders.values():
    recorders.append(recorder.to_dict())","for i, recorder in enumerate(self.recorders.values()):
    recorders.append(recorder.to_dict())"
angr,https://github.com/angr/angr/tree/master/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/storage/memory_mixins/paged_memory/paged_memory_mixin.py,PagedMemoryMixin,"def merge(self, others: Iterable['PagedMemoryMixin'], merge_conditions, common_ancestor=None) -> bool:
    changed_pages_and_offsets: Dict[int, Optional[Set[int]]] = {}
    for o in others:
        for (changed_page, changed_offsets) in self.changed_pages(o).items():
            if changed_offsets is None:
                changed_pages_and_offsets[changed_page] = None
            elif changed_page not in changed_pages_and_offsets:
                changed_pages_and_offsets[changed_page] = changed_offsets
            elif changed_pages_and_offsets[changed_page] is None:
                pass
            else:
                changed_pages_and_offsets[changed_page] = changed_pages_and_offsets[changed_page].union(changed_offsets)
    if merge_conditions is None:
        merge_conditions = [None] * (len(list(others)) + 1)
    merged_bytes = set()
    for page_no in sorted(changed_pages_and_offsets.keys()):
        l.debug('... on page %x', page_no)
        page = self._get_page(page_no, True)
        other_pages = []
        for o in others:
            if page_no in o._pages:
                other_pages.append(o._get_page(page_no, False))
        page_addr = page_no * self.page_size
        changed_offsets = changed_pages_and_offsets[page_no]
        merged_offsets = page.merge(other_pages, merge_conditions, page_addr=page_addr, memory=self, changed_offsets=changed_offsets)
        for off in merged_offsets:
            merged_bytes.add(page_addr + off)
    return bool(merged_bytes)","for off in merged_offsets:
    merged_bytes.add(page_addr + off)","for i, off in enumerate(merged_offsets):
    merged_bytes.add(page_addr + off)"
pygorithm,https://github.com/OmkarPathak/pygorithm/tree/master/pygorithm/dynamic_programming/lcs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygorithm/pygorithm/dynamic_programming/lcs.py,,"def longest_common_subsequence(s1, s2):
    """"""
    :param s1: string
    :param s2: string
    :return: int
    """"""
    (m, n) = (len(s1), len(s2))
    dp = [[0] * (n + 1)] * (m + 1)
    '\n    dp[i][j] : contains length of LCS of s1[0..i-1] and s2[0..j-1]\n    '
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i - 1] == s2[j - 1]:
                dp[i][j] = dp[i - 1][j - 1] + 1
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
    return dp[m][n]","for i in range(1, m + 1):
    for j in range(1, n + 1):
        if s1[i - 1] == s2[j - 1]:
            dp[i][j] = dp[i - 1][j - 1] + 1
        else:
            dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])","for i, num1 in enumerate(range(1, m + 1)):
    for j, num2 in enumerate(range(1, n + 1)):
        if s1[i - 1] == s2[j - 1]:
            dp[i][j] = dp[i - 1][j - 1] + 1
        else:
            dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for name in subreddit_names:
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]","for i, name in enumerate(subreddit_names):
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[i] += [reddit_res]"
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/os/map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/os/map.py,OSMap,"def handle_fleet_resolve(self, revert=False):
    """"""
        Check each fleet if afflicted with the low
        resolve debuff
        If so, handle by completing an easy zone

        Args:
            revert (bool): If go back to previous zone.

        Returns:
            bool:
        """"""
    if self.is_in_special_zone():
        logger.info('OS is in a special zone type, skip fleet resolve')
        return False
    for index in [1, 2, 3, 4]:
        if not self.fleet_set(index):
            self.device.screenshot()
        if self.fleet_low_resolve_appear():
            logger.info('At least one fleet is afflicted with the low resolve debuff')
            self.fleet_resolve(revert)
            return True
    logger.info('None of the fleets are afflicted with the low resolve debuff')
    return False","for index in [1, 2, 3, 4]:
    if not self.fleet_set(index):
        self.device.screenshot()
    if self.fleet_low_resolve_appear():
        logger.info('At least one fleet is afflicted with the low resolve debuff')
        self.fleet_resolve(revert)
        return True","for index, num in enumerate([1, 2, 3, 4]):
    if not self.fleet_set(index):
        self.device.screenshot()
    if self.fleet_low_resolve_appear():
        logger.info('At least one fleet is afflicted with the low resolve debuff')
        self.fleet_resolve(revert)
        return True"
nlp-beginner-finish,https://github.com/Alic-yuan/nlp-beginner-finish/tree/master/task5/dataHandler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-beginner-finish/task5/dataHandler.py,,"if __name__ == '__main__':
    from config import Config
    config = Config()
    (pad_data, char_to_ix, ix_to_chars) = get_data(config)
    for l in pad_data[:10]:
        print(l)
    n = 0
    for (k, v) in char_to_ix.items():
        print(k, v)
        if n > 10:
            break
        n += 1
    n = 0
    for (k, v) in ix_to_chars.items():
        print(k, v)
        if n > 10:
            break
        n += 1","for (k, v) in char_to_ix.items():
    print(k, v)
    if n > 10:
        break
    n += 1","for i, (k, v) in enumerate(char_to_ix.items()):
    print(k, v)
    if i > 10:
        break"
Deep-Reinforcement-Learning-Algorithms-with-PyTorch,https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/tree/master/environments/ant_environments/maze_env.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/environments/ant_environments/maze_env.py,MazeEnv,"def __init__(self, maze_id=None, maze_height=0.5, maze_size_scaling=8, n_bins=0, sensor_range=3.0, sensor_span=2 * math.pi, observe_blocks=False, put_spin_near_agent=False, top_down_view=False, manual_collision=False, *args, **kwargs):
    self._maze_id = maze_id
    model_cls = self.__class__.MODEL_CLASS
    if model_cls is None:
        raise 'MODEL_CLASS unspecified!'
    xml_path = os.path.join(MODEL_DIR, model_cls.FILE)
    import sys
    sys.path.insert(0, '/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments')
    sys.path.insert(0, '/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments/environments')
    print(os.getcwd())
    print(xml_path)
    tree = ET.parse('/Users/petroschristodoulou/Documents/Deep_RL_Implementations/Environments/ant_environments/' + xml_path[13:])
    worldbody = tree.find('.//worldbody')
    self.MAZE_HEIGHT = height = maze_height
    self.MAZE_SIZE_SCALING = size_scaling = maze_size_scaling
    self._n_bins = n_bins
    self._sensor_range = sensor_range * size_scaling
    self._sensor_span = sensor_span
    self._observe_blocks = observe_blocks
    self._put_spin_near_agent = put_spin_near_agent
    self._top_down_view = top_down_view
    self._manual_collision = manual_collision
    self.MAZE_STRUCTURE = structure = maze_env_utils.construct_maze(maze_id=self._maze_id)
    self.elevated = any((-1 in row for row in structure))
    self.blocks = any((any((maze_env_utils.can_move(r) for r in row)) for row in structure))
    (torso_x, torso_y) = self._find_robot()
    self._init_torso_x = torso_x
    self._init_torso_y = torso_y
    self._init_positions = [(x - torso_x, y - torso_y) for (x, y) in self._find_all_robots()]
    self._xy_to_rowcol = lambda x, y: (2 + (y + size_scaling / 2) / size_scaling, 2 + (x + size_scaling / 2) / size_scaling)
    self._view = np.zeros([5, 5, 3])
    height_offset = 0.0
    if self.elevated:
        height_offset = height * size_scaling
        torso = tree.find("".//body[@name='torso']"")
        torso.set('pos', '0 0 %.2f' % (0.75 + height_offset))
    if self.blocks:
        default = tree.find('.//default')
        default.find('.//geom').set('solimp', '.995 .995 .01')
    self.movable_blocks = []
    for i in range(len(structure)):
        for j in range(len(structure[0])):
            struct = structure[i][j]
            if struct == 'r' and self._put_spin_near_agent:
                struct = maze_env_utils.Move.SpinXY
            if self.elevated and struct not in [-1]:
                ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
            if struct == 1:
                ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
            elif maze_env_utils.can_move(struct):
                name = 'movable_%d_%d' % (i, j)
                self.movable_blocks.append((name, struct))
                falling = maze_env_utils.can_move_z(struct)
                spinning = maze_env_utils.can_spin(struct)
                x_offset = 0.25 * size_scaling if spinning else 0.0
                y_offset = 0.0
                shrink = 0.1 if spinning else 0.99 if falling else 1.0
                height_shrink = 0.1 if spinning else 1.0
                movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
                ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
                if maze_env_utils.can_move_x(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_move_y(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_move_z(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
                if maze_env_utils.can_spin(struct):
                    ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')
    torso = tree.find("".//body[@name='torso']"")
    geoms = torso.findall('.//geom')
    for geom in geoms:
        if 'name' not in geom.attrib:
            raise Exception('Every geom of the torso must have a name defined')
    (_, file_path) = tempfile.mkstemp(text=True, suffix='.xml')
    tree.write(file_path)
    self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)","for i in range(len(structure)):
    for j in range(len(structure[0])):
        struct = structure[i][j]
        if struct == 'r' and self._put_spin_near_agent:
            struct = maze_env_utils.Move.SpinXY
        if self.elevated and struct not in [-1]:
            ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
        if struct == 1:
            ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
        elif maze_env_utils.can_move(struct):
            name = 'movable_%d_%d' % (i, j)
            self.movable_blocks.append((name, struct))
            falling = maze_env_utils.can_move_z(struct)
            spinning = maze_env_utils.can_spin(struct)
            x_offset = 0.25 * size_scaling if spinning else 0.0
            y_offset = 0.0
            shrink = 0.1 if spinning else 0.99 if falling else 1.0
            height_shrink = 0.1 if spinning else 1.0
            movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
            ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
            if maze_env_utils.can_move_x(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_y(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_z(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_spin(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')","for i, row in enumerate(structure):
    for j, struct in enumerate(row):
        if struct == 'r' and self._put_spin_near_agent:
            struct = maze_env_utils.Move.SpinXY
        if self.elevated and struct not in [-1]:
            ET.SubElement(worldbody, 'geom', name='elevated_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.9 0.9 0.9 1')
        if struct == 1:
            ET.SubElement(worldbody, 'geom', name='block_%d_%d' % (i, j), pos='%f %f %f' % (j * size_scaling - torso_x, i * size_scaling - torso_y, height_offset + height / 2 * size_scaling), size='%f %f %f' % (0.5 * size_scaling, 0.5 * size_scaling, height / 2 * size_scaling), type='box', material='', contype='1', conaffinity='1', rgba='0.4 0.4 0.4 1')
        elif maze_env_utils.can_move(struct):
            name = 'movable_%d_%d' % (i, j)
            self.movable_blocks.append((name, struct))
            falling = maze_env_utils.can_move_z(struct)
            spinning = maze_env_utils.can_spin(struct)
            x_offset = 0.25 * size_scaling if spinning else 0.0
            y_offset = 0.0
            shrink = 0.1 if spinning else 0.99 if falling else 1.0
            height_shrink = 0.1 if spinning else 1.0
            movable_body = ET.SubElement(worldbody, 'body', name=name, pos='%f %f %f' % (j * size_scaling - torso_x + x_offset, i * size_scaling - torso_y + y_offset, height_offset + height / 2 * size_scaling * height_shrink))
            ET.SubElement(movable_body, 'geom', name='block_%d_%d' % (i, j), pos='0 0 0', size='%f %f %f' % (0.5 * size_scaling * shrink, 0.5 * size_scaling * shrink, height / 2 * size_scaling * height_shrink), type='box', material='', mass='0.001' if falling else '0.0002', contype='1', conaffinity='1', rgba='0.9 0.1 0.1 1')
            if maze_env_utils.can_move_x(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='1 0 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_x_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_y(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 1 0', damping='0.0', limited='true' if falling else 'false', range='%f %f' % (-size_scaling, size_scaling), margin='0.01', name='movable_y_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_move_z(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='true', range='%f 0' % -height_offset, margin='0.01', name='movable_z_%d_%d' % (i, j), pos='0 0 0', type='slide')
            if maze_env_utils.can_spin(struct):
                ET.SubElement(movable_body, 'joint', armature='0', axis='0 0 1', damping='0.0', limited='false', name='spinable_%d_%d' % (i, j), pos='0 0 0', type='ball')"
baserow,https://github.com/bram2w/baserow/tree/master/premium/backend/src/baserow_premium/license/handler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/baserow/premium/backend/src/baserow_premium/license/handler.py,LicenseHandler,"def send_license_info_and_fetch_license_status_with_authority(cls, license_objects: List[License]):
    license_payloads = []
    extra_license_info = []
    for license_object in license_objects:
        license_payloads.append(license_object.license)
        try:
            license_type = license_object.license_type
            extra_info = {'id': license_object.license_id, 'seats_taken': license_type.get_seats_taken(license_object), 'free_users_count': license_type.get_free_users_count(license_object)}
            extra_license_info.append(extra_info)
        except (InvalidLicenseError, UnsupportedLicenseError, DatabaseError):
            pass
    return cls.fetch_license_status_with_authority(license_payloads, extra_license_info)","for license_object in license_objects:
    license_payloads.append(license_object.license)
    try:
        license_type = license_object.license_type
        extra_info = {'id': license_object.license_id, 'seats_taken': license_type.get_seats_taken(license_object), 'free_users_count': license_type.get_free_users_count(license_object)}
        extra_license_info.append(extra_info)
    except (InvalidLicenseError, UnsupportedLicenseError, DatabaseError):
        pass","for i, license_object in enumerate(license_objects):
    license_payloads.append(license_object.license)
    try:
        license_type = license_object.license_type
        extra_info = {'id': license_object.license_id, 'seats_taken': license_type.get_seats_taken(license_object), 'free_users_count': license_type.get_free_users_count(license_object)}
        extra_license_info.append(extra_info)
    except (InvalidLicenseError, UnsupportedLicenseError, DatabaseError):
        pass"
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/db/ClientDB.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/db/ClientDB.py,DB,"def _DuplicatesDissolveMediaIdFromHashes(self, hashes):
    hash_ids = self.modules_hashes_local_cache.GetHashIds(hashes)
    for hash_id in hash_ids:
        media_id = self._DuplicatesGetMediaId(hash_id, do_not_create=True)
        if media_id is not None:
            self._DuplicatesDissolveMediaId(media_id)","for hash_id in hash_ids:
    media_id = self._DuplicatesGetMediaId(hash_id, do_not_create=True)
    if media_id is not None:
        self._DuplicatesDissolveMediaId(media_id)","for i, hash_id in enumerate(hash_ids):
    media_id = self._DuplicatesGetMediaId(hash_id, do_not_create=True)
    if media_id is not None:
        self._DuplicatesDissolveMediaId(media_id)"
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(1, c.shape[1]):
    f = f + c[:, k] * Pg ** k","for k, val in enumerate(range(1, c.shape[1])):
    f = f + c[:, k] * Pg ** val"
django-cms,https://github.com/django-cms/django-cms/tree/master/cms/cache/page.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-cms/cms/cache/page.py,,"def set_page_cache(response):
    from django.core.cache import cache
    request = response._request
    toolbar = get_toolbar_from_request(request)
    is_authenticated = request.user.is_authenticated
    if is_authenticated or toolbar._cache_disabled or (not get_cms_setting('PAGE_CACHE')):
        add_never_cache_headers(response)
        return response
    timestamp = now()
    placeholders = toolbar.content_renderer.get_rendered_placeholders()
    ttl_list = []
    vary_cache_on_set = set()
    for ph in placeholders:
        ttl = ph.get_cache_expiration(request, timestamp)
        vary_cache_on = ph.get_vary_cache_on(request)
        ttl_list.append(ttl)
        if ttl and vary_cache_on:
            vary_cache_on_set |= set(vary_cache_on)
    if EXPIRE_NOW not in ttl_list:
        ttl_list.append(get_cms_setting('CACHE_DURATIONS')['content'])
        ttl_list.append(MAX_EXPIRATION_TTL)
        if hasattr(settings, 'CMS_LIMIT_TTL_CACHE_FUNCTION'):
            extension_point = settings.CMS_LIMIT_TTL_CACHE_FUNCTION
            (module, func_name) = extension_point.rsplit('.', 1)
            module = import_module(module)
            limit_ttl_cache_function = getattr(module, func_name)
            limit_ttl = limit_ttl_cache_function(response)
            if isinstance(limit_ttl, int):
                ttl_list.append(limit_ttl)
        ttl = min(ttl_list)
        if ttl > 0:
            patch_response_headers(response, cache_timeout=ttl)
            patch_vary_headers(response, sorted(vary_cache_on_set))
            version = _get_cache_version()
            expires_datetime = timestamp + timedelta(seconds=ttl)
            response_headers = get_response_headers(response)
            cache.set(_page_cache_key(request), (response.content, response_headers, expires_datetime), ttl, version=version)
            _set_cache_version(version)
    return response","for ph in placeholders:
    ttl = ph.get_cache_expiration(request, timestamp)
    vary_cache_on = ph.get_vary_cache_on(request)
    ttl_list.append(ttl)
    if ttl and vary_cache_on:
        vary_cache_on_set |= set(vary_cache_on)","for i, ph in enumerate(placeholders):
    ttl = ph.get_cache_expiration(request, timestamp)
    vary_cache_on = ph.get_vary_cache_on(request)
    ttl_list.append(ttl)
    if ttl and vary_cache_on:
        vary_cache_on_set |= set(vary_cache_on)"
salt,https://github.com/saltstack/salt/tree/master/salt/states/grafana4_dashboard.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/grafana4_dashboard.py,,"def _dashboard_diff(_new_dashboard, _old_dashboard):
    """"""Return a dictionary of changes between dashboards.""""""
    diff = {}
    new_dashboard = copy.deepcopy(_new_dashboard)
    old_dashboard = copy.deepcopy(_old_dashboard)
    dashboard_diff = DictDiffer(new_dashboard, old_dashboard)
    diff['dashboard'] = _stripped({'changed': list(dashboard_diff.changed()) or None, 'added': list(dashboard_diff.added()) or None, 'removed': list(dashboard_diff.removed()) or None})
    new_rows = new_dashboard.get('rows', [])
    old_rows = old_dashboard.get('rows', [])
    new_rows_by_title = {}
    old_rows_by_title = {}
    for row in new_rows:
        if 'title' in row:
            new_rows_by_title[row['title']] = row
    for row in old_rows:
        if 'title' in row:
            old_rows_by_title[row['title']] = row
    rows_diff = DictDiffer(new_rows_by_title, old_rows_by_title)
    diff['rows'] = _stripped({'added': list(rows_diff.added()) or None, 'removed': list(rows_diff.removed()) or None})
    for changed_row_title in rows_diff.changed():
        old_row = old_rows_by_title[changed_row_title]
        new_row = new_rows_by_title[changed_row_title]
        row_diff = DictDiffer(new_row, old_row)
        diff['rows'].setdefault('changed', {})
        diff['rows']['changed'][changed_row_title] = _stripped({'changed': list(row_diff.changed()) or None, 'added': list(row_diff.added()) or None, 'removed': list(row_diff.removed()) or None})
    old_panels_by_id = {}
    new_panels_by_id = {}
    for row in old_dashboard.get('rows', []):
        for panel in row.get('panels', []):
            if 'id' in panel:
                old_panels_by_id[panel['id']] = panel
    for row in new_dashboard.get('rows', []):
        for panel in row.get('panels', []):
            if 'id' in panel:
                new_panels_by_id[panel['id']] = panel
    panels_diff = DictDiffer(new_panels_by_id, old_panels_by_id)
    diff['panels'] = _stripped({'added': list(panels_diff.added()) or None, 'removed': list(panels_diff.removed()) or None})
    for changed_panel_id in panels_diff.changed():
        old_panel = old_panels_by_id[changed_panel_id]
        new_panel = new_panels_by_id[changed_panel_id]
        panels_diff = DictDiffer(new_panel, old_panel)
        diff['panels'].setdefault('changed', {})
        diff['panels']['changed'][changed_panel_id] = _stripped({'changed': list(panels_diff.changed()) or None, 'added': list(panels_diff.added()) or None, 'removed': list(panels_diff.removed()) or None})
    return diff","for row in new_dashboard.get('rows', []):
    for panel in row.get('panels', []):
        if 'id' in panel:
            new_panels_by_id[panel['id']] = panel","for i, row in enumerate(new_dashboard.get('rows', [])):
    for panel in row.get('panels', []):
        if 'id' in panel:
            new_panels_by_id[panel['id']] = panel"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/ranking/TBPR.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/ranking/TBPR.py,TBPR,"def initModel(self):
    super(TBPR, self).initModel()
    self.strength = defaultdict(dict)
    self.weakTies = defaultdict(dict)
    self.strongTies = defaultdict(dict)
    self.weights = []
    for u1 in self.social.user:
        N_u1 = list(self.social.getFollowees(u1).keys())
        for u2 in self.social.getFollowees(u1):
            if u1 == u2:
                continue
            N_u2 = list(self.social.getFollowees(u2).keys())
            s = len(set(N_u1).intersection(set(N_u2))) / (len(set(N_u1).union(set(N_u2))) + 0.0)
            self.strength[u1][u2] = s
            self.weights.append(s)
    self.weights.sort()
    self.weights = np.array(self.weights)
    self.theta = np.median(self.weights)
    for u1 in self.strength:
        for u2 in self.strength[u1]:
            if self.strength[u1][u2] > self.theta:
                self.strongTies[u1][u2] = self.strength[u1][u2]
            else:
                self.weakTies[u1][u2] = self.strength[u1][u2]
    self.t_s = self.weights[len(self.weights) // 2 + 1:].sum() / (len(self.weights[len(self.weights) // 2 + 1:]) + 0.0)
    self.t_w = self.weights[0:len(self.weights) // 2].sum() / (len(self.weights[0:len(self.weights) // 2]) + 0.0)","for u2 in self.social.getFollowees(u1):
    if u1 == u2:
        continue
    N_u2 = list(self.social.getFollowees(u2).keys())
    s = len(set(N_u1).intersection(set(N_u2))) / (len(set(N_u1).union(set(N_u2))) + 0.0)
    self.strength[u1][u2] = s
    self.weights.append(s)","for i, u2 in enumerate(self.social.getFollowees(u1)):
    if u1 == u2:
        continue
    N_u2 = list(self.social.getFollowees(u2).keys())
    s = len(set(N_u1).intersection(set(N_u2))) / (len(set(N_u1).union(set(N_u2))) + 0.0)
    self.strength[u1][u2] = s
    self.weights.append(s)"
joinmarket-clientserver,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmclient/jmclient/wallet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmclient/jmclient/wallet.py,PSBTWalletMixin,"def sign_psbt(self, in_psbt, with_sign_result=False):
    """""" Given a serialized PSBT in raw binary format,
        iterate over the inputs and sign all that we can sign with this wallet.
        NB IT IS UP TO CALLERS TO ENSURE THAT THEY ACTUALLY WANT TO SIGN
        THIS TRANSACTION!
        The above is important especially in coinjoin scenarios.
        Return: (psbt, msg)
        msg: error message or None
        if not `with_sign_result`:
        psbt: signed psbt in binary serialization, or None if error.
        if `with_sign_result` True:
        psbt: (PSBT_SignResult object, psbt (deserialized) object)
        """"""
    try:
        new_psbt = btc.PartiallySignedTransaction.from_binary(in_psbt)
    except Exception as e:
        return (None, 'Unable to deserialize binary PSBT, error: ' + repr(e))
    privkeys = []
    for (k, v) in self._utxos._utxo.items():
        for (k2, v2) in v.items():
            key = self._get_key_from_path(v2[0])
            if FidelityBondMixin.is_timelocked_path(v2[0]) and len(key[0]) == 2:
                key = (key[0][0], key[1])
            privkeys.append(key)
    for vin in new_psbt.inputs:
        try:
            path = self.script_to_path(vin.utxo.scriptPubKey)
            key = self._get_key_from_path(path)
            if key not in privkeys:
                privkeys.append(key)
        except AssertionError:
            continue
        except AttributeError:
            continue
    jmckeys = list((btc.JMCKey(x[0][:-1]) for x in privkeys))
    new_keystore = btc.KeyStore.from_iterable(jmckeys)
    if isinstance(self, SegwitLegacyWallet):
        for (i, txinput) in enumerate(new_psbt.inputs):
            tu = txinput.witness_utxo
            if isinstance(tu, btc.CTxOut):
                if tu.scriptPubKey.is_witness_scriptpubkey():
                    continue
                elif tu.scriptPubKey.is_p2sh():
                    try:
                        path = self.script_to_path(tu.scriptPubKey)
                    except AssertionError:
                        continue
                    (privkey, _) = self._get_key_from_path(path)
                    txinput.redeem_script = btc.pubkey_to_p2wpkh_script(btc.privkey_to_pubkey(privkey))
    try:
        signresult = new_psbt.sign(new_keystore)
    except Exception as e:
        return (None, repr(e))
    if not with_sign_result:
        return (new_psbt.serialize(), None)
    else:
        return ((signresult, new_psbt), None)","for (k2, v2) in v.items():
    key = self._get_key_from_path(v2[0])
    if FidelityBondMixin.is_timelocked_path(v2[0]) and len(key[0]) == 2:
        key = (key[0][0], key[1])
    privkeys.append(key)","for i, (k2, v2) in enumerate(v.items()):
    key = self._get_key_from_path(v2[0])
    if FidelityBondMixin.is_timelocked_path(v2[0]) and len(key[0]) == 2:
        key = (key[0][0], key[1])
    privkeys.append(key)"
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for fn in possibleFiles:
    mass_store.append([False, fn[len(path):]])","for i, fn in enumerate(possibleFiles):
    mass_store.append([False, fn[len(path):]])"
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines","for check in valid_checks:
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')","for i, check in enumerate(valid_checks):
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')"
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","for data in ori_data:
    vertexSet = data['vertexSet']
    for label in data['labels']:
        rel = label['r']
        for n1 in vertexSet[label['h']]:
            for n2 in vertexSet[label['t']]:
                fact_in_train.add((n1['name'], n2['name'], rel))","for i, data in enumerate(ori_data):
    vertexSet = data['vertexSet']
    for j, label in enumerate(data['labels']):
        rel = label['r']
        for n1 in vertexSet[label['h']]:
            for n2 in vertexSet[label['t']]:
                fact_in_train.add((n1['name'], n2['name'], rel))"
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master/Target Offer/带锁的门.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython/Target Offer/带锁的门.py,,"def openDoor(n):
    if n == None or n <= 0:
        return
    doorList = [0] * (n + 1)
    for i in range(1, len(doorList)):
        j = i
        while j <= n:
            doorList[j] = 1 - doorList[j]
            j += i
    output = [i for (i, x) in enumerate(doorList) if x != 0]
    return output","for i in range(1, len(doorList)):
    j = i
    while j <= n:
        doorList[j] = 1 - doorList[j]
        j += i","for i, door in enumerate(doorList[1:], start=1):
    j = i
    while j <= n:
        doorList[j] = 1 - doorList[j]
        j += i"
