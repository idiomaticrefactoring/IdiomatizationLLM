repo_name,file_path,file_html,class_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
chainer,https://github.com/chainer/chainer/tree/master/examples/chainermn/seq2seq/seq2seq_mp1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chainer/examples/chainermn/seq2seq/seq2seq_mp1.py,Decoder,"def translate(self, xs, max_length=100):
    batch = len(xs)
    with chainer.no_backprop_mode():
        with chainer.using_config('train', False):
            result = []
            ys = self.xp.zeros(batch, self.xp.int32)
            eys = self.embed_y(ys)
            eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
            (h, c, ys, _) = self.mn_decoder(eys)
            cys = chainer.functions.concat(ys, axis=0)
            wy = self.W(cys)
            ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
            result.append(ys)
            for i in range(1, max_length):
                eys = self.embed_y(ys)
                eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
                (h, c, ys) = self.mn_decoder.actual_rnn(h, c, eys)
                cys = chainer.functions.concat(ys, axis=0)
                wy = self.W(cys)
                ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
                result.append(ys)
    result = cuda.to_cpu(self.xp.stack(result).T)
    outs = []
    for y in result:
        inds = numpy.argwhere(y == 0)
        if len(inds) > 0:
            y = y[:inds[0, 0]]
        outs.append(y)
    return outs","for i in range(1, max_length):
    eys = self.embed_y(ys)
    eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
    (h, c, ys) = self.mn_decoder.actual_rnn(h, c, eys)
    cys = chainer.functions.concat(ys, axis=0)
    wy = self.W(cys)
    ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
    result.append(ys)","for i in enumerate(range(1, max_length)):
    eys = self.embed_y(ys)
    eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
    (h, c, ys) = self.mn_decoder.actual_rnn(h, c, eys)
    cys = chainer.functions.concat(ys, axis=0)
    wy = self.W(cys)
    ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
    result.append(ys)",1
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model","for (name, array) in zip(names, init_params):
    name = name[6:]
    assert name[-2:] == ':0'
    name = name[:-2]
    name = name.split('/')
    pointer = model
    for m_name in name:
        if re.fullmatch('[A-Za-z]+\\d+', m_name):
            l = re.split('(\\d+)', m_name)
        else:
            l = [m_name]
        if l[0] == 'g':
            pointer = getattr(pointer, 'weight')
        elif l[0] == 'b':
            pointer = getattr(pointer, 'bias')
        elif l[0] == 'w':
            pointer = getattr(pointer, 'weight')
        else:
            pointer = getattr(pointer, l[0])
        if len(l) >= 2:
            num = int(l[1])
            pointer = pointer[num]
    try:
        assert pointer.shape == array.shape
    except AssertionError as e:
        e.args += (pointer.shape, array.shape)
        raise
    try:
        assert pointer.shape == array.shape
    except AssertionError as e:
        e.args += (pointer.shape, array.shape)
        raise
    logger.info('Initialize PyTorch weight {}'.format(name))
    pointer.data = torch.from_numpy(array)","for (i, (name, array)) in enumerate(zip(names, init_params)):
    name = name[6:]
    assert name[-2:] == ':0'
    name = name[:-2]
    name = name.split('/')
    pointer = model
    for m_name in name:
        if re.fullmatch('[A-Za-z]+\\d+', m_name):
            l = re.split('(\\d+)', m_name)
        else:
            l = [m_name]
        if l[0] == 'g':
            pointer = getattr(pointer, 'weight')
        elif l[0] == 'b':
            pointer = getattr(pointer, 'bias')
        elif l[0] == 'w':
            pointer = getattr(pointer, 'weight')
        else:
            pointer = getattr(pointer, l[0])
        if len(l) >= 2:
            num = int(l[1])
            pointer = pointer[num]
    try:
        assert pointer.shape == array.shape
    except AssertionError as e:
        e.args += (pointer.shape, array.shape)
        raise
    try:
        assert pointer.shape == array.shape
    except AssertionError as e:
        e.args += (pointer.shape, array.shape)
        raise
    logger.info('Initialize PyTorch weight {}'.format(name))
    pointer.data = torch.from_numpy(array)",1
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)","for inv_y in (False, True):
    for (vc, result) in results.items():
        net = pp.create_empty_network(sn_mva=16)
        add_network(net, vc)
        try:
            sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
        except Exception as e:
            raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
        check_results(net, vc, result)","for i, inv_y in enumerate((False, True)):
    for (vc, result) in results.items():
        net = pp.create_empty_network(sn_mva=16)
        add_network(net, vc)
        try:
            sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
        except Exception as e:
            raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
        check_results(net, vc, result)",1
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)","for (vc, result) in results.items():
    net = pp.create_empty_network(sn_mva=16)
    add_network(net, vc)
    try:
        sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
    except Exception as e:
        raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
    check_results(net, vc, result)","for i, (vc, result) in enumerate(results.items()):
    net = pp.create_empty_network(sn_mva=16)
    add_network(net, vc)
    try:
        sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
    except Exception as e:
        raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
    check_results(net, vc, result)",1
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","for line in f:
    infos = line.split(' ')
    length = len(infos)
    for i in range(1, length):
        width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
        height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
        dataSet.append([width, height])","for i, line in enumerate(f):
    infos = line.split(' ')
    length = len(infos)
    for j in range(1, length):
        width = int(infos[j].split(',')[2]) - int(infos[j].split(',')[0])
        height = int(infos[j].split(',')[3]) - int(infos[j].split(',')[1])
        dataSet.append([width, height])",1
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","for i in range(1, length):
    width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
    height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
    dataSet.append([width, height])","for i, info in enumerate(infos[1:], start=1):
    width = int(info.split(',')[2]) - int(info.split(',')[0])
    height = int(info.split(',')[3]) - int(info.split(',')[1])
    dataSet.append([width, height])",1
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/ToolsPage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/ToolsPage.py,Tools,"def __init__(self, gcode):
    self.gcode = gcode
    self.inches = False
    self.digits = 4
    self.active = StringVar()
    self.tools = {}
    self.buttons = {}
    self.widget = {}
    self.listbox = None
    for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
        tool = cls(self)
        self.addTool(tool)
    for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
        (name, ext) = os.path.splitext(os.path.basename(f))
        try:
            exec(f'import {name}')
            tool = eval(f'{name}.Tool(self)')
            self.addTool(tool)
        except (ImportError, AttributeError):
            (typ, val, tb) = sys.exc_info()
            traceback.print_exception(typ, val, tb)","for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
    tool = cls(self)
    self.addTool(tool)","for i, cls in enumerate([Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]):
    tool = cls(self)
    self.addTool(tool)",1
HUNT,https://github.com/bugcrowd/HUNT/tree/master/Burp/lib/issues.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HUNT/Burp/lib/issues.py,Issues,"def create_scanner_issues(self, view, callbacks, helpers, vuln_parameters, request_response):
    issues = self.issues
    json = self.json
    for vuln_parameter in vuln_parameters:
        issue_name = vuln_parameter['vuln_name']
        vuln_param = vuln_parameter['vuln_param']
        param_name = vuln_parameter['param']
        param_value = vuln_parameter['value']
        url = helpers.analyzeRequest(request_response).getUrl()
        url = urlparse.urlsplit(str(url))
        hostname = url.hostname
        path = url.path
        url = url.scheme + '://' + url.hostname + url.path
        http_service = request_response.getHttpService()
        http_messages = [callbacks.applyMarkers(request_response, None, None)]
        detail = json['issues'][issue_name]['detail']
        severity = 'Medium'
        scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
        is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
        if is_scanner_issue_dupe:
            continue
        else:
            self.set_scanner_issues(scanner_issue)
        issue_count = self.set_issue_count(issue_name, vuln_param)
        total_count = self.total_count[issue_name]
        view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
        view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)","for vuln_parameter in vuln_parameters:
    issue_name = vuln_parameter['vuln_name']
    vuln_param = vuln_parameter['vuln_param']
    param_name = vuln_parameter['param']
    param_value = vuln_parameter['value']
    url = helpers.analyzeRequest(request_response).getUrl()
    url = urlparse.urlsplit(str(url))
    hostname = url.hostname
    path = url.path
    url = url.scheme + '://' + url.hostname + url.path
    http_service = request_response.getHttpService()
    http_messages = [callbacks.applyMarkers(request_response, None, None)]
    detail = json['issues'][issue_name]['detail']
    severity = 'Medium'
    scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
    is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
    if is_scanner_issue_dupe:
        continue
    else:
        self.set_scanner_issues(scanner_issue)
    issue_count = self.set_issue_count(issue_name, vuln_param)
    total_count = self.total_count[issue_name]
    view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
    view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)","for i, vuln_parameter in enumerate(vuln_parameters):
    issue_name = vuln_parameter['vuln_name']
    vuln_param = vuln_parameter['vuln_param']
    param_name = vuln_parameter['param']
    param_value = vuln_parameter['value']
    url = helpers.analyzeRequest(request_response).getUrl()
    url = urlparse.urlsplit(str(url))
    hostname = url.hostname
    path = url.path
    url = url.scheme + '://' + url.hostname + url.path
    http_service = request_response.getHttpService()
    http_messages = [callbacks.applyMarkers(request_response, None, None)]
    detail = json['issues'][issue_name]['detail']
    severity = 'Medium'
    scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
    is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
    if is_scanner_issue_dupe:
        continue
    else:
        self.set_scanner_issues(scanner_issue)
    issue_count = self.set_issue_count(issue_name, vuln_param)
    total_count = self.total_count[issue_name]
    view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
    view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)",1
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for item in arguments:
    if item.startswith('choice'):
        if arguments[item][0] != '':
            uuidsplit = item.split('uuid-')
            if len(uuidsplit) > 1:
                choiceitems[uuidsplit[1]] = arguments[item][0]
            else:
                for flagoption in arguments[item]:
                    if len(flagoption) > 0:
                        FlagChoice.create_choice(flag, decode(flagoption))","for i, item in enumerate(arguments):
    if item.startswith('choice'):
        if arguments[item][0] != '':
            uuidsplit = item.split('uuid-')
            if len(uuidsplit) > 1:
                choiceitems[uuidsplit[1]] = arguments[item][0]
            else:
                for flagoption in arguments[item]:
                    if len(flagoption) > 0:
                        FlagChoice.create_choice(flag, decode(flagoption))",1
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for d in range(1, der + 1):
    if c.shape[1] >= 2:
        c = c[:, 1:maxN - d + 1]
    else:
        c = zeros((ng, 1))
        break
    for k in range(2, maxN - d + 1):
        c[:, k - 1] = c[:, k - 1] * k","for i in range(1, der + 1):
    if c.shape[1] >= 2:
        c = c[:, 1:maxN - i + 1]
    else:
        c = zeros((ng, 1))
        break
    for k in range(2, maxN - i + 1):
        c[:, k - 1] = c[:, k - 1] * k",1
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(2, maxN - d + 1):
    c[:, k - 1] = c[:, k - 1] * k","for i, k in enumerate(range(2, maxN - d + 1)):
    c[:, k - 1] = c[:, k - 1] * k",1
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(1, c.shape[1]):
    f = f + c[:, k] * Pg ** k","for i,k in enumerate(range(1, c.shape[1])):
    f = f + c[:, k] * Pg ** k",1
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-en/d2l/tensorflow.py,,"def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):
    """"""Defined in :numref:`sec_minibatches`""""""
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01), trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)
    (net, loss) = (lambda X: d2l.linreg(X, w, b), d2l.squared_loss)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35])
    (n, timer) = (0, d2l.Timer())
    for _ in range(num_epochs):
        for (X, y) in data_iter:
            with tf.GradientTape() as g:
                l = tf.math.reduce_mean(loss(net(X), y))
            (dw, db) = g.gradient(l, [w, b])
            trainer_fn([w, b], [dw, db], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n / X.shape[0]
                q = p / tf.data.experimental.cardinality(data_iter).numpy()
                r = (d2l.evaluate_loss(net, data_iter, loss),)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch')
    return (timer.cumsum(), animator.Y[0])","for _ in range(num_epochs):
    for (X, y) in data_iter:
        with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))
        (dw, db) = g.gradient(l, [w, b])
        trainer_fn([w, b], [dw, db], states, hyperparams)
        n += X.shape[0]
        if n % 200 == 0:
            timer.stop()
            p = n / X.shape[0]
            q = p / tf.data.experimental.cardinality(data_iter).numpy()
            r = (d2l.evaluate_loss(net, data_iter, loss),)
            animator.add(q, r)
            timer.start()","for epoch in range(num_epochs):
    for (X, y) in data_iter:
        with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))
        (dw, db) = g.gradient(l, [w, b])
        trainer_fn([w, b], [dw, db], states, hyperparams)
        n += X.shape[0]
        if n % 200 == 0:
            timer.stop()
            p = n / X.shape[0]
            q = p / tf.data.experimental.cardinality(data_iter).numpy()
            r = (d2l.evaluate_loss(net, data_iter, loss),)
            animator.add(q, r)
            timer.start()",1
faust,https://github.com/robinhood/faust/tree/master/t/stress/producer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/stress/producer.py,,"def install_produce_command(app) -> None:

    @app.command(option('--max-latency', type=float, default=0.5, envvar='PRODUCE_LATENCY', help='Add delay of (at most) n seconds between publishing.'), option('--max-messages', type=int, default=None, help='Send at most N messages or 0 for infinity.'))
    async def produce(self, max_latency: float, max_messages: int):
        """"""Produce example events.""""""
        prods = {aiter(p(max_messages)) for p in app.stress_producers}
        i = 0
        while not app.should_stop:
            to_remove: Set[Any] = set()
            for producer in prods:
                i += 1
                try:
                    await anext(producer)
                except StopAsyncIteration:
                    to_remove.add(producer)
                if not max_latency:
                    if not i % 10000:
                        self.say(f'+SEND {i}')
                elif not i % 10:
                    self.say(f'+SEND {i}')
            if not prods:
                await asyncio.sleep(1.0)
            if max_latency:
                await asyncio.sleep(random.uniform(0, max_latency))
            for producer in to_remove:
                prods.discard(producer)
        print('No more producers - exiting', file=sys.stderr)","for producer in prods:
    i += 1
    try:
        await anext(producer)
    except StopAsyncIteration:
        to_remove.add(producer)
    if not max_latency:
        if not i % 10000:
            self.say(f'+SEND {i}')
    elif not i % 10:
        self.say(f'+SEND {i}')","for i, producer in enumerate(prods):
    try:
        await anext(producer)
    except StopAsyncIteration:
        to_remove.add(producer)
    if not max_latency:
        if not (i+1) % 10000:
            self.say(f'+SEND {i+1}')
    elif not (i+1) % 10:
        self.say(f'+SEND {i+1}')",1
poutyne,https://github.com/GRAAL-Research/poutyne/tree/master/tests/framework/callbacks/test_mlflow_logger.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poutyne/tests/framework/callbacks/test_mlflow_logger.py,MLFlowLoggerTest,"def _assert_has_granularity_calls(self, ml_flow_client_patch):
    for _ in range(1, self.num_epochs):
        for step_number in range(1, self.steps_per_epoch):
            ml_flow_client_step_calls = []
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
            ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for _ in range(1, self.num_epochs):
    for step_number in range(1, self.steps_per_epoch):
        ml_flow_client_step_calls = []
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
        ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for epoch_number in range(1, self.num_epochs):
    for step_number in range(1, self.steps_per_epoch):
        ml_flow_client_step_calls = []
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
        ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)",1
poutyne,https://github.com/GRAAL-Research/poutyne/tree/master/tests/framework/callbacks/test_mlflow_logger.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poutyne/tests/framework/callbacks/test_mlflow_logger.py,MLFlowLoggerTest,"def _assert_has_granularity_calls(self, ml_flow_client_patch):
    for _ in range(1, self.num_epochs):
        for step_number in range(1, self.steps_per_epoch):
            ml_flow_client_step_calls = []
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
            ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for step_number in range(1, self.steps_per_epoch):
    ml_flow_client_step_calls = []
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
    ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for step_number, _ in enumerate(range(1, self.steps_per_epoch), start=1):
    ml_flow_client_step_calls = []
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
    ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)",1
curator,https://github.com/elastic/curator/tree/master/curator/indexlist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/curator/curator/indexlist.py,IndexList,"def filter_by_size(self, size_threshold=None, threshold_behavior='greater_than', exclude=False, size_behavior='primary'):
    """"""
        Remove indices from the actionable list based on index size.

        `threshold_behavior`, when set to `greater_than` (default), includes if it the index
        tests to be larger than `size_threshold`. When set to `less_than`, it includes if
        the index is smaller than `size_threshold`

        :arg size_threshold: Filter indices over *n* gigabytes
        :arg threshold_behavior: Size to filter, either ``greater_than`` or ``less_than``. Defaults
            to ``greater_than`` to preserve backwards compatability.
        :arg size_behavior: Size that used to filter, either ``primary`` or ``total``. Defaults to ``primary``
        :arg exclude: If `exclude` is `True`, this filter will remove matching
            indices from `indices`. If `exclude` is `False`, then only matching
            indices will be kept in `indices`.
            Default is `False`
        """"""
    self.loggit.debug('Filtering indices by index size')
    if not size_threshold:
        raise exceptions.MissingArgument('No value for ""size_threshold"" provided')
    if size_behavior not in ['primary', 'total']:
        raise ValueError('Invalid value for ""size_behavior"": {0}'.format(size_behavior))
    if threshold_behavior not in ['greater_than', 'less_than']:
        raise ValueError('Invalid value for ""threshold_behavior"": {0}'.format(threshold_behavior))
    index_size_limit = float(size_threshold) * 2 ** 30
    self.loggit.debug('Cannot get disk usage info from closed indices.  Omitting any closed indices.')
    self.filter_closed()
    working_list = self.working_list()
    for index in working_list:
        if size_behavior == 'primary':
            index_size = self.index_info[index]['primary_size_in_bytes']
        else:
            index_size = self.index_info[index]['size_in_bytes']
        msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
        if threshold_behavior == 'greater_than':
            self.__excludify(index_size > index_size_limit, exclude, index, msg)
        elif threshold_behavior == 'less_than':
            self.__excludify(index_size < index_size_limit, exclude, index, msg)","for index in working_list:
    if size_behavior == 'primary':
        index_size = self.index_info[index]['primary_size_in_bytes']
    else:
        index_size = self.index_info[index]['size_in_bytes']
    msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
    if threshold_behavior == 'greater_than':
        self.__excludify(index_size > index_size_limit, exclude, index, msg)
    elif threshold_behavior == 'less_than':
        self.__excludify(index_size < index_size_limit, exclude, index, msg)","for i, index in enumerate(working_list):
    if size_behavior == 'primary':
        index_size = self.index_info[index]['primary_size_in_bytes']
    else:
        index_size = self.index_info[index]['size_in_bytes']
    msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
    if threshold_behavior == 'greater_than':
        self.__excludify(index_size > index_size_limit, exclude, index, msg)
    elif threshold_behavior == 'less_than':
        self.__excludify(index_size < index_size_limit, exclude, index, msg)",1
Machine-Learning-Collection,https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Machine-Learning-Collection/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,,"def check_accuracy(loader, model, device='cuda'):
    num_correct = 0
    num_samples = 0
    model.eval()
    with torch.no_grad():
        for (x, y) in loader:
            x = x.to(device=device)
            y = y.to(device=device)
            scores = torch.sigmoid(model(x))
            predictions = (scores > 0.5).float()
            num_correct += (predictions == y).sum()
            num_samples += predictions.shape[0]
        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')
    model.train()","for (x, y) in loader:
    x = x.to(device=device)
    y = y.to(device=device)
    scores = torch.sigmoid(model(x))
    predictions = (scores > 0.5).float()
    num_correct += (predictions == y).sum()
    num_samples += predictions.shape[0]","for i, (x, y) in enumerate(loader):
    x = x.to(device=device)
    y = y.to(device=device)
    scores = torch.sigmoid(model(x))
    predictions = (scores > 0.5).float()
    num_correct += (predictions == y).sum()
    num_samples += predictions.shape[0]",1
EasyTransfer,https://github.com/alibaba/EasyTransfer/tree/master/scripts/fashion_bert/image_feature_extract.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/scripts/fashion_bert/image_feature_extract.py,PredictorImpl,"def search_pb(self, directory):
    """"""
    search pb file recursively, if multiple pb files exist, exception will be
    raised

    Returns:
      directory contain pb file
    """"""
    dir_list = []
    for (root, dirs, files) in tf.gfile.Walk(directory):
        for f in files:
            (_, ext) = os.path.splitext(f)
            if ext == '.pb':
                dir_list.append(root)
    if len(dir_list) == 0:
        raise ValueError('savedmodel is not found in directory %s' % directory)
    elif len(dir_list) > 1:
        raise ValueError('multiple saved model found in directory %s' % directory)
    return dir_list[0]","for f in files:
    (_, ext) = os.path.splitext(f)
    if ext == '.pb':
        dir_list.append(root)","for i, f in enumerate(files):
    (_, ext) = os.path.splitext(f)
    if ext == '.pb':
        dir_list.append(root)",1
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_dragonbones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_dragonbones.py,,"def get_bone_index(self, armature, bone_name):
    armature_bones = []
    for bone in armature.data.bones:
        armature_bones.append(bone)
    for (i, bone) in enumerate(armature_bones):
        if bone_name == bone.name:
            return i","for bone in armature.data.bones:
    armature_bones.append(bone)","for i, bone in enumerate(armature.data.bones):
    armature_bones.append(bone)",1
freeipa,https://github.com/freeipa/freeipa/tree/master/ipatests/test_webui/test_loginscreen.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_webui/test_loginscreen.py,TestLoginScreen,"def assert_form_equals(self, actual_form, expected_form):
    """"""
        Compare two forms
        """"""
    assert len(actual_form) == len(expected_form)
    for (act_row, exp_row) in zip(actual_form, expected_form):
        assert self.get_data_from_form_row(act_row) == exp_row","for (act_row, exp_row) in zip(actual_form, expected_form):
    assert self.get_data_from_form_row(act_row) == exp_row","for i, (act_row, exp_row) in enumerate(zip(actual_form, expected_form)):
    assert self.get_data_from_form_row(act_row) == exp_row",1
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for _index in range(len(domain_segments)):
    full_domain = '.'.join(domain_segments)
    potential_domain = Domain.query.filter(Domain.name == full_domain).first()
    if potential_domain in domains:
        domain = potential_domain
        break
    domain_segments.pop(0)","for _index, _ in enumerate(domain_segments):
    full_domain = '.'.join(domain_segments)
    potential_domain = Domain.query.filter(Domain.name == full_domain).first()
    if potential_domain in domains:
        domain = potential_domain
        break
    domain_segments.pop(0)",1
espresso,https://github.com/freewym/espresso/tree/master/fairseq/criterions/sentence_ranking.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/criterions/sentence_ranking.py,SentenceRankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute ranking loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    assert hasattr(model, 'classification_heads') and self.ranking_head_name in model.classification_heads, 'model must provide sentence ranking head for --criterion=sentence_ranking'
    scores = []
    for idx in range(self.num_classes):
        (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
        scores.append(score)
    logits = torch.cat(scores, dim=1)
    sample_size = logits.size(0)
    if 'target' in sample:
        targets = model.get_targets(sample, [logits]).view(-1)
        lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
        loss = F.nll_loss(lprobs, targets, reduction='sum')
    else:
        targets = None
        loss = torch.tensor(0.0, requires_grad=True)
    if self.prediction_h is not None:
        preds = logits.argmax(dim=1)
        for (i, (id, pred)) in enumerate(zip(sample['id'].tolist(), preds.tolist())):
            if targets is not None:
                label = targets[i].item()
                print('{}\t{}\t{}'.format(id, pred, label), file=self.prediction_h)
            else:
                print('{}\t{}'.format(id, pred), file=self.prediction_h)
    logging_output = {'loss': loss.data, 'ntokens': sample['ntokens'], 'nsentences': sample_size, 'sample_size': sample_size}
    if targets is not None:
        logging_output['ncorrect'] = (logits.argmax(dim=1) == targets).sum()
    return (loss, sample_size, logging_output)","for idx in range(self.num_classes):
    (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
    scores.append(score)","for idx in range(self.num_classes):
    (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
    scores.append(score)",1
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for i in range(len(x)):
    d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))","for i, _ in enumerate(x):
    d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))",1
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for j in range(len(y)):
    for i in range(len(x)):
        d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))","for j, y_j in enumerate(y):
    for i, x_i in enumerate(x):
        d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x_i != y_j), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))",1
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","for i in range(len(x)):
    d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))","for i, _ in enumerate(x):
    d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))",1
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/dns_data_management.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/dns_data_management.py,IPASystemRecords,"def _get_location_dns_records_for_server(self, zone_obj, hostname, locations, roles=None, include_master_role=True, include_kerberos_realm=True):
    server = self.servers_data[hostname]
    if roles:
        eff_roles = server['roles'] & roles
    else:
        eff_roles = server['roles']
    hostname_abs = DNSName(hostname).make_absolute()
    for location in locations:
        if location == self.servers_data[hostname]['location']:
            priority = self.PRIORITY_HIGH
        else:
            priority = self.PRIORITY_LOW
        if include_kerberos_realm:
            self.__add_kerberos_txt_rec(zone_obj, location)
        if include_master_role:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
            self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
        if 'AD trust controller' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
        if 'NTP server' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)
    return zone_obj","for location in locations:
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)","for i, location in enumerate(locations):
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)",1
napalm,https://github.com/napalm-automation/napalm/tree/master/napalm/iosxr_netconf/iosxr_netconf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/iosxr_netconf/iosxr_netconf.py,IOSXRNETCONFDriver,"def get_snmp_information(self):
    """"""Return the SNMP configuration.""""""
    snmp_information = {}
    rpc_reply = self.device.get_config(source='running', filter=('subtree', C.SNMP_RPC_REQ_FILTER)).xml
    snmp_result_tree = ETREE.fromstring(rpc_reply)
    _PRIVILEGE_MODE_MAP_ = {'read-only': 'ro', 'read-write': 'rw'}
    snmp_information = {'chassis_id': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:chassis-id', default='', namespaces=C.NS), 'contact': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:contact', default='', namespaces=C.NS), 'location': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:location', default='', namespaces=C.NS), 'community': {}}
    for community in snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS):
        name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
        privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
        acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
        snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}
    return snmp_information","for community in snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS):
    name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
    privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
    acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
    snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}","for i, community in enumerate(snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS)):
    name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
    privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
    acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
    snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}",1
youtube-dl,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/rtve.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/rtve.py,RTVEALaCartaIE,"def _extract_png_formats(self, video_id):
    png = self._download_webpage('http://www.rtve.es/ztnr/movil/thumbnail/%s/videos/%s.png' % (self._manager, video_id), video_id, 'Downloading url information', query={'q': 'v2'})
    q = qualities(['Media', 'Alta', 'HQ', 'HD_READY', 'HD_FULL'])
    formats = []
    for (quality, video_url) in self._decrypt_url(png):
        ext = determine_ext(video_url)
        if ext == 'm3u8':
            formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
        elif ext == 'mpd':
            formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
        else:
            formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})
    self._sort_formats(formats)
    return formats","for (quality, video_url) in self._decrypt_url(png):
    ext = determine_ext(video_url)
    if ext == 'm3u8':
        formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
    elif ext == 'mpd':
        formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
    else:
        formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})","for i, (quality, video_url) in enumerate(self._decrypt_url(png)):
    ext = determine_ext(video_url)
    if ext == 'm3u8':
        formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
    elif ext == 'mpd':
        formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
    else:
        formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})",1
ActualVim,https://github.com/lunixbochs/ActualVim/tree/master//edit.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ActualVim//edit.py,EditStep,"def resolve_args(self, view, edit):
    args = []
    for arg in self.args:
        if isinstance(arg, EditFuture):
            arg = arg.resolve(view, edit)
        args.append(arg)
    return args","for arg in self.args:
    if isinstance(arg, EditFuture):
        arg = arg.resolve(view, edit)
    args.append(arg)","for i,arg in enumerate(self.args):
    if isinstance(arg, EditFuture):
        arg = arg.resolve(view, edit)
    args.append(arg)",1
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for subdir in os.listdir(dir1):
    if osp.isdir(osp.join(dir1, subdir)):
        if check_keywords(subdir, keywords):
            dir_path1 = osp.join(dir1, subdir)
            for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                img_list = list()
                for dir in dir_list:
                    dir_path = dir_path1.replace(dir1, dir)
                    img_path = osp.join(dir_path, img_name)
                    img = cv2.imread(img_path)
                    img_list.append(img)
                    if img_path.find(dir1) >= 0:
                        res_img_path = img_path.replace(dir1, res_dir)
                if any([img is None for img in img_list]):
                    continue
                res_img = np.concatenate(img_list, axis=0)
                (h, w) = res_img.shape[:2]
                res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                res_img_path = res_img_path.replace('.png', '.jpg')
                ry_utils.make_subdir(res_img_path)
                cv2.imwrite(res_img_path, res_img)
                print(res_img_path)","for (i, subdir) in enumerate(os.listdir(dir1)):
    if osp.isdir(osp.join(dir1, subdir)):
        if check_keywords(subdir, keywords):
            dir_path1 = osp.join(dir1, subdir)
            for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                img_list = list()
                for dir in dir_list:
                    dir_path = dir_path1.replace(dir1, dir)
                    img_path = osp.join(dir_path, img_name)
                    img = cv2.imread(img_path)
                    img_list.append(img)
                    if img_path.find(dir1) >= 0:
                        res_img_path = img_path.replace(dir1, res_dir)
                if any([img is None for img in img_list]):
                    continue
                res_img = np.concatenate(img_list, axis=0)
                (h, w) = res_img.shape[:2]
                res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                res_img_path = res_img_path.replace('.png', '.jpg')
                ry_utils.make_subdir(res_img_path)
                cv2.imwrite(res_img_path, res_img)
                print(res_img_path)",1
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for dir in dir_list:
    dir_path = dir_path1.replace(dir1, dir)
    img_path = osp.join(dir_path, img_name)
    img = cv2.imread(img_path)
    img_list.append(img)
    if img_path.find(dir1) >= 0:
        res_img_path = img_path.replace(dir1, res_dir)","for i, dir in enumerate(dir_list):
    dir_path = dir_path1.replace(dir1, dir)
    img_path = osp.join(dir_path, img_name)
    img = cv2.imread(img_path)
    img_list.append(img)
    if img_path.find(dir1) >= 0:
        res_img_path = img_path.replace(dir1, res_dir)",1
manuskript,https://github.com/olivierkes/manuskript/tree/master/manuskript/functions/spellchecker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/manuskript/manuskript/functions/spellchecker.py,Spellchecker,"def getDictionary(dictionary):
    if not dictionary:
        dictionary = Spellchecker.getDefaultDictionary()
    if not dictionary:
        return None
    values = dictionary.split(':', 1)
    if len(values) == 1:
        (lib, name) = (Spellchecker.implementations[0].getLibraryName(), dictionary)
        dictionary = Spellchecker.normalizeDictName(lib, name)
    else:
        (lib, name) = values
    try:
        d = Spellchecker.dictionaries.get(dictionary, None)
        if d == None:
            for impl in Spellchecker.implementations:
                if impl.isInstalled() and lib == impl.getLibraryName():
                    d = impl(name)
                    Spellchecker.dictionaries[dictionary] = d
                    break
        return d
    except Exception as e:
        pass
    return None","for impl in Spellchecker.implementations:
    if impl.isInstalled() and lib == impl.getLibraryName():
        d = impl(name)
        Spellchecker.dictionaries[dictionary] = d
        break","for i, impl in enumerate(Spellchecker.implementations):
    if impl.isInstalled() and lib == impl.getLibraryName():
        d = impl(name)
        Spellchecker.dictionaries[dictionary] = d
        break",1
pootle,https://github.com/translate/pootle/tree/master/tests/views/admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pootle/tests/views/admin.py,,"def test_admin_view_project_delete_tp(english, client, admin):
    user = admin
    project = Project.objects.get(code='project0')
    tp = TranslationProject.objects.create(language=english, project=project)
    project.config['pootle.core.lang_mapping'] = {tp.language.code: 'foo'}
    client.login(username=user.username, password=TEST_USERS['admin']['password'])
    get_response = _admin_view_get(client, project)
    post_data = {}
    formset = get_response.context['formset']
    forms = formset.forms + formset.extra_forms + [formset.management_form]
    for form in forms:
        for field in form.fields:
            post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')
    tp_pk = post_data['form-0-id']
    post_data['form-0-DELETE'] = 'true'
    response = _admin_view_post(client, project, **post_data)
    assert tp_pk not in project.translationproject_set.values_list('pk', flat=True)
    _test_admin_view(response, project)
    assert project.config['pootle.core.lang_mapping'] == {}","for form in forms:
    for field in form.fields:
        post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')","for i, form in enumerate(forms):
    for field in form.fields:
        post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')",1
pootle,https://github.com/translate/pootle/tree/master/tests/views/admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pootle/tests/views/admin.py,,"def test_admin_view_project_delete_tp(english, client, admin):
    user = admin
    project = Project.objects.get(code='project0')
    tp = TranslationProject.objects.create(language=english, project=project)
    project.config['pootle.core.lang_mapping'] = {tp.language.code: 'foo'}
    client.login(username=user.username, password=TEST_USERS['admin']['password'])
    get_response = _admin_view_get(client, project)
    post_data = {}
    formset = get_response.context['formset']
    forms = formset.forms + formset.extra_forms + [formset.management_form]
    for form in forms:
        for field in form.fields:
            post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')
    tp_pk = post_data['form-0-id']
    post_data['form-0-DELETE'] = 'true'
    response = _admin_view_post(client, project, **post_data)
    assert tp_pk not in project.translationproject_set.values_list('pk', flat=True)
    _test_admin_view(response, project)
    assert project.config['pootle.core.lang_mapping'] == {}","for field in form.fields:
    post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')","for i, field in enumerate(form.fields):
    post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')",1
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","for index in self._ops.keys():
    term_strs.append('%s%s' % (self[index], index))","for i, index in enumerate(self._ops.keys()):
    term_strs.append('%s%s' % (self[index], index))",1
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i, _ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate",1
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i, _ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate",1
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i, _ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate",1
robosuite,https://github.com/ARISE-Initiative/robosuite/tree/master/robosuite/demos/demo_collect_and_playback_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/robosuite/robosuite/demos/demo_collect_and_playback_data.py,,"def collect_random_trajectory(env, timesteps=1000):
    """"""Run a random policy to collect trajectories.

    The rollout trajectory is saved to files in npz format.
    Modify the DataCollectionWrapper wrapper to add new fields or change data formats.

    Args:
        env (MujocoEnv): environment instance to collect trajectories from
        timesteps(int): how many environment timesteps to run for a given trajectory
    """"""
    env.reset()
    dof = env.action_dim
    for t in range(timesteps):
        action = np.random.randn(dof)
        env.step(action)
        env.render()
        if t % 100 == 0:
            print(t)","for t in range(timesteps):
    action = np.random.randn(dof)
    env.step(action)
    env.render()
    if t % 100 == 0:
        print(t)","for (i, t) in enumerate(range(timesteps)):
    action = np.random.randn(dof)
    env.step(action)
    env.render()
    if t % 100 == 0:
        print(t)",1
moto,https://github.com/spulec/moto/tree/master/moto/s3/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/s3/models.py,FakeMultipart,"def list_parts(self, part_number_marker, max_parts):
    max_marker = part_number_marker + max_parts
    for part_id in self.partlist[part_number_marker:max_marker]:
        yield self.parts[part_id]","for part_id in self.partlist[part_number_marker:max_marker]:
    yield self.parts[part_id]","for i, part_id in enumerate(self.partlist[part_number_marker:max_marker]):
    yield self.parts[part_id]",1
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","for (model, classes) in models:
    net = model(pretrained=pretrained)
    ctx = mx.cpu()
    if not pretrained:
        net.initialize(ctx=ctx)
    net_params = net.collect_params()
    weight_count = 0
    for param in net_params.values():
        if param.shape is None or not param._differentiable:
            continue
        weight_count += np.prod(param.shape)
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resdropresnet20_cifar10 or weight_count == 272474
    assert model != resdropresnet20_cifar100 or weight_count == 278324
    assert model != resdropresnet20_svhn or weight_count == 272474
    x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
    y = net(x)
    assert y.shape == (14, classes)","for (i, (model, classes)) in enumerate(models):
    net = model(pretrained=pretrained)
    ctx = mx.cpu()
    if not pretrained:
        net.initialize(ctx=ctx)
    net_params = net.collect_params()
    weight_count = 0
    for param in net_params.values():
        if param.shape is None or not param._differentiable:
            continue
        weight_count += np.prod(param.shape)
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resdropresnet20_cifar10 or weight_count == 272474
    assert model != resdropresnet20_cifar100 or weight_count == 278324
    assert model != resdropresnet20_svhn or weight_count == 272474
    x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
    y = net(x)
    assert y.shape == (14, classes)",1
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","for param in net_params.values():
    if param.shape is None or not param._differentiable:
        continue
    weight_count += np.prod(param.shape)","for i, param in enumerate(net_params.values()):
    if param.shape is None or not param._differentiable:
        continue
    weight_count += np.prod(param.shape)",1
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","for i in range(self.FILE_HEADER.NumberOfSections):
    if i >= MAX_SECTIONS:
        self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
        break
    simultaneous_errors = 0
    section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
    if not section:
        break
    section_offset = offset + section.sizeof() * i
    section.set_file_offset(section_offset)
    section_data = self.__data__[section_offset:section_offset + section.sizeof()]
    if count_zeroes(section_data) == section.sizeof():
        self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
        break
    if not section_data:
        self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
        break
    section.__unpack__(section_data)
    self.__structures__.append(section)
    if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
    if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
    if section.Misc_VirtualSize > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
    if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
    if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
    if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
        self.__warnings.append('Too many warnings parsing section. Aborting.')
        break
    section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
    set_flags(section, section.Characteristics, section_flags)
    if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
        if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
            pass
        else:
            self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
    self.sections.append(section)","for i in range(self.FILE_HEADER.NumberOfSections):
    if i >= MAX_SECTIONS:
        self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
        break
    simultaneous_errors = 0
    section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
    if not section:
        break
    section_offset = offset + section.sizeof() * i
    section.set_file_offset(section_offset)
    section_data = self.__data__[section_offset:section_offset + section.sizeof()]
    if count_zeroes(section_data) == section.sizeof():
        self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
        break
    if not section_data:
        self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
        break
    section.__unpack__(section_data)
    self.__structures__.append(section)
    if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
    if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
    if section.Misc_VirtualSize > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
    if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
    if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
    if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
        self.__warnings.append('Too many warnings parsing section. Aborting.')
        break
    section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
    set_flags(section, section.Characteristics, section_flags)
    if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
        if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
            pass
        else:
            self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
    self.sections.append(section)",1
contrastive-unpaired-translation,https://github.com/taesungp/contrastive-unpaired-translation/tree/master/datasets/make_dataset_aligned.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/contrastive-unpaired-translation/datasets/make_dataset_aligned.py,,"def align_images(a_file_paths, b_file_paths, target_path):
    if not os.path.exists(target_path):
        os.makedirs(target_path)
    for i in range(len(a_file_paths)):
        img_a = Image.open(a_file_paths[i])
        img_b = Image.open(b_file_paths[i])
        assert img_a.size == img_b.size
        aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
        aligned_image.paste(img_a, (0, 0))
        aligned_image.paste(img_b, (img_a.size[0], 0))
        aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))","for i in range(len(a_file_paths)):
    img_a = Image.open(a_file_paths[i])
    img_b = Image.open(b_file_paths[i])
    assert img_a.size == img_b.size
    aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
    aligned_image.paste(img_a, (0, 0))
    aligned_image.paste(img_b, (img_a.size[0], 0))
    aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))","for i, (a_file_path, b_file_path) in enumerate(zip(a_file_paths, b_file_paths)):
    img_a = Image.open(a_file_path)
    img_b = Image.open(b_file_path)
    assert img_a.size == img_b.size
    aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
    aligned_image.paste(img_a, (0, 0))
    aligned_image.paste(img_b, (img_a.size[0], 0))
    aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))",1
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resnet_cub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/resnet_cub.py,,"def _test():
    import numpy as np
    import chainer
    chainer.global_config.train = False
    pretrained = False
    models = [resnet10_cub, resnet12_cub, resnet14_cub, resnetbc14b_cub, resnet16_cub, resnet18_cub, resnet26_cub, resnetbc26b_cub, resnet34_cub, resnetbc38b_cub, resnet50_cub, resnet50b_cub, resnet101_cub, resnet101b_cub, resnet152_cub, resnet152b_cub, resnet200_cub, resnet200b_cub]
    for model in models:
        net = model(pretrained=pretrained)
        weight_count = net.count_params()
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resnet10_cub or weight_count == 5008392
        assert model != resnet12_cub or weight_count == 5082376
        assert model != resnet14_cub or weight_count == 5377800
        assert model != resnetbc14b_cub or weight_count == 8425736
        assert model != resnet16_cub or weight_count == 6558472
        assert model != resnet18_cub or weight_count == 11279112
        assert model != resnet26_cub or weight_count == 17549832
        assert model != resnetbc26b_cub or weight_count == 14355976
        assert model != resnet34_cub or weight_count == 21387272
        assert model != resnetbc38b_cub or weight_count == 20286216
        assert model != resnet50_cub or weight_count == 23917832
        assert model != resnet50b_cub or weight_count == 23917832
        assert model != resnet101_cub or weight_count == 42909960
        assert model != resnet101b_cub or weight_count == 42909960
        assert model != resnet152_cub or weight_count == 58553608
        assert model != resnet152b_cub or weight_count == 58553608
        assert model != resnet200_cub or weight_count == 63034632
        assert model != resnet200b_cub or weight_count == 63034632
        x = np.zeros((1, 3, 224, 224), np.float32)
        y = net(x)
        assert y.shape == (1, 200)","for model in models:
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)","for i, model in enumerate(models):
    net = model(pretrained=pretrained)
    weight_count = net.count_params()
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resnet10_cub or weight_count == 5008392
    assert model != resnet12_cub or weight_count == 5082376
    assert model != resnet14_cub or weight_count == 5377800
    assert model != resnetbc14b_cub or weight_count == 8425736
    assert model != resnet16_cub or weight_count == 6558472
    assert model != resnet18_cub or weight_count == 11279112
    assert model != resnet26_cub or weight_count == 17549832
    assert model != resnetbc26b_cub or weight_count == 14355976
    assert model != resnet34_cub or weight_count == 21387272
    assert model != resnetbc38b_cub or weight_count == 20286216
    assert model != resnet50_cub or weight_count == 23917832
    assert model != resnet50b_cub or weight_count == 23917832
    assert model != resnet101_cub or weight_count == 42909960
    assert model != resnet101b_cub or weight_count == 42909960
    assert model != resnet152_cub or weight_count == 58553608
    assert model != resnet152b_cub or weight_count == 58553608
    assert model != resnet200_cub or weight_count == 63034632
    assert model != resnet200b_cub or weight_count == 63034632
    x = np.zeros((1, 3, 224, 224), np.float32)
    y = net(x)
    assert y.shape == (1, 200)",1
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break",1
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape[1:] != subspace.shape:
        all_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape[1:] != subspace.shape:
        all_vectorized = False
        break",1
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for i in range(len(newArr)):
    for j in range(len(newArr[i])):
        tempList.append(newArr[i][j])","for i, arr in enumerate(newArr):
    for j in range(len(arr)):
        tempList.append(arr[j])",1
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for j in range(len(newArr[i])):
    tempList.append(newArr[i][j])","for j, val in enumerate(newArr[i]):
    tempList.append(val)",1
MAML-Pytorch,https://github.com/dragen1860/MAML-Pytorch/tree/master/backup/csmlv0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MAML-Pytorch/backup/csmlv0.py,,"def inner_train(K, gpuidx, support_x, support_y, query_x, query_y, concepts, Q):
    """"""
    inner-loop train function.
    :param K: train iterations
    :param gpuidx: which gpu to train
    :param support_x:   [b, setsz, c_, h, w]
    :param support_y:   []
    :param query_x:     [b, querysz]
    :param query_y:
    :param concepts:    concepts network
    :param Q:           Queue to receive result
    :return:
    """"""
    assert support_x.size(0) == query_x.size(0)
    support_x = support_x.cuda(gpuidx)
    support_y = support_y.cuda(gpuidx)
    query_x = query_x.cuda(gpuidx)
    query_y = query_y.cuda(gpuidx)
    support_db = TensorDataset(support_x, support_y)
    query_db = TensorDataset(query_x, query_y)
    outlayer = OutLayer().cuda(gpuidx)
    criteon = nn.CrossEntropyLoss().cuda(gpuidx)
    optimizer = optim.Adam(outlayer.parameters(), lr=0.001)
    right = Variable(torch.zeros(1).cuda(gpuidx))
    loss = Variable(torch.zeros(1).cuda(gpuidx))
    for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
        for i in range(K):
            x = concepts[gpuidx](support_xb)
            x = x.detach()
            logits = outlayer(x)
            loss = criteon(logits, support_yb)
            outlayer.zero_grad()
            loss.backward()
            optimizer.step()
        x = concepts[gpuidx](query_xb)
        logits = outlayer(x)
        (_, idx) = logits.max(1)
        pred = idx.long()
        right += torch.eq(pred, query_yb).sum().float()
        loss += criteon(logits, query_yb)
    accuracy = right.data[0] / np.array(query_y.size()).prod()
    print(gpuidx, loss.data[0], accuracy)
    Q.put([gpuidx, loss.data[0], accuracy])
    del outlayer, criteon
    print('removed outlayer and criteon.')","for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
    for i in range(K):
        x = concepts[gpuidx](support_xb)
        x = x.detach()
        logits = outlayer(x)
        loss = criteon(logits, support_yb)
        outlayer.zero_grad()
        loss.backward()
        optimizer.step()
    x = concepts[gpuidx](query_xb)
    logits = outlayer(x)
    (_, idx) = logits.max(1)
    pred = idx.long()
    right += torch.eq(pred, query_yb).sum().float()
    loss += criteon(logits, query_yb)","for (i, ((support_xb, support_yb), (query_xb, query_yb))) in enumerate(zip(support_db, query_db)):
    for j in range(K):
        x = concepts[gpuidx](support_xb)
        x = x.detach()
        logits = outlayer(x)
        loss = criteon(logits, support_yb)
        outlayer.zero_grad()
        loss.backward()
        optimizer.step()
    x = concepts[gpuidx](query_xb)
    logits = outlayer(x)
    (_, idx) = logits.max(1)
    pred = idx.long()
    right += torch.eq(pred, query_yb).sum().float()
    loss += criteon(logits, query_yb)",1
MAML-Pytorch,https://github.com/dragen1860/MAML-Pytorch/tree/master/backup/csmlv0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MAML-Pytorch/backup/csmlv0.py,,"def inner_train(K, gpuidx, support_x, support_y, query_x, query_y, concepts, Q):
    """"""
    inner-loop train function.
    :param K: train iterations
    :param gpuidx: which gpu to train
    :param support_x:   [b, setsz, c_, h, w]
    :param support_y:   []
    :param query_x:     [b, querysz]
    :param query_y:
    :param concepts:    concepts network
    :param Q:           Queue to receive result
    :return:
    """"""
    assert support_x.size(0) == query_x.size(0)
    support_x = support_x.cuda(gpuidx)
    support_y = support_y.cuda(gpuidx)
    query_x = query_x.cuda(gpuidx)
    query_y = query_y.cuda(gpuidx)
    support_db = TensorDataset(support_x, support_y)
    query_db = TensorDataset(query_x, query_y)
    outlayer = OutLayer().cuda(gpuidx)
    criteon = nn.CrossEntropyLoss().cuda(gpuidx)
    optimizer = optim.Adam(outlayer.parameters(), lr=0.001)
    right = Variable(torch.zeros(1).cuda(gpuidx))
    loss = Variable(torch.zeros(1).cuda(gpuidx))
    for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
        for i in range(K):
            x = concepts[gpuidx](support_xb)
            x = x.detach()
            logits = outlayer(x)
            loss = criteon(logits, support_yb)
            outlayer.zero_grad()
            loss.backward()
            optimizer.step()
        x = concepts[gpuidx](query_xb)
        logits = outlayer(x)
        (_, idx) = logits.max(1)
        pred = idx.long()
        right += torch.eq(pred, query_yb).sum().float()
        loss += criteon(logits, query_yb)
    accuracy = right.data[0] / np.array(query_y.size()).prod()
    print(gpuidx, loss.data[0], accuracy)
    Q.put([gpuidx, loss.data[0], accuracy])
    del outlayer, criteon
    print('removed outlayer and criteon.')","for i in range(K):
    x = concepts[gpuidx](support_xb)
    x = x.detach()
    logits = outlayer(x)
    loss = criteon(logits, support_yb)
    outlayer.zero_grad()
    loss.backward()
    optimizer.step()","for i in range(K):
    x = concepts[gpuidx](support_xb)
    x = x.detach()
    logits = outlayer(x)
    loss = criteon(logits, support_yb)
    outlayer.zero_grad()
    loss.backward()
    optimizer.step()",1
django-postgres-extra,https://github.com/SectorLabs/django-postgres-extra/tree/master/psqlextra/backend/schema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-postgres-extra/psqlextra/backend/schema.py,PostgresSchemaEditor,"def replace_materialized_view_model(self, model: Model) -> None:
    """"""Replaces a materialized view with a newer version.

        This is used to alter the backing query of a materialized view.

        Replacing a materialized view is a lot trickier than a normal view.
        For normal views we can use `CREATE OR REPLACE VIEW`, but for
        materialized views, we have to create the new view, copy all
        indexes and constraints and drop the old one.

        This operation is atomic as it runs in a transaction.
        """"""
    with self.connection.cursor() as cursor:
        constraints = self.introspection.get_constraints(cursor, model._meta.db_table)
    with transaction.atomic():
        self.delete_materialized_view_model(model)
        self.create_materialized_view_model(model)
        for (constraint_name, constraint_options) in constraints.items():
            if not constraint_options['definition']:
                raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
            self.execute(constraint_options['definition'])","for (constraint_name, constraint_options) in constraints.items():
    if not constraint_options['definition']:
        raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
    self.execute(constraint_options['definition'])","for i, (constraint_name, constraint_options) in enumerate(constraints.items()):
    if not constraint_options['definition']:
        raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
    self.execute(constraint_options['definition'])",1
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for i in range(self['nx']):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx","for i, _ in enumerate(range(self['nx'])):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx",1
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for bid in blocks:
    undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
    newblocks.append((pos, None))
    pos += 1","for i, bid in enumerate(blocks):
    undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
    newblocks.append((pos, None))
    pos += 1",1
luigi,https://github.com/spotify/luigi/tree/master/luigi/contrib/scalding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/luigi/contrib/scalding.py,ScaldingJobRunner,"def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    for line in open(source).readlines():
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')","for line in open(source).readlines():
    p = re.search('package\\s+([^\\s\\(]+)', line)
    if p:
        package = p.groups()[0]
    p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
    if p:
        job_class = p.groups()[0]
        if job_class == job_name:
            break","for i, line in enumerate(open(source).readlines()):
    p = re.search('package\\s+([^\\s\\(]+)', line)
    if p:
        package = p.groups()[0]
    p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
    if p:
        job_class = p.groups()[0]
        if job_class == job_name:
            break",1
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/modnas/estim/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/modnas/estim/base.py,,"def build_criterions_all(crit_configs, device_ids=None):
    """"""Build Criterions from configs.""""""
    crits_all = []
    crits_train = []
    crits_eval = []
    crits_valid = []
    for crit_conf in streamline_spec(crit_configs):
        crit = backend.get_criterion(crit_conf, device_ids=device_ids)
        crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
        if not isinstance(crit_mode, list):
            crit_mode = [crit_mode]
        if 'all' in crit_mode:
            crits_all.append(crit)
        if 'train' in crit_mode:
            crits_train.append(crit)
        if 'eval' in crit_mode:
            crits_eval.append(crit)
        if 'valid' in crit_mode:
            crits_valid.append(crit)
    return (crits_all, crits_train, crits_eval, crits_valid)","for crit_conf in streamline_spec(crit_configs):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)","for i, crit_conf in enumerate(streamline_spec(crit_configs)):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)",1
rdflib,https://github.com/RDFLib/rdflib/tree/master/rdflib/compare.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rdflib/rdflib/compare.py,_TripleCanonicalizer,"def _traces(self, coloring: List[Color], stats: Optional[Stats]=None, depth: List[int]=[0]) -> List[Color]:
    if stats is not None and 'prunings' not in stats:
        stats['prunings'] = 0
    depth[0] += 1
    candidates = self._get_candidates(coloring)
    best: List[List[Color]] = []
    best_score = None
    best_experimental = None
    best_experimental_score = None
    last_coloring = None
    generator: Dict[Node, Set[Node]] = defaultdict(set)
    visited: Set[Node] = set()
    for (candidate, color) in candidates:
        if candidate in generator:
            v = generator[candidate] & visited
            if len(v) > 0:
                visited.add(candidate)
                continue
        visited.add(candidate)
        coloring_copy: List[Color] = []
        color_copy = None
        for c in coloring:
            c_copy = c.copy()
            coloring_copy.append(c_copy)
            if c == color:
                color_copy = c_copy
        new_color = self._individuate(color_copy, candidate)
        coloring_copy.append(new_color)
        refined_coloring = self._refine(coloring_copy, [new_color])
        color_score = tuple([c.key() for c in refined_coloring])
        experimental = self._experimental_path(coloring_copy)
        experimental_score = set([c.key() for c in experimental])
        if last_coloring:
            generator = self._create_generator([last_coloring, experimental], generator)
        last_coloring = experimental
        if best_score is None or best_score < color_score:
            best = [refined_coloring]
            best_score = color_score
            best_experimental_score = experimental_score
        elif best_score > color_score:
            if stats is not None:
                stats['prunings'] += 1
        elif experimental_score != best_experimental_score:
            best.append(refined_coloring)
        elif stats is not None:
            stats['prunings'] += 1
    discrete: List[List[Color]] = [x for x in best if self._discrete(x)]
    if len(discrete) == 0:
        best_score = None
        best_depth = None
        for coloring in best:
            d = [depth[0]]
            new_color = self._traces(coloring, stats=stats, depth=d)
            color_score = tuple([c.key() for c in refined_coloring])
            if best_score is None or color_score > best_score:
                discrete = [new_color]
                best_score = color_score
                best_depth = d[0]
        depth[0] = best_depth
    return discrete[0]","for (candidate, color) in candidates:
    if candidate in generator:
        v = generator[candidate] & visited
        if len(v) > 0:
            visited.add(candidate)
            continue
    visited.add(candidate)
    coloring_copy: List[Color] = []
    color_copy = None
    for c in coloring:
        c_copy = c.copy()
        coloring_copy.append(c_copy)
        if c == color:
            color_copy = c_copy
    new_color = self._individuate(color_copy, candidate)
    coloring_copy.append(new_color)
    refined_coloring = self._refine(coloring_copy, [new_color])
    color_score = tuple([c.key() for c in refined_coloring])
    experimental = self._experimental_path(coloring_copy)
    experimental_score = set([c.key() for c in experimental])
    if last_coloring:
        generator = self._create_generator([last_coloring, experimental], generator)
    last_coloring = experimental
    if best_score is None or best_score < color_score:
        best = [refined_coloring]
        best_score = color_score
        best_experimental_score = experimental_score
    elif best_score > color_score:
        if stats is not None:
            stats['prunings'] += 1
    elif experimental_score != best_experimental_score:
        best.append(refined_coloring)
    elif stats is not None:
        stats['prunings'] += 1","for (i, (candidate, color)) in enumerate(candidates):
    if candidate in generator:
        v = generator[candidate] & visited
        if len(v) > 0:
            visited.add(candidate)
            continue
    visited.add(candidate)
    coloring_copy: List[Color] = []
    color_copy = None
    for c in coloring:
        c_copy = c.copy()
        coloring_copy.append(c_copy)
        if c == color:
            color_copy = c_copy
    new_color = self._individuate(color_copy, candidate)
    coloring_copy.append(new_color)
    refined_coloring = self._refine(coloring_copy, [new_color])
    color_score = tuple([c.key() for c in refined_coloring])
    experimental = self._experimental_path(coloring_copy)
    experimental_score = set([c.key() for c in experimental])
    if last_coloring:
        generator = self._create_generator([last_coloring, experimental], generator)
    last_coloring = experimental
    if best_score is None or best_score < color_score:
        best = [refined_coloring]
        best_score = color_score
        best_experimental_score = experimental_score
    elif best_score > color_score:
        if stats is not None:
            stats['prunings'] += 1
    elif experimental_score != best_experimental_score:
        best.append(refined_coloring)
    elif stats is not None:
        stats['prunings'] += 1",1
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/monitoradds.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/monitoradds.py,,"def delmod(bot, event, *args):
    """"""remove user id(s) from the whitelist of who can add to a hangout""""""
    if not bot.get_config_option('mods'):
        return
    mods = bot.get_config_option('mods')
    mods_new = []
    for mod in mods:
        if args[0] != mod:
            mods_new.append(mod)
    bot.config.set_by_path(['mods'], mods_new)
    bot.config.save()
    html_message = _('<i>Moderators updated: {} removed</i>')
    yield from bot.coro_send_message(event.conv, html_message.format(args[0]))","for mod in mods:
    if args[0] != mod:
        mods_new.append(mod)","for i, mod in enumerate(mods):
    if args[0] != mod:
        mods_new.append(mod)",1
videoflow,https://github.com/videoflow/videoflow/tree/master/tests/test_release_resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videoflow/tests/test_release_resources.py,,"def test_bboxannotator_resources():
    for datasetid in BoundingBoxAnnotator.supported_datasets:
        filename = f'labels_{datasetid}.pbtxt'
        url_path = BASE_URL_DETECTION + filename
        get_file(filename, url_path)","for datasetid in BoundingBoxAnnotator.supported_datasets:
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)","for i, datasetid in enumerate(BoundingBoxAnnotator.supported_datasets):
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)",1
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for post_id in post_ids:
    for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if i % 1000000 == 0:
            print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)","for (i, post_id) in enumerate(post_ids):
    for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if i % 1000000 == 0:
            print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)",1
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for name in subreddit_names:
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]","for i, name in enumerate(subreddit_names):
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]",1
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/test_streams.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/tests/test_streams.py,,"def test_all_linked_stream_parameters_owners():
    """"""Test to ensure operations can accept parameters in streams dictionary""""""
    stream_classes = param.concrete_descendents(LinkedStream)
    for stream_class in stream_classes.values():
        for (name, p) in stream_class.param.params().items():
            if name != 'name' and p.owner != stream_class:
                msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
                raise Exception(msg % (stream_class, name, p.owner))","for (name, p) in stream_class.param.params().items():
    if name != 'name' and p.owner != stream_class:
        msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
        raise Exception(msg % (stream_class, name, p.owner))","for i, (name, p) in enumerate(stream_class.param.params().items()):
    if name != 'name' and p.owner != stream_class:
        msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
        raise Exception(msg % (stream_class, name, p.owner))",1
hydra,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/defaults_list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/defaults_list.py,,"def _update_overrides(defaults_list: List[InputDefault], overrides: Overrides, parent: InputDefault, interpolated_subtree: bool) -> None:
    seen_override = False
    last_override_seen = None
    for d in defaults_list:
        if d.is_self():
            continue
        d.update_parent(parent.get_group_path(), parent.get_final_package())
        legacy_hydra_override = False
        if isinstance(d, GroupDefault):
            assert d.group is not None
            if not version.base_at_least('1.2'):
                legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
        if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
            assert isinstance(last_override_seen, GroupDefault)
            pcp = parent.get_config_path()
            okey = last_override_seen.get_override_key()
            oval = last_override_seen.get_name()
            raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
        if isinstance(d, GroupDefault):
            if legacy_hydra_override:
                d.override = True
                url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
                msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
                deprecation_warning(msg)
            if d.override:
                if not legacy_hydra_override:
                    seen_override = True
                last_override_seen = d
                if interpolated_subtree:
                    raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
                overrides.add_override(parent.get_config_path(), d)","for d in defaults_list:
    if d.is_self():
        continue
    d.update_parent(parent.get_group_path(), parent.get_final_package())
    legacy_hydra_override = False
    if isinstance(d, GroupDefault):
        assert d.group is not None
        if not version.base_at_least('1.2'):
            legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
    if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
        assert isinstance(last_override_seen, GroupDefault)
        pcp = parent.get_config_path()
        okey = last_override_seen.get_override_key()
        oval = last_override_seen.get_name()
        raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
    if isinstance(d, GroupDefault):
        if legacy_hydra_override:
            d.override = True
            url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
            msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
            deprecation_warning(msg)
        if d.override:
            if not legacy_hydra_override:
                seen_override = True
            last_override_seen = d
            if interpolated_subtree:
                raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
            overrides.add_override(parent.get_config_path(), d)","for (i, d) in enumerate(defaults_list):
    if d.is_self():
        continue
    d.update_parent(parent.get_group_path(), parent.get_final_package())
    legacy_hydra_override = False
    if isinstance(d, GroupDefault):
        assert d.group is not None
        if not version.base_at_least('1.2'):
            legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
    if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
        assert isinstance(last_override_seen, GroupDefault)
        pcp = parent.get_config_path()
        okey = last_override_seen.get_override_key()
        oval = last_override_seen.get_name()
        raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
    if isinstance(d, GroupDefault):
        if legacy_hydra_override:
            d.override = True
            url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
            msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
            deprecation_warning(msg)
        if d.override:
            if not legacy_hydra_override:
                seen_override = True
            last_override_seen = d
            if interpolated_subtree:
                raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
            overrides.add_override(parent.get_config_path(), d)",1
s3fs,https://github.com/fsspec/s3fs/tree/master/s3fs/tests/test_s3fs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3fs/s3fs/tests/test_s3fs.py,,"def test_s3_big_ls(s3):
    for x in range(1200):
        s3.touch(test_bucket_name + '/thousand/%i.part' % x)
    assert len(s3.find(test_bucket_name)) > 1200
    s3.rm(test_bucket_name + '/thousand/', recursive=True)
    assert len(s3.find(test_bucket_name + '/thousand/')) == 0","for x in range(1200):
    s3.touch(test_bucket_name + '/thousand/%i.part' % x)","for i in range(1200):
    s3.touch(test_bucket_name + '/thousand/%i.part' % i)",1
git-imerge,https://github.com/mhagger/git-imerge/tree/master//gitimerge.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/git-imerge//gitimerge.py,MergeState,"def find_index(self, commit):
    """"""Return (i1,i2) for the specified commit.

        Raise CommitNotFoundError if it is not known.""""""
    for i2 in range(0, self.len2):
        for i1 in range(0, self.len1):
            if (i1, i2) in self:
                record = self[i1, i2]
                if record.sha1 == commit:
                    return (i1, i2)
    raise CommitNotFoundError(commit)","for i2 in range(0, self.len2):
    for i1 in range(0, self.len1):
        if (i1, i2) in self:
            record = self[i1, i2]
            if record.sha1 == commit:
                return (i1, i2)","for i2, _ in enumerate(range(0, self.len2)):
    for i1 in range(0, self.len1):
        if (i1, i2) in self:
            record = self[i1, i2]
            if record.sha1 == commit:
                return (i1, i2)",1
torch-points3d,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/base_architectures/backbone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/base_architectures/backbone.py,BackboneBasedModel,"def _flatten_compact_options(self, opt):
    """"""Converts from a dict of lists, to a list of dicts
        """"""
    flattenedOpts = []
    for index in range(int(1000000.0)):
        try:
            flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
        except IndexError:
            break
    return flattenedOpts","for index in range(int(1000000.0)):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break","for (i, index) in enumerate(range(int(1000000.0))):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break",1
DeepPrivacy,https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/dataset/places2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPrivacy/deep_privacy/dataset/places2.py,Places2Dataset,"def _load_impaths(self):
    relevant_suffixes = ['.png', '.jpg', '.jpeg']
    image_dir = self.dirpath
    image_paths = []
    for (dirpath, dirnames, filenames) in os.walk(image_dir):
        for filename in filenames:
            path = pathlib.Path(dirpath, filename)
            if path.suffix in relevant_suffixes:
                assert path.is_file()
                image_paths.append(path)
    image_paths.sort(key=lambda x: int(x.stem.split('_')[-1]))
    return image_paths","for filename in filenames:
    path = pathlib.Path(dirpath, filename)
    if path.suffix in relevant_suffixes:
        assert path.is_file()
        image_paths.append(path)","for i, filename in enumerate(filenames):
    path = pathlib.Path(dirpath, filename)
    if path.suffix in relevant_suffixes:
        assert path.is_file()
        image_paths.append(path)",1
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for item in self.itemNet:
    for user in self.itemNet[item]:
        if self.itemNet[item][user] >= 1:
            self.filteredRatings[user].append(item)","for i, item in enumerate(self.itemNet):
    for user in self.itemNet[item]:
        if self.itemNet[item][user] >= 1:
            self.filteredRatings[user].append(item)",1
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.filteredRatings:
    s1 = set(self.filteredRatings[user1])
    for user2 in self.filteredRatings:
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight","for i, user1 in enumerate(self.filteredRatings):
    s1 = set(self.filteredRatings[user1])
    for j, user2 in enumerate(self.filteredRatings):
        if i != j:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight",1
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.CUNet:
    sims = []
    u1 = self.data.user[user1]
    self.W[u1] = model.wv[user1]
    for user2 in self.CUNet:
        if user1 != user2:
            u2 = self.data.user[user2]
            self.W[u2] = model.wv[user2]
            sims.append((user2, cosine(self.W[u1], self.W[u2])))
    self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
    i += 1
    if i % 200 == 0:
        print('progress:', i, '/', len(self.CUNet))","for i, user1 in enumerate(self.CUNet):
    sims = []
    u1 = self.data.user[user1]
    self.W[u1] = model.wv[user1]
    for user2 in self.CUNet:
        if user1 != user2:
            u2 = self.data.user[user2]
            self.W[u2] = model.wv[user2]
            sims.append((user2, cosine(self.W[u1], self.W[u2])))
    self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
    if (i+1) % 200 == 0:
        print('progress:', i+1, '/', len(self.CUNet))",1
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for entry in self.data.trainingData:
    (user, item, rating) = entry
    u = self.data.user[user]
    i = self.data.item[item]
    error = rating - self.P[u].dot(self.Q[i])
    self.loss += error ** 2
    p = self.P[u]
    q = self.Q[i]
    self.P[u] += self.lRate * (error * q - self.regU * p)
    self.Q[i] += self.lRate * (error * p - self.regI * q)","for i, entry in enumerate(self.data.trainingData):
    (user, item, rating) = entry
    u = self.data.user[user]
    i = self.data.item[item]
    error = rating - self.P[u].dot(self.Q[i])
    self.loss += error ** 2
    p = self.P[u]
    q = self.Q[i]
    self.P[u] += self.lRate * (error * q - self.regU * p)
    self.Q[i] += self.lRate * (error * p - self.regI * q)",1
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/loom/loom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/loom/loom.py,Loom,"def _setup_metadata(self):
    """"""Construct the serialized metadata about this loom for the scheduler.""""""
    loom_metadata = loom_pb2.LoomMetadata()
    loom_metadata.max_depth = self._max_depth
    for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
        type_shape_metadata = loom_metadata.type_shape_metadata.add()
        type_shape_metadata.dtype = ts.dtype_enum
        type_shape_metadata.shape.extend(ts.shape)
        type_shape_metadata.tag = ts.tag
        type_shape_metadata.name = str(ts)
        type_shape_metadata.tensor_names.extend(tensor_names)
        type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict
    for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
        op_metadata = loom_metadata.op_metadata.add()
        op_metadata.name = op_name
        op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
        op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))
    self._loom_metadata_str = loom_metadata.SerializeToString()","for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
    type_shape_metadata = loom_metadata.type_shape_metadata.add()
    type_shape_metadata.dtype = ts.dtype_enum
    type_shape_metadata.shape.extend(ts.shape)
    type_shape_metadata.tag = ts.tag
    type_shape_metadata.name = str(ts)
    type_shape_metadata.tensor_names.extend(tensor_names)
    type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict","for (i, (ts, tensor_names)) in enumerate(zip(self._type_shapes, self._ts_idx_to_tensor_names)):
    type_shape_metadata = loom_metadata.type_shape_metadata.add()
    type_shape_metadata.dtype = ts.dtype_enum
    type_shape_metadata.shape.extend(ts.shape)
    type_shape_metadata.tag = ts.tag
    type_shape_metadata.name = str(ts)
    type_shape_metadata.tensor_names.extend(tensor_names)
    type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict",1
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/loom/loom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/loom/loom.py,Loom,"def _setup_metadata(self):
    """"""Construct the serialized metadata about this loom for the scheduler.""""""
    loom_metadata = loom_pb2.LoomMetadata()
    loom_metadata.max_depth = self._max_depth
    for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
        type_shape_metadata = loom_metadata.type_shape_metadata.add()
        type_shape_metadata.dtype = ts.dtype_enum
        type_shape_metadata.shape.extend(ts.shape)
        type_shape_metadata.tag = ts.tag
        type_shape_metadata.name = str(ts)
        type_shape_metadata.tensor_names.extend(tensor_names)
        type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict
    for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
        op_metadata = loom_metadata.op_metadata.add()
        op_metadata.name = op_name
        op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
        op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))
    self._loom_metadata_str = loom_metadata.SerializeToString()","for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
    op_metadata = loom_metadata.op_metadata.add()
    op_metadata.name = op_name
    op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
    op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))","for i, (op_name, op) in enumerate(zip(self._loom_op_names, self._loom_ops)):
    op_metadata = loom_metadata.op_metadata.add()
    op_metadata.name = op_name
    op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
    op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))",1
electrum,https://github.com/spesmilo/electrum/tree/master/electrum/lnrater.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/lnrater.py,LNRater,"def _rate_nodes(self):
    """"""Rate nodes by collected statistics.""""""
    max_capacity = 0
    max_num_chan = 0
    min_fee_rate = float('inf')
    for stats in self._node_stats.values():
        max_capacity = max(max_capacity, stats.total_capacity_msat)
        max_num_chan = max(max_num_chan, stats.number_channels)
        min_fee_rate = min(min_fee_rate, stats.mean_fee_rate)
    for (n, stats) in self._node_stats.items():
        heuristics = []
        heuristics_weights = []
        heuristics.append(stats.number_channels / max_num_chan)
        heuristics_weights.append(0.2)
        heuristics.append(stats.total_capacity_msat / max_capacity)
        heuristics_weights.append(0.8)
        fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
        heuristics.append(fees)
        heuristics_weights.append(1.0)
        self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)","for (n, stats) in self._node_stats.items():
    heuristics = []
    heuristics_weights = []
    heuristics.append(stats.number_channels / max_num_chan)
    heuristics_weights.append(0.2)
    heuristics.append(stats.total_capacity_msat / max_capacity)
    heuristics_weights.append(0.8)
    fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
    heuristics.append(fees)
    heuristics_weights.append(1.0)
    self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)","for (i, (n, stats)) in enumerate(self._node_stats.items()):
    heuristics = []
    heuristics_weights = []
    heuristics.append(stats.number_channels / max_num_chan)
    heuristics_weights.append(0.2)
    heuristics.append(stats.total_capacity_msat / max_capacity)
    heuristics_weights.append(0.8)
    fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
    heuristics.append(fees)
    heuristics_weights.append(1.0)
    self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)",1
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/build_gen.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/build_gen.py,,"def _build_gen_ppc(net, ppc):
    """"""
    Takes the empty ppc network and fills it with the gen values. The gen
    datatype will be float afterwards.

    **INPUT**:
        **net** -The pandapower format network

        **ppc** - The PYPOWER format network to fill in values
    """"""
    mode = net['_options']['mode']
    distributed_slack = net['_options']['distributed_slack']
    if mode == 'estimate':
        return
    _is_elements = net['_is_elements']
    gen_order = dict()
    f = 0
    for element in ['ext_grid', 'gen']:
        f = add_gen_order(gen_order, element, _is_elements, f)
    if mode == 'opf':
        if len(net.dcline) > 0:
            ppc['dcline'] = net.dcline[['loss_mw', 'loss_percent']].values
        for element in ['sgen_controllable', 'load_controllable', 'storage_controllable']:
            f = add_gen_order(gen_order, element, _is_elements, f)
    f = add_gen_order(gen_order, 'xward', _is_elements, f)
    _init_ppc_gen(net, ppc, f)
    for (element, (f, t)) in gen_order.items():
        add_element_to_gen(net, ppc, element, f, t)
    net._gen_order = gen_order
    if distributed_slack:
        xward_pq_buses = _get_xward_pq_buses(net, ppc)
        (gen_mask, xward_mask) = _gen_xward_mask(net, ppc)
        _normalise_slack_weights(ppc, gen_mask, xward_mask, xward_pq_buses)","for (element, (f, t)) in gen_order.items():
    add_element_to_gen(net, ppc, element, f, t)","for i, ((element, (f, t))) in enumerate(gen_order.items()):
    add_element_to_gen(net, ppc, element, f, t)",1
dlrm,https://github.com/facebookresearch/dlrm/tree/master//dlrm_data_pytorch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dlrm//dlrm_data_pytorch.py,,"def generate_dist_input_batch(m_den, ln_emb, n, num_indices_per_lookup, num_indices_per_lookup_fixed, rand_data_dist, rand_data_min, rand_data_max, rand_data_mu, rand_data_sigma):
    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))
    lS_emb_offsets = []
    lS_emb_indices = []
    for size in ln_emb:
        lS_batch_offsets = []
        lS_batch_indices = []
        offset = 0
        for _ in range(n):
            if num_indices_per_lookup_fixed:
                sparse_group_size = np.int64(num_indices_per_lookup)
            else:
                r = ra.random(1)
                sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
            if rand_data_dist == 'gaussian':
                if rand_data_mu == -1:
                    rand_data_mu = (rand_data_max + rand_data_min) / 2.0
                r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
                sparse_group = np.clip(r, rand_data_min, rand_data_max)
                sparse_group = np.unique(sparse_group).astype(np.int64)
            elif rand_data_dist == 'uniform':
                r = ra.random(sparse_group_size)
                sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
            else:
                raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
            sparse_group_size = np.int64(sparse_group.size)
            lS_batch_offsets += [offset]
            lS_batch_indices += sparse_group.tolist()
            offset += sparse_group_size
        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
        lS_emb_indices.append(torch.tensor(lS_batch_indices))
    return (Xt, lS_emb_offsets, lS_emb_indices)","for size in ln_emb:
    lS_batch_offsets = []
    lS_batch_indices = []
    offset = 0
    for _ in range(n):
        if num_indices_per_lookup_fixed:
            sparse_group_size = np.int64(num_indices_per_lookup)
        else:
            r = ra.random(1)
            sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
        if rand_data_dist == 'gaussian':
            if rand_data_mu == -1:
                rand_data_mu = (rand_data_max + rand_data_min) / 2.0
            r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
            sparse_group = np.clip(r, rand_data_min, rand_data_max)
            sparse_group = np.unique(sparse_group).astype(np.int64)
        elif rand_data_dist == 'uniform':
            r = ra.random(sparse_group_size)
            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
        else:
            raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
        sparse_group_size = np.int64(sparse_group.size)
        lS_batch_offsets += [offset]
        lS_batch_indices += sparse_group.tolist()
        offset += sparse_group_size
    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
    lS_emb_indices.append(torch.tensor(lS_batch_indices))","for i, size in enumerate(ln_emb):
    lS_batch_offsets = []
    lS_batch_indices = []
    offset = 0
    for _ in range(n):
        if num_indices_per_lookup_fixed:
            sparse_group_size = np.int64(num_indices_per_lookup)
        else:
            r = ra.random(1)
            sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
        if rand_data_dist == 'gaussian':
            if rand_data_mu == -1:
                rand_data_mu = (rand_data_max + rand_data_min) / 2.0
            r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
            sparse_group = np.clip(r, rand_data_min, rand_data_max)
            sparse_group = np.unique(sparse_group).astype(np.int64)
        elif rand_data_dist == 'uniform':
            r = ra.random(sparse_group_size)
            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
        else:
            raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
        sparse_group_size = np.int64(sparse_group.size)
        lS_batch_offsets += [offset]
        lS_batch_indices += sparse_group.tolist()
        offset += sparse_group_size
    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
    lS_emb_indices.append(torch.tensor(lS_batch_indices))",1
neural_sp,https://github.com/hirofumi0810/neural_sp/tree/master/test/decoders/test_las_decoder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_sp/test/decoders/test_las_decoder.py,,"def test_decoding(backward, lm_fusion, params):
    args = make_args()
    args['backward'] = backward
    args['lm_fusion'] = lm_fusion
    params = make_decode_params(**params)
    bs = params['recog_batch_size']
    emax = 40
    device = 'cpu'
    eouts = np.random.randn(bs, emax, ENC_N_UNITS).astype(np.float32)
    elens = torch.IntTensor([len(x) for x in eouts])
    eouts = pad_list([np2tensor(x, device).float() for x in eouts], 0.0)
    ylens = [4, 5, 3, 7]
    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int32) for ylen in ylens]
    ctc_log_probs = None
    if params['recog_ctc_weight'] > 0:
        ctc_logits = torch.FloatTensor(bs, emax, VOCAB, device=device)
        ctc_log_probs = torch.softmax(ctc_logits, dim=-1)
    if params['recog_ctc_weight'] == 1:
        args['ctc_weight'] = 1
    args_lm = make_args_rnnlm()
    module_rnnlm = importlib.import_module('neural_sp.models.lm.rnnlm')
    lm = None
    lm_second = None
    lm_second_bwd = None
    if params['recog_lm_weight'] > 0:
        lm = module_rnnlm.RNNLM(args_lm).to(device)
    if params['recog_lm_second_weight'] > 0:
        lm_second = module_rnnlm.RNNLM(args_lm).to(device)
    if params['recog_lm_bwd_weight'] > 0:
        lm_second_bwd = module_rnnlm.RNNLM(args_lm).to(device)
    if args['lm_fusion']:
        args['external_lm'] = module_rnnlm.RNNLM(args_lm).to(device)
    module = importlib.import_module('neural_sp.models.seq2seq.decoders.las')
    dec = module.RNNDecoder(**args)
    dec = dec.to(device)
    dec.eval()
    with torch.no_grad():
        if params['recog_ctc_weight'] == 1:
            if params['recog_beam_width'] == 1:
                nbest_hyps = dec.ctc.greedy(eouts, elens)
            else:
                nbest_hyps = dec.ctc.beam_search(eouts, elens, params, idx2token, lm, lm_second, lm_second_bwd, nbest=1)
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
        elif params['recog_beam_width'] == 1:
            out = dec.greedy(eouts, elens, max_len_ratio=1.0, idx2token=idx2token, exclude_eos=params['exclude_eos'], refs_id=ys, utt_ids=None, speakers=None)
            assert len(out) == 2
            (nbest_hyps, aws) = out
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
            assert isinstance(aws, list)
            assert aws[0].shape == (args['attn_n_heads'], len(nbest_hyps[0]), emax)
        else:
            out = dec.beam_search(eouts, elens, params, idx2token, lm, lm_second, lm_second_bwd, ctc_log_probs, nbest=params['nbest'], exclude_eos=params['exclude_eos'], refs_id=None, utt_ids=None, speakers=None, cache_states=True)
            assert len(out) == 3
            (nbest_hyps, aws, scores) = out
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
            assert len(nbest_hyps[0]) == params['nbest']
            ymax = len(nbest_hyps[0][0])
            assert isinstance(aws, list)
            assert aws[0][0].shape == (args['attn_n_heads'], ymax, emax)
            assert isinstance(scores, list)
            assert len(scores) == bs
            assert len(scores[0]) == params['nbest']
            (ensmbl_eouts, ensmbl_elens, ensmbl_decs) = ([], [], [])
            for _ in range(3):
                ensmbl_eouts += [eouts]
                ensmbl_elens += [elens]
                ensmbl_decs += [dec]
            out = dec.beam_search(eouts, elens, params, idx2token=idx2token, lm=lm, lm_second=lm_second, lm_second_bwd=lm_second_bwd, ctc_log_probs=ctc_log_probs, nbest=params['nbest'], exclude_eos=params['exclude_eos'], refs_id=None, utt_ids=None, speakers=None, ensmbl_eouts=ensmbl_eouts, ensmbl_elens=ensmbl_elens, ensmbl_decs=ensmbl_decs, cache_states=True)
            assert len(out) == 3
            (nbest_hyps, aws, scores) = out
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
            assert len(nbest_hyps[0]) == params['nbest']
            ymax = len(nbest_hyps[0][0])
            assert isinstance(aws, list)
            assert aws[0][0].shape == (args['attn_n_heads'], ymax, emax)
            assert isinstance(scores, list)
            assert len(scores) == bs
            assert len(scores[0]) == params['nbest']","for _ in range(3):
    ensmbl_eouts += [eouts]
    ensmbl_elens += [elens]
    ensmbl_decs += [dec]","for i in range(3):
    ensmbl_eouts += [eouts]
    ensmbl_elens += [elens]
    ensmbl_decs += [dec]",1
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/env_dict.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/metaworld/envs/mujoco/env_dict.py,,"def create_hidden_goal_envs():
    hidden_goal_envs = {}
    for (env_name, env_cls) in ALL_V2_ENVIRONMENTS.items():
        d = {}

        def initialize(env, seed=None):
            if seed is not None:
                st0 = np.random.get_state()
                np.random.seed(seed)
            super(type(env), env).__init__()
            env._partially_observable = True
            env._freeze_rand_vec = False
            env._set_task_called = True
            env.reset()
            env._freeze_rand_vec = True
            if seed is not None:
                env.seed(seed)
                np.random.set_state(st0)
        d['__init__'] = initialize
        hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
        hg_env_name = hg_env_name.replace('-', '')
        hg_env_key = '{}-goal-hidden'.format(env_name)
        hg_env_name = '{}GoalHidden'.format(hg_env_name)
        HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
        hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls
    return OrderedDict(hidden_goal_envs)","for (env_name, env_cls) in ALL_V2_ENVIRONMENTS.items():
    d = {}

    def initialize(env, seed=None):
        if seed is not None:
            st0 = np.random.get_state()
            np.random.seed(seed)
        super(type(env), env).__init__()
        env._partially_observable = True
        env._freeze_rand_vec = False
        env._set_task_called = True
        env.reset()
        env._freeze_rand_vec = True
        if seed is not None:
            env.seed(seed)
            np.random.set_state(st0)
    d['__init__'] = initialize
    hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
    hg_env_name = hg_env_name.replace('-', '')
    hg_env_key = '{}-goal-hidden'.format(env_name)
    hg_env_name = '{}GoalHidden'.format(hg_env_name)
    HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
    hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls","for (i, (env_name, env_cls)) in enumerate(ALL_V2_ENVIRONMENTS.items()):
    d = {}

    def initialize(env, seed=None):
        if seed is not None:
            st0 = np.random.get_state()
            np.random.seed(seed)
        super(type(env), env).__init__()
        env._partially_observable = True
        env._freeze_rand_vec = False
        env._set_task_called = True
        env.reset()
        env._freeze_rand_vec = True
        if seed is not None:
            env.seed(seed)
            np.random.set_state(st0)
    d['__init__'] = initialize
    hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
    hg_env_name = hg_env_name.replace('-', '')
    hg_env_key = '{}-goal-hidden'.format(env_name)
    hg_env_name = '{}GoalHidden'.format(hg_env_name)
    HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
    hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls",1
airflow,https://github.com/apache/airflow/tree/master/airflow/www/fab_security/manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/fab_security/manager.py,BaseSecurityManager,"def get_role_permissions(self, role) -> Set[Tuple[str, str]]:
    """"""Get all permissions for a certain role""""""
    result = set()
    if role.name in self.builtin_roles:
        for permission in self.builtin_roles[role.name]:
            result.add((permission[1], permission[0]))
    else:
        for permission in self.get_role_permissions_from_db(role.id):
            result.add((permission.action.name, permission.resource.name))
    return result","for permission in self.get_role_permissions_from_db(role.id):
    result.add((permission.action.name, permission.resource.name))","for i, permission in enumerate(self.get_role_permissions_from_db(role.id)):
    result.add((permission.action.name, permission.resource.name))",1
KILT,https://github.com/facebookresearch/KILT/tree/master/kilt/readers/t5/base_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KILT/kilt/readers/t5/base_transformer.py,LoggingCallback,"def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
    logger.info('***** Validation results *****')
    if pl_module.is_logger():
        metrics = trainer.callback_metrics
        for key in sorted(metrics):
            if key not in ['log', 'progress_bar']:
                logger.info('{} = {}\n'.format(key, str(metrics[key])))","for key in sorted(metrics):
    if key not in ['log', 'progress_bar']:
        logger.info('{} = {}\n'.format(key, str(metrics[key])))","for i, key in enumerate(sorted(metrics)):
    if key not in ['log', 'progress_bar']:
        logger.info('{} = {}\n'.format(key, str(metrics[key])))",1
slither,https://github.com/crytic/slither/tree/master/slither/printers/call/call_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/slither/slither/printers/call/call_graph.py,PrinterCallGraph,"def output(self, filename):
    """"""
        Output the graph in filename
        Args:
            filename(string)
        """"""
    all_contracts_filename = ''
    if not filename.endswith('.dot'):
        all_contracts_filename = f'{filename}.all_contracts.call-graph.dot'
    if filename == '.dot':
        all_contracts_filename = 'all_contracts.dot'
    info = ''
    results = []
    with open(all_contracts_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {all_contracts_filename}\n'
        all_functionss = [compilation_unit.functions for compilation_unit in self.slither.compilation_units]
        all_functions = [item for sublist in all_functionss for item in sublist]
        all_functions_as_dict = {function.canonical_name: function for function in all_functions}
        content = '\n'.join(['strict digraph {'] + [_process_functions(all_functions_as_dict.values())] + ['}'])
        f.write(content)
        results.append((all_contracts_filename, content))
    for derived_contract in self.slither.contracts_derived:
        derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
        with open(derived_output_filename, 'w', encoding='utf8') as f:
            info += f'Call Graph: {derived_output_filename}\n'
            content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
            f.write(content)
            results.append((derived_output_filename, content))
    self.info(info)
    res = self.generate_output(info)
    for (filename_result, content) in results:
        res.add_file(filename_result, content)
    return res","for derived_contract in self.slither.contracts_derived:
    derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
    with open(derived_output_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {derived_output_filename}\n'
        content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
        f.write(content)
        results.append((derived_output_filename, content))","for i, derived_contract in enumerate(self.slither.contracts_derived):
    derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
    with open(derived_output_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {derived_output_filename}\n'
        content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
        f.write(content)
        results.append((derived_output_filename, content))",1
jittor,https://github.com/Jittor/jittor/tree/master/python/jittor/test/test_transform.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jittor/python/jittor/test/test_transform.py,Tester,"def test_1_channel_ndarray_to_pil_image(self):
    img_data_float = np.random.rand(4, 4, 1).astype(np.float32)
    img_data_byte = np.random.randint(0, 255, (4, 4, 1)).astype(np.uint8)
    img_data_short = np.random.randint(0, 32767, (4, 4, 1)).astype(np.int16)
    img_data_int = np.random.randint(0, 2147483647, (4, 4, 1)).astype(np.int32)
    inputs = [img_data_float, img_data_byte, img_data_short, img_data_int]
    expected_modes = ['F', 'L', 'I;16', 'I']
    for (img_data, mode) in zip(inputs, expected_modes):
        for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
            img = t(img_data)
            self.assertEqual(img.mode, mode)
            self.assertTrue(np.allclose(img_data[:, :, 0], img))","for (img_data, mode) in zip(inputs, expected_modes):
    for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
        img = t(img_data)
        self.assertEqual(img.mode, mode)
        self.assertTrue(np.allclose(img_data[:, :, 0], img))","for i, (img_data, mode) in enumerate(zip(inputs, expected_modes)):
    for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
        img = t(img_data)
        self.assertEqual(img.mode, mode)
        self.assertTrue(np.allclose(img_data[:, :, 0], img))",1
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
    if line.startswith('Question_ID'):
        questions += 1
    if line.startswith('Answer_to_Question_ID'):
        answers += 1","with open(data_folder / (file + '.txt'), mode='r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1",1
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/test/training/test_synthetic_reward_training.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/test/training/test_synthetic_reward_training.py,,"def create_sequence_data(state_dim, action_dim, seq_len, batch_size, num_batches):
    SCALE = 2
    weight = SCALE * torch.randn(state_dim + action_dim)
    data = [None for _ in range(num_batches)]
    for i in range(num_batches):
        state = SCALE * torch.randn(seq_len, batch_size, state_dim)
        action = SCALE * torch.randn(seq_len, batch_size, action_dim)
        valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
        feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
        feature_mask = (feature_mask >= seq_len - valid_step).float()
        assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
        feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
        assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
        feature = torch.cat((state, action), dim=2)
        masked_feature = feature * feature_mask
        left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
        right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
        reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
        mask = torch.arange(seq_len).repeat(batch_size, 1)
        mask = (mask >= seq_len - valid_step).float()
        reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
        data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))
    return (weight, data)","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))",1
dbt-core,https://github.com/dbt-labs/dbt-core/tree/master/core/dbt/adapters/base/relation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dbt-core/core/dbt/adapters/base/relation.py,BaseRelation,"def matches(self, database: Optional[str]=None, schema: Optional[str]=None, identifier: Optional[str]=None) -> bool:
    search = filter_null_values({ComponentName.Database: database, ComponentName.Schema: schema, ComponentName.Identifier: identifier})
    if not search:
        raise dbt.exceptions.RuntimeException('Tried to match relation, but no search path was passed!')
    exact_match = True
    approximate_match = True
    for (k, v) in search.items():
        if not self._is_exactish_match(k, v):
            exact_match = False
        if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
            approximate_match = False
    if approximate_match and (not exact_match):
        target = self.create(database=database, schema=schema, identifier=identifier)
        dbt.exceptions.approximate_relation_match(target, self)
    return exact_match","for (k, v) in search.items():
    if not self._is_exactish_match(k, v):
        exact_match = False
    if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
        approximate_match = False","for i, (k, v) in enumerate(search.items()):
    if not self._is_exactish_match(k, v):
        exact_match = False
    if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
        approximate_match = False",1
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines","for check in valid_checks:
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')","for i, check in enumerate(valid_checks):
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')",1
Kats,https://github.com/facebookresearch/Kats/tree/master/kats/tests/test_consts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kats/kats/tests/test_consts.py,TimeSeriesDataOpsTest,"def test_get_item(self) -> None:
    self.assertEqual(self.ts_date_transform_concat_univ[:len(self.ts_univ_1)], self.ts_univ_1)
    self.assertEqual(self.ts_date_transform_concat_multi[:len(self.ts_multi_1)], self.ts_multi_1)
    for col in self.ts_date_transform_concat_multi.value.columns:
        ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
        self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)
    self.assertEqual(self.ts_date_transform_concat_multi[MULTIVAR_VALUE_DF_COLS], self.ts_date_transform_concat_multi)
    self.assertEqual(self.ts_univ_1[:], self.ts_univ_1)
    self.assertEqual(self.ts_univ_1[0:0], TimeSeriesData(time=pd.Series(name=TIME_COL_NAME), value=pd.Series(name=VALUE_COL_NAME), time_col_name=TIME_COL_NAME))","for col in self.ts_date_transform_concat_multi.value.columns:
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)","for i, col in enumerate(self.ts_date_transform_concat_multi.value.columns):
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)",1
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/distributions/gaussian_scale_mixture.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyro/pyro/distributions/gaussian_scale_mixture.py,GaussianScaleMixture,"def _compute_coeffs(self):
    """"""
        These coefficients are used internally in the backward call.
        """"""
    dimov2 = int(self.dim / 2)
    coeffs = torch.ones(dimov2)
    for k in range(dimov2 - 1):
        coeffs[k + 1:] *= self.dim - 2 * (k + 1)
    return coeffs","for k in range(dimov2 - 1):
    coeffs[k + 1:] *= self.dim - 2 * (k + 1)","for i in range(dimov2 - 1):
    coeffs[i + 1:] *= self.dim - 2 * (i + 1)",1
onnxmltools,https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/libsvm/_parse.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnxmltools/onnxmltools/convert/libsvm/_parse.py,,"def parse_libsvm(model, initial_types=None, target_opset=None, custom_conversion_functions=None, custom_shape_calculators=None):
    raw_model_container = LibSvmModelContainer(model)
    topology = Topology(raw_model_container, default_batch_size='None', initial_types=initial_types, target_opset=target_opset, custom_conversion_functions=custom_conversion_functions, custom_shape_calculators=custom_shape_calculators)
    scope = topology.declare_scope('__root__')
    inputs = []
    for (var_name, initial_type) in initial_types:
        inputs.append(scope.declare_local_variable(var_name, initial_type))
    for variable in inputs:
        raw_model_container.add_input(variable)
    outputs = _parse_libsvm(scope, model, inputs)
    for variable in outputs:
        raw_model_container.add_output(variable)
    return topology","for (var_name, initial_type) in initial_types:
    inputs.append(scope.declare_local_variable(var_name, initial_type))","for i, (var_name, initial_type) in enumerate(initial_types):
    inputs.append(scope.declare_local_variable(var_name, initial_type))",1
Project_CodeNet,https://github.com/IBM/Project_CodeNet/tree/master/model-experiments/masked-language-model/infer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Project_CodeNet/model-experiments/masked-language-model/infer.py,,"def encode(text):
    R = [0] * config.MAX_LEN
    text = tokenize(text)
    for i in range(len(text)):
        w = text[i]
        if w in token2id:
            R[i] = token2id[w]
        else:
            R[i] = 1
    return np.array(R)","for i in range(len(text)):
    w = text[i]
    if w in token2id:
        R[i] = token2id[w]
    else:
        R[i] = 1","for i, w in enumerate(text):
    if w in token2id:
        R[i] = token2id[w]
    else:
        R[i] = 1",1
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tagstore/snuba/backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/tagstore/snuba/backend.py,SnubaTagStorage,"def __get_tag_keys_for_projects(self, projects, group, environments, start, end, limit=1000, keys=None, include_values_seen=True, use_cache=False, include_transactions=False, denylist=None, **kwargs):
    """"""Query snuba for tag keys based on projects

        When use_cache is passed, we'll attempt to use the cache. There's an exception if group_id was passed
        which refines the query enough caching isn't required.
        The cache key is based on the filters being passed so that different queries don't hit the same cache, with
        exceptions for start and end dates. Since even a microsecond passing would result in a different caching
        key, which means always missing the cache.
        Instead, to keep the cache key the same for a short period we append the duration, and the end time rounded
        with a certain jitter to the cache key.
        This jitter is based on the hash of the key before duration/end time is added for consistency per query.
        The jitter's intent is to avoid a dogpile effect of many queries being invalidated at the same time.
        This is done by changing the rounding of the end key to a random offset. See snuba.quantize_time for
        further explanation of how that is done.
        """"""
    (default_start, default_end) = default_start_end_dates()
    if start is None:
        start = default_start
    if end is None:
        end = default_end
    dataset = Dataset.Events
    if include_transactions:
        dataset = Dataset.Discover
    conditions = []
    aggregations = [['count()', '', 'count']]
    filters = {'project_id': sorted(projects)}
    if environments:
        filters['environment'] = sorted(environments)
    if group is not None:
        (dataset, conditions, filters) = self.apply_group_filters_conditions(group, conditions, filters)
    if keys is not None:
        filters['tags_key'] = sorted(keys)
    if include_values_seen:
        aggregations.append(['uniq', 'tags_value', 'values_seen'])
    should_cache = use_cache and group is None
    result = None
    cache_key = None
    if should_cache:
        filtering_strings = [f'{key}={value}' for (key, value) in filters.items()]
        filtering_strings.append(f'dataset={dataset.name}')
        cache_key = 'tagstore.__get_tag_keys:{}'.format(md5_text(*filtering_strings).hexdigest())
        key_hash = hash(cache_key)
        duration = (end - start).total_seconds()
        end = snuba.quantize_time(end, key_hash)
        cache_key += f':{duration}@{end.isoformat()}'
        result = cache.get(cache_key, None)
        if result is not None:
            metrics.incr('testing.tagstore.cache_tag_key.hit')
        else:
            metrics.incr('testing.tagstore.cache_tag_key.miss')
    if result is None:
        result = snuba.query(dataset=dataset, start=start, end=end, groupby=['tags_key'], conditions=conditions, filter_keys=filters, aggregations=aggregations, limit=limit, orderby='-count', referrer='tagstore.__get_tag_keys', **kwargs)
        if should_cache:
            cache.set(cache_key, result, 300)
            metrics.incr('testing.tagstore.cache_tag_key.len', amount=len(result))
    if group is None:
        ctor = TagKey
    else:
        ctor = functools.partial(GroupTagKey, group_id=group.id)
    results = set()
    for (key, data) in result.items():
        if denylist is not None and key in denylist:
            continue
        params = {'key': key}
        if include_values_seen:
            params['values_seen'] = data['values_seen']
            params['count'] = data['count']
        else:
            params['count'] = data
        results.add(ctor(**params))
    return results","for (key, data) in result.items():
    if denylist is not None and key in denylist:
        continue
    params = {'key': key}
    if include_values_seen:
        params['values_seen'] = data['values_seen']
        params['count'] = data['count']
    else:
        params['count'] = data
    results.add(ctor(**params))","for i, (key, data) in enumerate(result.items()):
    if denylist is not None and key in denylist:
        continue
    params = {'key': key}
    if include_values_seen:
        params['values_seen'] = data['values_seen']
        params['count'] = data['count']
    else:
        params['count'] = data
    results.add(ctor(**params))",1
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr","for var in self._variables:
    if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
        if var.name is not None:
            continue
        if var.ident.startswith('iarg'):
            var.name = 'arg_%x' % var.offset
        else:
            var.name = 's_%x' % -var.offset
    elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
        if var.name is not None:
            continue
        var.name = var.ident
    elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
        if var.name is not None:
            continue
        if labels is not None and var.addr in labels:
            var.name = labels[var.addr]
            if '@@' in var.name:
                var.name = var.name[:var.name.index('@@')]
        elif isinstance(var.addr, int):
            var.name = 'g_%x' % var.addr
        elif var.ident is not None:
            var.name = var.ident
        else:
            var.name = 'g_%s' % var.addr","for i,var in enumerate(self._variables):
    if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
        if var.name is not None:
            continue
        if var.ident.startswith('iarg'):
            var.name = 'arg_%x' % var.offset
        else:
            var.name = 's_%x' % -var.offset
    elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
        if var.name is not None:
            continue
        var.name = var.ident
    elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
        if var.name is not None:
            continue
        if labels is not None and var.addr in labels:
            var.name = labels[var.addr]
            if '@@' in var.name:
                var.name = var.name[:var.name.index('@@')]
        elif isinstance(var.addr, int):
            var.name = 'g_%x' % var.addr
        elif var.ident is not None:
            var.name = var.ident
        else:
            var.name = 'g_%s' % var.addr",1
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    runner = self._get_fextractor_instance(fextractor_type)
    runner.run(parallelize=self.parallelize, processes=self.processes)
    results = runner.results
    self.type2results_dict[fextractor_type] = results","for i, fextractor_type in enumerate(self.feature_dict):
    runner = self._get_fextractor_instance(fextractor_type)
    runner.run(parallelize=self.parallelize, processes=self.processes)
    results = runner.results
    self.type2results_dict[fextractor_type] = results",1
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]","for i, fextractor_type in enumerate(self.feature_dict):
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]",1
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for atom_feature in self._get_atom_features(fextractor_type):
    scores_key = self._get_scores_key(fextractor_type, atom_feature)
    for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
        try:
            result_dicts[result_index][scores_key] = result[scores_key]
        except KeyError:
            scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
            result_dicts[result_index][scores_key] = result[scores_key_alt]","for (i, atom_feature) in enumerate(self._get_atom_features(fextractor_type)):
    scores_key = self._get_scores_key(fextractor_type, atom_feature)
    for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
        try:
            result_dicts[result_index][scores_key] = result[scores_key]
        except KeyError:
            scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
            result_dicts[result_index][scores_key] = result[scores_key_alt]",1
FakeNewsNet,https://github.com/KaiDMML/FakeNewsNet/tree/master/code/tweet_collection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FakeNewsNet/code/tweet_collection.py,TweetCollector,"def collect_data(self, choices):
    for choice in choices:
        news_list = self.load_news_file(choice)
        collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for choice in choices:
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for i, choice in enumerate(choices):
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)",1
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","for message in messages:
    signal_declarations = []
    for signal in message.signals:
        signal_declaration = ''
        if floating_point_numbers:
            signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declarations.append(signal_declaration)
    declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
    if signal_declarations:
        declaration += '\n' + '\n'.join(signal_declarations)
    declarations.append(declaration)","for i, message in enumerate(messages):
    signal_declarations = []
    for signal in message.signals:
        signal_declaration = ''
        if floating_point_numbers:
            signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declarations.append(signal_declaration)
    declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
    if signal_declarations:
        declaration += '\n' + '\n'.join(signal_declarations)
    declarations.append(declaration)",1
toapi,https://github.com/gaojiuli/toapi/tree/master/toapi/item.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toapi/toapi/item.py,ItemType,"def __new__(cls, what, bases=None, attrdict=None):
    __fields__ = OrderedDict()
    for (name, selector) in attrdict.items():
        if isinstance(selector, Selector):
            __fields__[name] = selector
    for name in __fields__.keys():
        del attrdict[name]
    instance = type.__new__(cls, what, bases, attrdict)
    instance._list = None
    instance._site = None
    instance._selector = None
    instance.__fields__ = __fields__
    return instance","for (name, selector) in attrdict.items():
    if isinstance(selector, Selector):
        __fields__[name] = selector","for i, (name, selector) in enumerate(attrdict.items()):
    if isinstance(selector, Selector):
        __fields__[name] = selector",1
vid2vid,https://github.com/NVIDIA/vid2vid/tree/master/models/networks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vid2vid/models/networks.py,MultiscaleDiscriminator,"def forward(self, input):
    num_D = self.num_D
    result = []
    input_downsampled = input
    for i in range(num_D):
        if self.getIntermFeat:
            model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
        else:
            model = getattr(self, 'layer' + str(num_D - 1 - i))
        result.append(self.singleD_forward(model, input_downsampled))
        if i != num_D - 1:
            input_downsampled = self.downsample(input_downsampled)
    return result","for i in range(num_D):
    if self.getIntermFeat:
        model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
    else:
        model = getattr(self, 'layer' + str(num_D - 1 - i))
    result.append(self.singleD_forward(model, input_downsampled))
    if i != num_D - 1:
        input_downsampled = self.downsample(input_downsampled)","for i in range(num_D):
    if self.getIntermFeat:
        model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
    else:
        model = getattr(self, 'layer' + str(num_D - 1 - i))
    result.append(self.singleD_forward(model, input_downsampled))
    if i != num_D - 1:
        input_downsampled = self.downsample(input_downsampled)",1
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/addons/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mitmproxy/mitmproxy/addons/core.py,Core,"def revert(self, flows: typing.Sequence[flow.Flow]) -> None:
    """"""
            Revert flow changes.
        """"""
    updated = []
    for f in flows:
        if f.modified():
            f.revert()
            updated.append(f)
    ctx.log.alert('Reverted %s flows.' % len(updated))
    ctx.master.addons.trigger(hooks.UpdateHook(updated))","for f in flows:
    if f.modified():
        f.revert()
        updated.append(f)","for i, f in enumerate(flows):
    if f.modified():
        f.revert()
        updated.append(f)",1
configuration,https://github.com/edx/configuration/tree/master/util/jenkins/helm_update_checker/helm_update_checker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/configuration/util/jenkins/helm_update_checker/helm_update_checker.py,,"def get_repo_name(repo_url):
    try:
        get_repo_cmd = 'helm repo list -o json'
        repositories = subprocess.check_output(get_repo_cmd, shell=True).strip()
        repo_list = json.loads(repositories.decode())
        for repo in repo_list:
            if repo['url'] == repo_url:
                return repo['name']
    except subprocess.CalledProcessError as e:
        print(e.output)","for repo in repo_list:
    if repo['url'] == repo_url:
        return repo['name']","for i, repo in enumerate(repo_list):
    if repo['url'] == repo_url:
        return repo['name']",1
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})","for file_id in ids:
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})","for i, file_id in enumerate(ids):
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})",1
espresso,https://github.com/freewym/espresso/tree/master/fairseq/data/multilingual/multilingual_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/data/multilingual/multilingual_utils.py,,"def augment_dictionary(dictionary: Dictionary, language_list: List[str], lang_tok_style: str, langtoks_specs: Sequence[str]=(LangTokSpec.main.value,), extra_data: Optional[Dict[str, str]]=None) -> None:
    for spec in langtoks_specs:
        for language in language_list:
            dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))
    if lang_tok_style == LangTokStyle.mbart.value or (extra_data is not None and LangTokSpec.mono_dae.value in extra_data):
        dictionary.add_symbol('<mask>')","for spec in langtoks_specs:
    for language in language_list:
        dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))","for (i, spec) in enumerate(langtoks_specs):
    for language in language_list:
        dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))",1
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/actions_wdgt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fgmk/fgmk/actions_wdgt.py,tinyActionsWdgt,"def removeAction(self):
    if self.ssettings == {}:
        return
    if len(self.ActionList.selectedItems()) < 1:
        return
    previous_actions = self.getValue()
    for item in self.ActionList.selectedItems():
        itemIndex = self.ActionList.row(item)
        self.ActionList.takeItem(itemIndex)
    current_actions = self.getValue()
    self.somethingChanged.emit(previous_actions, current_actions, 'remove', 'remove action')","for item in self.ActionList.selectedItems():
    itemIndex = self.ActionList.row(item)
    self.ActionList.takeItem(itemIndex)","for i in reversed(range(self.ActionList.count())):
    if self.ActionList.item(i).isSelected():
        self.ActionList.takeItem(i)",1
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()","for stat_set in stat_sets:
    for stat in self.stats[stat_set]:
        self.stats[stat_set][stat].reset()","for i, stat_set in enumerate(stat_sets):
    for stat in self.stats[stat_set]:
        self.stats[stat_set][stat].reset()",1
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()","for stat in self.stats[stat_set]:
    self.stats[stat_set][stat].reset()","for i, stat in enumerate(self.stats[stat_set]):
    self.stats[stat_set][stat].reset()",1
mars,https://github.com/mars-project/mars/tree/master/mars/dataframe/arithmetic/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/dataframe/arithmetic/core.py,DataFrameBinOpMixin,"def _tile_dataframe_series(cls, op):
    (left, right) = (op.lhs, op.rhs)
    df = op.outputs[0]
    (nsplits, out_shape, left_chunks, right_chunks) = align_dataframe_series(left, right, axis=op.axis)
    out_chunk_indexes = itertools.product(*(range(s) for s in out_shape))
    out_chunks = []
    for (out_idx, df_chunk) in zip(out_chunk_indexes, left_chunks):
        if op.axis == 'columns' or op.axis == 1:
            series_chunk = right_chunks[out_idx[1]]
            kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
        else:
            series_chunk = right_chunks[out_idx[0]]
            kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
        out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
        out_chunks.append(out_chunk)
    new_op = op.copy()
    return new_op.new_dataframes(op.inputs, df.shape, nsplits=tuple((tuple(ns) for ns in nsplits)), chunks=out_chunks, dtypes=df.dtypes, index_value=df.index_value, columns_value=df.columns_value)","for (out_idx, df_chunk) in zip(out_chunk_indexes, left_chunks):
    if op.axis == 'columns' or op.axis == 1:
        series_chunk = right_chunks[out_idx[1]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
    else:
        series_chunk = right_chunks[out_idx[0]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
    out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
    out_chunks.append(out_chunk)","for i, (out_idx, df_chunk) in enumerate(zip(out_chunk_indexes, left_chunks)):
    if op.axis == 'columns' or op.axis == 1:
        series_chunk = right_chunks[out_idx[1]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
    else:
        series_chunk = right_chunks[out_idx[0]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
    out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
    out_chunks.append(out_chunk)",1
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/solo/utils/knn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/solo/utils/knn.py,WeightedKNNClassifier,"def compute(self) -> Tuple[float]:
    """"""Computes weighted k-NN accuracy @1 and @5. If cosine distance is selected,
        the weight is computed using the exponential of the temperature scaled cosine
        distance of the samples. If euclidean distance is selected, the weight corresponds
        to the inverse of the euclidean distance.

        Returns:
            Tuple[float]: k-NN accuracy @1 and @5.
        """"""
    train_features = torch.cat(self.train_features)
    train_targets = torch.cat(self.train_targets)
    test_features = torch.cat(self.test_features)
    test_targets = torch.cat(self.test_targets)
    if self.distance_fx == 'cosine':
        train_features = F.normalize(train_features)
        test_features = F.normalize(test_features)
    num_classes = torch.unique(test_targets).numel()
    num_train_images = train_targets.size(0)
    num_test_images = test_targets.size(0)
    num_train_images = train_targets.size(0)
    chunk_size = min(max(1, self.max_distance_matrix_size // num_train_images), num_test_images)
    k = min(self.k, num_train_images)
    (top1, top5, total) = (0.0, 0.0, 0)
    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)
    for idx in range(0, num_test_images, chunk_size):
        features = test_features[idx:min(idx + chunk_size, num_test_images), :]
        targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
        batch_size = targets.size(0)
        if self.distance_fx == 'cosine':
            similarities = torch.mm(features, train_features.t())
        elif self.distance_fx == 'euclidean':
            similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
        else:
            raise NotImplementedError
        (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
        candidates = train_targets.view(1, -1).expand(batch_size, -1)
        retrieved_neighbors = torch.gather(candidates, 1, indices)
        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
        if self.distance_fx == 'cosine':
            similarities = similarities.clone().div_(self.T).exp_()
        probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
        (_, predictions) = probs.sort(1, True)
        correct = predictions.eq(targets.data.view(-1, 1))
        top1 = top1 + correct.narrow(1, 0, 1).sum().item()
        top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
        total += targets.size(0)
    top1 = top1 * 100.0 / total
    top5 = top5 * 100.0 / total
    self.reset()
    return (top1, top5)","for idx in range(0, num_test_images, chunk_size):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)","for i, idx in enumerate(range(0, num_test_images, chunk_size)):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)",1
mayavi,https://github.com/enthought/mayavi/tree/master/tvtk/indenter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/tvtk/indenter.py,VTKDocMassager,"def _rename_methods(self, doc):
    lines = doc.split('\n')
    nl = []
    for line in lines:
        words = line.split(' ')
        nw = []
        for word in words:
            if word[:3] == 'vtk':
                nw.append(word)
            else:
                nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
        nl.append(' '.join(nw))
    return '\n'.join(nl)","for line in lines:
    words = line.split(' ')
    nw = []
    for word in words:
        if word[:3] == 'vtk':
            nw.append(word)
        else:
            nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
    nl.append(' '.join(nw))","for i, line in enumerate(lines):
    words = line.split(' ')
    nw = []
    for word in words:
        if word[:3] == 'vtk':
            nw.append(word)
        else:
            nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
    nl.append(' '.join(nw))",1
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial","for file in files:
    if re.match(self.vbox_name, file):
        file = os.path.join(path, file)
        vbox.append(file)","for i, file in enumerate(files):
    if re.match(self.vbox_name, file):
        file = os.path.join(path, file)
        vbox.append(file)",1
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial","for line in f.readlines():
    res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
    if res:
        serial.append(f'127.0.0.1:{res.group(1)}')","for i, line in enumerate(f.readlines()):
    res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
    if res:
        serial.append(f'127.0.0.1:{res.group(1)}')",1
xalpha,https://github.com/refraction-ray/xalpha/tree/master/xalpha/indicator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xalpha/xalpha/indicator.py,indicator,"def max_drawdown(self, date=yesterdayobj()):
    """"""
        

        :param date: date obj or string
        :returns: three elements tuple, the first two are the date obj of
            start and end of the time window, the third one is the drawdown amplitude in unit 1.
        """"""
    li = [(row['date'], row['netvalue']) for (i, row) in self.price[self.price['date'] <= date].iterrows()]
    res = []
    for (i, _) in enumerate(li):
        for j in range(i + 1, len(li)):
            res.append((li[i][0], li[j][0], (li[j][1] - li[i][1]) / li[i][1]))
    return min(res, key=lambda x: x[2])","for j in range(i + 1, len(li)):
    res.append((li[i][0], li[j][0], (li[j][1] - li[i][1]) / li[i][1]))","for i, (item1, value1) in enumerate(li):
    for j, (item2, value2) in enumerate(li[i+1:], i+1):
        res.append((item1, item2, (value2 - value1) / value1))",1
DataStructure_Algorithm_ZJU,https://github.com/CYBruce/DataStructure_Algorithm_ZJU/tree/master//04-4 .py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DataStructure_Algorithm_ZJU//04-4 .py,,"def get_input(N, L):
    global same
    LIST = []
    real = input().split()
    for i in range(N):
        real[i] = int(real[i])
    for i in range(L):
        test = input().split()
        for j in range(N):
            test[j] = int(test[j])
        same = 0
        IsBST(real, test)
        if same == 0:
            LIST.append('Yes')
        else:
            LIST.append('No')
    for i in range(len(LIST)):
        print(LIST[i])","for i in range(N):
    real[i] = int(real[i])","for i, val in enumerate(real):
    real[i] = int(val)",1
DataStructure_Algorithm_ZJU,https://github.com/CYBruce/DataStructure_Algorithm_ZJU/tree/master//04-4 .py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DataStructure_Algorithm_ZJU//04-4 .py,,"def get_input(N, L):
    global same
    LIST = []
    real = input().split()
    for i in range(N):
        real[i] = int(real[i])
    for i in range(L):
        test = input().split()
        for j in range(N):
            test[j] = int(test[j])
        same = 0
        IsBST(real, test)
        if same == 0:
            LIST.append('Yes')
        else:
            LIST.append('No')
    for i in range(len(LIST)):
        print(LIST[i])","for i in range(len(LIST)):
    print(LIST[i])","for i, item in enumerate(LIST):
    print(item)",1
DataStructure_Algorithm_ZJU,https://github.com/CYBruce/DataStructure_Algorithm_ZJU/tree/master//04-4 .py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DataStructure_Algorithm_ZJU//04-4 .py,,"def get_input(N, L):
    global same
    LIST = []
    real = input().split()
    for i in range(N):
        real[i] = int(real[i])
    for i in range(L):
        test = input().split()
        for j in range(N):
            test[j] = int(test[j])
        same = 0
        IsBST(real, test)
        if same == 0:
            LIST.append('Yes')
        else:
            LIST.append('No')
    for i in range(len(LIST)):
        print(LIST[i])","for j in range(N):
    test[j] = int(test[j])","for i, val in enumerate(test):
    test[i] = int(val)",1
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/videa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/videa.py,VideaIE,"def rc4(cipher_text, key):
    res = b''
    key_len = len(key)
    S = list(range(256))
    j = 0
    for i in range(256):
        j = (j + S[i] + ord(key[i % key_len])) % 256
        (S[i], S[j]) = (S[j], S[i])
    i = 0
    j = 0
    for m in range(len(cipher_text)):
        i = (i + 1) % 256
        j = (j + S[i]) % 256
        (S[i], S[j]) = (S[j], S[i])
        k = S[(S[i] + S[j]) % 256]
        res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))
    return res.decode()","for m in range(len(cipher_text)):
    i = (i + 1) % 256
    j = (j + S[i]) % 256
    (S[i], S[j]) = (S[j], S[i])
    k = S[(S[i] + S[j]) % 256]
    res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))","for (m, cipher_char) in enumerate(cipher_text):
    i = (i + 1) % 256
    j = (j + S[i]) % 256
    (S[i], S[j]) = (S[j], S[i])
    k = S[(S[i] + S[j]) % 256]
    res += compat_struct_pack('B', k ^ compat_ord(cipher_char))",1
GPT2-chitchat,https://github.com/yangjianxin1/GPT2-chitchat/tree/master//train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GPT2-chitchat//train.py,,"def train(model, logger, train_dataset, validate_dataset, args):
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    validate_dataloader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    early_stopping = EarlyStopping(args.patience, verbose=True, save_path=args.save_model_path)
    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs
    optimizer = transformers.AdamW(model.parameters(), lr=args.lr, eps=args.eps)
    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)
    logger.info('starting training')
    (train_losses, validate_losses) = ([], [])
    best_val_loss = 10000
    for epoch in range(args.epochs):
        train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
        train_losses.append(train_loss)
        validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
        validate_losses.append(validate_loss)
        if validate_loss < best_val_loss:
            best_val_loss = validate_loss
            logger.info('saving current best model for epoch {}'.format(epoch + 1))
            model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
            if not os.path.exists(model_path):
                os.mkdir(model_path)
            model_to_save = model.module if hasattr(model, 'module') else model
            model_to_save.save_pretrained(model_path)
        if args.patience == 0:
            continue
        early_stopping(validate_loss, model)
        if early_stopping.early_stop:
            logger.info('Early stopping')
            break
    logger.info('training finished')
    logger.info('train_losses:{}'.format(train_losses))
    logger.info('validate_losses:{}'.format(validate_losses))","for epoch in range(args.epochs):
    train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
    train_losses.append(train_loss)
    validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
    validate_losses.append(validate_loss)
    if validate_loss < best_val_loss:
        best_val_loss = validate_loss
        logger.info('saving current best model for epoch {}'.format(epoch + 1))
        model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
        if not os.path.exists(model_path):
            os.mkdir(model_path)
        model_to_save = model.module if hasattr(model, 'module') else model
        model_to_save.save_pretrained(model_path)
    if args.patience == 0:
        continue
    early_stopping(validate_loss, model)
    if early_stopping.early_stop:
        logger.info('Early stopping')
        break","for epoch in range(args.epochs):
    train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
    train_losses.append(train_loss)
    validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
    validate_losses.append(validate_loss)
    if validate_loss < best_val_loss:
        best_val_loss = validate_loss
        logger.info('saving current best model for epoch {}'.format(epoch + 1))
        model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
        if not os.path.exists(model_path):
            os.mkdir(model_path)
        model_to_save = model.module if hasattr(model, 'module') else model
        model_to_save.save_pretrained(model_path)
    if args.patience == 0:
        continue
    early_stopping(validate_loss, model)
    if early_stopping.early_stop:
        logger.info('Early stopping')
        break",1
espnet,https://github.com/espnet/espnet/tree/master/test/test_e2e_asr.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/test/test_e2e_asr.py,,"def test_torch_save_and_load():
    args = make_arg()
    model = th_asr.E2E(10, 5, args)
    for p in model.parameters():
        p.data.uniform_()
    if not os.path.exists('.pytest_cache'):
        os.makedirs('.pytest_cache')
    tmppath = tempfile.mktemp()
    asr_utils.torch_save(tmppath, model)
    p_saved = [p.data.numpy() for p in model.parameters()]
    for p in model.parameters():
        p.data.zero_()
    asr_utils.torch_load(tmppath, model)
    for (p1, p2) in zip(p_saved, model.parameters()):
        np.testing.assert_array_equal(p1, p2.data.numpy())
    if os.path.exists(tmppath):
        os.remove(tmppath)","for (p1, p2) in zip(p_saved, model.parameters()):
    np.testing.assert_array_equal(p1, p2.data.numpy())","for i, (p1, p2) in enumerate(zip(p_saved, model.parameters())):
    np.testing.assert_array_equal(p1, p2.data.numpy())",1
iou-tracker,https://github.com/bochinski/iou-tracker/tree/master//viou_tracker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/iou-tracker//viou_tracker.py,,"def track_viou_matlab_wrapper(frames_path, detections, sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio=1.0):
    """"""
    Matlab wrapper of the v-iou tracker for the detrac evaluation toolkit.

    Args:
         detections (numpy.array): numpy array of detections, usually supplied by run_tracker.m
         sigma_l (float): low detection threshold.
         sigma_h (float): high detection threshold.
         sigma_iou (float): IOU threshold.
         t_min (float): minimum track length in frames.

    Returns:
        float: speed in frames per second.
        list: list of tracks.
    """"""
    detections = detections.reshape((7, -1)).transpose()
    dets = load_mot(detections, with_classes=False)
    start = time()
    tracks = track_viou(frames_path + 'img{:05d}.jpg', dets, sigma_l, sigma_h, sigma_iou, int(t_min), int(ttl), tracker_type, keep_upper_height_ratio)
    end = time()
    id_ = 1
    out = []
    for track in tracks:
        for (i, bbox) in enumerate(track['bboxes']):
            out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
        id_ += 1
    num_frames = len(dets)
    speed = num_frames / (end - start)
    return (speed, out)","for track in tracks:
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
    id_ += 1","for (id_, track) in enumerate(tracks):
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]",1
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for (component_name, component_specs) in rte_components.items():
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for (i, (component_name, component_specs)) in enumerate(rte_components.items()):
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)",1
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/visuals.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/visuals.py,,"def get_context_from_uri_vars(only_infos: Optional[List[InfoName]]=None) -> VisualContext:
    context = {}
    for (filter_name, filter_object) in filter_registry.items():
        if only_infos is not None and filter_object.info not in only_infos:
            continue
        this_filter_vars = {}
        for varname in filter_object.htmlvars:
            if not request.has_var(varname):
                continue
            filter_value = request.get_str_input_mandatory(varname)
            if not filter_value:
                continue
            this_filter_vars[varname] = filter_value
        if this_filter_vars:
            context[filter_name] = this_filter_vars
    return context","for (filter_name, filter_object) in filter_registry.items():
    if only_infos is not None and filter_object.info not in only_infos:
        continue
    this_filter_vars = {}
    for varname in filter_object.htmlvars:
        if not request.has_var(varname):
            continue
        filter_value = request.get_str_input_mandatory(varname)
        if not filter_value:
            continue
        this_filter_vars[varname] = filter_value
    if this_filter_vars:
        context[filter_name] = this_filter_vars","for (i, (filter_name, filter_object)) in enumerate(filter_registry.items()):
    if only_infos is not None and filter_object.info not in only_infos:
        continue
    this_filter_vars = {}
    for varname in filter_object.htmlvars:
        if not request.has_var(varname):
            continue
        filter_value = request.get_str_input_mandatory(varname)
        if not filter_value:
            continue
        this_filter_vars[varname] = filter_value
    if this_filter_vars:
        context[filter_name] = this_filter_vars",1
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,FrameToTFE,"def add_point_cloud(self, feature, laser_names, range_image_pose):
    """"""Convert the range images in `feature` to 3D point clouds.

    Adds the point cloud data to the tf.Example feature map.

    Args:
      feature: A tf.Example feature map.
      laser_names: A list of laser names (e.g., 'TOP', 'REAR', 'SIDE_LEFT').
      range_image_pose: A range image pose Tensor for the top laser.
    """"""
    self.laser_info = {}
    for laser_name in laser_names:
        beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
        if beam_inclinations.size == 0:
            beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
            beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
            laser_ri_name = '%s_ri1' % laser_name
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            height = tf.cast(range_image_shape[0], tf.float32)
            beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
            beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
        beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
        for ri_type in ['ri1', 'ri2']:
            laser_ri_name = '%s_%s' % (laser_name, ri_type)
            range_image = np.array(feature[laser_ri_name].float_list.value[:])
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            range_image = range_image.reshape(range_image_shape)
            range_image_mask = range_image[..., 0] >= 0
            range_image_range = range_image[..., 0]
            batched_pixel_pose = None
            batched_frame_pose = None
            if laser_name == 'TOP' and range_image_pose is not None:
                batched_pixel_pose = range_image_pose[tf.newaxis, ...]
                batched_frame_pose = self.frame_pose[tf.newaxis, ...]
            batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
            batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
            range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
            info = py_utils.NestedMap()
            self.laser_info[laser_ri_name] = info
            info.range_image = range_image
            info.range_image_shape = range_image_shape
            ri_indices = tf.where(range_image_mask)
            points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
            info.num_points = tf.shape(points_xyz).numpy()[0]
            points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
            if self._use_range_image_index_as_lidar_feature:
                points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
            else:
                points_data = tf.concat([points_xyz, points_features], axis=-1)
            points_list = list(points_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
            laser_ri_flow_name = '%s_flow' % laser_ri_name
            if laser_ri_flow_name in feature:
                range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
                range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
                range_image_flow = range_image_flow.reshape(range_image_flow_shape)
                flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
                flow_list = list(flow_data.numpy().reshape([-1]))
                feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","for laser_name in laser_names:
    beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
    if beam_inclinations.size == 0:
        beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
        beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
        laser_ri_name = '%s_ri1' % laser_name
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        height = tf.cast(range_image_shape[0], tf.float32)
        beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
        beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
    beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
    for ri_type in ['ri1', 'ri2']:
        laser_ri_name = '%s_%s' % (laser_name, ri_type)
        range_image = np.array(feature[laser_ri_name].float_list.value[:])
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        range_image = range_image.reshape(range_image_shape)
        range_image_mask = range_image[..., 0] >= 0
        range_image_range = range_image[..., 0]
        batched_pixel_pose = None
        batched_frame_pose = None
        if laser_name == 'TOP' and range_image_pose is not None:
            batched_pixel_pose = range_image_pose[tf.newaxis, ...]
            batched_frame_pose = self.frame_pose[tf.newaxis, ...]
        batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
        batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
        range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
        info = py_utils.NestedMap()
        self.laser_info[laser_ri_name] = info
        info.range_image = range_image
        info.range_image_shape = range_image_shape
        ri_indices = tf.where(range_image_mask)
        points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
        info.num_points = tf.shape(points_xyz).numpy()[0]
        points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
        if self._use_range_image_index_as_lidar_feature:
            points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
        else:
            points_data = tf.concat([points_xyz, points_features], axis=-1)
        points_list = list(points_data.numpy().reshape([-1]))
        feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
        laser_ri_flow_name = '%s_flow' % laser_ri_name
        if laser_ri_flow_name in feature:
            range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
            range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
            range_image_flow = range_image_flow.reshape(range_image_flow_shape)
            flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
            flow_list = list(flow_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","for i,laser_name in enumerate(laser_names):
    beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
    if beam_inclinations.size == 0:
        beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
        beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
        laser_ri_name = '%s_ri1' % laser_name
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        height = tf.cast(range_image_shape[0], tf.float32)
        beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
        beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
    beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
    for ri_type in ['ri1', 'ri2']:
        laser_ri_name = '%s_%s' % (laser_name, ri_type)
        range_image = np.array(feature[laser_ri_name].float_list.value[:])
        range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
        range_image = range_image.reshape(range_image_shape)
        range_image_mask = range_image[..., 0] >= 0
        range_image_range = range_image[..., 0]
        batched_pixel_pose = None
        batched_frame_pose = None
        if laser_name == 'TOP' and range_image_pose is not None:
            batched_pixel_pose = range_image_pose[tf.newaxis, ...]
            batched_frame_pose = self.frame_pose[tf.newaxis, ...]
        batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
        batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
        batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
        range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
        info = py_utils.NestedMap()
        self.laser_info[laser_ri_name] = info
        info.range_image = range_image
        info.range_image_shape = range_image_shape
        ri_indices = tf.where(range_image_mask)
        points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
        info.num_points = tf.shape(points_xyz).numpy()[0]
        points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
        if self._use_range_image_index_as_lidar_feature:
            points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
        else:
            points_data = tf.concat([points_xyz, points_features], axis=-1)
        points_list = list(points_data.numpy().reshape([-1]))
        feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
        laser_ri_flow_name = '%s_flow' % laser_ri_name
        if laser_ri_flow_name in feature:
            range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
            range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
            range_image_flow = range_image_flow.reshape(range_image_flow_shape)
            flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
            flow_list = list(flow_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list",1
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/core/torch_generator_agent.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/core/torch_generator_agent.py,TopKSampling,"def select_paths(self, logprobs, prior_scores, current_length) -> _PathSelection:
    (values, indices) = logprobs.topk(self.k, dim=-1)
    probs = torch.softmax(values, dim=-1)
    choices = torch.multinomial(probs, 1)[:, 0]
    hyp_ids = torch.arange(logprobs.size(0)).to(logprobs.device)
    tok_ids = indices[hyp_ids, choices]
    scores = values[hyp_ids, choices]
    best_scores = prior_scores.expand_as(scores) + scores
    token_details: Optional[List[_PathSelectionTokenDetails]] = None
    if self.verbose:
        tok_logprobs = probs[hyp_ids, choices].log().view(-1).cpu().numpy()
        tok_ranks = choices.view(-1).cpu().numpy()
        token_details = []
        for (tok_logprob, tok_rank) in zip(tok_logprobs, tok_ranks):
            token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})
    return _PathSelection(hypothesis_ids=hyp_ids, token_ids=tok_ids, scores=best_scores, token_details=token_details)","for (tok_logprob, tok_rank) in zip(tok_logprobs, tok_ranks):
    token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})","for i, (tok_logprob, tok_rank) in enumerate(zip(tok_logprobs, tok_ranks)):
    token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})",1
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/tools/_paga.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scanpy/scanpy/tools/_paga.py,PAGA,"def _compute_connectivities_v1_2(self):
    import igraph
    ones = self._neighbors.distances.copy()
    ones.data = np.ones(len(ones.data))
    g = _utils.get_igraph_from_adjacency(ones, directed=True)
    vc = igraph.VertexClustering(g, membership=self._adata.obs[self._groups_key].cat.codes.values)
    ns = vc.sizes()
    n = sum(ns)
    es_inner_cluster = [vc.subgraph(i).ecount() for i in range(len(ns))]
    cg = vc.cluster_graph(combine_edges='sum')
    inter_es = _utils.get_sparse_from_igraph(cg, weight_attr='weight')
    es = np.array(es_inner_cluster) + inter_es.sum(axis=1).A1
    inter_es = inter_es + inter_es.T
    connectivities = inter_es.copy()
    expected_n_edges = inter_es.copy()
    inter_es = inter_es.tocoo()
    for (i, j, v) in zip(inter_es.row, inter_es.col, inter_es.data):
        expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
        if expected_random_null != 0:
            scaled_value = v / expected_random_null
        else:
            scaled_value = 1
        if scaled_value > 1:
            scaled_value = 1
        connectivities[i, j] = scaled_value
        expected_n_edges[i, j] = expected_random_null
    self.ns = ns
    self.expected_n_edges_random = expected_n_edges
    self.connectivities = connectivities
    self.connectivities_tree = self._get_connectivities_tree_v1_2()
    return (inter_es.tocsr(), connectivities)","for (i, j, v) in zip(inter_es.row, inter_es.col, inter_es.data):
    expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
    if expected_random_null != 0:
        scaled_value = v / expected_random_null
    else:
        scaled_value = 1
    if scaled_value > 1:
        scaled_value = 1
    connectivities[i, j] = scaled_value
    expected_n_edges[i, j] = expected_random_null","for (k, (i, j, v)) in enumerate(zip(inter_es.row, inter_es.col, inter_es.data)):
    expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
    if expected_random_null != 0:
        scaled_value = v / expected_random_null
    else:
        scaled_value = 1
    if scaled_value > 1:
        scaled_value = 1
    connectivities[i, j] = scaled_value
    expected_n_edges[i, j] = expected_random_null",1
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/compatibility/v2/api/_quantum_computer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/compatibility/v2/api/_quantum_computer.py,,"def _symmetrization(program: Program, meas_qubits: List[int], symm_type: int=3) -> Tuple[List[Program], List[Tuple[bool]]]:
    """"""
    For the input program generate new programs which flip the measured qubits with an X gate in
    certain combinations in order to symmetrize readout.

    An expanded list of programs is returned along with a list of bools which indicates which
    qubits are flipped in each program.

    The symmetrization types are specified by an int; the types available are:

    * -1 -- exhaustive symmetrization uses every possible combination of flips
    *  0 -- trivial that is no symmetrization
    *  1 -- symmetrization using an OA with strength 1
    *  2 -- symmetrization using an OA with strength 2
    *  3 -- symmetrization using an OA with strength 3

    In the context of readout symmetrization the strength of the orthogonal array enforces the
    symmetry of the marginal confusion matrices.

    By default a strength 3 OA is used; this ensures expectations of the form <b_k * b_j * b_i>
    for bits any bits i,j,k will have symmetric readout errors. Here expectation of a random
    variable x as is denote <x> = sum_i Pr(i) x_i. It turns out that a strength 3 OA is also a
    strength 2 and strength 1 OA it also ensures <b_j * b_i> and <b_i> have symmetric readout
    errors for any bits b_j and b_i.

    :param programs: a program which will be symmetrized.
    :param meas_qubits: the groups of measurement qubits. Only these qubits will be symmetrized
        over, even if the program acts on other qubits.
    :param sym_type: an int determining the type of symmetrization performed.
    :return: a list of symmetrized programs, the corresponding array of bools indicating which
        qubits were flipped.
    """"""
    if symm_type < -1 or symm_type > 3:
        raise ValueError('symm_type must be one of the following ints [-1, 0, 1, 2, 3].')
    elif symm_type == -1:
        flip_matrix = np.asarray(list(itertools.product([0, 1], repeat=len(meas_qubits))))
    elif symm_type >= 0:
        flip_matrix = _construct_orthogonal_array(len(meas_qubits), symm_type)
    flip_matrix = flip_matrix[:, :len(meas_qubits)]
    symm_programs = []
    flip_arrays = []
    for flip_array in flip_matrix:
        total_prog_symm = program.copy()
        prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
        total_prog_symm += prog_symm
        symm_programs.append(total_prog_symm)
        flip_arrays.append(flip_array)
    return (symm_programs, flip_arrays)","for flip_array in flip_matrix:
    total_prog_symm = program.copy()
    prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
    total_prog_symm += prog_symm
    symm_programs.append(total_prog_symm)
    flip_arrays.append(flip_array)","for i, flip_array in enumerate(flip_matrix):
    total_prog_symm = program.copy()
    prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
    total_prog_symm += prog_symm
    symm_programs.append(total_prog_symm)
    flip_arrays.append(flip_array)",1
videos,https://github.com/3b1b/videos/tree/master/_2017/waves.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/waves.py,EMWave,"def __init__(self, **kwargs):
    digest_config(self, kwargs)
    if not all(self.propogation_direction == RIGHT):
        self.matrix_transform = np.dot(z_to_vector(self.propogation_direction), np.linalg.inv(z_to_vector(RIGHT)))
    else:
        self.matrix_transform = None
    vector_oscillations = []
    self.E_vects = VGroup()
    self.M_vects = VGroup()
    self.A_vect = np.array(self.A_vect) / get_norm(self.A_vect)
    self.A_vect *= self.amplitude
    for alpha in np.linspace(0, 1, self.n_vectors):
        tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
        phase = -alpha * self.length * self.wave_number
        kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
        E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
        M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
        vector_oscillations += [E_ov, M_ov]
        self.E_vects.add(E_ov.vector)
        self.M_vects.add(M_ov.vector)
    ContinualAnimationGroup.__init__(self, *vector_oscillations)","for alpha in np.linspace(0, 1, self.n_vectors):
    tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
    phase = -alpha * self.length * self.wave_number
    kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
    E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
    M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
    vector_oscillations += [E_ov, M_ov]
    self.E_vects.add(E_ov.vector)
    self.M_vects.add(M_ov.vector)","for i, alpha in enumerate(np.linspace(0, 1, self.n_vectors)):
    tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
    phase = -alpha * self.length * self.wave_number
    kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
    E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
    M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
    vector_oscillations += [E_ov, M_ov]
    self.E_vects.add(E_ov.vector)
    self.M_vects.add(M_ov.vector)",1
justpy,https://github.com/elimintz/justpy/tree/master/justpy/htmlcomponents.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/justpy/justpy/htmlcomponents.py,Div,"def to_html(self, indent=0, indent_step=0, format=True):
    block_indent = ' ' * indent
    if format:
        ws = '\n'
    else:
        ws = ''
    s = f'{block_indent}<{self.html_tag} '
    d = self.convert_object_to_dict()
    for (attr, value) in d['attrs'].items():
        if value:
            s = f'{s}{attr}=""{value}"" '
    if self.style:
        s = f'{s}style=""{self.style}""'
    if self.classes:
        s = f'{s}class=""{self.classes}"">{ws}'
    else:
        s = f'{s}>{ws}'
    if self.inner_html:
        s = f'{s}{self.inner_html}</{self.html_tag}>{ws}'
        return s
    try:
        s = f'{s}{self.text}{ws}'
    except:
        pass
    for c in self.components:
        s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'
    s = f'{s}{block_indent}</{self.html_tag}>{ws}'
    return s","for (attr, value) in d['attrs'].items():
    if value:
        s = f'{s}{attr}=""{value}"" '","for i, (attr, value) in enumerate(d['attrs'].items()):
    if value:
        s = f'{s}{attr}=""{value}"" '",1
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/thrift/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/thrift/all/conanfile.py,ThriftConan,"def export_sources(self):
    for p in self.conan_data.get('patches', {}).get(self.version, []):
        copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)","for p in self.conan_data.get('patches', {}).get(self.version, []):
    copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)","for i, p in enumerate(self.conan_data.get('patches', {}).get(self.version, [])):
    copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)",1
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_imperative_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_imperative_optimizer.py,TestOptimizerLearningRate,"def func_test_set_lr(self):
    with fluid.dygraph.guard():
        a = np.random.uniform(-0.1, 0.1, [10, 10]).astype('float32')
        linear = paddle.nn.Linear(10, 10)
        a = fluid.dygraph.to_variable(a)
        b = linear(a)
        loss = paddle.mean(b)
        adam = fluid.optimizer.Adam(0.1, parameter_list=linear.parameters())
        lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
        for i in range(5):
            adam.set_lr(lr_list[i])
            adam.minimize(loss)
            lr = adam.current_step_lr()
            np.testing.assert_allclose(lr, lr_list[i], rtol=1e-06, atol=0.0)
        lr_var = fluid.layers.create_global_var(shape=[1], value=0.7, dtype='float32')
        adam.set_lr(lr_var)
        adam.minimize(loss)
        lr = adam.current_step_lr()
        np.testing.assert_allclose(lr, 0.7, rtol=1e-06, atol=0.0)
        with self.assertRaises(RuntimeError):
            adam = fluid.optimizer.Adam(fluid.dygraph.NaturalExpDecay(learning_rate=0.1, decay_steps=3, decay_rate=0.5, staircase=True), parameter_list=linear.parameters())
            adam.set_lr(0.01)","for i in range(5):
    adam.set_lr(lr_list[i])
    adam.minimize(loss)
    lr = adam.current_step_lr()
    np.testing.assert_allclose(lr, lr_list[i], rtol=1e-06, atol=0.0)","for i, lr in enumerate(lr_list):
    adam.set_lr(lr)
    adam.minimize(loss)
    current_lr = adam.current_step_lr()
    np.testing.assert_allclose(current_lr, lr, rtol=1e-06, atol=0.0)",1
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_or_warn (the default; fix file if we can, warn otherwise)')
    postproc.add_option('--prefer-avconv', action='store_false', dest='prefer_ffmpeg', help='Prefer avconv over ffmpeg for running the postprocessors')
    postproc.add_option('--prefer-ffmpeg', action='store_true', dest='prefer_ffmpeg', help='Prefer ffmpeg over avconv for running the postprocessors (default)')
    postproc.add_option('--ffmpeg-location', '--avconv-location', metavar='PATH', dest='ffmpeg_location', help='Location of the ffmpeg/avconv binary; either the path to the binary or its containing directory.')
    postproc.add_option('--exec', metavar='CMD', dest='exec_cmd', help=""Execute a command on the file after downloading and post-processing, similar to find's -exec syntax. Example: --exec 'adb push {} /sdcard/Music/ && rm {}'"")
    postproc.add_option('--convert-subs', '--convert-subtitles', metavar='FORMAT', dest='convertsubtitles', default=None, help='Convert the subtitles to other format (currently supported: srt|ass|vtt|lrc)')
    parser.add_option_group(general)
    parser.add_option_group(network)
    parser.add_option_group(geo)
    parser.add_option_group(selection)
    parser.add_option_group(downloader)
    parser.add_option_group(filesystem)
    parser.add_option_group(thumbnail)
    parser.add_option_group(verbosity)
    parser.add_option_group(workarounds)
    parser.add_option_group(video_format)
    parser.add_option_group(subtitles)
    parser.add_option_group(authentication)
    parser.add_option_group(adobe_pass)
    parser.add_option_group(postproc)
    if overrideArguments is not None:
        (opts, args) = parser.parse_args(overrideArguments)
        if opts.verbose:
            write_string('[debug] Override config: ' + repr(overrideArguments) + '\n')
    else:

        def compat_conf(conf):
            if sys.version_info < (3,):
                return [a.decode(preferredencoding(), 'replace') for a in conf]
            return conf
        command_line_conf = compat_conf(sys.argv[1:])
        (opts, args) = parser.parse_args(command_line_conf)
        system_conf = user_conf = custom_conf = []
        if '--config-location' in command_line_conf:
            location = compat_expanduser(opts.config_location)
            if os.path.isdir(location):
                location = os.path.join(location, 'youtube-dl.conf')
            if not os.path.exists(location):
                parser.error('config-location %s does not exist.' % location)
            custom_conf = _readOptions(location)
        elif '--ignore-config' in command_line_conf:
            pass
        else:
            system_conf = _readOptions('/etc/youtube-dl.conf')
            if '--ignore-config' not in system_conf:
                user_conf = _readUserConf()
        argv = system_conf + user_conf + custom_conf + command_line_conf
        (opts, args) = parser.parse_args(argv)
        if opts.verbose:
            for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
                write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))
    return (parser, opts, args)","for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))","for (i, (conf_label, conf)) in enumerate((('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf))):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))",1
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for f in filenames:
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))","for i, f in enumerate(filenames):
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))",1
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for line in input_file:
    filenames.append(line.rstrip())","for i, line in enumerate(input_file):
    filenames.append(line.rstrip())",1
TSD,https://github.com/Sense-X/TSD/tree/master/mmdet/models/detectors/reppoints_detector.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/detectors/reppoints_detector.py,RepPointsDetector,"def merge_aug_results(self, aug_bboxes, aug_scores, img_metas):
    """"""Merge augmented detection bboxes and scores.

        Args:
            aug_bboxes (list[Tensor]): shape (n, 4*#class)
            aug_scores (list[Tensor] or None): shape (n, #class)
            img_shapes (list[Tensor]): shape (3, ).

        Returns:
            tuple: (bboxes, scores)
        """"""
    recovered_bboxes = []
    for (bboxes, img_info) in zip(aug_bboxes, img_metas):
        img_shape = img_info[0]['img_shape']
        scale_factor = img_info[0]['scale_factor']
        flip = img_info[0]['flip']
        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
        recovered_bboxes.append(bboxes)
    bboxes = torch.cat(recovered_bboxes, dim=0)
    if aug_scores is None:
        return bboxes
    else:
        scores = torch.cat(aug_scores, dim=0)
        return (bboxes, scores)","for (bboxes, img_info) in zip(aug_bboxes, img_metas):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)","for i, (bboxes, img_info) in enumerate(zip(aug_bboxes, img_metas)):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)",1
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/sr_evaluator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/sr_evaluator.py,ModelOVModel,"def fit_to_input(self, input_data):
    fitted = {}
    for (name, info) in self.inputs.items():
        data = input_data[self._name_to_idx[name]]
        data = np.expand_dims(data, axis=0)
        data = np.transpose(data, [0, 3, 1, 2])
        if not info.get_partial_shape.is_dynamic:
            assert parse_partial_shape(info.input_data.shape) == np.shape(data)
        fitted[name] = data
    return fitted","for (name, info) in self.inputs.items():
    data = input_data[self._name_to_idx[name]]
    data = np.expand_dims(data, axis=0)
    data = np.transpose(data, [0, 3, 1, 2])
    if not info.get_partial_shape.is_dynamic:
        assert parse_partial_shape(info.input_data.shape) == np.shape(data)
    fitted[name] = data","for i, (name, info) in enumerate(self.inputs.items()):
    data = input_data[self._name_to_idx[name]]
    data = np.expand_dims(data, axis=0)
    data = np.transpose(data, [0, 3, 1, 2])
    if not info.get_partial_shape.is_dynamic:
        assert parse_partial_shape(info.input_data.shape) == np.shape(data)
    fitted[name] = data",1
nnFormer,https://github.com/282857341/nnFormer/tree/master/nnformer/preprocessing/preprocessing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/preprocessing/preprocessing.py,GenericPreprocessor,"def resample_and_normalize(self, data, target_spacing, properties, seg=None, force_separate_z=None):
    """"""
        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values
        (spacing etc)
        :param data:
        :param target_spacing:
        :param properties:
        :param seg:
        :param force_separate_z:
        :return:
        """"""
    original_spacing_transposed = np.array(properties['original_spacing'])[self.transpose_forward]
    before = {'spacing': properties['original_spacing'], 'spacing_transposed': original_spacing_transposed, 'data.shape (data is transposed)': data.shape}
    data[np.isnan(data)] = 0
    (data, seg) = resample_patient(data, seg, np.array(original_spacing_transposed), target_spacing, 3, 1, force_separate_z=force_separate_z, order_z_data=0, order_z_seg=0, separate_z_anisotropy_threshold=self.resample_separate_z_anisotropy_threshold)
    after = {'spacing': target_spacing, 'data.shape (data is resampled)': data.shape}
    print('before:', before, '\nafter: ', after, '\n')
    if seg is not None:
        seg[seg < -1] = 0
    properties['size_after_resampling'] = data[0].shape
    properties['spacing_after_resampling'] = target_spacing
    use_nonzero_mask = self.use_nonzero_mask
    assert len(self.normalization_scheme_per_modality) == len(data), 'self.normalization_scheme_per_modality must have as many entries as data has modalities'
    assert len(self.use_nonzero_mask) == len(data), 'self.use_nonzero_mask must have as many entries as data has modalities'
    for c in range(len(data)):
        scheme = self.normalization_scheme_per_modality[c]
        if scheme == 'CT':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            mean_intensity = self.intensityproperties[c]['mean']
            std_intensity = self.intensityproperties[c]['sd']
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            data[c] = (data[c] - mean_intensity) / std_intensity
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        elif scheme == 'CT2':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            mask = (data[c] > lower_bound) & (data[c] < upper_bound)
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            mn = data[c][mask].mean()
            sd = data[c][mask].std()
            data[c] = (data[c] - mn) / sd
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        else:
            if use_nonzero_mask[c]:
                mask = seg[-1] >= 0
            else:
                mask = np.ones(seg.shape[1:], dtype=bool)
            data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
            data[c][mask == 0] = 0
    return (data, seg, properties)","for c in range(len(data)):
    scheme = self.normalization_scheme_per_modality[c]
    if scheme == 'CT':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        mean_intensity = self.intensityproperties[c]['mean']
        std_intensity = self.intensityproperties[c]['sd']
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        data[c] = (data[c] - mean_intensity) / std_intensity
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    elif scheme == 'CT2':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        mask = (data[c] > lower_bound) & (data[c] < upper_bound)
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        mn = data[c][mask].mean()
        sd = data[c][mask].std()
        data[c] = (data[c] - mn) / sd
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    else:
        if use_nonzero_mask[c]:
            mask = seg[-1] >= 0
        else:
            mask = np.ones(seg.shape[1:], dtype=bool)
        data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
        data[c][mask == 0] = 0","for c, _ in enumerate(data):
    scheme = self.normalization_scheme_per_modality[c]
    if scheme == 'CT':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        mean_intensity = self.intensityproperties[c]['mean']
        std_intensity = self.intensityproperties[c]['sd']
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        data[c] = (data[c] - mean_intensity) / std_intensity
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    elif scheme == 'CT2':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        mask = (data[c] > lower_bound) & (data[c] < upper_bound)
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        mn = data[c][mask].mean()
        sd = data[c][mask].std()
        data[c] = (data[c] - mn) / sd
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    else:
        if use_nonzero_mask[c]:
            mask = seg[-1] >= 0
        else:
            mask = np.ones(seg.shape[1:], dtype=bool)
        data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
        data[c][mask == 0] = 0",1
text_classification,https://github.com/brightmart/text_classification/tree/master/aa1_data_util/data_util_zhihu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/text_classification/aa1_data_util/data_util_zhihu.py,,"def create_voabulary_label(voabulary_label='train-zhihu4-only-title-all.txt', name_scope='', use_seq2seq=False):
    print('create_voabulary_label_sorted.started.traning_data_path:', voabulary_label)
    cache_path = 'cache_vocabulary_label_pik/' + name_scope + '_label_voabulary.pik'
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as data_f:
            (vocabulary_word2index_label, vocabulary_index2word_label) = pickle.load(data_f)
            return (vocabulary_word2index_label, vocabulary_index2word_label)
    else:
        zhihu_f_train = codecs.open(voabulary_label, 'r', 'utf8')
        lines = zhihu_f_train.readlines()
        count = 0
        vocabulary_word2index_label = {}
        vocabulary_index2word_label = {}
        vocabulary_label_count_dict = {}
        for (i, line) in enumerate(lines):
            if '__label__' in line:
                label = line[line.index('__label__') + len('__label__'):].strip().replace('\n', '')
                if vocabulary_label_count_dict.get(label, None) is not None:
                    vocabulary_label_count_dict[label] = vocabulary_label_count_dict[label] + 1
                else:
                    vocabulary_label_count_dict[label] = 1
        list_label = sort_by_value(vocabulary_label_count_dict)
        print('length of list_label:', len(list_label))
        countt = 0
        if use_seq2seq:
            i_list = [0, 1, 2]
            label_special_list = [_GO, _END, _PAD]
            for (i, label) in zip(i_list, label_special_list):
                vocabulary_word2index_label[label] = i
                vocabulary_index2word_label[i] = label
        for (i, label) in enumerate(list_label):
            if i < 10:
                count_value = vocabulary_label_count_dict[label]
                print('label:', label, 'count_value:', count_value)
                countt = countt + count_value
            indexx = i + 3 if use_seq2seq else i
            vocabulary_word2index_label[label] = indexx
            vocabulary_index2word_label[indexx] = label
        print('count top10:', countt)
        if not os.path.exists(cache_path):
            with open(cache_path, 'a') as data_f:
                pickle.dump((vocabulary_word2index_label, vocabulary_index2word_label), data_f)
    print('create_voabulary_label_sorted.ended.len of vocabulary_label:', len(vocabulary_index2word_label))
    return (vocabulary_word2index_label, vocabulary_index2word_label)","for (i, label) in zip(i_list, label_special_list):
    vocabulary_word2index_label[label] = i
    vocabulary_index2word_label[i] = label","for i, label in enumerate(label_special_list):
    vocabulary_word2index_label[label] = i
    vocabulary_index2word_label[i] = label",1
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for marker in self.tracker.markers:
    marker_type = marker.marker_type
    if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
        continue
    centroid = marker.centroid()
    if marker.uid in surface.registered_markers_dist.keys():
        active_markers = active_markers_by_type.get(marker_type, [])
        active_markers.append(centroid)
        active_markers_by_type[marker_type] = active_markers
    else:
        inactive_markers = inactive_markers_by_type.get(marker_type, [])
        inactive_markers.append(centroid)
        inactive_markers_by_type[marker_type] = inactive_markers","for i, marker in enumerate(self.tracker.markers):
    marker_type = marker.marker_type
    if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
        continue
    centroid = marker.centroid()
    if marker.uid in surface.registered_markers_dist.keys():
        active_markers = active_markers_by_type.get(marker_type, [])
        active_markers.append(centroid)
        active_markers_by_type[marker_type] = active_markers
    else:
        inactive_markers = inactive_markers_by_type.get(marker_type, [])
        inactive_markers.append(centroid)
        inactive_markers_by_type[marker_type] = inactive_markers",1
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0","for (counter, count) in capa.perf.counters.most_common():
    logger.debug('perf: counter: {:}: {:,}'.format(counter, count))","for (i, (counter, count)) in enumerate(capa.perf.counters.most_common()):
    logger.debug('perf: counter: {:}: {:,}'.format(counter, count))",1
MultiQC,https://github.com/ewels/MultiQC/tree/master/multiqc/utils/log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/utils/log.py,,"def get_log_stream(logger):
    """"""
    Returns a stream to the root log file.
    If there is no logfile return the stderr log stream

    Returns:
        A stream to the root log file or stderr stream.
    """"""
    file_stream = None
    log_stream = None
    for handler in logger.handlers:
        if isinstance(handler, logging.FileHandler):
            file_stream = handler.stream
        else:
            log_stream = handler.stream
    if file_stream:
        return file_stream
    return log_stream","for handler in logger.handlers:
    if isinstance(handler, logging.FileHandler):
        file_stream = handler.stream
    else:
        log_stream = handler.stream","for i, handler in enumerate(logger.handlers):
    if isinstance(handler, logging.FileHandler):
        file_stream = handler.stream
    else:
        log_stream = handler.stream",1
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/experiment/_main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/experiment/_main.py,Experiment,"def build_symmetrization_memory_maps(self, qubits: Sequence[int], label: str='symmetrization') -> List[Dict[str, List[float]]]:
    """"""
        Build a list of memory maps to be used in a program that is trying to perform readout
        symmetrization via parametric compilation. For example, if we have the following program:

            RX(symmetrization[0]) 0
            RX(symmetrization[1]) 1
            MEASURE 0 ro[0]
            MEASURE 1 ro[1]

        We can perform exhaustive readout symmetrization on our two qubits by providing the four
        following memory maps, and then appropriately flipping the resultant bitstrings:

            {'symmetrization': [0.0, 0.0]} -> XOR results with [0,0]
            {'symmetrization': [0.0, pi]}  -> XOR results with [0,1]
            {'symmetrization': [pi, 0.0]}  -> XOR results with [1,0]
            {'symmetrization': [pi, pi]}   -> XOR results with [1,1]

        :param qubits: List of qubits to symmetrize readout for.
        :param label: Name of the declared memory region. Defaults to ""symmetrization"".
        :return: List of memory maps that performs the desired level of symmetrization.
        """"""
    num_meas_registers = len(self.get_meas_qubits())
    symm_registers = self.get_meas_registers(qubits)
    if self.symmetrization == SymmetrizationLevel.NONE:
        return [{}]
    if self.symmetrization != SymmetrizationLevel.EXHAUSTIVE:
        raise ValueError('We only support exhaustive symmetrization for now.')
    import numpy as np
    import itertools
    assignments = itertools.product(np.array([0, np.pi]), repeat=len(symm_registers))
    memory_maps = []
    for a in assignments:
        zeros = np.zeros(num_meas_registers)
        for (idx, r) in enumerate(symm_registers):
            zeros[r] = a[idx]
        memory_maps.append({f'{label}': list(zeros)})
    return memory_maps","for a in assignments:
    zeros = np.zeros(num_meas_registers)
    for (idx, r) in enumerate(symm_registers):
        zeros[r] = a[idx]
    memory_maps.append({f'{label}': list(zeros)})","for i,a in enumerate(assignments):
    zeros = np.zeros(num_meas_registers)
    for (idx, r) in enumerate(symm_registers):
        zeros[r] = a[idx]
    memory_maps.append({f'{label}': list(zeros)})",1
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","for i in range(bbox.shape[0]):
    if easy_view and label[i] not in g_easy_view_labels:
        continue
    if exclude_table and label[i] == g_classes.index('table'):
        continue
    length = bbox[i, 3:6] - bbox[i, 0:3]
    a = length[0]
    b = length[1]
    c = length[2]
    x = bbox[i, 0]
    y = bbox[i, 1]
    z = bbox[i, 2]
    color = np.array(g_label2color[label[i]], dtype=float) / 255.0
    material = 'material%d' % ins_cnt
    fout_obj.write('usemtl %s\n' % material)
    fout_obj.write('v %f %f %f\n' % (x, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y, z))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z))
    fout_obj.write('g default\n')
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
    fout_obj.write('\n')
    fout_mtl.write('newmtl %s\n' % material)
    fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
    fout_mtl.write('\n')
    v_cnt += 8
    ins_cnt += 1","for i in range(bbox.shape[0]):
    if easy_view and label[i] not in g_easy_view_labels:
        continue
    if exclude_table and label[i] == g_classes.index('table'):
        continue
    length = bbox[i, 3:6] - bbox[i, 0:3]
    a = length[0]
    b = length[1]
    c = length[2]
    x = bbox[i, 0]
    y = bbox[i, 1]
    z = bbox[i, 2]
    color = np.array(g_label2color[label[i]], dtype=float) / 255.0
    material = 'material%d' % ins_cnt
    fout_obj.write('usemtl %s\n' % material)
    fout_obj.write('v %f %f %f\n' % (x, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y, z))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z))
    fout_obj.write('g default\n')
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
    fout_obj.write('\n')
    fout_mtl.write('newmtl %s\n' % material)
    fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
    fout_mtl.write('\n')
    v_cnt += 8
    ins_cnt += 1",1
tensorflow-rnn-shakespeare,https://github.com/martin-gorner/tensorflow-rnn-shakespeare/tree/master//tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-rnn-shakespeare//tests.py,RnnMinibatchSequencerTest,"def test_batches(self):
    start = True
    prev_x = np.zeros([TST_BATCHSIZE, TST_SEQLEN], np.int32)
    prev_y = np.zeros([TST_BATCHSIZE, TST_SEQLEN], np.int32)
    nb_errors = 0
    nb_batches = 0
    for (x, y, epoch) in txt.rnn_minibatch_sequencer(self.data, TST_BATCHSIZE, TST_SEQLEN, TST_EPOCHS):
        if not start:
            nb_errors += self.check_seq_batch(prev_x, x)
            nb_errors += self.check_seq_batch(prev_y, y)
        prev_x = x
        prev_y = y
        start = False
        nb_batches += 1
    self.assertLessEqual(nb_errors, 2 * TST_EPOCHS, msg='Sequences should be correctly continued, even between epochs. Only one sequence is allowed to not continue from one epoch to the next.')
    self.assertLess(TST_TXTSIZE - nb_batches * TST_BATCHSIZE * TST_SEQLEN, TST_BATCHSIZE * TST_SEQLEN * TST_EPOCHS, msg='Text ignored at the end of an epoch must be smaller than one batch of sequences')","for (x, y, epoch) in txt.rnn_minibatch_sequencer(self.data, TST_BATCHSIZE, TST_SEQLEN, TST_EPOCHS):
    if not start:
        nb_errors += self.check_seq_batch(prev_x, x)
        nb_errors += self.check_seq_batch(prev_y, y)
    prev_x = x
    prev_y = y
    start = False
    nb_batches += 1","for (i, (x, y, epoch)) in enumerate(txt.rnn_minibatch_sequencer(self.data, TST_BATCHSIZE, TST_SEQLEN, TST_EPOCHS)):
    if not start:
        nb_errors += self.check_seq_batch(prev_x, x)
        nb_errors += self.check_seq_batch(prev_y, y)
    prev_x = x
    prev_y = y
    start = False
    nb_batches += 1",1
zentral,https://github.com/zentralopensource/zentral/tree/master/tests/inventory/test_metrics_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zentral/tests/inventory/test_metrics_views.py,PrometheusViewsTestCase,"def test_prometheus_metrics_osx_apps_bundle_names(self):
    old_config = settings._collection['apps']['zentral.contrib.inventory'].pop('metrics_options', None)
    settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = ConfigDict({'osx_apps': {'sources': ['zentral tests'], 'bundle_names': ['Baller']}})
    response = self.client.get(reverse('inventory_metrics:all'), HTTP_AUTHORIZATION='Bearer CHANGE ME!!!')
    self.assertEqual(response.status_code, 200)
    seen = False
    for family in text_string_to_metric_families(response.content.decode('utf-8')):
        if family.name == 'zentral_inventory_active_machines_bucket':
            continue
        self.assertEqual(len(family.samples), 7)
        for sample in family.samples:
            self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
            le = sample.labels['le']
            self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
            if le == '1':
                self.assertEqual(sample.value, 0)
            else:
                self.assertEqual(sample.value, 1)
        self.assertFalse(seen)
        seen = True
    self.assertTrue(seen)
    if old_config:
        settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = old_config","for family in text_string_to_metric_families(response.content.decode('utf-8')):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True","for i, family in enumerate(text_string_to_metric_families(response.content.decode('utf-8'))):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True",1
pretrained-models.pytorch,https://github.com/Cadene/pretrained-models.pytorch/tree/master/pretrainedmodels/datasets/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretrained-models.pytorch/pretrainedmodels/datasets/utils.py,AveragePrecisionMeter,"def value(self):
    """"""Returns the model's average precision for each class
        Return:
            ap (FloatTensor): 1xK tensor, with avg precision for each class k
        """"""
    if self.scores.numel() == 0:
        return 0
    ap = torch.zeros(self.scores.size(1))
    rg = torch.arange(1, self.scores.size(0)).float()
    for k in range(self.scores.size(1)):
        scores = self.scores[:, k]
        targets = self.targets[:, k]
        ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)
    return ap","for k in range(self.scores.size(1)):
    scores = self.scores[:, k]
    targets = self.targets[:, k]
    ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)","for k, _ in enumerate(range(self.scores.size(1))):
    scores = self.scores[:, k]
    targets = self.targets[:, k]
    ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)",1
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
    for obj in getattr(self, prop).all():
        obj.clear_cache()
        obj.pre_cache()","for i, prop in enumerate([prop for prop in dir(self) if prop[-8:] == '_related']):
    for obj in getattr(self, prop).all():
        obj.clear_cache()
        obj.pre_cache()",1
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for i in range(len(olist) // 2):
    olist[i * 2] = F.softmax(olist[i * 2], dim=1)","for i in enumerate(range(len(olist) // 2)):
    olist[i[0] * 2] = F.softmax(olist[i[0] * 2], dim=1)",1
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for i in range(len(olist) // 2):
    (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
    (FB, FC, FH, FW) = ocls.size()
    stride = 2 ** (i + 2)
    anchor = stride * 4
    poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
    for (Iindex, hindex, windex) in poss:
        (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
        score = ocls[0, 1, hindex, windex]
        loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
        priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
        variances = [0.1, 0.2]
        box = decode(loc, priors, variances)
        (x1, y1, x2, y2) = box[0]
        bboxlist.append([x1, y1, x2, y2, score])","for i, _ in enumerate(range(len(olist) // 2)):
    (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
    (FB, FC, FH, FW) = ocls.size()
    stride = 2 ** (i + 2)
    anchor = stride * 4
    poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
    for (Iindex, hindex, windex) in poss:
        (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
        score = ocls[0, 1, hindex, windex]
        loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
        priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
        variances = [0.1, 0.2]
        box = decode(loc, priors, variances)
        (x1, y1, x2, y2) = box[0]
        bboxlist.append([x1, y1, x2, y2, score])",1
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for (Iindex, hindex, windex) in poss:
    (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
    score = ocls[0, 1, hindex, windex]
    loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
    priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
    variances = [0.1, 0.2]
    box = decode(loc, priors, variances)
    (x1, y1, x2, y2) = box[0]
    bboxlist.append([x1, y1, x2, y2, score])","for i, (Iindex, hindex, windex) in enumerate(poss):
    (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
    score = ocls[0, 1, hindex, windex]
    loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
    priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
    variances = [0.1, 0.2]
    box = decode(loc, priors, variances)
    (x1, y1, x2, y2) = box[0]
    bboxlist.append([x1, y1, x2, y2, score])",1
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
    (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
    self.writer.add_summary(summary, i)
    (accuracy, _) = self.accuracy(predictions, labels)
    if i % 50 == 0:
        print('Minibatch loss at step %d: %f' % (i, l))
        print('Minibatch accuracy: %.1f%%' % accuracy)","for i, (samples, labels) in enumerate(data_iterator(train_samples, train_labels, self.train_batch_size)):
    (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
    self.writer.add_summary(summary, i)
    (accuracy, _) = self.accuracy(predictions, labels)
    if i % 50 == 0:
        print('Minibatch loss at step %d: %f' % (i, l))
        print('Minibatch accuracy: %.1f%%' % accuracy)",1
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
    print('samples shape', samples.shape)
    (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
    self.writer.add_summary(summary, i)
    (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
    accuracies.append(accuracy)
    confusionMatrices.append(cm)
    print('Test Accuracy: %.1f%%' % accuracy)","for i, (samples, labels) in enumerate(data_iterator(test_samples, test_labels, self.test_batch_size)):
    print('samples shape', samples.shape)
    (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
    self.writer.add_summary(summary, i)
    (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
    accuracies.append(accuracy)
    confusionMatrices.append(cm)
    print('Test Accuracy: %.1f%%' % accuracy)",1
mypy,https://github.com/python/mypy/tree/master/mypy/join.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/join.py,,"def combine_arg_names(t: CallableType, s: CallableType) -> List[Optional[str]]:
    """"""Produces a list of argument names compatible with both callables.

    For example, suppose 't' and 's' have the following signatures:

    - t: (a: int, b: str, X: str) -> None
    - s: (a: int, b: str, Y: str) -> None

    This function would return [""a"", ""b"", None]. This information
    is then used above to compute the join of t and s, which results
    in a signature of (a: int, b: str, str) -> None.

    Note that the third argument's name is omitted and 't' and 's'
    are both valid subtypes of this inferred signature.

    Precondition: is_similar_types(t, s) is true.
    """"""
    num_args = len(t.arg_types)
    new_names = []
    for i in range(num_args):
        t_name = t.arg_names[i]
        s_name = s.arg_names[i]
        if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
            new_names.append(t_name)
        else:
            new_names.append(None)
    return new_names","for i in range(num_args):
    t_name = t.arg_names[i]
    s_name = s.arg_names[i]
    if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
        new_names.append(t_name)
    else:
        new_names.append(None)","for i, (t_name, s_name) in enumerate(zip(t.arg_names, s.arg_names)):
    if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
        new_names.append(t_name)
    else:
        new_names.append(None)",1
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str","for index in range(service_count):
    service = services[index]
    index += 1
    commandData = ''
    if service.upper() == 'ZOOKEEPER':
        commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
    else:
        commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
    payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})","for (index, service) in enumerate(services):
    index += 1
    commandData = ''
    if service.upper() == 'ZOOKEEPER':
        commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
    else:
        commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
    payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})",1
nltk,https://github.com/nltk/nltk/tree/master/nltk/tokenize/punkt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tokenize/punkt.py,PunktBaseClass,"def _tokenize_words(self, plaintext):
    """"""
        Divide the given text into tokens, using the punkt word
        segmentation regular expression, and generate the resulting list
        of tokens augmented as three-tuples with two boolean values for whether
        the given token occurs at the start of a paragraph or a new line,
        respectively.
        """"""
    parastart = False
    for line in plaintext.split('\n'):
        if line.strip():
            line_toks = iter(self._lang_vars.word_tokenize(line))
            try:
                tok = next(line_toks)
            except StopIteration:
                continue
            yield self._Token(tok, parastart=parastart, linestart=True)
            parastart = False
            for tok in line_toks:
                yield self._Token(tok)
        else:
            parastart = True","for line in plaintext.split('\n'):
    if line.strip():
        line_toks = iter(self._lang_vars.word_tokenize(line))
        try:
            tok = next(line_toks)
        except StopIteration:
            continue
        yield self._Token(tok, parastart=parastart, linestart=True)
        parastart = False
        for tok in line_toks:
            yield self._Token(tok)
    else:
        parastart = True","for i, line in enumerate(plaintext.split('\n')):
    if line.strip():
        line_toks = iter(self._lang_vars.word_tokenize(line))
        try:
            tok = next(line_toks)
        except StopIteration:
            continue
        yield self._Token(tok, parastart=parastart, linestart=True)
        parastart = False
        for tok in line_toks:
            yield self._Token(tok)
    else:
        parastart = True",1
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for nonterminal in nonterminals:
    if nonterminal not in first_plans:
        _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)","for i, nonterminal in enumerate(nonterminals):
    if nonterminal not in first_plans:
        _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)",1
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for dfas in nonterminal_to_dfas.values():
    for dfa_state in dfas:
        transitions = dfa_state.transitions
        for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
            for (transition, pushes) in first_plans[nonterminal].items():
                if transition in transitions:
                    prev_plan = transitions[transition]
                    choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                    raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                transitions[transition] = DFAPlan(next_dfa, pushes)","for i, dfas in enumerate(nonterminal_to_dfas.values()):
    for dfa_state in dfas:
        transitions = dfa_state.transitions
        for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
            for (transition, pushes) in first_plans[nonterminal].items():
                if transition in transitions:
                    prev_plan = transitions[transition]
                    choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                    raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                transitions[transition] = DFAPlan(next_dfa, pushes)",1
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for dfa_state in dfas:
    transitions = dfa_state.transitions
    for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
        for (transition, pushes) in first_plans[nonterminal].items():
            if transition in transitions:
                prev_plan = transitions[transition]
                choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
            transitions[transition] = DFAPlan(next_dfa, pushes)","for (i, dfa_state) in enumerate(dfas):
    transitions = dfa_state.transitions
    for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
        for (transition, pushes) in first_plans[nonterminal].items():
            if transition in transitions:
                prev_plan = transitions[transition]
                choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
            transitions[transition] = DFAPlan(next_dfa, pushes)",1
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
    for (transition, pushes) in first_plans[nonterminal].items():
        if transition in transitions:
            prev_plan = transitions[transition]
            choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
            raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
        transitions[transition] = DFAPlan(next_dfa, pushes)","for (i, (nonterminal, next_dfa)) in enumerate(dfa_state.nonterminal_arcs.items()):
    for (transition, pushes) in first_plans[nonterminal].items():
        if transition in transitions:
            prev_plan = transitions[transition]
            choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
            raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
        transitions[transition] = DFAPlan(next_dfa, pushes)",1
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for (transition, pushes) in first_plans[nonterminal].items():
    if transition in transitions:
        prev_plan = transitions[transition]
        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
    transitions[transition] = DFAPlan(next_dfa, pushes)","for i, (transition, pushes) in enumerate(first_plans[nonterminal].items()):
    if transition in transitions:
        prev_plan = transitions[transition]
        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
    transitions[transition] = DFAPlan(next_dfa, pushes)",1
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for person in peoplelist:
    if args.vv == True or args.debug == True:
        print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
    else:
        sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
        sys.stdout.flush()
    count = count + 1
    if person.person_image:
        try:
            target_image = face_recognition.load_image_file(person.person_image)
            target_encoding = face_recognition.face_encodings(target_image)[0]
            profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
            if args.debug == True:
                print(profilelist)
        except:
            continue
    else:
        continue
    early_break = False
    updatedlist = []
    for (profilelink, profilepic, distance) in profilelist:
        try:
            os.remove('potential_target_image.jpg')
        except:
            pass
        if early_break:
            break
        image_link = profilepic
        if image_link:
            try:
                urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                try:
                    potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                except:
                    continue
                results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                for result in results:
                    if args.mode == 'fast':
                        if result < threshold:
                            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                            if args.vv == True:
                                print('\tMatch found: ' + person.full_name)
                                print('\tLinkedIn: ' + person.linkedin)
                            early_break = True
                            break
                    elif args.mode == 'accurate':
                        if result < threshold:
                            updatedlist.append([profilelink, image_link, result])
            except Exception as e:
                print(e)
    if args.mode == 'accurate':
        highestdistance = 1.0
        bestprofilelink = ''
        bestimagelink = ''
        for (profilelink, image_link, distance) in updatedlist:
            if distance < highestdistance:
                highestdistance = distance
                bestprofilelink = profilelink
                bestimagelink = image_link
        if highestdistance < threshold:
            person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)","for (i, person) in enumerate(peoplelist):
    if args.vv == True or args.debug == True:
        print('LinkedIn Check %i/%i : %s' % (i+1, len(peoplelist), person.full_name))
    else:
        sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (i+1, len(peoplelist), person.full_name))
        sys.stdout.flush()
    if person.person_image:
        try:
            target_image = face_recognition.load_image_file(person.person_image)
            target_encoding = face_recognition.face_encodings(target_image)[0]
            profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
            if args.debug == True:
                print(profilelist)
        except:
            continue
    else:
        continue
    early_break = False
    updatedlist = []
    for (profilelink, profilepic, distance) in profilelist:
        try:
            os.remove('potential_target_image.jpg')
        except:
            pass
        if early_break:
            break
        image_link = profilepic
        if image_link:
            try:
                urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                try:
                    potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                except:
                    continue
                results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                for result in results:
                    if args.mode == 'fast':
                        if result < threshold:
                            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                            if args.vv == True:
                                print('\tMatch found: ' + person.full_name)
                                print('\tLinkedIn: ' + person.linkedin)
                            early_break = True
                            break
                    elif args.mode == 'accurate':
                        if result < threshold:
                            updatedlist.append([profilelink, image_link, result])
            except Exception as e:
                print(e)
    if args.mode == 'accurate':
        highestdistance = 1.0
        bestprofilelink = ''
        bestimagelink = ''
        for (profilelink, image_link, distance) in updatedlist:
            if distance < highestdistance:
                highestdistance = distance
                bestprofilelink = profilelink
                bestimagelink = image_link
        if highestdistance < threshold:
            person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)",1
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for (profilelink, profilepic, distance) in profilelist:
    try:
        os.remove('potential_target_image.jpg')
    except:
        pass
    if early_break:
        break
    image_link = profilepic
    if image_link:
        try:
            urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
            potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
            try:
                potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
            except:
                continue
            results = face_recognition.face_distance([target_encoding], potential_target_encoding)
            for result in results:
                if args.mode == 'fast':
                    if result < threshold:
                        person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                        person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                        if args.vv == True:
                            print('\tMatch found: ' + person.full_name)
                            print('\tLinkedIn: ' + person.linkedin)
                        early_break = True
                        break
                elif args.mode == 'accurate':
                    if result < threshold:
                        updatedlist.append([profilelink, image_link, result])
        except Exception as e:
            print(e)","for (i, (profilelink, profilepic, distance)) in enumerate(profilelist):
    try:
        os.remove('potential_target_image.jpg')
    except:
        pass
    if early_break:
        break
    image_link = profilepic
    if image_link:
        try:
            urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
            potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
            try:
                potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
            except:
                continue
            results = face_recognition.face_distance([target_encoding], potential_target_encoding)
            for result in results:
                if args.mode == 'fast':
                    if result < threshold:
                        person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                        person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                        if args.vv == True:
                            print('\tMatch found: ' + person.full_name)
                            print('\tLinkedIn: ' + person.linkedin)
                        early_break = True
                        break
                elif args.mode == 'accurate':
                    if result < threshold:
                        updatedlist.append([profilelink, image_link, result])
        except Exception as e:
            print(e)",1
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for (profilelink, image_link, distance) in updatedlist:
    if distance < highestdistance:
        highestdistance = distance
        bestprofilelink = profilelink
        bestimagelink = image_link","for i, (profilelink, image_link, distance) in enumerate(updatedlist):
    if distance < highestdistance:
        highestdistance = distance
        bestprofilelink = profilelink
        bestimagelink = image_link",1
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","for result in results:
    if args.mode == 'fast':
        if result < threshold:
            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)
            early_break = True
            break
    elif args.mode == 'accurate':
        if result < threshold:
            updatedlist.append([profilelink, image_link, result])","for i, result in enumerate(results):
    if args.mode == 'fast':
        if result < threshold:
            person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
            person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
            if args.vv == True:
                print('\tMatch found: ' + person.full_name)
                print('\tLinkedIn: ' + person.linkedin)
            early_break = True
            break
    elif args.mode == 'accurate':
        if result < threshold:
            updatedlist.append([profilelink, image_link, result])",1
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for id_i in tgt_ids:
    cocoEval = COCOeval(coco, coco_dets, iou_type)
    if res_type == 'proposal':
        cocoEval.params.useCats = 0
        cocoEval.params.maxDets = list(max_dets)
    cocoEval.params.imgIds = [id_i]
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
    res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})","for i, id_i in enumerate(tgt_ids):
    cocoEval = COCOeval(coco, coco_dets, iou_type)
    if res_type == 'proposal':
        cocoEval.params.useCats = 0
        cocoEval.params.maxDets = list(max_dets)
    cocoEval.params.imgIds = [id_i]
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
    res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})",1
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for sel in view.sel():
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))","for i, sel in enumerate(view.sel()):
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))",1
nltk,https://github.com/nltk/nltk/tree/master/nltk/tag/senna.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tag/senna.py,SennaNERTagger,"def tag_sents(self, sentences):
    """"""
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        """"""
    tagged_sents = super().tag_sents(sentences)
    for i in range(len(tagged_sents)):
        for j in range(len(tagged_sents[i])):
            annotations = tagged_sents[i][j]
            tagged_sents[i][j] = (annotations['word'], annotations['ner'])
    return tagged_sents","for i in range(len(tagged_sents)):
    for j in range(len(tagged_sents[i])):
        annotations = tagged_sents[i][j]
        tagged_sents[i][j] = (annotations['word'], annotations['ner'])","for i, sent in enumerate(tagged_sents):
    for j, annotations in enumerate(sent):
        tagged_sents[i][j] = (annotations['word'], annotations['ner'])",1
nltk,https://github.com/nltk/nltk/tree/master/nltk/tag/senna.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tag/senna.py,SennaNERTagger,"def tag_sents(self, sentences):
    """"""
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        """"""
    tagged_sents = super().tag_sents(sentences)
    for i in range(len(tagged_sents)):
        for j in range(len(tagged_sents[i])):
            annotations = tagged_sents[i][j]
            tagged_sents[i][j] = (annotations['word'], annotations['ner'])
    return tagged_sents","for j in range(len(tagged_sents[i])):
    annotations = tagged_sents[i][j]
    tagged_sents[i][j] = (annotations['word'], annotations['ner'])","for j, annotations in enumerate(tagged_sents[i]):
    tagged_sents[i][j] = (annotations['word'], annotations['ner'])",1
pdpipe,https://github.com/pdpipe/pdpipe/tree/master/pdpipe/sklearn_stages.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pdpipe/pdpipe/sklearn_stages.py,Encode,"def _transform(self, df, verbose):
    inter_df = df
    for colname in self.encoders:
        lbl_enc = self.encoders[colname]
        source_col = df[colname]
        loc = df.columns.get_loc(colname) + 1
        new_name = colname + '_enc'
        if self._drop:
            inter_df = inter_df.drop(colname, axis=1)
            new_name = colname
            loc -= 1
        inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)
    return inter_df","for colname in self.encoders:
    lbl_enc = self.encoders[colname]
    source_col = df[colname]
    loc = df.columns.get_loc(colname) + 1
    new_name = colname + '_enc'
    if self._drop:
        inter_df = inter_df.drop(colname, axis=1)
        new_name = colname
        loc -= 1
    inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)","for i, colname in enumerate(self.encoders):
    lbl_enc = self.encoders[colname]
    source_col = df[colname]
    loc = df.columns.get_loc(colname) + 1
    new_name = colname + '_enc'
    if self._drop:
        inter_df = inter_df.drop(colname, axis=1)
        new_name = colname
        loc -= 1
    inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)",1
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/tests/synthesis/test_deep_feature_synthesis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/featuretools/featuretools/tests/synthesis/test_deep_feature_synthesis.py,,"def test_abides_by_max_depth_param(es):
    for i in [0, 1, 2, 3]:
        dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
        features = dfs_obj.build_features()
        for f in features:
            assert f.get_depth() <= i","for i in [0, 1, 2, 3]:
    dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
    features = dfs_obj.build_features()
    for f in features:
        assert f.get_depth() <= i","for i in range(4):
    dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
    features = dfs_obj.build_features()
    for f in features:
        assert f.get_depth() <= i",1
chia-rosechain,https://github.com/snight1983/chia-rosechain/tree/master/chia/daemon/server.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chia-rosechain/chia/daemon/server.py,WebSocketServer,"def remove_connection(self, websocket: WebSocketServerProtocol):
    service_name = None
    if websocket in self.remote_address_map:
        service_name = self.remote_address_map[websocket]
        self.remote_address_map.pop(websocket)
    if service_name in self.connections:
        after_removal = []
        for connection in self.connections[service_name]:
            if connection == websocket:
                continue
            else:
                after_removal.append(connection)
        self.connections[service_name] = after_removal","for connection in self.connections[service_name]:
    if connection == websocket:
        continue
    else:
        after_removal.append(connection)","for i, connection in enumerate(self.connections[service_name]):
    if connection == websocket:
        continue
    else:
        after_removal.append(connection)",1
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for rname in recorder_names:
    if not self.recorders.get(rname):
        raise NoSuchConfigurationRecorderException(rname)
    recorders.append(self.recorders[rname].to_dict())","for i, rname in enumerate(recorder_names):
    if not self.recorders.get(rname):
        raise NoSuchConfigurationRecorderException(rname)
    recorders.append(self.recorders[rname].to_dict())",1
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for recorder in self.recorders.values():
    recorders.append(recorder.to_dict())","for i, recorder in enumerate(self.recorders.values()):
    recorders.append(recorder.to_dict())",1
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/democracynow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/democracynow.py,DemocracynowIE,"def _real_extract(self, url):
    display_id = self._match_id(url)
    webpage = self._download_webpage(url, display_id)
    json_data = self._parse_json(self._search_regex('<script[^>]+type=""text/json""[^>]*>\\s*({[^>]+})', webpage, 'json'), display_id)
    title = json_data['title']
    formats = []
    video_id = None
    for key in ('file', 'audio', 'video', 'high_res_video'):
        media_url = json_data.get(key, '')
        if not media_url:
            continue
        media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
        video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
        formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})
    self._sort_formats(formats)
    default_lang = 'en'
    subtitles = {}

    def add_subtitle_item(lang, info_dict):
        if lang not in subtitles:
            subtitles[lang] = []
        subtitles[lang].append(info_dict)
    if 'caption_file' in json_data:
        add_subtitle_item(default_lang, {'url': compat_urlparse.urljoin(url, json_data['caption_file'])})
    for subtitle_item in json_data.get('captions', []):
        lang = subtitle_item.get('language', '').lower() or default_lang
        add_subtitle_item(lang, {'url': compat_urlparse.urljoin(url, subtitle_item['url'])})
    description = self._og_search_description(webpage, default=None)
    return {'id': video_id or display_id, 'title': title, 'description': description, 'thumbnail': json_data.get('image'), 'subtitles': subtitles, 'formats': formats}","for key in ('file', 'audio', 'video', 'high_res_video'):
    media_url = json_data.get(key, '')
    if not media_url:
        continue
    media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
    video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
    formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})","for i, key in enumerate(('file', 'audio', 'video', 'high_res_video')):
    media_url = json_data.get(key, '')
    if not media_url:
        continue
    media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
    video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
    formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})",1
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/neuron_view.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/neuron_view.py,,"def format_delimiters(tokens, tokenizer):
    formatted_tokens = []
    for t in tokens:
        if tokenizer.sep_token:
            t = t.replace(tokenizer.sep_token, '[SEP]')
        if tokenizer.cls_token:
            t = t.replace(tokenizer.cls_token, '[CLS]')
        formatted_tokens.append(t)
    return formatted_tokens","for t in tokens:
    if tokenizer.sep_token:
        t = t.replace(tokenizer.sep_token, '[SEP]')
    if tokenizer.cls_token:
        t = t.replace(tokenizer.cls_token, '[CLS]')
    formatted_tokens.append(t)","for i,t in enumerate(tokens):
    if tokenizer.sep_token:
        t = t.replace(tokenizer.sep_token, '[SEP]')
    if tokenizer.cls_token:
        t = t.replace(tokenizer.cls_token, '[CLS]')
    formatted_tokens.append(t)",1
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for (arn, role_entry_j) in all_roles.items():
    role_entry = json.loads(role_entry_j)
    if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
        roles_to_delete_from_cache.append(arn)","for i, (arn, role_entry_j) in enumerate(all_roles.items()):
    role_entry = json.loads(role_entry_j)
    if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
        roles_to_delete_from_cache.append(arn)",1
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for account_id in accounts_d.keys():
    if config.get('environment') in ['prod', 'dev']:
        tasks.append(cache_iam_resources_for_account.s(account_id))
    else:
        log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
        if account_id in config.get('celery.test_account_ids', []):
            tasks.append(cache_iam_resources_for_account.s(account_id))","for i, account_id in enumerate(accounts_d.keys()):
    if config.get('environment') in ['prod', 'dev']:
        tasks.append(cache_iam_resources_for_account.s(account_id))
    else:
        log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
        if account_id in config.get('celery.test_account_ids', []):
            tasks.append(cache_iam_resources_for_account.s(account_id))",1
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for arn in roles_to_delete_from_cache:
    all_roles.pop(arn, None)","for i, arn in enumerate(roles_to_delete_from_cache):
    all_roles.pop(arn, None)",1
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/map_detection/grid_predictor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/map_detection/grid_predictor.py,GridPredictor,"def predict_enemy_genre(self):
    image_dic = {}
    scaling_dic = self.config.MAP_ENEMY_GENRE_DETECTION_SCALING
    for (name, template) in self.template_enemy_genre.items():
        if template is None:
            logger.warning(f'Enemy detection template not found: {name}')
            logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
            raise ScriptError(f'Enemy detection template not found: {name}')
        short_name = name[6:] if name.startswith('Siren_') else name
        scaling = scaling_dic.get(short_name, 1)
        scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
        for scale in scaling:
            if scale not in image_dic:
                shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
                image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
            if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
                return name
    return None","for (name, template) in self.template_enemy_genre.items():
    if template is None:
        logger.warning(f'Enemy detection template not found: {name}')
        logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
        raise ScriptError(f'Enemy detection template not found: {name}')
    short_name = name[6:] if name.startswith('Siren_') else name
    scaling = scaling_dic.get(short_name, 1)
    scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
    for scale in scaling:
        if scale not in image_dic:
            shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
            image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
        if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
            return name","for i, (name, template) in enumerate(self.template_enemy_genre.items()):
    if template is None:
        logger.warning(f'Enemy detection template not found: {name}')
        logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
        raise ScriptError(f'Enemy detection template not found: {name}')
    short_name = name[6:] if name.startswith('Siren_') else name
    scaling = scaling_dic.get(short_name, 1)
    scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
    for scale in scaling:
        if scale not in image_dic:
            shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
            image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
        if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
            return name",1
PathPicker,https://github.com/facebook/PathPicker/tree/master/src/tests/test_parsing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPicker/src/tests/test_parsing.py,TestParseFunction,"def test_all_input_matches(self) -> None:
    for test_case in ALL_INPUT_TEST_CASES:
        result = parse.match_line(test_case.test_input, False, True)
        if not result:
            self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
            continue
        (match, _, _) = result
        self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')
    print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.')","for test_case in ALL_INPUT_TEST_CASES:
    result = parse.match_line(test_case.test_input, False, True)
    if not result:
        self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
        continue
    (match, _, _) = result
    self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')","for i, test_case in enumerate(ALL_INPUT_TEST_CASES):
    result = parse.match_line(test_case.test_input, False, True)
    if not result:
        self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
        continue
    (match, _, _) = result
    self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')",1
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_saving_in_empty_folder(self):
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs:
            if isinstance(temp_fs, TempFS):
                self.eopatch.save(temp_fs.root_path)
            else:
                self.eopatch.save('/', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
            subfolder = 'new-subfolder'
            self.eopatch.save('new-subfolder', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for fs_loader in self.filesystem_loaders:
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for i, fs_loader in enumerate(self.filesystem_loaders):
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))",1
binderhub,https://github.com/jupyterhub/binderhub/tree/master/helm-chart/binderhub/files/binderhub_config.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/binderhub/helm-chart/binderhub/files/binderhub_config.py,,"def get_value(key, default=None):
    """"""
    Find an item in values.yaml of a given name & return it

    get_value(""a.b.c"") returns values['a']['b']['c']
    """"""
    value = _load_values()
    for level in key.split('.'):
        if not isinstance(value, dict):
            return default
        if level not in value:
            return default
        else:
            value = value[level]
    return value","for level in key.split('.'):
    if not isinstance(value, dict):
        return default
    if level not in value:
        return default
    else:
        value = value[level]","for i, level in enumerate(key.split('.')):
    if not isinstance(value, dict):
        return default
    if level not in value:
        return default
    else:
        value = value[level]",1
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/os/operation_siren.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/os/operation_siren.py,OperationSiren,"def _os_explore(self):
    """"""
        Explore all dangerous zones at the beginning of month.
        """"""

    def end():
        logger.info('OS explore finished, delay to next reset')
        next_reset = get_os_next_reset()
        logger.attr('OpsiNextReset', next_reset)
        logger.info('To run again, clear OpsiExplore.Scheduler.NextRun and set OpsiExplore.OpsiExplore.LastZone=0')
        with self.config.multi_set():
            self.config.OpsiExplore_LastZone = 0
            self.config.task_delay(target=next_reset)
            self.config.task_call('OpsiDaily', force_call=False)
            self.config.task_call('OpsiShop', force_call=False)
        self.config.task_stop()
    logger.hr('OS explore', level=1)
    order = [int(f.strip(' \t\r\n')) for f in self.config.OS_EXPLORE_FILTER.split('>')]
    try:
        last_zone = self.name_to_zone(self.config.OpsiExplore_LastZone).zone_id
    except ScriptError:
        logger.warning(f'Invalid OpsiExplore_LastZone={self.config.OpsiExplore_LastZone}, re-explore')
        last_zone = 0
    if last_zone in order:
        order = order[order.index(last_zone) + 1:]
        logger.info(f'Last zone: {self.name_to_zone(last_zone)}, next zone: {order[:1]}')
    elif last_zone == 0:
        logger.info(f'First run, next zone: {order[:1]}')
    else:
        raise ScriptError(f'Invalid last_zone: {last_zone}')
    if not len(order):
        end()
    for zone in order:
        if not self.globe_goto(zone, stop_if_safe=True):
            logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
            self.config.OpsiExplore_LastZone = zone
            continue
        logger.hr(f'OS explore {zone}', level=1)
        if not self.config.OpsiExplore_SpecialRadar:
            self.tuning_sample_use()
        self.fleet_set(self.config.OpsiFleet_Fleet)
        self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
        self._os_explore_task_delay()
        self.run_auto_search()
        self.config.OpsiExplore_LastZone = zone
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.handle_after_auto_search()
        self.config.check_task_switch()
        if zone == order[-1]:
            end()","for zone in order:
    if not self.globe_goto(zone, stop_if_safe=True):
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.config.OpsiExplore_LastZone = zone
        continue
    logger.hr(f'OS explore {zone}', level=1)
    if not self.config.OpsiExplore_SpecialRadar:
        self.tuning_sample_use()
    self.fleet_set(self.config.OpsiFleet_Fleet)
    self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
    self._os_explore_task_delay()
    self.run_auto_search()
    self.config.OpsiExplore_LastZone = zone
    logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
    self.handle_after_auto_search()
    self.config.check_task_switch()
    if zone == order[-1]:
        end()","for i, zone in enumerate(order):
    if not self.globe_goto(zone, stop_if_safe=True):
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.config.OpsiExplore_LastZone = zone
        continue
    logger.hr(f'OS explore {zone}', level=1)
    if not self.config.OpsiExplore_SpecialRadar:
        self.tuning_sample_use()
    self.fleet_set(self.config.OpsiFleet_Fleet)
    self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
    self._os_explore_task_delay()
    self.run_auto_search()
    self.config.OpsiExplore_LastZone = zone
    logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
    self.handle_after_auto_search()
    self.config.check_task_switch()
    if i == len(order)-1:
        end()",1
swift,https://github.com/openstack/swift/tree/master/swift/obj/diskfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/obj/diskfile.py,ECDiskFile,"def validate_metadata(self):
    required_metadata = ['Content-Length', 'X-Object-Sysmeta-Ec-Frag-Index', 'X-Object-Sysmeta-Ec-Etag']
    for header in required_metadata:
        if not self._datafile_metadata.get(header):
            return False
    return True","for header in required_metadata:
    if not self._datafile_metadata.get(header):
        return False","for i, header in enumerate(required_metadata):
    if not self._datafile_metadata.get(header):
        return False",1
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for el in edge_lists:
    for e in el:
        self.delete(e)","for i, el in enumerate(edge_lists):
    for e in el:
        self.delete(e)",1
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for e in el:
    self.delete(e)","for i,e in enumerate(el):
    self.delete(e)",1
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ctci-solutions/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,,"def show(placement):
    parts = ['\n+-----------------+\n']
    for row in xrange(8):
        parts.append('| ')
        for col in xrange(8):
            bit = 1 << row * 8 + col
            if bit & placement:
                parts.append('Q ')
            else:
                parts.append('  ')
        parts.append('|\n')
    parts.append('+-----------------+\n')
    return ''.join(parts)","for row in xrange(8):
    parts.append('| ')
    for col in xrange(8):
        bit = 1 << row * 8 + col
        if bit & placement:
            parts.append('Q ')
        else:
            parts.append('  ')
    parts.append('|\n')","for row in xrange(8):
    parts.append('| ')
    for col in xrange(8):
        bit = 1 << row * 8 + col
        if bit & placement:
            parts.append('Q ')
        else:
            parts.append('  ')
    parts.append('|\n')",1
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ctci-solutions/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,,"def show(placement):
    parts = ['\n+-----------------+\n']
    for row in xrange(8):
        parts.append('| ')
        for col in xrange(8):
            bit = 1 << row * 8 + col
            if bit & placement:
                parts.append('Q ')
            else:
                parts.append('  ')
        parts.append('|\n')
    parts.append('+-----------------+\n')
    return ''.join(parts)","for col in xrange(8):
    bit = 1 << row * 8 + col
    if bit & placement:
        parts.append('Q ')
    else:
        parts.append('  ')","for (col, _) in enumerate(xrange(8)):
    bit = 1 << row * 8 + col
    if bit & placement:
        parts.append('Q ')
    else:
        parts.append('  ')",1
DeepPavlov,https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/core/common/params_search.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPavlov/deeppavlov/core/common/params_search.py,ParamsSearch,"def remove_key_from_config(config: dict, path: list) -> Tuple[dict, Any]:
    """"""
        Remove config element determined by path

        Args:
            config: dictionary
            path: list of keys and/or integers (for list)

        Returns:
            dictionary without value from path, value from path
        """"""
    config_copy = deepcopy(config)
    config_pointer = config_copy
    for el in path[:-1]:
        if isinstance(config_pointer, dict):
            config_pointer = config_pointer.setdefault(el, {})
        elif isinstance(config_pointer, list):
            config_pointer = config_pointer[el]
        else:
            pass
    value = config_pointer.pop(path[-1])
    return (config_copy, value)","for el in path[:-1]:
    if isinstance(config_pointer, dict):
        config_pointer = config_pointer.setdefault(el, {})
    elif isinstance(config_pointer, list):
        config_pointer = config_pointer[el]
    else:
        pass","for i, el in enumerate(path[:-1]):
    if isinstance(config_pointer, dict):
        config_pointer = config_pointer.setdefault(el, {})
    elif isinstance(config_pointer, list):
        config_pointer = config_pointer[el]
    else:
        pass",1
rasa,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/core/domain.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/rasa/shared/core/domain.py,Domain,"def _add_categorical_slot_default_value(self) -> None:
    """"""Add a default value to all categorical slots.

        All unseen values found for the slot will be mapped to this default value
        for featurization.
        """"""
    for slot in [s for s in self.slots if isinstance(s, CategoricalSlot)]:
        slot.add_default_value()","for slot in [s for s in self.slots if isinstance(s, CategoricalSlot)]:
    slot.add_default_value()","for i, slot in enumerate([s for s in self.slots if isinstance(s, CategoricalSlot)]):
    slot.add_default_value()",1
pyrsistent,https://github.com/tobgu/pyrsistent/tree/master/pyrsistent/_pbag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyrsistent/pyrsistent/_pbag.py,PBag,"def __iter__(self):
    """"""
        Return an iterator of all elements, including duplicates.

        >>> list(pbag([1, 1, 2]))
        [1, 1, 2]
        >>> list(pbag([1, 2]))
        [1, 2]
        """"""
    for (elt, count) in self._counts.iteritems():
        for i in range(count):
            yield elt","for (elt, count) in self._counts.iteritems():
    for i in range(count):
        yield elt","for (i, (elt, count)) in enumerate(self._counts.iteritems()):
    for j in range(count):
        yield elt",1
kube-janitor,https://github.com/hjacobs/kube-janitor/tree/master/kube_janitor/resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kube-janitor/kube_janitor/resources.py,,"def get_namespaced_resource_types(api):
    for (api_version, resource) in discover_namespaced_api_resources(api):
        clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
        yield clazz","for (api_version, resource) in discover_namespaced_api_resources(api):
    clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
    yield clazz","for i, (api_version, resource) in enumerate(discover_namespaced_api_resources(api)):
    clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
    yield clazz",1
SDV,https://github.com/sdv-dev/SDV/tree/master//tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV//tasks.py,,"def install_minimum(c):
    with open('setup.py', 'r') as setup_py:
        lines = setup_py.read().splitlines()
    versions = []
    started = False
    for line in lines:
        if started:
            if line == ']':
                started = False
                continue
            line = line.strip()
            if _validate_python_version(line):
                requirement = re.match('[^>]*', line).group(0)
                requirement = re.sub('[\'"",]', '', requirement)
                version = re.search('>=?[^(,|#)]*', line).group(0)
                if version:
                    version = re.sub('>=?', '==', version)
                    version = re.sub('[\'"",]', '', version)
                    requirement += version
                versions.append(requirement)
        elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
            started = True
    c.run(f""python -m pip install {' '.join(versions)}"")","for line in lines:
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True","for i, line in enumerate(lines):
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True",1
enumerate-iam,https://github.com/andresriancho/enumerate-iam/tree/master/enumerate_iam/generate_bruteforce_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enumerate-iam/enumerate_iam/generate_bruteforce_tests.py,,"def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)","for filename in os.listdir(API_DEFINITIONS):
    if not filename.endswith('.min.json'):
        continue
    api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
    api_json = json.loads(api_json_data)
    service_name = extract_service_name(filename, api_json)
    if service_name is None:
        print('%s does not define a service name' % filename)
        continue
    operations = extract_operations(api_json)
    if not operations:
        continue
    if service_name in bruteforce_tests:
        bruteforce_tests[service_name].extend(operations)
    else:
        bruteforce_tests[service_name] = operations","for i, filename in enumerate(os.listdir(API_DEFINITIONS)):
    if not filename.endswith('.min.json'):
        continue
    api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
    api_json = json.loads(api_json_data)
    service_name = extract_service_name(filename, api_json)
    if service_name is None:
        print('%s does not define a service name' % filename)
        continue
    operations = extract_operations(api_json)
    if not operations:
        continue
    if service_name in bruteforce_tests:
        bruteforce_tests[service_name].extend(operations)
    else:
        bruteforce_tests[service_name] = operations",1
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)","for (img_path, annotation_path) in tqdm(filename_pairs):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)","for i, (img_path, annotation_path) in enumerate(tqdm(filename_pairs)):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)",1
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/test_client_regress/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v22/tests/test_client_regress/tests.py,AssertFormsetErrorTests,"def test_no_error_field(self):
    """"""An assertion is raised if the field doesn't have any errors""""""
    for (prefix, kwargs) in self.msg_prefixes:
        msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
        with self.assertRaisesMessage(AssertionError, msg):
            self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)","for (prefix, kwargs) in self.msg_prefixes:
    msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
    with self.assertRaisesMessage(AssertionError, msg):
        self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)","for i, (prefix, kwargs) in enumerate(self.msg_prefixes):
    msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
    with self.assertRaisesMessage(AssertionError, msg):
        self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)",1
transformers,https://github.com/huggingface/transformers/tree/master/examples/flax/question-answering/run_qa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/examples/flax/question-answering/run_qa.py,,"def eval_data_collator(dataset: Dataset, batch_size: int):
    """"""Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.""""""
    batch_idx = np.arange(len(dataset))
    steps_per_epoch = math.ceil(len(dataset) / batch_size)
    batch_idx = np.array_split(batch_idx, steps_per_epoch)
    for idx in batch_idx:
        batch = dataset[idx]
        batch = {k: np.array(v) for (k, v) in batch.items()}
        yield batch","for idx in batch_idx:
    batch = dataset[idx]
    batch = {k: np.array(v) for (k, v) in batch.items()}
    yield batch","for i, idx in enumerate(batch_idx):
    batch = dataset[idx]
    batch = {k: np.array(v) for (k, v) in batch.items()}
    yield batch",1
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for fn in possibleFiles:
    mass_store.append([False, fn[len(path):]])","for i, fn in enumerate(possibleFiles):
    mass_store.append([False, fn[len(path):]])",1
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for iso in ISO3166_LIST:
    if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
        widget.set_active(idx)
        break
    idx += 1","for (idx, iso) in enumerate(ISO3166_LIST):
    if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
        widget.set_active(idx)
        break",1
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","for iso in ISO3166_LIST:
    if iso.iso2 == country:
        self.widgets['engine_country_combo'].set_active(idx)
        break
    idx += 1","for idx, iso in enumerate(ISO3166_LIST):
    if iso.iso2 == country:
        self.widgets['engine_country_combo'].set_active(idx)
        break",1
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for l in range(max_depth):
    outdegree = self.outdegree(cur_nodes)
    mask = outdegree != 0
    if np.any(mask):
        cur_walk_ids = cur_walk_ids[mask]
        cur_nodes = cur_nodes[mask]
        outdegree = outdegree[mask]
    else:
        break
    succ = self.successor(cur_nodes)
    sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
    nxt_cur_nodes = []
    for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
        walk[walk_id].append(s[ind])
        nxt_cur_nodes.append(s[ind])
    cur_nodes = np.array(nxt_cur_nodes)","for l in range(max_depth):
    outdegree = self.outdegree(cur_nodes)
    mask = outdegree != 0
    if np.any(mask):
        cur_walk_ids = cur_walk_ids[mask]
        cur_nodes = cur_nodes[mask]
        outdegree = outdegree[mask]
    else:
        break
    succ = self.successor(cur_nodes)
    sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
    nxt_cur_nodes = []
    for (i, (s, ind, walk_id)) in enumerate(zip(succ, sample_index, cur_walk_ids)):
        walk[walk_id].append(s[ind])
        nxt_cur_nodes.append(s[ind])
    cur_nodes = np.array(nxt_cur_nodes)",1
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])","for (i, (s, ind, walk_id)) in enumerate(zip(succ, sample_index, cur_walk_ids)):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])",1
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/lstm.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/fairseq/models/lstm.py,LSTMDecoder,"def extract_features(self, prev_output_tokens, encoder_out: Optional[Tuple[Tensor, Tensor, Tensor, Tensor]]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):
    """"""
        Similar to *forward* but only return features.
        """"""
    if encoder_out is not None:
        encoder_outs = encoder_out[0]
        encoder_hiddens = encoder_out[1]
        encoder_cells = encoder_out[2]
        encoder_padding_mask = encoder_out[3]
    else:
        encoder_outs = torch.empty(0)
        encoder_hiddens = torch.empty(0)
        encoder_cells = torch.empty(0)
        encoder_padding_mask = torch.empty(0)
    srclen = encoder_outs.size(0)
    if incremental_state is not None and len(incremental_state) > 0:
        prev_output_tokens = prev_output_tokens[:, -1:]
    (bsz, seqlen) = prev_output_tokens.size()
    x = self.embed_tokens(prev_output_tokens)
    x = self.dropout_in_module(x)
    x = x.transpose(0, 1)
    if incremental_state is not None and len(incremental_state) > 0:
        (prev_hiddens, prev_cells, input_feed) = self.get_cached_state(incremental_state)
    elif encoder_out is not None:
        prev_hiddens = [encoder_hiddens[i] for i in range(self.num_layers)]
        prev_cells = [encoder_cells[i] for i in range(self.num_layers)]
        if self.encoder_hidden_proj is not None:
            prev_hiddens = [self.encoder_hidden_proj(y) for y in prev_hiddens]
            prev_cells = [self.encoder_cell_proj(y) for y in prev_cells]
        input_feed = x.new_zeros(bsz, self.hidden_size)
    else:
        zero_state = x.new_zeros(bsz, self.hidden_size)
        prev_hiddens = [zero_state for i in range(self.num_layers)]
        prev_cells = [zero_state for i in range(self.num_layers)]
        input_feed = None
    assert srclen > 0 or self.attention is None, 'attention is not supported if there are no encoder outputs'
    attn_scores: Optional[Tensor] = x.new_zeros(srclen, seqlen, bsz) if self.attention is not None else None
    outs = []
    for j in range(seqlen):
        if input_feed is not None:
            input = torch.cat((x[j, :, :], input_feed), dim=1)
        else:
            input = x[j]
        for (i, rnn) in enumerate(self.layers):
            (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
            input = self.dropout_out_module(hidden)
            if self.residuals:
                input = input + prev_hiddens[i]
            prev_hiddens[i] = hidden
            prev_cells[i] = cell
        if self.attention is not None:
            assert attn_scores is not None
            (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
        else:
            out = hidden
        out = self.dropout_out_module(out)
        if input_feed is not None:
            input_feed = out
        outs.append(out)
    prev_hiddens_tensor = torch.stack(prev_hiddens)
    prev_cells_tensor = torch.stack(prev_cells)
    cache_state = torch.jit.annotate(Dict[str, Optional[Tensor]], {'prev_hiddens': prev_hiddens_tensor, 'prev_cells': prev_cells_tensor, 'input_feed': input_feed})
    self.set_incremental_state(incremental_state, 'cached_state', cache_state)
    x = torch.cat(outs, dim=0).view(seqlen, bsz, self.hidden_size)
    x = x.transpose(1, 0)
    if hasattr(self, 'additional_fc') and self.adaptive_softmax is None:
        x = self.additional_fc(x)
        x = self.dropout_out_module(x)
    if not self.training and self.need_attn and (self.attention is not None):
        assert attn_scores is not None
        attn_scores = attn_scores.transpose(0, 2)
    else:
        attn_scores = None
    return (x, attn_scores)","for j in range(seqlen):
    if input_feed is not None:
        input = torch.cat((x[j, :, :], input_feed), dim=1)
    else:
        input = x[j]
    for (i, rnn) in enumerate(self.layers):
        (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
        input = self.dropout_out_module(hidden)
        if self.residuals:
            input = input + prev_hiddens[i]
        prev_hiddens[i] = hidden
        prev_cells[i] = cell
    if self.attention is not None:
        assert attn_scores is not None
        (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
    else:
        out = hidden
    out = self.dropout_out_module(out)
    if input_feed is not None:
        input_feed = out
    outs.append(out)","for (j, _) in enumerate(range(seqlen)):
    if input_feed is not None:
        input = torch.cat((x[j, :, :], input_feed), dim=1)
    else:
        input = x[j]
    for (i, rnn) in enumerate(self.layers):
        (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
        input = self.dropout_out_module(hidden)
        if self.residuals:
            input = input + prev_hiddens[i]
        prev_hiddens[i] = hidden
        prev_cells[i] = cell
    if self.attention is not None:
        assert attn_scores is not None
        (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
    else:
        out = hidden
    out = self.dropout_out_module(out)
    if input_feed is not None:
        input_feed = out
    outs.append(out)",1
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/modeling/data_handler/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/haystack/haystack/modeling/data_handler/processor.py,TextClassificationProcessor,"def convert_labels(self, dictionary: Dict):
    ret: Dict = {}
    for (task_name, task) in self.tasks.items():
        label_name = task['label_name']
        label_raw = dictionary[label_name]
        label_list = task['label_list']
        if task['task_type'] == 'classification':
            label_ids = [label_list.index(label_raw)]
        elif task['task_type'] == 'multilabel_classification':
            label_ids = [0] * len(label_list)
            for l in label_raw.split(','):
                if l != '':
                    label_ids[label_list.index(l)] = 1
        ret[task['label_tensor_name']] = label_ids
    return ret","for (task_name, task) in self.tasks.items():
    label_name = task['label_name']
    label_raw = dictionary[label_name]
    label_list = task['label_list']
    if task['task_type'] == 'classification':
        label_ids = [label_list.index(label_raw)]
    elif task['task_type'] == 'multilabel_classification':
        label_ids = [0] * len(label_list)
        for l in label_raw.split(','):
            if l != '':
                label_ids[label_list.index(l)] = 1
    ret[task['label_tensor_name']] = label_ids","for (i, (task_name, task)) in enumerate(self.tasks.items()):
    label_name = task['label_name']
    label_raw = dictionary[label_name]
    label_list = task['label_list']
    if task['task_type'] == 'classification':
        label_ids = [label_list.index(label_raw)]
    elif task['task_type'] == 'multilabel_classification':
        label_ids = [0] * len(label_list)
        for l in label_raw.split(','):
            if l != '':
                label_ids[label_list.index(l)] = 1
    ret[task['label_tensor_name']] = label_ids",1
pyray,https://github.com/ryu577/pyray/tree/master/pyray/shapes/twod/plot.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyray/pyray/shapes/twod/plot.py,Canvas,"def draw_grid_s(draw, r=np.eye(2), scale=64, origin=np.array([8, 8]), im_size=np.array([1024, 1024])):
    lo_range = -scale
    hi_range = im_size[0] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
        pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)
    hi_range = im_size[1] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
        pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for i in np.arange(lo_range, hi_range, scale):
    pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
    pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for (idx, i) in enumerate(np.arange(lo_range, hi_range, scale)):
    pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
    pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)",1
pyray,https://github.com/ryu577/pyray/tree/master/pyray/shapes/twod/plot.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyray/pyray/shapes/twod/plot.py,Canvas,"def draw_grid_s(draw, r=np.eye(2), scale=64, origin=np.array([8, 8]), im_size=np.array([1024, 1024])):
    lo_range = -scale
    hi_range = im_size[0] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
        pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)
    hi_range = im_size[1] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
        pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for i in np.arange(lo_range, hi_range, scale):
    pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
    pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for (j, i) in enumerate(np.arange(lo_range, hi_range, scale)):
    pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
    pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)",1
Tuxemon,https://github.com/Tuxemon/Tuxemon/tree/master/tuxemon/cli/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tuxemon/tuxemon/cli/processor.py,CommandProcessor,"def collect_commands(self, folder: str) -> Iterable[CLICommand]:
    """"""
        Use plugins to load CLICommand classes for commands.

        Parameters:
            folder: Folder to search.

        """"""
    pm = PluginManager()
    pm.setPluginPlaces([folder])
    pm.include_patterns = ['commands']
    pm.exclude_classes = ['CLICommand']
    pm.collectPlugins()
    for cmd_class in get_available_classes(pm, interface=CLICommand):
        if cmd_class.usable_from_root:
            yield cmd_class()","for cmd_class in get_available_classes(pm, interface=CLICommand):
    if cmd_class.usable_from_root:
        yield cmd_class()","for i, cmd_class in enumerate(get_available_classes(pm, interface=CLICommand)):
    if cmd_class.usable_from_root:
        yield cmd_class()",1
swift,https://github.com/openstack/swift/tree/master/test/unit/common/middleware/s3api/test_service.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/common/middleware/s3api/test_service.py,TestS3ApiService,"def test_service_GET_with_blind_resource(self):
    buckets = (('apple', 1, 200), ('orange', 3, 430), ('apple+segment', 1, 200))
    expected = buckets[:-1]
    bucket_list = create_bucket_list_json(buckets)
    self.swift.register('GET', '/v1/AUTH_test', swob.HTTPOk, {}, bucket_list)
    req = Request.blank('/', environ={'REQUEST_METHOD': 'GET'}, headers={'Authorization': 'AWS test:tester:hmac', 'Date': self.get_date_header()})
    (status, headers, body) = self.call_s3api(req)
    self.assertEqual(status.split()[0], '200')
    elem = fromstring(body, 'ListAllMyBucketsResult')
    all_buckets = elem.find('./Buckets')
    buckets = all_buckets.iterchildren('Bucket')
    listing = list(list(buckets)[0])
    self.assertEqual(len(listing), 2)
    names = []
    for b in all_buckets.iterchildren('Bucket'):
        names.append(b.find('./Name').text)
    self.assertEqual(len(names), len(expected))
    for i in expected:
        self.assertIn(i[0], names)","for i in expected:
    self.assertIn(i[0], names)","for i, val in enumerate(expected):
    self.assertIn(val[0], names)",1
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/kfserving/transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kale/backend/kale/kfserving/transformer.py,KaleTransformer,"def _load_transformer_assets(self):
    marshal.set_data_dir(serveutils.TRANSFORMER_ASSETS_DIR)
    log.info('Loading transformer function...')
    _fn = marshal.load(serveutils.TRANSFORMER_FN_ASSET_NAME)
    self.fn = types.FunctionType(_fn.__code__, globals(), _fn.__name__, _fn.__defaults__, _fn.__closure__)
    log.info('Processing source notebook for imports and functions...')
    processor = NotebookProcessor(nb_path=os.path.join(serveutils.TRANSFORMER_ASSETS_DIR, serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME), skip_validation=True)
    self.init_code = processor.get_imports_and_functions()
    log.info('Initialization code:\n%s' % self.init_code)
    log.info('Running initialization code...')
    exec(self.init_code, globals())
    log.info(""Loading transformer's assets..."")
    for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
        if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
            continue
        basename = os.path.splitext(file)[0]
        self.assets[basename] = marshal.load(basename)
    log.info('Assets successfully loaded: %s' % self.assets.keys())
    log.info('Initializing assets...')
    for (asset_name, asset_value) in self.assets.items():
        globals()[asset_name] = asset_value","for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
    if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
        continue
    basename = os.path.splitext(file)[0]
    self.assets[basename] = marshal.load(basename)","for i, file in enumerate(os.listdir(serveutils.TRANSFORMER_ASSETS_DIR)):
    if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
        continue
    basename = os.path.splitext(file)[0]
    self.assets[basename] = marshal.load(basename)",1
sunpy,https://github.com/sunpy/sunpy/tree/master/examples/time_series/timeseries_peak_finding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/examples/time_series/timeseries_peak_finding.py,,"def findpeaks(series, DELTA):
    """"""
    Finds extrema in a pandas series data.

    Parameters
    ----------
    series : `pandas.Series`
        The data series from which we need to find extrema.

    DELTA : `float`
        The minimum difference between data values that defines a peak.

    Returns
    -------
    minpeaks, maxpeaks : `list`
        Lists consisting of pos, val pairs for both local minima points and
        local maxima points.
    """"""
    (mn, mx) = (np.Inf, -np.Inf)
    minpeaks = []
    maxpeaks = []
    lookformax = True
    start = True
    for (time_pos, value) in series.iteritems():
        if value > mx:
            mx = value
            mxpos = time_pos
        if value < mn:
            mn = value
            mnpos = time_pos
        if lookformax:
            if value < mx - DELTA:
                maxpeaks.append((mxpos, mx))
                mn = value
                mnpos = time_pos
                lookformax = False
            elif start:
                minpeaks.append((mnpos, mn))
                mx = value
                mxpos = time_pos
                start = False
        elif value > mn + DELTA:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            lookformax = True
    if value > mn + DELTA:
        maxpeaks.append((mxpos, mx))
    elif value < mx - DELTA:
        minpeaks.append((mnpos, mn))
    return (minpeaks, maxpeaks)","for (time_pos, value) in series.iteritems():
    if value > mx:
        mx = value
        mxpos = time_pos
    if value < mn:
        mn = value
        mnpos = time_pos
    if lookformax:
        if value < mx - DELTA:
            maxpeaks.append((mxpos, mx))
            mn = value
            mnpos = time_pos
            lookformax = False
        elif start:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            start = False
    elif value > mn + DELTA:
        minpeaks.append((mnpos, mn))
        mx = value
        mxpos = time_pos
        lookformax = True","for i, (time_pos, value) in enumerate(series.iteritems()):
    if value > mx:
        mx = value
        mxpos = time_pos
    if value < mn:
        mn = value
        mnpos = time_pos
    if lookformax:
        if value < mx - DELTA:
            maxpeaks.append((mxpos, mx))
            mn = value
            mnpos = time_pos
            lookformax = False
        elif start:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            start = False
    elif value > mn + DELTA:
        minpeaks.append((mnpos, mn))
        mx = value
        mxpos = time_pos
        lookformax = True",1
glance,https://github.com/openstack/glance/tree/master/glance/db/simple/api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/db/simple/api.py,,"def metadef_tag_update(context, namespace_name, id, values):
    """"""Update a metadef tag""""""
    global DATA
    namespace = metadef_namespace_get(context, namespace_name)
    _check_namespace_visibility(context, namespace, namespace_name)
    tag = metadef_tag_get_by_id(context, namespace_name, id)
    if tag['name'] != values['name']:
        for db_tag in DATA['metadef_tags']:
            if db_tag['name'] == values['name'] and db_tag['namespace_id'] == namespace['id']:
                LOG.debug('Invalid update. It would result in a duplicate metadata definition tag with same name=%(name)s  in namespace=%(namespace_name)s.', {'name': tag['name'], 'namespace_name': namespace_name})
                raise exception.MetadefDuplicateTag(name=tag['name'], namespace_name=namespace_name)
    DATA['metadef_tags'].remove(tag)
    tag.update(values)
    tag['updated_at'] = timeutils.utcnow()
    DATA['metadef_tags'].append(tag)
    return tag","for db_tag in DATA['metadef_tags']:
    if db_tag['name'] == values['name'] and db_tag['namespace_id'] == namespace['id']:
        LOG.debug('Invalid update. It would result in a duplicate metadata definition tag with same name=%(name)s  in namespace=%(namespace_name)s.', {'name': tag['name'], 'namespace_name': namespace_name})
        raise exception.MetadefDuplicateTag(name=tag['name'], namespace_name=namespace_name)","for i, db_tag in enumerate(DATA['metadef_tags']):
    if db_tag['name'] == values['name'] and db_tag['namespace_id'] == namespace['id']:
        LOG.debug('Invalid update. It would result in a duplicate metadata definition tag with same name=%(name)s  in namespace=%(namespace_name)s.', {'name': tag['name'], 'namespace_name': namespace_name})
        raise exception.MetadefDuplicateTag(name=tag['name'], namespace_name=namespace_name)",1
cubes,https://github.com/DataBrewery/cubes/tree/master/cubes/query/cells.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cubes/cubes/query/cells.py,,"def cuts_from_string(cube, string, member_converters=None, role_member_converters=None):
    """"""Return list of cuts specified in `string`. You can use this function to
    parse cuts encoded in a URL.

    Arguments:

    * `string`  string containing the cut descritption (see below)
    * `cube`  cube for which the cuts are being created
    * `member_converters`  callables converting single-item values into paths.
      Keys are dimension names.
    * `role_member_converters`  callables converting single-item values into
      paths. Keys are dimension role names (`Dimension.role`).

    Examples::

        date:2004
        date:2004,1
        date:2004,1|class=5
        date:2004,1,1|category:5,10,12|class:5

    Ranges are in form ``from-to`` with possibility of open range::

        date:2004-2010
        date:2004,5-2010,3
        date:2004,5-2010
        date:2004,5-
        date:-2010

    Sets are in form ``path1;path2;path3`` (none of the paths should be
    empty)::

        date:2004;2010
        date:2004;2005,1;2010,10

    Grammar::

        <list> ::= <cut> | <cut> '|' <list>
        <cut> ::= <dimension> ':' <path>
        <dimension> ::= <identifier>
        <path> ::= <value> | <value> ',' <path>

    The characters '|', ':' and ',' are configured in `CUT_STRING_SEPARATOR`,
    `DIMENSION_STRING_SEPARATOR`, `PATH_STRING_SEPARATOR` respectively.
    """"""
    if not string:
        return []
    cuts = []
    dim_cuts = CUT_STRING_SEPARATOR.split(string)
    for dim_cut in dim_cuts:
        cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
        cuts.append(cut)
    return cuts","for dim_cut in dim_cuts:
    cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
    cuts.append(cut)","for i, dim_cut in enumerate(dim_cuts):
    cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
    cuts.append(cut)",1
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/examples/pointnet2_segmentation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_geometric/examples/pointnet2_segmentation.py,,"def test(loader):
    model.eval()
    y_mask = loader.dataset.y_mask
    ious = [[] for _ in range(len(loader.dataset.categories))]
    for data in loader:
        data = data.to(device)
        pred = model(data).argmax(dim=1)
        (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
        iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
        iou[torch.isnan(iou)] = 1
        for (iou, category) in zip(iou.unbind(), data.category.unbind()):
            ious[category.item()].append(iou[y_mask[category]])
    ious = [torch.stack(iou).mean(0).mean(0) for iou in ious]
    return torch.tensor(ious).mean().item()","for data in loader:
    data = data.to(device)
    pred = model(data).argmax(dim=1)
    (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
    iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
    iou[torch.isnan(iou)] = 1
    for (iou, category) in zip(iou.unbind(), data.category.unbind()):
        ious[category.item()].append(iou[y_mask[category]])","for (i, data) in enumerate(loader):
    data = data.to(device)
    pred = model(data).argmax(dim=1)
    (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
    iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
    iou[torch.isnan(iou)] = 1
    for (iou, category) in zip(iou.unbind(), data.category.unbind()):
        ious[category.item()].append(iou[y_mask[category]])",1
numpy,https://github.com/numpy/numpy/tree/master/tools/refguide_check.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/tools/refguide_check.py,,"def check_documentation(base_path, results, args, dots):
    """"""
    Check examples in any *.rst located inside `base_path`.
    Add the output to `results`.

    See Also
    --------
    check_doctests_testfile
    """"""
    for filename in iter_included_files(base_path, args.verbose):
        if dots:
            sys.stderr.write(filename + ' ')
            sys.stderr.flush()
        tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

        def scratch():
            pass
        scratch.__name__ = filename
        results.append((scratch, tut_results))
        if dots:
            sys.stderr.write('\n')
            sys.stderr.flush()","for filename in iter_included_files(base_path, args.verbose):
    if dots:
        sys.stderr.write(filename + ' ')
        sys.stderr.flush()
    tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

    def scratch():
        pass
    scratch.__name__ = filename
    results.append((scratch, tut_results))
    if dots:
        sys.stderr.write('\n')
        sys.stderr.flush()","for i, filename in enumerate(iter_included_files(base_path, args.verbose)):
    if dots:
        sys.stderr.write(filename + ' ')
        sys.stderr.flush()
    tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

    def scratch():
        pass
    scratch.__name__ = filename
    results.append((scratch, tut_results))
    if dots:
        sys.stderr.write('\n')
        sys.stderr.flush()",1
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for glyphName in glyphNames:
    try:
        gid = ttFont.getGlyphID(glyphName)
    except:
        assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
    vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])","for i, glyphName in enumerate(glyphNames):
    try:
        gid = ttFont.getGlyphID(glyphName)
    except:
        assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
    vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])",1
speechbrain,https://github.com/speechbrain/speechbrain/tree/master/recipes/KsponSpeech/ASR/transformer/train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/speechbrain/recipes/KsponSpeech/ASR/transformer/train.py,,"if __name__ == '__main__':
    (hparams_file, run_opts, overrides) = sb.parse_arguments(sys.argv[1:])
    with open(hparams_file) as fin:
        hparams = load_hyperpyyaml(fin, overrides)
    sb.utils.distributed.ddp_init_group(run_opts)
    from ksponspeech_prepare import prepare_ksponspeech
    sb.create_experiment_directory(experiment_directory=hparams['output_folder'], hyperparams_to_save=hparams_file, overrides=overrides)
    run_on_main(prepare_ksponspeech, kwargs={'data_folder': hparams['data_folder'], 'tr_splits': hparams['train_splits'], 'dev_splits': hparams['dev_splits'], 'te_splits': hparams['test_splits'], 'save_folder': hparams['data_folder'], 'merge_lst': hparams['train_splits'], 'merge_name': hparams['train_csv'], 'skip_prep': hparams['skip_prep']})
    (train_data, valid_data, test_datasets, tokenizer) = dataio_prepare(hparams)
    run_on_main(hparams['pretrainer'].collect_files)
    hparams['pretrainer'].load_collected(device=run_opts['device'])
    asr_brain = ASR(modules=hparams['modules'], opt_class=hparams['Adam'], hparams=hparams, run_opts=run_opts, checkpointer=hparams['checkpointer'])
    asr_brain.tokenizer = hparams['tokenizer']
    asr_brain.fit(asr_brain.hparams.epoch_counter, train_data, valid_data, train_loader_kwargs=hparams['train_dataloader_opts'], valid_loader_kwargs=hparams['valid_dataloader_opts'])
    for k in test_datasets.keys():
        asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
        asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])","for k in test_datasets.keys():
    asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
    asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])","for i, k in enumerate(test_datasets.keys()):
    asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
    asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])",1
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for item in items:
    if not isinstance(item, dict):
        continue
    year = int_or_none(item.get('year'))
    if year is None:
        continue
    months = item.get('months')
    if not isinstance(months, list):
        continue
    for month_dict in months:
        if not isinstance(month_dict, dict) or not month_dict:
            continue
        month_number = int_or_none(list(month_dict.keys())[0])
        if month_number is None:
            continue
        entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))","for i, item in enumerate(items):
    if not isinstance(item, dict):
        continue
    year = int_or_none(item.get('year'))
    if year is None:
        continue
    months = item.get('months')
    if not isinstance(months, list):
        continue
    for j, month_dict in enumerate(months):
        if not isinstance(month_dict, dict) or not month_dict:
            continue
        month_number = int_or_none(list(month_dict.keys())[0])
        if month_number is None:
            continue
        entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))",1
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,,"def make_accrual_interest_entry_for_term_loans(posting_date, process_loan_interest, term_loan=None, loan_type=None, accrual_type='Regular'):
    curr_date = posting_date or add_days(nowdate(), 1)
    term_loans = get_term_loans(curr_date, term_loan, loan_type)
    accrued_entries = []
    for loan in term_loans:
        accrued_entries.append(loan.payment_entry)
        args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
        make_loan_interest_accrual_entry(args)
    if accrued_entries:
        frappe.db.sql('UPDATE `tabRepayment Schedule`\n\t\t\tSET is_accrued = 1 where name in (%s)' % ', '.join(['%s'] * len(accrued_entries)), tuple(accrued_entries))","for loan in term_loans:
    accrued_entries.append(loan.payment_entry)
    args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
    make_loan_interest_accrual_entry(args)","for i, loan in enumerate(term_loans):
    accrued_entries.append(loan.payment_entry)
    args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
    make_loan_interest_accrual_entry(args)",1
openpilot,https://github.com/commaai/openpilot/tree/master/tools/sim/lib/manual_ctrl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/sim/lib/manual_ctrl.py,,"def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')","for _axis in buf[:num_axes]:
    axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
    axis_map.append(axis_name)
    axis_states[axis_name] = 0.0","for i, _axis in enumerate(buf[:num_axes]):
    axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
    axis_map.append(axis_name)
    axis_states[axis_name] = 0.0",1
openpilot,https://github.com/commaai/openpilot/tree/master/tools/sim/lib/manual_ctrl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/sim/lib/manual_ctrl.py,,"def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')","for btn in buf[:num_buttons]:
    btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
    button_map.append(btn_name)
    button_states[btn_name] = 0","for i, btn in enumerate(buf[:num_buttons]):
    btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
    button_map.append(btn_name)
    button_states[btn_name] = 0",1
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/transpiler/passes/basis/basis_translator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/transpiler/passes/basis/basis_translator.py,BasisTranslator,"def _extract_basis_target(self, dag, qarg_indices, source_basis=None, qargs_local_source_basis=None):
    if source_basis is None:
        source_basis = set()
    if qargs_local_source_basis is None:
        qargs_local_source_basis = defaultdict(set)
    for node in dag.op_nodes():
        qargs = tuple((qarg_indices[bit] for bit in node.qargs))
        if dag.has_calibration_for(node):
            continue
        if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
            qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
        else:
            source_basis.add((node.name, node.op.num_qubits))
        if isinstance(node.op, ControlFlowOp):
            for block in node.op.blocks:
                block_dag = circuit_to_dag(block)
                (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)
    return (source_basis, qargs_local_source_basis)","for node in dag.op_nodes():
    qargs = tuple((qarg_indices[bit] for bit in node.qargs))
    if dag.has_calibration_for(node):
        continue
    if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
        qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
    else:
        source_basis.add((node.name, node.op.num_qubits))
    if isinstance(node.op, ControlFlowOp):
        for block in node.op.blocks:
            block_dag = circuit_to_dag(block)
            (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)","for i, node in enumerate(dag.op_nodes()):
    qargs = tuple((qarg_indices[bit] for bit in node.qargs))
    if dag.has_calibration_for(node):
        continue
    if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
        qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
    else:
        source_basis.add((node.name, node.op.num_qubits))
    if isinstance(node.op, ControlFlowOp):
        for block in node.op.blocks:
            block_dag = circuit_to_dag(block)
            (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)",1
RigNet,https://github.com/zhan-xu/RigNet/tree/master/utils/rig_parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RigNet/utils/rig_parser.py,Skel,"def save(self, filename):
    fout = open(filename, 'w')
    this_level = [self.root]
    hier_level = 1
    while this_level:
        next_level = []
        for p_node in this_level:
            pos = p_node.pos
            parent = p_node.parent.name if p_node.parent is not None else 'None'
            line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
            fout.write(line)
            for c_node in p_node.children:
                next_level.append(c_node)
        this_level = next_level
        hier_level += 1
    fout.close()","for p_node in this_level:
    pos = p_node.pos
    parent = p_node.parent.name if p_node.parent is not None else 'None'
    line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
    fout.write(line)
    for c_node in p_node.children:
        next_level.append(c_node)","for i, p_node in enumerate(this_level):
    pos = p_node.pos
    parent = p_node.parent.name if p_node.parent is not None else 'None'
    line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
    fout.write(line)
    for c_node in p_node.children:
        next_level.append(c_node)",1
espresso,https://github.com/freewym/espresso/tree/master/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,KLDivergenceRerankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    sample_size = sample['id'].numel()
    assert sample_size % self.task.cfg.mt_beam == 0, f'sample_size ({sample_size}) cannot be divided by beam size ({self.task.cfg.mt_beam}).Please set --required-batch-size-multiple={self.task.cfg.mt_beam}.'
    batch_out = []
    for i in range(0, sample_size, self.forward_batch_size):
        j = min(i + self.forward_batch_size, sample_size)
        out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
        batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))
    batch_out = torch.cat(batch_out, dim=0).view(self.task.cfg.mt_beam, sample_size // self.task.cfg.mt_beam, -1)
    if model.joint_classification == 'sent':
        batch_out = model.joint_forward(batch_out)
    scores = model.classification_forward(batch_out.view(sample_size, 1, -1)).view(-1, self.task.cfg.mt_beam)
    loss = self.compute_kl_loss(scores, sample['target'][:, 0].view(-1, self.task.cfg.mt_beam))
    sample_size = sample_size // self.task.cfg.mt_beam
    logging_output = {'loss': loss.detach(), 'ntokens': sample['ntokens'], 'nsentences': sample_size * self.task.cfg.mt_beam, 'sample_size': sample_size, 'scores': scores.detach()}
    return (loss, sample_size, logging_output)","for i in range(0, sample_size, self.forward_batch_size):
    j = min(i + self.forward_batch_size, sample_size)
    out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
    batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))","for i in range(0, sample_size, self.forward_batch_size):
    j = min(i + self.forward_batch_size, sample_size)
    out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
    batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))",1
FARM,https://github.com/deepset-ai/FARM/tree/master/farm/modeling/biadaptive_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FARM/farm/modeling/biadaptive_model.py,BaseBiAdaptiveModel,"def connect_heads_with_processor(self, tasks, require_labels=True):
    """"""
        Populates prediction head with information coming from tasks.

        :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)
        :param require_labels: If True, an error will be thrown when a task is not supplied with labels)
        :return:
        """"""
    for head in self.prediction_heads:
        head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
        label_list = tasks[head.task_name]['label_list']
        if not label_list and require_labels:
            raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
        label_list = tasks[head.task_name]['label_list']
        head.label_list = label_list
        num_labels = len(label_list)
        head.metric = tasks[head.task_name]['metric']","for head in self.prediction_heads:
    head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
    label_list = tasks[head.task_name]['label_list']
    if not label_list and require_labels:
        raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
    label_list = tasks[head.task_name]['label_list']
    head.label_list = label_list
    num_labels = len(label_list)
    head.metric = tasks[head.task_name]['metric']","for i, head in enumerate(self.prediction_heads):
    head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
    label_list = tasks[head.task_name]['label_list']
    if not label_list and require_labels:
        raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
    label_list = tasks[head.task_name]['label_list']
    head.label_list = label_list
    num_labels = len(label_list)
    head.metric = tasks[head.task_name]['metric']",1
BLINK,https://github.com/facebookresearch/BLINK/tree/master/blink/biencoder/zeshel_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BLINK/blink/biencoder/zeshel_utils.py,Stats,"def __init__(self, top_k=1000):
    self.cnt = 0
    self.hits = []
    self.top_k = top_k
    self.rank = [1, 4, 8, 16, 32, 64, 100, 128, 256, 512]
    self.LEN = len(self.rank)
    for i in range(self.LEN):
        self.hits.append(0)","for i in range(self.LEN):
    self.hits.append(0)","for i, _ in enumerate(range(self.LEN)):
    self.hits.append(0)",1
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)","for item in builder.widgets.items():
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)","for i, item in enumerate(builder.widgets.items()):
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)",1
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","for i in range(batch_size):
    width_begin = map_width * i
    width_end = map_width * (i + 1)
    image = dataset.reverse_transform(batch_images[i])
    grid_image[:, width_begin:width_end, :] = image
    if 'semantic' in target_keys:
        gt_sem = batch_targets['semantic'][i].cpu().numpy()
        gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
        grid_target[:map_height, width_begin:width_end, :] = gt_sem
    if 'center' in target_keys:
        gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
        gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        gt_ctr = gt_ctr.clip(0, 255)
        grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
    if 'offset' in target_keys:
        gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
        gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
        grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
    if 'semantic_weights' in target_keys:
        gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
    if 'center_weights' in target_keys:
        gt_ign = batch_targets['center_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
    if 'offset_weights' in target_keys:
        gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
    if 'foreground' in target_keys:
        gt_fg = batch_targets['foreground'][i].cpu().numpy()
        gt_fg = gt_fg[:, :, None] * 255
        grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
    if 'semantic' in output_keys:
        pred_sem = semantic_pred[i].cpu().numpy()
        pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
        grid_output[:map_height, width_begin:width_end, :] = pred_sem
    if 'center' in output_keys:
        pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
        pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        pred_ctr = pred_ctr.clip(0, 255)
        grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
    if 'offset' in output_keys:
        pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
        pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
        grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
    if 'foreground' in output_keys:
        if foreground_pred is not None:
            pred_fg = foreground_pred[i].cpu().numpy()
            pred_fg = pred_fg[:, :, None] * 255
            grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg","for i in range(batch_size):
    width_begin = map_width * i
    width_end = map_width * (i + 1)
    image = dataset.reverse_transform(batch_images[i])
    grid_image[:, width_begin:width_end, :] = image
    if 'semantic' in target_keys:
        gt_sem = batch_targets['semantic'][i].cpu().numpy()
        gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
        grid_target[:map_height, width_begin:width_end, :] = gt_sem
    if 'center' in target_keys:
        gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
        gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        gt_ctr = gt_ctr.clip(0, 255)
        grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
    if 'offset' in target_keys:
        gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
        gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
        grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
    if 'semantic_weights' in target_keys:
        gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
    if 'center_weights' in target_keys:
        gt_ign = batch_targets['center_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
    if 'offset_weights' in target_keys:
        gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
    if 'foreground' in target_keys:
        gt_fg = batch_targets['foreground'][i].cpu().numpy()
        gt_fg = gt_fg[:, :, None] * 255
        grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
    if 'semantic' in output_keys:
        pred_sem = semantic_pred[i].cpu().numpy()
        pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
        grid_output[:map_height, width_begin:width_end, :] = pred_sem
    if 'center' in output_keys:
        pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
        pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        pred_ctr = pred_ctr.clip(0, 255)
        grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
    if 'offset' in output_keys:
        pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
        pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
        grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
    if 'foreground' in output_keys:
        if foreground_pred is not None:
            pred_fg = foreground_pred[i].cpu().numpy()
            pred_fg = pred_fg[:, :, None] * 255
            grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg",1
tvm,https://github.com/apache/tvm/tree/master/vta/tests/python/unittest/test_vta_insn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/vta/tests/python/unittest/test_vta_insn.py,,"def _run(env, remote):

    def check_alu(tvm_op, np_op=None, use_imm=False, test_name=None):
        """"""Test ALU""""""
        m = 8
        n = 8
        imm = np.random.randint(1, 5)
        a = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='a', dtype=env.acc_dtype)
        a_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: a(*i), 'a_buf')
        if use_imm:
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), imm), 'res_buf')
        else:
            b = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='b', dtype=env.acc_dtype)
            b_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: b(*i), 'b_buf')
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), b_buf(*i)), 'res_buf')
        res = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: res_buf(*i).astype(env.inp_dtype), 'res')
        s = te.create_schedule(res.op)
        s[a_buf].set_scope(env.acc_scope)
        s[a_buf].pragma(a_buf.op.axis[0], env.dma_copy)
        s[res_buf].set_scope(env.acc_scope)
        s[res_buf].pragma(res_buf.op.axis[0], env.alu)
        s[res].pragma(res.op.axis[0], env.dma_copy)
        if not use_imm:
            s[b_buf].set_scope(env.acc_scope)
            s[b_buf].pragma(b_buf.op.axis[0], env.dma_copy)
        if not remote:
            return
        with vta.build_config():
            if use_imm:
                mod = vta.build(s, [a, res], tvm.target.Target('ext_dev', host=env.target_host))
            else:
                mod = vta.build(s, [a, b, res], tvm.target.Target('ext_dev', host=env.target_host))
        temp = utils.tempdir()
        mod.save(temp.relpath('load_act.o'))
        remote.upload(temp.relpath('load_act.o'))
        f = remote.load_module('load_act.o')
        dev = remote.ext_dev(0)
        a_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(a.dtype)
        if use_imm:
            res_np = np_op(a_np, imm) if np_op else tvm_op(a_np, imm)
        else:
            b_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(b.dtype)
            res_np = np_op(a_np, b_np) if np_op else tvm_op(a_np, b_np)
        res_np = res_np.astype(res.dtype)
        a_nd = tvm.nd.array(a_np, dev)
        res_nd = tvm.nd.array(np.zeros((m, n, env.BATCH, env.BLOCK_OUT)).astype(res.dtype), dev)
        if env.TARGET in ['sim', 'tsim']:
            simulator.clear_stats()
        if use_imm:
            f(a_nd, res_nd)
        else:
            b_nd = tvm.nd.array(b_np, dev)
            f(a_nd, b_nd, res_nd)
        np.testing.assert_equal(res_np, res_nd.numpy())
        if env.TARGET in ['sim', 'tsim']:
            sim_stats = simulator.stats()
            print('ALU {} execution statistics:'.format(test_name))
            for (k, v) in sim_stats.items():
                print('\t{:<16}: {:>16}'.format(k, v))
    check_alu(lambda x, y: x << y, np.left_shift, use_imm=True, test_name='SHL')
    check_alu(tvm.te.max, np.maximum, use_imm=True, test_name='MAX')
    check_alu(tvm.te.max, np.maximum, test_name='MAX')
    check_alu(lambda x, y: x + y, use_imm=True, test_name='ADD')
    check_alu(lambda x, y: x + y, test_name='ADD')
    check_alu(lambda x, y: x >> y, np.right_shift, use_imm=True, test_name='SHR')","for (k, v) in sim_stats.items():
    print('\t{:<16}: {:>16}'.format(k, v))","for i, (k, v) in enumerate(sim_stats.items()):
    print('\t{:<16}: {:>16}'.format(k, v))",1
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug","for (param, expection) in expections.items():
    url = reverse('milestones-list') + '?' + param
    response = client.get(url)
    number_of_milestones = len(response.data)
    assert response.status_code == 200
    assert number_of_milestones == expection, param
    if number_of_milestones > 0:
        assert response.data[0]['slug'] == milestone.slug","for i, (param, expection) in enumerate(expections.items()):
    url = reverse('milestones-list') + '?' + param
    response = client.get(url)
    number_of_milestones = len(response.data)
    assert response.status_code == 200
    assert number_of_milestones == expection, param
    if number_of_milestones > 0:
        assert response.data[0]['slug'] == milestone.slug",1
kaggle_ndsb2017,https://github.com/juliandewit/kaggle_ndsb2017/tree/master//helpers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kaggle_ndsb2017//helpers.py,,"def get_segmented_lungs(im, plot=False):
    binary = im < -400
    cleared = clear_border(binary)
    label_image = label(cleared)
    areas = [r.area for r in regionprops(label_image)]
    areas.sort()
    if len(areas) > 2:
        for region in regionprops(label_image):
            if region.area < areas[-2]:
                for coordinates in region.coords:
                    label_image[coordinates[0], coordinates[1]] = 0
    binary = label_image > 0
    selem = disk(2)
    binary = binary_erosion(binary, selem)
    selem = disk(10)
    binary = binary_closing(binary, selem)
    edges = roberts(binary)
    binary = ndi.binary_fill_holes(edges)
    get_high_vals = binary == 0
    im[get_high_vals] = -2000
    return (im, binary)","for coordinates in region.coords:
    label_image[coordinates[0], coordinates[1]] = 0","for i, coordinates in enumerate(region.coords):
    label_image[coordinates[0], coordinates[1]] = 0",1
doit,https://github.com/pydoit/doit/tree/master/doit/task.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/doit/doit/task.py,Task,"def clean(self, outstream, dryrun):
    """"""Execute task's clean
        @ivar outstream: 'write' output into this stream
        @ivar dryrun (bool): if True clean tasks are not executed
                             (just print out what would be executed)
        """"""
    self.init_options()
    if self._remove_targets is True:
        clean_targets(self, dryrun)
    else:
        for action in self.clean_actions:
            msg = ""%s - executing '%s'\n""
            outstream.write(msg % (self.name, action))
            execute_on_dryrun = False
            if isinstance(action, PythonAction):
                action_sig = inspect.signature(action.py_callable)
                if 'dryrun' in action_sig.parameters:
                    execute_on_dryrun = True
                    action.kwargs['dryrun'] = dryrun
            if not dryrun or execute_on_dryrun:
                result = action.execute(out=outstream)
                if isinstance(result, CatchedException):
                    sys.stderr.write(str(result))","for action in self.clean_actions:
    msg = ""%s - executing '%s'\n""
    outstream.write(msg % (self.name, action))
    execute_on_dryrun = False
    if isinstance(action, PythonAction):
        action_sig = inspect.signature(action.py_callable)
        if 'dryrun' in action_sig.parameters:
            execute_on_dryrun = True
            action.kwargs['dryrun'] = dryrun
    if not dryrun or execute_on_dryrun:
        result = action.execute(out=outstream)
        if isinstance(result, CatchedException):
            sys.stderr.write(str(result))","for i, action in enumerate(self.clean_actions):
    msg = ""%s - executing '%s'\n""
    outstream.write(msg % (self.name, action))
    execute_on_dryrun = False
    if isinstance(action, PythonAction):
        action_sig = inspect.signature(action.py_callable)
        if 'dryrun' in action_sig.parameters:
            execute_on_dryrun = True
            action.kwargs['dryrun'] = dryrun
    if not dryrun or execute_on_dryrun:
        result = action.execute(out=outstream)
        if isinstance(result, CatchedException):
            sys.stderr.write(str(result))",1
MeshCNN,https://github.com/ranahanocka/MeshCNN/tree/master/util/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MeshCNN/util/util.py,,"def print_network(net):
    """"""Print the total number of parameters in the network
    Parameters:
        network
    """"""
    print('---------- Network initialized -------------')
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print('[Network] Total number of parameters : %.3f M' % (num_params / 1000000.0))
    print('-----------------------------------------------')","for param in net.parameters():
    num_params += param.numel()","for i, param in enumerate(net.parameters()):
    num_params += param.numel()",1
sentinelsat,https://github.com/sentinelsat/sentinelsat/tree/master/tests/test_docs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentinelsat/tests/test_docs.py,,"def test_rst(rst_file):
    with open(rst_file) as input_file:
        contents = input_file.read()
    all_errors = []
    errors = rstcheck.check(contents, report_level=2, ignore={'languages': ['python', 'bash']})
    for (line_number, error) in errors:
        if 'Title underline too short' in error:
            continue
        m = re.search('Unknown interpreted text role ""([^""]+)""', error)
        if m and m.group(1) in ['program', 'paramref']:
            continue
        m = re.search('Unknown directive type ""([^""]+)""', error)
        if m and m.group(1) in ['automodule']:
            continue
        all_errors.append((line_number, error))
    assert len(all_errors) == 0","for (line_number, error) in errors:
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))","for i, (line_number, error) in enumerate(errors):
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))",1
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for b in range(value.shape[0]):
    b_kernel = kernel[min(b, kernel.shape[0] - 1)]
    for o in range(kernel.shape[1]):
        for i in range(value.shape[1]):
            result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for b, _ in enumerate(range(value.shape[0])):
    b_kernel = kernel[min(b, kernel.shape[0] - 1)]
    for o in range(kernel.shape[1]):
        for i in range(value.shape[1]):
            result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)",1
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for o in range(kernel.shape[1]):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for o, _ in enumerate(range(kernel.shape[1])):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)",1
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for i in range(value.shape[1]):
    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for i, _ in enumerate(range(value.shape[1])):
    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)",1
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in embed_layer.collect_params().items():
    if '_embed' in k:
        arr_id = int(k[-len('_weight') - 1])
        embed_weights[arr_id] = v.data()[0].asnumpy()
        embed_grads[arr_id] = v.grad()[0].asnumpy()
    elif '_inter_proj' in k:
        arr_id = int(k[-len('_weight') - 1])
        proj_weights[arr_id] = v.data()[0].asnumpy()
        proj_grads[arr_id] = v.grad()[0].asnumpy()","for i, (k, v) in enumerate(embed_layer.collect_params().items()):
    if '_embed' in k:
        arr_id = int(k[-len('_weight') - 1])
        embed_weights[arr_id] = v.data()[0].asnumpy()
        embed_grads[arr_id] = v.grad()[0].asnumpy()
    elif '_inter_proj' in k:
        arr_id = int(k[-len('_weight') - 1])
        proj_weights[arr_id] = v.data()[0].asnumpy()
        proj_grads[arr_id] = v.grad()[0].asnumpy()",1
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_proj.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (i, (k, v)) in enumerate(layer_with_shared_proj.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])",1
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_embed.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])","for i, (k, v) in enumerate(layer_with_shared_embed.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])",1
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_proj_embed.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for i, (k, v) in enumerate(layer_with_shared_proj_embed.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])",1
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for building in self.production_buildings:
    producer = building.get_component(Producer)
    for production in producer.get_productions():
        if not production.get_produced_resources():
            continue
        all_full = True
        for (resource_id, min_amount) in production.get_produced_resources().items():
            if production.inventory.get_free_space_for(resource_id) >= min_amount:
                all_full = False
                break
        if all_full and (not isinstance(building, Mine)):
            for resource_id in production.get_consumed_resources():
                if production.inventory.get_free_space_for(resource_id) > 0:
                    all_full = False
                    break
        if all_full:
            if not production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
        elif production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for (i, building) in enumerate(self.production_buildings):
    producer = building.get_component(Producer)
    for production in producer.get_productions():
        if not production.get_produced_resources():
            continue
        all_full = True
        for (resource_id, min_amount) in production.get_produced_resources().items():
            if production.inventory.get_free_space_for(resource_id) >= min_amount:
                all_full = False
                break
        if all_full and (not isinstance(building, Mine)):
            for resource_id in production.get_consumed_resources():
                if production.inventory.get_free_space_for(resource_id) > 0:
                    all_full = False
                    break
        if all_full:
            if not production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
        elif production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)",1
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for production in producer.get_productions():
    if not production.get_produced_resources():
        continue
    all_full = True
    for (resource_id, min_amount) in production.get_produced_resources().items():
        if production.inventory.get_free_space_for(resource_id) >= min_amount:
            all_full = False
            break
    if all_full and (not isinstance(building, Mine)):
        for resource_id in production.get_consumed_resources():
            if production.inventory.get_free_space_for(resource_id) > 0:
                all_full = False
                break
    if all_full:
        if not production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
    elif production.is_paused():
        ToggleActive(producer, production).execute(self.land_manager.session)
        self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for (i, production) in enumerate(producer.get_productions()):
    if not production.get_produced_resources():
        continue
    all_full = True
    for (resource_id, min_amount) in production.get_produced_resources().items():
        if production.inventory.get_free_space_for(resource_id) >= min_amount:
            all_full = False
            break
    if all_full and (not isinstance(building, Mine)):
        for resource_id in production.get_consumed_resources():
            if production.inventory.get_free_space_for(resource_id) > 0:
                all_full = False
                break
    if all_full:
        if not production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
    elif production.is_paused():
        ToggleActive(producer, production).execute(self.land_manager.session)
        self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)",1
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for (resource_id, min_amount) in production.get_produced_resources().items():
    if production.inventory.get_free_space_for(resource_id) >= min_amount:
        all_full = False
        break","for i, (resource_id, min_amount) in enumerate(production.get_produced_resources().items()):
    if production.inventory.get_free_space_for(resource_id) >= min_amount:
        all_full = False
        break",1
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/once.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/once.py,OnceIE,"def _extract_once_formats(self, url, http_formats_preference=None):
    (domain_id, application_id, media_item_id) = re.match(OnceIE._VALID_URL, url).groups()
    formats = self._extract_m3u8_formats(self.ADAPTIVE_URL_TEMPLATE % (domain_id, application_id, media_item_id), media_item_id, 'mp4', m3u8_id='hls', fatal=False)
    progressive_formats = []
    for adaptive_format in formats:
        adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
        rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
        if rendition_id:
            progressive_format = adaptive_format.copy()
            progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
            progressive_formats.append(progressive_format)
    self._check_formats(progressive_formats, media_item_id)
    formats.extend(progressive_formats)
    return formats","for adaptive_format in formats:
    adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
    rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
    if rendition_id:
        progressive_format = adaptive_format.copy()
        progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
        progressive_formats.append(progressive_format)","for i, adaptive_format in enumerate(formats):
    adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
    rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
    if rendition_id:
        progressive_format = adaptive_format.copy()
        progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
        progressive_formats.append(progressive_format)",1
DFDNet,https://github.com/csxmli2016/DFDNet/tree/master/models/base_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DFDNet/models/base_model.py,BaseModel,"def print_networks(self, verbose):
    for name in self.model_names:
        if isinstance(name, str):
            net = getattr(self, 'net' + name)
            num_params = 0
            for param in net.parameters():
                num_params += param.numel()","for param in net.parameters():
    num_params += param.numel()","for i, param in enumerate(net.parameters()):
    num_params += param.numel()",1
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/query_labels.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ALiPy/alipy/query_strategy/query_labels.py,QueryInstanceQBC,"def calc_avg_KL_divergence(cls, predict_matrices):
    """"""Calculate the average Kullback-Leibler (KL) divergence for measuring the
        level of disagreement in QBC.

        Parameters
        ----------
        predict_matrices: list
            The prediction matrix for each committee.
            Each committee predict matrix should have the shape [n_samples, n_classes] for probabilistic output
            or [n_samples] for class output.

        Returns
        -------
        score: list
            Score for each instance. Shape [n_samples]

        References
        ----------
        [1] A. McCallum and K. Nigam. Employing EM in pool-based active learning for
            text classification. In Proceedings of the International Conference on Machine
            Learning (ICML), pages 359-367. Morgan Kaufmann, 1998.
        """"""
    score = []
    (input_shape, committee_size) = cls()._check_committee_results(predict_matrices)
    if len(input_shape) == 2:
        label_num = input_shape[1]
        for i in range(input_shape[0]):
            instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
            tmp = 0
            for lab in range(label_num):
                committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
                for committee in range(committee_size):
                    tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
            score.append(tmp)
    else:
        raise Exception('A 2D probabilistic prediction matrix must be provided, with the shape like [n_samples, n_class]')
    return score","for i in range(input_shape[0]):
    instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
    tmp = 0
    for lab in range(label_num):
        committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
        for committee in range(committee_size):
            tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
    score.append(tmp)","for i, _ in enumerate(range(input_shape[0])):
    instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
    tmp = 0
    for lab in range(label_num):
        committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
        for committee in range(committee_size):
            tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
    score.append(tmp)",1
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/query_labels.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ALiPy/alipy/query_strategy/query_labels.py,QueryInstanceQBC,"def calc_avg_KL_divergence(cls, predict_matrices):
    """"""Calculate the average Kullback-Leibler (KL) divergence for measuring the
        level of disagreement in QBC.

        Parameters
        ----------
        predict_matrices: list
            The prediction matrix for each committee.
            Each committee predict matrix should have the shape [n_samples, n_classes] for probabilistic output
            or [n_samples] for class output.

        Returns
        -------
        score: list
            Score for each instance. Shape [n_samples]

        References
        ----------
        [1] A. McCallum and K. Nigam. Employing EM in pool-based active learning for
            text classification. In Proceedings of the International Conference on Machine
            Learning (ICML), pages 359-367. Morgan Kaufmann, 1998.
        """"""
    score = []
    (input_shape, committee_size) = cls()._check_committee_results(predict_matrices)
    if len(input_shape) == 2:
        label_num = input_shape[1]
        for i in range(input_shape[0]):
            instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
            tmp = 0
            for lab in range(label_num):
                committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
                for committee in range(committee_size):
                    tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
            score.append(tmp)
    else:
        raise Exception('A 2D probabilistic prediction matrix must be provided, with the shape like [n_samples, n_class]')
    return score","for lab in range(label_num):
    committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
    for committee in range(committee_size):
        tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)","for i, lab in enumerate(range(label_num)):
    committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
    for committee in range(committee_size):
        tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)",1
ezdxf,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/entities/ltype.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/entities/ltype.py,Linetype,"def setup_pattern(self, pattern: Union[Sequence[float], str], length: float=0) -> None:
    complex_line_type = True if isinstance(pattern, str) else False
    if complex_line_type:
        tags = self._setup_complex_pattern(pattern, length)
    else:
        tags = Tags([DXFTag(72, 65), DXFTag(73, len(pattern) - 1), DXFTag(40, float(pattern[0]))])
        for element in pattern[1:]:
            tags.append(DXFTag(49, float(element)))
            tags.append(DXFTag(74, 0))
    self.pattern_tags = LinetypePattern(tags)","for element in pattern[1:]:
    tags.append(DXFTag(49, float(element)))
    tags.append(DXFTag(74, 0))","for i, element in enumerate(pattern[1:]):
    tags.append(DXFTag(49, float(element)))
    tags.append(DXFTag(74, 0))",1
grover,https://github.com/rowanz/grover/tree/master/realnews/prepare_lm_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grover/realnews/prepare_lm_data.py,,"def _stream_from_buffer(buffer, current_desired_size, pad_token=0, add_articles_to_end=False):
    """""" Combines short articles that are in a buffer """"""
    random.shuffle(buffer)
    i = 0
    while i < len(buffer):
        article = buffer[i]
        if add_articles_to_end:
            for article2add in buffer[i + 1:]:
                i += 1
                article['input_ids'].append(encoder.padding)
                article['input_ids'].append(encoder.reset_context)
                article['input_ids'].extend(article2add['input_ids'])
                if len(article['input_ids']) >= current_desired_size:
                    article['input_ids'] = article['input_ids'][:current_desired_size]
                    break
        amount_to_pad = current_desired_size - len(article['input_ids'])
        article['input_ids'].extend([pad_token] * amount_to_pad)
        article['sub_index'] = 0
        yield article
        i += 1","for article2add in buffer[i + 1:]:
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break","for j, article2add in enumerate(buffer[i + 1:]):
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break",1
heamy,https://github.com/rushter/heamy/tree/master/heamy/pipeline.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/heamy/heamy/pipeline.py,ModelsPipeline,"def stack(self, k=5, stratify=False, shuffle=True, seed=100, full_test=True, add_diff=False):
    """"""Stacks sequence of models.

        Parameters
        ----------

        k : int, default 5
            Number of folds.
        stratify : bool, default False
        shuffle : bool, default True
        seed : int, default 100
        full_test : bool, default True
            If True then evaluate test dataset on the full data otherwise take the mean of every fold.
        add_diff : bool, default False

        Returns
        -------
        `DataFrame`

        Examples
        --------
        >>> pipeline = ModelsPipeline(model_rf,model_lr)
        >>> stack_ds = pipeline.stack(k=10, seed=111)
        """"""
    result_train = []
    result_test = []
    y = None
    for model in self.models:
        result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
        train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
        test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
        result_train.append(train_df)
        result_test.append(test_df)
        if y is None:
            y = result.y_train
    result_train = pd.concat(result_train, axis=1)
    result_test = pd.concat(result_test, axis=1)
    if add_diff:
        result_train = feature_combiner(result_train)
        result_test = feature_combiner(result_test)
    ds = Dataset(X_train=result_train, y_train=y, X_test=result_test)
    return ds","for model in self.models:
    result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
    train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
    test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
    result_train.append(train_df)
    result_test.append(test_df)
    if y is None:
        y = result.y_train","for i, model in enumerate(self.models):
    result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
    train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
    test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
    result_train.append(train_df)
    result_test.append(test_df)
    if y is None:
        y = result.y_train",1
binaryalert,https://github.com/airbnb/binaryalert/tree/master/rules/clone_rules.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/binaryalert/rules/clone_rules.py,,"def _clone_repo(url: str, include: Optional[List[str]], exclude: Optional[List[str]]) -> int:
    """"""Clone the given repo and copy only the YARA files from the specified paths.

    Returns:
        Number of files copied.
    """"""
    cloned_repo_root = os.path.join(tempfile.gettempdir(), os.path.basename(url))
    if os.path.exists(cloned_repo_root):
        shutil.rmtree(cloned_repo_root)
    subprocess.check_call(['git', 'clone', '--quiet', '--depth', '1', url, cloned_repo_root])
    if '//' in url:
        target_repo_root = os.path.join(RULES_DIR, url.split('//')[1])
    else:
        target_repo_root = os.path.join(RULES_DIR, url.split('@')[1].replace(':', '/', 1))
    if os.path.exists(target_repo_root):
        shutil.rmtree(target_repo_root)
    files_copied = 0
    for relative_path in _files_to_copy(cloned_repo_root, include, exclude):
        os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
        src = os.path.join(cloned_repo_root, relative_path)
        dst = os.path.join(target_repo_root, relative_path)
        shutil.copy(src, dst)
        files_copied += 1
    shutil.rmtree(cloned_repo_root)
    return files_copied","for relative_path in _files_to_copy(cloned_repo_root, include, exclude):
    os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
    src = os.path.join(cloned_repo_root, relative_path)
    dst = os.path.join(target_repo_root, relative_path)
    shutil.copy(src, dst)
    files_copied += 1","for i, relative_path in enumerate(_files_to_copy(cloned_repo_root, include, exclude)):
    os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
    src = os.path.join(cloned_repo_root, relative_path)
    dst = os.path.join(target_repo_root, relative_path)
    shutil.copy(src, dst)
    files_copied += 1",1
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","for ci in qs.iterator():
    if ci.position:
        try:
            ia = ci.position.order.invoice_address
        except InvoiceAddress.DoesNotExist:
            ia = InvoiceAddress()
    yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","for i, ci in enumerate(qs.iterator()):
    if ci.position:
        try:
            ia = ci.position.order.invoice_address
        except InvoiceAddress.DoesNotExist:
            ia = InvoiceAddress()
    yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",1
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])","for i in range(RUN_STEP):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)","for i in enumerate(range(RUN_STEP)):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)",1
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (name, cloud) in clouds.items():
    for (resource_name, resource) in cloud.resources.items():
        issues = issues.union(check_deprecations(resource))
        for (fname, f) in resource.filter_registry.items():
            if fname in ('and', 'or', 'not'):
                continue
            issues = issues.union(check_deprecations(f))
        for (aname, a) in resource.action_registry.items():
            issues = issues.union(check_deprecations(a))","for (i, (name, cloud)) in enumerate(clouds.items()):
    for (resource_name, resource) in cloud.resources.items():
        issues = issues.union(check_deprecations(resource))
        for (fname, f) in resource.filter_registry.items():
            if fname in ('and', 'or', 'not'):
                continue
            issues = issues.union(check_deprecations(f))
        for (aname, a) in resource.action_registry.items():
            issues = issues.union(check_deprecations(a))",1
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (resource_name, resource) in cloud.resources.items():
    issues = issues.union(check_deprecations(resource))
    for (fname, f) in resource.filter_registry.items():
        if fname in ('and', 'or', 'not'):
            continue
        issues = issues.union(check_deprecations(f))
    for (aname, a) in resource.action_registry.items():
        issues = issues.union(check_deprecations(a))","for (i, (resource_name, resource)) in enumerate(cloud.resources.items()):
    issues = issues.union(check_deprecations(resource))
    for (fname, f) in resource.filter_registry.items():
        if fname in ('and', 'or', 'not'):
            continue
        issues = issues.union(check_deprecations(f))
    for (aname, a) in resource.action_registry.items():
        issues = issues.union(check_deprecations(a))",1
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (aname, a) in resource.action_registry.items():
    issues = issues.union(check_deprecations(a))","for i, (aname, a) in enumerate(resource.action_registry.items()):
    issues = issues.union(check_deprecations(a))",1
2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement,https://github.com/Mingtzge/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/tree/master/recognize_process/tools/test_crnn_jmz.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/recognize_process/tools/test_crnn_jmz.py,,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict_path=None):
    """"""
    listhttps://github.com/bai-shang/crnn_ctc_ocr.Tensorflow
    :param sparse_matrix:
    :param char_map_dict_path:
    :return:
    """"""
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    char_map_dict = json.load(open(char_map_dict_path, 'r', encoding='utf-8'))
    if char_map_dict is None:
        print('error')
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","for row in dense_matrix:
    string = []
    for val in row:
        string.append(_int_to_string(val, char_map_dict))
    string_list.append(''.join((s for s in string if s != '*')))","for i, row in enumerate(dense_matrix):
    string = []
    for val in row:
        string.append(_int_to_string(val, char_map_dict))
    string_list.append(''.join((s for s in string if s != '*')))",1
2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement,https://github.com/Mingtzge/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/tree/master/recognize_process/tools/test_crnn_jmz.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/recognize_process/tools/test_crnn_jmz.py,,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict_path=None):
    """"""
    listhttps://github.com/bai-shang/crnn_ctc_ocr.Tensorflow
    :param sparse_matrix:
    :param char_map_dict_path:
    :return:
    """"""
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    char_map_dict = json.load(open(char_map_dict_path, 'r', encoding='utf-8'))
    if char_map_dict is None:
        print('error')
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","for val in row:
    string.append(_int_to_string(val, char_map_dict))","for i, val in enumerate(row):
    string.append(_int_to_string(val, char_map_dict))",1
transformers,https://github.com/huggingface/transformers/tree/master/tests/test_modeling_tf_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/tests/test_modeling_tf_common.py,TFModelTesterMixin,"def test_numpy_arrays_inputs(self):
    (config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()

    def prepare_numpy_arrays(inputs_dict):
        inputs_np_dict = {}
        for (k, v) in inputs_dict.items():
            if tf.is_tensor(v):
                inputs_np_dict[k] = v.numpy()
            else:
                inputs_np_dict[k] = np.array(k)
        return inputs_np_dict
    for model_class in self.all_model_classes:
        model = model_class(config)
        inputs = self._prepare_for_class(inputs_dict, model_class)
        inputs_np = prepare_numpy_arrays(inputs)
        output_for_dict_input = model(inputs_np)
        output_for_kw_input = model(**inputs_np)
        self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for model_class in self.all_model_classes:
    model = model_class(config)
    inputs = self._prepare_for_class(inputs_dict, model_class)
    inputs_np = prepare_numpy_arrays(inputs)
    output_for_dict_input = model(inputs_np)
    output_for_kw_input = model(**inputs_np)
    self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for i, model_class in enumerate(self.all_model_classes):
    model = model_class(config)
    inputs = self._prepare_for_class(inputs_dict, model_class)
    inputs_np = prepare_numpy_arrays(inputs)
    output_for_dict_input = model(inputs_np)
    output_for_kw_input = model(**inputs_np)
    self.assert_outputs_same(output_for_dict_input, output_for_kw_input)",1
transformers,https://github.com/huggingface/transformers/tree/master/tests/test_modeling_tf_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/tests/test_modeling_tf_common.py,TFModelTesterMixin,"def test_numpy_arrays_inputs(self):
    (config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()

    def prepare_numpy_arrays(inputs_dict):
        inputs_np_dict = {}
        for (k, v) in inputs_dict.items():
            if tf.is_tensor(v):
                inputs_np_dict[k] = v.numpy()
            else:
                inputs_np_dict[k] = np.array(k)
        return inputs_np_dict
    for model_class in self.all_model_classes:
        model = model_class(config)
        inputs = self._prepare_for_class(inputs_dict, model_class)
        inputs_np = prepare_numpy_arrays(inputs)
        output_for_dict_input = model(inputs_np)
        output_for_kw_input = model(**inputs_np)
        self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for (k, v) in inputs_dict.items():
    if tf.is_tensor(v):
        inputs_np_dict[k] = v.numpy()
    else:
        inputs_np_dict[k] = np.array(k)","for i, (k, v) in enumerate(inputs_dict.items()):
    if tf.is_tensor(v):
        inputs_np_dict[k] = v.numpy()
    else:
        inputs_np_dict[k] = np.array(k)",1
amundsen,https://github.com/amundsen-io/amundsen/tree/master/common/tests/unit/utils/test_atlas_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/amundsen/common/tests/unit/utils/test_atlas_utils.py,TestAtlasColumnKey,"def test_table_column_key_details_from_qualified_name(self) -> None:
    params = [('db_name.table_name.column_name@cluster_name', dict(cluster='cluster_name', schema='db_name', table='table_name', column='column_name')), ('db_name.table_name.column_name.dot@cluster_name.dot', dict(cluster='cluster_name.dot', schema='db_name', table='table_name', column='column_name.dot'))]
    for (qn, details) in params:
        with self.subTest(f'Test extract details from qualified name: {qn}'):
            result = AtlasColumnKey(qn)
            self.assertEqual(details, result.get_details())","for (qn, details) in params:
    with self.subTest(f'Test extract details from qualified name: {qn}'):
        result = AtlasColumnKey(qn)
        self.assertEqual(details, result.get_details())","for i, (qn, details) in enumerate(params):
    with self.subTest(f'Test extract details from qualified name: {qn}'):
        result = AtlasColumnKey(qn)
        self.assertEqual(details, result.get_details())",1
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return","for row in fdf.itertuples():
    if block_start <= row.timestamp < block_end:
        readblock.append(row.file)
        continue
    if readblock or (schema.type == 'record' and block_start in state.poller_periods):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
    if wr_polling_period and readblock:
        state.poller_periods.add(block_start)
    if readblock:
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    block_start = block_end
    block_end = block_start + state.period
    readblock = []
    if schema.type != 'record':
        block_start = compute_block_start(row.timestamp)
        block_end = block_start + state.period
        if row.timestamp > block_end or block_end > utcnow:
            break
        readblock = [row.file]
        continue
    while row.timestamp > block_end:
        if block_start in state.poller_periods:
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        block_start = block_end
        block_end = block_start + state.period
    if block_end > utcnow:
        break
    readblock = [row.file]","for i, row in enumerate(fdf.itertuples()):
    if block_start <= row.timestamp < block_end:
        readblock.append(row.file)
        continue
    if readblock or (schema.type == 'record' and block_start in state.poller_periods):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
    if wr_polling_period and readblock:
        state.poller_periods.add(block_start)
    if readblock:
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    block_start = block_end
    block_end = block_start + state.period
    readblock = []
    if schema.type != 'record':
        block_start = compute_block_start(row.timestamp)
        block_end = block_start + state.period
        if row.timestamp > block_end or block_end > utcnow:
            break
        readblock = [row.file]
        continue
    while row.timestamp > block_end:
        if block_start in state.poller_periods:
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        block_start = block_end
        block_end = block_start + state.period
    if block_end > utcnow:
        break
    readblock = [row.file]",1
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return","for interval in polled_periods:
    if not fdf.empty and block_end < interval:
        break
    pre_block_start = compute_block_start(interval)
    pre_block_end = pre_block_start + state.period
    write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)","for i, interval in enumerate(polled_periods):
    if not fdf.empty and block_end < interval:
        break
    pre_block_start = compute_block_start(interval)
    pre_block_end = pre_block_start + state.period
    write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)",1
Invertible-Image-Rescaling,https://github.com/pkuxmq/Invertible-Image-Rescaling/tree/master/codes/models/modules/discriminator_vgg_arch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Invertible-Image-Rescaling/codes/models/modules/discriminator_vgg_arch.py,VGGFeatureExtractor,"def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, device=torch.device('cpu')):
    super(VGGFeatureExtractor, self).__init__()
    self.use_input_norm = use_input_norm
    if use_bn:
        model = torchvision.models.vgg19_bn(pretrained=True)
    else:
        model = torchvision.models.vgg19(pretrained=True)
    if self.use_input_norm:
        mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
        self.register_buffer('mean', mean)
        self.register_buffer('std', std)
    self.features = nn.Sequential(*list(model.features.children())[:feature_layer + 1])
    for (k, v) in self.features.named_parameters():
        v.requires_grad = False","for (k, v) in self.features.named_parameters():
    v.requires_grad = False","for i, (k, v) in enumerate(self.features.named_parameters()):
    v.requires_grad = False",1
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for doc_span_index in range(len(spans)):
    for j in range(spans[doc_span_index]['paragraph_len']):
        is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
        index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
        spans[doc_span_index]['token_is_max_context'][index] = is_max_context","for (doc_span_index, span) in enumerate(spans):
    for j in range(span['paragraph_len']):
        is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
        index = j if tokenizer.padding_side == 'left' else span['truncated_query_with_special_tokens_length'] + j
        span['token_is_max_context'][index] = is_max_context",1
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for span in spans:
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))","for i, span in enumerate(spans):
    cls_index = span['input_ids'].index(tokenizer.cls_token_id)
    p_mask = np.ones_like(span['token_type_ids'])
    if tokenizer.padding_side == 'right':
        p_mask[len(truncated_query) + sequence_added_tokens:] = 0
    else:
        p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
    pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
    special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
    p_mask[pad_token_indices] = 1
    p_mask[special_token_indices] = 1
    p_mask[cls_index] = 0
    span_is_impossible = self.is_impossible
    start_position = 0
    end_position = 0
    if is_training and (not span_is_impossible):
        doc_start = span['start']
        doc_end = span['start'] + span['length'] - 1
        out_of_span = False
        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True
        if out_of_span:
            start_position = cls_index
            end_position = cls_index
            span_is_impossible = True
        else:
            if tokenizer.padding_side == 'left':
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens
            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset
    features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))",1
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for sub_token in sub_tokens:
    tok_to_orig_index.append(i)
    all_doc_tokens.append(sub_token)","for i, sub_token in enumerate(sub_tokens):
    tok_to_orig_index.append(i)
    all_doc_tokens.append(sub_token)",1
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for i in range(paragraph_len):
    index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
    token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]","for i, _ in enumerate(range(paragraph_len)):
    index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
    token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]",1
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","for j in range(spans[doc_span_index]['paragraph_len']):
    is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
    index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
    spans[doc_span_index]['token_is_max_context'][index] = is_max_context","for (j, _) in enumerate(range(spans[doc_span_index]['paragraph_len'])):
    is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
    index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
    spans[doc_span_index]['token_is_max_context'][index] = is_max_context",1
routersploit,https://github.com/threat9/routersploit/tree/master/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/routersploit/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,Exploit,"def run(self):
    creds = []
    print_status('Requesting {}'.format(self.get_target_url()))
    response = self.http_request(method='GET', path='/password.cgi')
    if response is None:
        print_error('Exploit failed - empty response')
        return
    tokens = [('admin', ""pwdAdmin = '(.+?)'""), ('support', ""pwdSupport = '(.+?)'""), ('user', ""pwdUser = '(.+?)'"")]
    print_status('Trying to extract credentials')
    for token in tokens:
        res = re.findall(token[1], response.text)
        if res:
            creds.append((token[0], res[0]))
    if creds:
        print_success('Credentials found')
        print_table(('Login', 'Password'), *creds)
    else:
        print_error('Exploit failed - credentials could not be found')","for token in tokens:
    res = re.findall(token[1], response.text)
    if res:
        creds.append((token[0], res[0]))","for i, token in enumerate(tokens):
    res = re.findall(token[1], response.text)
    if res:
        creds.append((token[0], res[0]))",1
edx-platform,https://github.com/edx/edx-platform/tree/master/scripts/xblock/xblock_counts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/scripts/xblock/xblock_counts.py,,"def write_block_summary_report(course_data):
    """"""
    Generate a CSV file containing a summary of the xBlock usage

    Arguments:
        course_data (list of dicts): a list of course_data objects

    Returns:
        Nothing
    """"""
    (block_summary_counts, unique_course_counts) = _get_block_summary_totals(course_data)
    with open('xblock_summary_counts.csv', 'wb') as csvfile:
        summary_writer = csv.writer(csvfile, delimiter=',', quotechar='""', quoting=csv.QUOTE_MINIMAL)
        summary_writer.writerow(['XBLOCK_NAME', 'UNIQUE_COURSES', 'NUM_TOTAL_INSTANCES'])
        for block_type in sorted(block_summary_counts):
            block_count = block_summary_counts.get(block_type)
            summary_writer.writerow([block_type, str(unique_course_counts[block_type]), str(block_count)])
        csvfile.close()","for block_type in sorted(block_summary_counts):
    block_count = block_summary_counts.get(block_type)
    summary_writer.writerow([block_type, str(unique_course_counts[block_type]), str(block_count)])","for i, block_type in enumerate(sorted(block_summary_counts)):
    block_count = block_summary_counts.get(block_type)
    summary_writer.writerow([block_type, str(unique_course_counts[block_type]), str(block_count)])",1
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for epoch in range(hp.epochs):
    dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
    pbar = tqdm(dataloader)
    for (i, data) in enumerate(pbar):
        pbar.set_description('Processing at epoch %d' % epoch)
        global_step += 1
        if global_step < 400000:
            adjust_learning_rate(optimizer, global_step)
        (character, mel, mel_input, pos_text, pos_mel, _) = data
        stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
        character = character.cuda()
        mel = mel.cuda()
        mel_input = mel_input.cuda()
        pos_text = pos_text.cuda()
        pos_mel = pos_mel.cuda()
        (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
        mel_loss = nn.L1Loss()(mel_pred, mel)
        post_mel_loss = nn.L1Loss()(postnet_pred, mel)
        loss = mel_loss + post_mel_loss
        writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
        writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
        if global_step % hp.image_step == 1:
            for (i, prob) in enumerate(attn_probs):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_enc):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_dec):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        optimizer.step()
        if global_step % hp.save_step == 0:
            t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for (epoch) in enumerate(range(hp.epochs)):
    dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
    pbar = tqdm(dataloader)
    for (i, data) in enumerate(pbar):
        pbar.set_description('Processing at epoch %d' % epoch)
        global_step += 1
        if global_step < 400000:
            adjust_learning_rate(optimizer, global_step)
        (character, mel, mel_input, pos_text, pos_mel, _) = data
        stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
        character = character.cuda()
        mel = mel.cuda()
        mel_input = mel_input.cuda()
        pos_text = pos_text.cuda()
        pos_mel = pos_mel.cuda()
        (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
        mel_loss = nn.L1Loss()(mel_pred, mel)
        post_mel_loss = nn.L1Loss()(postnet_pred, mel)
        loss = mel_loss + post_mel_loss
        writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
        writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
        if global_step % hp.image_step == 1:
            for (i, prob) in enumerate(attn_probs):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_enc):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_dec):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        optimizer.step()
        if global_step % hp.save_step == 0:
            t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",1
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for j in range(4):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)","for j, _ in enumerate(range(4)):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)",1
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for j in range(4):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)","for j in enumerate(range(4)):
    x = vutils.make_grid(prob[j * 16] * 255)
    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)",1
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/modeling_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/modeling_utils.py,,"def find_pruneable_heads_and_indices(heads: List[int], n_heads: int, head_size: int, already_pruned_heads: Set[int]) -> Tuple[Set[int], torch.LongTensor]:
    """"""
    Finds the heads and their indices taking :obj:`already_pruned_heads` into account.

    Args:
        heads (:obj:`List[int]`): List of the indices of heads to prune.
        n_heads (:obj:`int`): The number of heads in the model.
        head_size (:obj:`int`): The size of each head.
        already_pruned_heads (:obj:`Set[int]`): A set of already pruned heads.

    Returns:
        :obj:`Tuple[Set[int], torch.LongTensor]`: A tuple with the remaining heads and their corresponding indices.
    """"""
    mask = torch.ones(n_heads, head_size)
    heads = set(heads) - already_pruned_heads
    for head in heads:
        head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
        mask[head] = 0
    mask = mask.view(-1).contiguous().eq(1)
    index: torch.LongTensor = torch.arange(len(mask))[mask].long()
    return (heads, index)","for head in heads:
    head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
    mask[head] = 0","for i, head in enumerate(heads):
    head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
    mask[head] = 0",1
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/tests/utils/test_auto_resumer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/tests/utils/test_auto_resumer.py,,"def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    loaded_cfg = json.load(open(args_path))
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)","for k in cfg_dict:
    assert cfg_dict[k] == loaded_cfg[k]","for i,k in enumerate(cfg_dict):
    assert cfg_dict[k] == loaded_cfg[k]",1
plaso,https://github.com/log2timeline/plaso/tree/master/plaso/parsers/winreg_plugins/usb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plaso/plaso/parsers/winreg_plugins/usb.py,USBPlugin,"def ExtractEvents(self, parser_mediator, registry_key, **kwargs):
    """"""Extracts events from a Windows Registry key.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfVFS.
      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.
    """"""
    event_data = WindowsUSBDeviceEventData()
    event_data.key_path = registry_key.path
    for subkey in registry_key.GetSubkeys():
        event_data.subkey_name = subkey.name
        vendor_identification = None
        product_identification = None
        try:
            subkey_name_parts = subkey.name.split('&')
            if len(subkey_name_parts) >= 2:
                vendor_identification = subkey_name_parts[0]
                product_identification = subkey_name_parts[1]
        except ValueError as exception:
            logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
        event_data.vendor = vendor_identification
        event_data.product = product_identification
        for devicekey in subkey.GetSubkeys():
            event_data.last_written_time = devicekey.last_written_time
            event_data.serial = devicekey.name
            parser_mediator.ProduceEventData(event_data)","for subkey in registry_key.GetSubkeys():
    event_data.subkey_name = subkey.name
    vendor_identification = None
    product_identification = None
    try:
        subkey_name_parts = subkey.name.split('&')
        if len(subkey_name_parts) >= 2:
            vendor_identification = subkey_name_parts[0]
            product_identification = subkey_name_parts[1]
    except ValueError as exception:
        logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
    event_data.vendor = vendor_identification
    event_data.product = product_identification
    for devicekey in subkey.GetSubkeys():
        event_data.last_written_time = devicekey.last_written_time
        event_data.serial = devicekey.name
        parser_mediator.ProduceEventData(event_data)","for i, subkey in enumerate(registry_key.GetSubkeys()):
    event_data.subkey_name = subkey.name
    vendor_identification = None
    product_identification = None
    try:
        subkey_name_parts = subkey.name.split('&')
        if len(subkey_name_parts) >= 2:
            vendor_identification = subkey_name_parts[0]
            product_identification = subkey_name_parts[1]
    except ValueError as exception:
        logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
    event_data.vendor = vendor_identification
    event_data.product = product_identification
    for devicekey in subkey.GetSubkeys():
        event_data.last_written_time = devicekey.last_written_time
        event_data.serial = devicekey.name
        parser_mediator.ProduceEventData(event_data)",1
SDV,https://github.com/sdv-dev/SDV/tree/master/sdv/timeseries/deepecho.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV/sdv/timeseries/deepecho.py,DeepEchoModel,"def _fit(self, timeseries_data):
    self._model = self._build_model()
    if self._sequence_index:
        timeseries_data = timeseries_data.rename(columns={self._sequence_index + '.value': self._sequence_index})
    self._output_columns = list(timeseries_data.columns)
    self._data_columns = [column for column in timeseries_data.columns if column not in self._entity_columns + self._context_columns]
    sequences = assemble_sequences(timeseries_data, self._entity_columns, self._context_columns, self._segment_size, self._sequence_index, drop_sequence_index=False)
    data_types = list()
    context_types = list()
    for field in self._output_columns:
        dtype = timeseries_data[field].dtype
        kind = dtype.kind
        if kind in ('i', 'f'):
            data_type = 'continuous'
        elif kind in ('O', 'b'):
            data_type = 'categorical'
        else:
            raise ValueError(f'Unsupported dtype {dtype}')
        if field in self._data_columns:
            data_types.append(data_type)
        elif field in self._context_columns:
            context_types.append(data_type)
    if self._sequence_index:
        self._transform_sequence_index(sequences)
        data_types.append('continuous')
    self._model.fit_sequences(sequences, context_types, data_types)","for field in self._output_columns:
    dtype = timeseries_data[field].dtype
    kind = dtype.kind
    if kind in ('i', 'f'):
        data_type = 'continuous'
    elif kind in ('O', 'b'):
        data_type = 'categorical'
    else:
        raise ValueError(f'Unsupported dtype {dtype}')
    if field in self._data_columns:
        data_types.append(data_type)
    elif field in self._context_columns:
        context_types.append(data_type)","for i, field in enumerate(self._output_columns):
    dtype = timeseries_data[field].dtype
    kind = dtype.kind
    if kind in ('i', 'f'):
        data_type = 'continuous'
    elif kind in ('O', 'b'):
        data_type = 'categorical'
    else:
        raise ValueError(f'Unsupported dtype {dtype}')
    if field in self._data_columns:
        data_types.append(data_type)
    elif field in self._context_columns:
        context_types.append(data_type)",1
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","for token in tokens:
    if len(token.strip()) == 0:
        return f'Invalid market. Ticker {token} has an invalid length.'
    if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
        return f'Invalid market. Ticker {token} contains invalid characters.'
    if token in tokens_list:
        return f'Duplicate market {token}.'
    tokens_list.append(token)","for i, token in enumerate(tokens):
    if len(token.strip()) == 0:
        return f'Invalid market. Ticker {token} has an invalid length.'
    if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
        return f'Invalid market. Ticker {token} contains invalid characters.'
    if token in tokens_list:
        return f'Duplicate market {token}.'
    tokens_list.append(token)",1
DeepRobust,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/defense/gcn_preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRobust/deeprobust/graph/defense/gcn_preprocess.py,,"def dropedge_both(A, iA, jA, features, threshold1=2.5, threshold2=0.01):
    removed_cnt = 0
    for row in range(len(iA) - 1):
        for i in range(iA[row], iA[row + 1]):
            n1 = row
            n2 = jA[i]
            C1 = np.linalg.norm(features[n1] - features[n2])
            (a, b) = (features[n1], features[n2])
            inner_product = (a * b).sum()
            C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
            if C1 > threshold1 or threshold2 < 0:
                A[i] = 0
                removed_cnt += 1
    return removed_cnt","for row in range(len(iA) - 1):
    for i in range(iA[row], iA[row + 1]):
        n1 = row
        n2 = jA[i]
        C1 = np.linalg.norm(features[n1] - features[n2])
        (a, b) = (features[n1], features[n2])
        inner_product = (a * b).sum()
        C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
        if C1 > threshold1 or threshold2 < 0:
            A[i] = 0
            removed_cnt += 1","for (row, iA_row) in enumerate(iA[:-1]):
    for i in range(iA_row, iA[row + 1]):
        n1 = row
        n2 = jA[i]
        C1 = np.linalg.norm(features[n1] - features[n2])
        (a, b) = (features[n1], features[n2])
        inner_product = (a * b).sum()
        C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
        if C1 > threshold1 or threshold2 < 0:
            A[i] = 0
            removed_cnt += 1",1
projects,https://github.com/explosion/projects/tree/master/tutorials/ner_fashion_brands/scripts/visualize_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/projects/tutorials/ner_fashion_brands/scripts/visualize_data.py,,"def load_data(filepath):
    examples = list(srsly.read_jsonl(filepath))
    rows = []
    n_total_ents = 0
    n_no_ents = 0
    labels = set()
    for eg in examples:
        row = {'text': eg['text'], 'ents': eg.get('spans', [])}
        n_total_ents += len(row['ents'])
        if not row['ents']:
            n_no_ents += 1
        labels.update([span['label'] for span in row['ents']])
        rows.append(row)
    return (rows, labels, n_total_ents, n_no_ents)","for eg in examples:
    row = {'text': eg['text'], 'ents': eg.get('spans', [])}
    n_total_ents += len(row['ents'])
    if not row['ents']:
        n_no_ents += 1
    labels.update([span['label'] for span in row['ents']])
    rows.append(row)","for i, eg in enumerate(examples):
    row = {'text': eg['text'], 'ents': eg.get('spans', [])}
    n_total_ents += len(row['ents'])
    if not row['ents']:
        n_no_ents += 1
    labels.update([span['label'] for span in row['ents']])
    rows.append(row)",1
