repo_name,file_path,file_html,class_name,me_code,old_code,new_codeacc
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/modeling_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/modeling_utils.py,,"def find_pruneable_heads_and_indices(heads: List[int], n_heads: int, head_size: int, already_pruned_heads: Set[int]) -> Tuple[Set[int], torch.LongTensor]:
    """"""
    Finds the heads and their indices taking :obj:`already_pruned_heads` into account.

    Args:
        heads (:obj:`List[int]`): List of the indices of heads to prune.
        n_heads (:obj:`int`): The number of heads in the model.
        head_size (:obj:`int`): The size of each head.
        already_pruned_heads (:obj:`Set[int]`): A set of already pruned heads.

    Returns:
        :obj:`Tuple[Set[int], torch.LongTensor]`: A tuple with the remaining heads and their corresponding indices.
    """"""
    mask = torch.ones(n_heads, head_size)
    heads = set(heads) - already_pruned_heads
    for head in heads:
        head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
        mask[head] = 0
    mask = mask.view(-1).contiguous().eq(1)
    index: torch.LongTensor = torch.arange(len(mask))[mask].long()
    return (heads, index)","for head in heads:
    head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
    mask[head] = 0","for i, head in enumerate(heads):
    head = head - sum((1 if h < head else 0 for h in already_pruned_heads))
    mask[head] = 0"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/dependencies/repository/relation_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/dependencies/repository/relation_builder.py,RelationBuilder,"def get_updated_changeset_revisions_for_repository_dependencies(self, key_rd_dicts):
    updated_key_rd_dicts = []
    for key_rd_dict in key_rd_dicts:
        key = next(iter(key_rd_dict))
        repository_dependency = key_rd_dict[key]
        (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
        if suc.tool_shed_is_this_tool_shed(rd_toolshed):
            repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
            if repository:
                repository_id = self.app.security.encode_id(repository.id)
                repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = repository_dependency
                    updated_key_rd_dicts.append(key_rd_dict)
                else:
                    changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                    if changeset_revision != rd_changeset_revision:
                        repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                    if repository_metadata:
                        new_key_rd_dict = {}
                        new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                        updated_key_rd_dicts.append(new_key_rd_dict)
                    else:
                        repository_components_tuple = container_util.get_components_from_key(key)
                        components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                        (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                        if len(components_list) in (4, 5):
                            rd_only_if_compiling_contained_td = 'False'
                        message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                        message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                        log.debug(message)
            else:
                repository_components_tuple = container_util.get_components_from_key(key)
                components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                log.debug(message)
    return updated_key_rd_dicts","for key_rd_dict in key_rd_dicts:
    key = next(iter(key_rd_dict))
    repository_dependency = key_rd_dict[key]
    (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
    if suc.tool_shed_is_this_tool_shed(rd_toolshed):
        repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
        if repository:
            repository_id = self.app.security.encode_id(repository.id)
            repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
            if repository_metadata:
                new_key_rd_dict = {}
                new_key_rd_dict[key] = repository_dependency
                updated_key_rd_dicts.append(key_rd_dict)
            else:
                changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                if changeset_revision != rd_changeset_revision:
                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                    updated_key_rd_dicts.append(new_key_rd_dict)
                else:
                    repository_components_tuple = container_util.get_components_from_key(key)
                    components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                    (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                    if len(components_list) in (4, 5):
                        rd_only_if_compiling_contained_td = 'False'
                    message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                    message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                    log.debug(message)
        else:
            repository_components_tuple = container_util.get_components_from_key(key)
            components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
            (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
            message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
            message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
            log.debug(message)","for i,key_rd_dict in enumerate(key_rd_dicts):
    key = next(iter(key_rd_dict))
    repository_dependency = key_rd_dict[key]
    (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
    if suc.tool_shed_is_this_tool_shed(rd_toolshed):
        repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
        if repository:
            repository_id = self.app.security.encode_id(repository.id)
            repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
            if repository_metadata:
                new_key_rd_dict = {}
                new_key_rd_dict[key] = repository_dependency
                updated_key_rd_dicts.append(key_rd_dict)
            else:
                changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                if changeset_revision != rd_changeset_revision:
                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                    updated_key_rd_dicts.append(new_key_rd_dict)
                else:
                    repository_components_tuple = container_util.get_components_from_key(key)
                    components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                    (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                    if len(components_list) in (4, 5):
                        rd_only_if_compiling_contained_td = 'False'
                    message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                    message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                    log.debug(message)
        else:
            repository_components_tuple = container_util.get_components_from_key(key)
            components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
            (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
            message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
            message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
            log.debug(message)"
airflow,https://github.com/apache/airflow/tree/master/airflow/www/fab_security/manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/fab_security/manager.py,BaseSecurityManager,"def get_role_permissions(self, role) -> Set[Tuple[str, str]]:
    """"""Get all permissions for a certain role""""""
    result = set()
    if role.name in self.builtin_roles:
        for permission in self.builtin_roles[role.name]:
            result.add((permission[1], permission[0]))
    else:
        for permission in self.get_role_permissions_from_db(role.id):
            result.add((permission.action.name, permission.resource.name))
    return result","for permission in self.builtin_roles[role.name]:
    result.add((permission[1], permission[0]))","for i, permission in enumerate(self.builtin_roles[role.name]):
    result.add((permission[1], permission[0]))"
airflow,https://github.com/apache/airflow/tree/master/airflow/www/fab_security/manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/fab_security/manager.py,BaseSecurityManager,"def get_role_permissions(self, role) -> Set[Tuple[str, str]]:
    """"""Get all permissions for a certain role""""""
    result = set()
    if role.name in self.builtin_roles:
        for permission in self.builtin_roles[role.name]:
            result.add((permission[1], permission[0]))
    else:
        for permission in self.get_role_permissions_from_db(role.id):
            result.add((permission.action.name, permission.resource.name))
    return result","for permission in self.get_role_permissions_from_db(role.id):
    result.add((permission.action.name, permission.resource.name))","for i, permission in enumerate(self.get_role_permissions_from_db(role.id)):
    result.add((permission.action.name, permission.resource.name))"
WhatWaf,https://github.com/Ekultek/WhatWaf/tree/master/content/tampers/enclosebrackets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WhatWaf/content/tampers/enclosebrackets.py,,"def tamper(payload, **kwargs):
    payload = str(payload)
    to_enclose = string.digits
    if not any((i in list(payload) for i in to_enclose)):
        return payload
    retval = ''
    for char in payload:
        if char in to_enclose:
            retval += '[{}]'.format(char)
        else:
            retval += char
    return retval","for char in payload:
    if char in to_enclose:
        retval += '[{}]'.format(char)
    else:
        retval += char","for i,char in enumerate(payload):
    if char in to_enclose:
        retval += '[{}]'.format(char)
    else:
        retval += char"
docassemble,https://github.com/jhpyle/docassemble/tree/master/tests/features/steps/docassemble.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/tests/features/steps/docassemble.py,,"def select_nth_option(step, value, ordinal, label):
    try:
        elem = world.browser.find_element_by_id(world.browser.find_element_by_xpath('(//label[text()=""' + label + '""])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']').get_attribute('for'))
    except:
        label += '\xa0'
        elem = world.browser.find_element_by_id(world.browser.find_element_by_xpath('(//label[text()=""' + label + '""])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']').get_attribute('for'))
    found = False
    for option in elem.find_elements_by_tag_name('option'):
        if option.text == value:
            found = True
            option.click()
            break
    assert found","for option in elem.find_elements_by_tag_name('option'):
    if option.text == value:
        found = True
        option.click()
        break","for i, option in enumerate(elem.find_elements_by_tag_name('option')):
    if option.text == value:
        found = True
        option.click()
        break"
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/monitoradds.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/monitoradds.py,,"def delmod(bot, event, *args):
    """"""remove user id(s) from the whitelist of who can add to a hangout""""""
    if not bot.get_config_option('mods'):
        return
    mods = bot.get_config_option('mods')
    mods_new = []
    for mod in mods:
        if args[0] != mod:
            mods_new.append(mod)
    bot.config.set_by_path(['mods'], mods_new)
    bot.config.save()
    html_message = _('<i>Moderators updated: {} removed</i>')
    yield from bot.coro_send_message(event.conv, html_message.format(args[0]))","for mod in mods:
    if args[0] != mod:
        mods_new.append(mod)","for i, mod in enumerate(mods):
    if args[0] != mod:
        mods_new.append(mod)"
freeipa,https://github.com/freeipa/freeipa/tree/master/ipatests/test_webui/test_loginscreen.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_webui/test_loginscreen.py,TestLoginScreen,"def assert_form_equals(self, actual_form, expected_form):
    """"""
        Compare two forms
        """"""
    assert len(actual_form) == len(expected_form)
    for (act_row, exp_row) in zip(actual_form, expected_form):
        assert self.get_data_from_form_row(act_row) == exp_row","for (act_row, exp_row) in zip(actual_form, expected_form):
    assert self.get_data_from_form_row(act_row) == exp_row","for i, (act_row, exp_row) in enumerate(zip(actual_form, expected_form)):
    assert self.get_data_from_form_row(act_row) == exp_row"
binderhub,https://github.com/jupyterhub/binderhub/tree/master/helm-chart/binderhub/files/binderhub_config.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/binderhub/helm-chart/binderhub/files/binderhub_config.py,,"def get_value(key, default=None):
    """"""
    Find an item in values.yaml of a given name & return it

    get_value(""a.b.c"") returns values['a']['b']['c']
    """"""
    value = _load_values()
    for level in key.split('.'):
        if not isinstance(value, dict):
            return default
        if level not in value:
            return default
        else:
            value = value[level]
    return value","for level in key.split('.'):
    if not isinstance(value, dict):
        return default
    if level not in value:
        return default
    else:
        value = value[level]","for i, level in enumerate(key.split('.')):
    if not isinstance(value, dict):
        return default
    if level not in value:
        return default
    else:
        value = value[level]"
slither,https://github.com/crytic/slither/tree/master/slither/printers/call/call_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/slither/slither/printers/call/call_graph.py,PrinterCallGraph,"def output(self, filename):
    """"""
        Output the graph in filename
        Args:
            filename(string)
        """"""
    all_contracts_filename = ''
    if not filename.endswith('.dot'):
        all_contracts_filename = f'{filename}.all_contracts.call-graph.dot'
    if filename == '.dot':
        all_contracts_filename = 'all_contracts.dot'
    info = ''
    results = []
    with open(all_contracts_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {all_contracts_filename}\n'
        all_functionss = [compilation_unit.functions for compilation_unit in self.slither.compilation_units]
        all_functions = [item for sublist in all_functionss for item in sublist]
        all_functions_as_dict = {function.canonical_name: function for function in all_functions}
        content = '\n'.join(['strict digraph {'] + [_process_functions(all_functions_as_dict.values())] + ['}'])
        f.write(content)
        results.append((all_contracts_filename, content))
    for derived_contract in self.slither.contracts_derived:
        derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
        with open(derived_output_filename, 'w', encoding='utf8') as f:
            info += f'Call Graph: {derived_output_filename}\n'
            content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
            f.write(content)
            results.append((derived_output_filename, content))
    self.info(info)
    res = self.generate_output(info)
    for (filename_result, content) in results:
        res.add_file(filename_result, content)
    return res","for derived_contract in self.slither.contracts_derived:
    derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
    with open(derived_output_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {derived_output_filename}\n'
        content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
        f.write(content)
        results.append((derived_output_filename, content))","results = []
for i, derived_contract in enumerate(self.slither.contracts_derived):
    derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
    with open(derived_output_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {derived_output_filename}\n'
        content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
        f.write(content)
        results.append((derived_output_filename, content))"
sdc,https://github.com/IntelPython/sdc/tree/master/sdc/tests/indexes/test_int64_index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sdc/sdc/tests/indexes/test_int64_index.py,TestInt64Index,"def test_int64_index_iterator_2(self):

    def test_impl(index):
        res = []
        for label in index:
            if not label % 2:
                res.append(label)
        return res
    sdc_func = self.jit(test_impl)
    index = pd.Int64Index([5, 3, 2, 1, 7, 4])
    result = sdc_func(index)
    result_ref = test_impl(index)
    self.assertEqual(result, result_ref)","for label in index:
    if not label % 2:
        res.append(label)","for i,label in enumerate(index):
    if not i % 2:
        res.append(label)"
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for building in self.production_buildings:
    producer = building.get_component(Producer)
    for production in producer.get_productions():
        if not production.get_produced_resources():
            continue
        all_full = True
        for (resource_id, min_amount) in production.get_produced_resources().items():
            if production.inventory.get_free_space_for(resource_id) >= min_amount:
                all_full = False
                break
        if all_full and (not isinstance(building, Mine)):
            for resource_id in production.get_consumed_resources():
                if production.inventory.get_free_space_for(resource_id) > 0:
                    all_full = False
                    break
        if all_full:
            if not production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
        elif production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for i, building in enumerate(self.production_buildings):
    producer = building.get_component(Producer)
    for production in producer.get_productions():
        if not production.get_produced_resources():
            continue
        all_full = True
        for (resource_id, min_amount) in production.get_produced_resources().items():
            if production.inventory.get_free_space_for(resource_id) >= min_amount:
                all_full = False
                break
        if all_full and (not isinstance(building, Mine)):
            for resource_id in production.get_consumed_resources():
                if production.inventory.get_free_space_for(resource_id) > 0:
                    all_full = False
                    break
        if all_full:
            if not production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
        elif production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)"
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for production in producer.get_productions():
    if not production.get_produced_resources():
        continue
    all_full = True
    for (resource_id, min_amount) in production.get_produced_resources().items():
        if production.inventory.get_free_space_for(resource_id) >= min_amount:
            all_full = False
            break
    if all_full and (not isinstance(building, Mine)):
        for resource_id in production.get_consumed_resources():
            if production.inventory.get_free_space_for(resource_id) > 0:
                all_full = False
                break
    if all_full:
        if not production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
    elif production.is_paused():
        ToggleActive(producer, production).execute(self.land_manager.session)
        self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for i, production in enumerate(producer.get_productions()):
    if not production.get_produced_resources():
        continue
    all_full = True
    for (resource_id, min_amount) in production.get_produced_resources().items():
        if production.inventory.get_free_space_for(resource_id) >= min_amount:
            all_full = False
            break
    if all_full and (not isinstance(building, Mine)):
        for resource_id in production.get_consumed_resources():
            if production.inventory.get_free_space_for(resource_id) > 0:
                all_full = False
                break
    if all_full:
        if not production.is_paused():
            ToggleActive(producer, production).execute(self.land_manager.session)
            self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
    elif production.is_paused():
        ToggleActive(producer, production).execute(self.land_manager.session)
        self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)"
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unknown-horizons/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,"def manage_production(self):
    """"""Pauses and resumes production buildings when they have full input and output inventories.""""""
    for building in self.production_buildings:
        producer = building.get_component(Producer)
        for production in producer.get_productions():
            if not production.get_produced_resources():
                continue
            all_full = True
            for (resource_id, min_amount) in production.get_produced_resources().items():
                if production.inventory.get_free_space_for(resource_id) >= min_amount:
                    all_full = False
                    break
            if all_full and (not isinstance(building, Mine)):
                for resource_id in production.get_consumed_resources():
                    if production.inventory.get_free_space_for(resource_id) > 0:
                        all_full = False
                        break
            if all_full:
                if not production.is_paused():
                    ToggleActive(producer, production).execute(self.land_manager.session)
                    self.log.info('%s paused a production at %s/%d', self, building.name, building.worldid)
            elif production.is_paused():
                ToggleActive(producer, production).execute(self.land_manager.session)
                self.log.info('%s resumed a production at %s/%d', self, building.name, building.worldid)","for resource_id in production.get_consumed_resources():
    if production.inventory.get_free_space_for(resource_id) > 0:
        all_full = False
        break","for i, resource_id in enumerate(production.get_consumed_resources()):
    if production.inventory.get_free_space_for(resource_id) > 0:
        all_full = False
        break"
BLINK,https://github.com/facebookresearch/BLINK/tree/master/blink/biencoder/zeshel_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BLINK/blink/biencoder/zeshel_utils.py,Stats,"def __init__(self, top_k=1000):
    self.cnt = 0
    self.hits = []
    self.top_k = top_k
    self.rank = [1, 4, 8, 16, 32, 64, 100, 128, 256, 512]
    self.LEN = len(self.rank)
    for i in range(self.LEN):
        self.hits.append(0)","for i in range(self.LEN):
    self.hits.append(0)","for i,_ in enumerate(range(self.LEN)):
    self.hits.append(0)"
tfc,https://github.com/maqp/tfc/tree/master/tests/common/test_crypto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tfc/tests/common/test_crypto.py,TestArgon2KDF,"def test_argon2_cffi_using_the_official_command_line_utility(self) -> None:
    min_password_length = 1
    max_password_length = 127
    min_salt_length = 8
    min_parallelism = 1
    max_parallelism = multiprocessing.cpu_count()
    min_time_cost = 1
    min_memory_cost = 7
    min_key_length = 4
    max_salt_length = 128
    max_time_cost = 3
    max_memory_cost = 15
    max_key_length = 64
    sys_rand = random.SystemRandom()
    for _ in range(self.number_of_tests):
        len_password = sys_rand.randint(min_password_length, max_password_length)
        len_salt = sys_rand.randint(min_salt_length, max_salt_length)
        parallelism = sys_rand.randint(min_parallelism, max_parallelism)
        time_cost = sys_rand.randint(min_time_cost, max_time_cost)
        memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
        key_length = sys_rand.randint(min_key_length, max_key_length)
        password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
        salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
        output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
        key_test_vector = output.split('\n')[4].split('\t')[-1]
        purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
        self.assertEqual(purported_key, key_test_vector)","for _ in range(self.number_of_tests):
    len_password = sys_rand.randint(min_password_length, max_password_length)
    len_salt = sys_rand.randint(min_salt_length, max_salt_length)
    parallelism = sys_rand.randint(min_parallelism, max_parallelism)
    time_cost = sys_rand.randint(min_time_cost, max_time_cost)
    memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
    key_length = sys_rand.randint(min_key_length, max_key_length)
    password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
    salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
    output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
    key_test_vector = output.split('\n')[4].split('\t')[-1]
    purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
    self.assertEqual(purported_key, key_test_vector)","for i in range(self.number_of_tests):
    len_password = sys_rand.randint(min_password_length, max_password_length)
    len_salt = sys_rand.randint(min_salt_length, max_salt_length)
    parallelism = sys_rand.randint(min_parallelism, max_parallelism)
    time_cost = sys_rand.randint(min_time_cost, max_time_cost)
    memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
    key_length = sys_rand.randint(min_key_length, max_key_length)
    password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
    salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
    output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
    key_test_vector = output.split('\n')[4].split('\t')[-1]
    purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
    self.assertEqual(purported_key, key_test_vector)"
keras,https://github.com/keras-team/keras/tree/master/keras/layers/preprocessing/image_preprocessing_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras/keras/layers/preprocessing/image_preprocessing_test.py,RandomZoomTest,"def test_random_zoom_in_numeric(self):
    for dtype in (np.int64, np.float32):
        with testing_utils.use_gpu():
            input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
            layer = image_preprocessing.RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation='nearest')
            output_image = layer(np.expand_dims(input_image, axis=0))
            expected_output = np.asarray([[6, 7, 7, 8, 8], [11, 12, 12, 13, 13], [11, 12, 12, 13, 13], [16, 17, 17, 18, 18], [16, 17, 17, 18, 18]]).astype(dtype)
            expected_output = np.reshape(expected_output, (1, 5, 5, 1))
            self.assertAllEqual(expected_output, output_image)","for dtype in (np.int64, np.float32):
    with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([[6, 7, 7, 8, 8], [11, 12, 12, 13, 13], [11, 12, 12, 13, 13], [16, 17, 17, 18, 18], [16, 17, 17, 18, 18]]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)","for i,dtype in enumerate((np.int64, np.float32)):
    with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([[6, 7, 7, 8, 8], [11, 12, 12, 13, 13], [11, 12, 12, 13, 13], [16, 17, 17, 18, 18], [16, 17, 17, 18, 18]]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)"
Invertible-Image-Rescaling,https://github.com/pkuxmq/Invertible-Image-Rescaling/tree/master/codes/models/modules/discriminator_vgg_arch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Invertible-Image-Rescaling/codes/models/modules/discriminator_vgg_arch.py,VGGFeatureExtractor,"def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, device=torch.device('cpu')):
    super(VGGFeatureExtractor, self).__init__()
    self.use_input_norm = use_input_norm
    if use_bn:
        model = torchvision.models.vgg19_bn(pretrained=True)
    else:
        model = torchvision.models.vgg19(pretrained=True)
    if self.use_input_norm:
        mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)
        self.register_buffer('mean', mean)
        self.register_buffer('std', std)
    self.features = nn.Sequential(*list(model.features.children())[:feature_layer + 1])
    for (k, v) in self.features.named_parameters():
        v.requires_grad = False","for (k, v) in self.features.named_parameters():
    v.requires_grad = False","for i, (k, v) in enumerate(self.features.named_parameters()):
    v.requires_grad = False"
transformers,https://github.com/huggingface/transformers/tree/master/examples/flax/question-answering/run_qa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/examples/flax/question-answering/run_qa.py,,"def eval_data_collator(dataset: Dataset, batch_size: int):
    """"""Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.""""""
    batch_idx = np.arange(len(dataset))
    steps_per_epoch = math.ceil(len(dataset) / batch_size)
    batch_idx = np.array_split(batch_idx, steps_per_epoch)
    for idx in batch_idx:
        batch = dataset[idx]
        batch = {k: np.array(v) for (k, v) in batch.items()}
        yield batch","for idx in batch_idx:
    batch = dataset[idx]
    batch = {k: np.array(v) for (k, v) in batch.items()}
    yield batch","for i,idx in enumerate(batch_idx):
    batch = dataset[idx]
    batch = {k: np.array(v) for (k, v) in batch.items()}
    yield batch"
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/dygraph/prune/filter_pruner.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/paddleslim/dygraph/prune/filter_pruner.py,FilterPruner,"def prune_var(self, var_name, pruned_axis, pruned_ratio, apply='impretive'):
    """"""
        Pruning a variable.
        Parameters:
            var_name(str): The name of variable.
            pruned_axis(int): The axis to be pruned. For convolution with format [out_c, in_c, k, k],
                             'axis=0' means pruning filters.
            pruned_ratio(float): The ratio of pruned values in one variable.
            apply(str): How to apply pruning plan to graph. It can be 'impretive', 'lazy' or None. None
                        means just returning an instance of 'PruningPlan' but not applying it to graph.

        Returns:
            plan: An instance of PruningPlan that can be applied on model by calling 'plan.apply(model)'.

        """"""
    pruned_axis = pruned_axis[0] if isinstance(pruned_axis, list) else pruned_axis
    assert isinstance(pruned_axis, int)
    if var_name in self.skip_vars:
        _logger.warn(f'{var_name} is skiped beacause it is not supported for pruning directly.')
        return
    collection = self.collections.find_collection_by_master(var_name, pruned_axis)
    plan = PruningPlan(self.model.full_name)
    if collection is None:
        _logger.debug(f""Can not find collection with master ['name': {var_name}, 'axis': {pruned_axis}]"")
        return plan
    _logger.info(f'Pruning variable [{var_name}] and its relatives {list(collection.variables())}')
    mask = self.cal_mask(pruned_ratio, collection, num_head=self.num_head)
    for _detail in collection.all_pruning_details():
        src_mask = copy.deepcopy(mask)
        var_shape = _detail.var.shape()
        for tran in _detail.transform:
            src_mask = self._transform_mask(src_mask, tran)
        current_mask = src_mask
        groups = _detail.op.attr('groups')
        if groups is None or groups == 1:
            assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
        plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))
    if apply == 'lazy':
        plan.apply(self.model, lazy=True)
    elif apply == 'impretive':
        plan.apply(self.model, lazy=False, opt=self.opt, prune_type=self.prune_type)
    return plan","for _detail in collection.all_pruning_details():
    src_mask = copy.deepcopy(mask)
    var_shape = _detail.var.shape()
    for tran in _detail.transform:
        src_mask = self._transform_mask(src_mask, tran)
    current_mask = src_mask
    groups = _detail.op.attr('groups')
    if groups is None or groups == 1:
        assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
    plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))","for i,_detail in enumerate(collection.all_pruning_details()):
    src_mask = copy.deepcopy(mask)
    var_shape = _detail.var.shape()
    for tran in _detail.transform:
        src_mask = self._transform_mask(src_mask, tran)
    current_mask = src_mask
    groups = _detail.op.attr('groups')
    if groups is None or groups == 1:
        assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
    plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))"
poetry,https://github.com/sheepzh/poetry/tree/master/src/poetry/utils/setup_reader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poetry/src/poetry/utils/setup_reader.py,SetupReader,"def _find_install_requires(self, call: ast.Call, body: Iterable[Any]) -> List[str]:
    install_requires = []
    value = self._find_in_call(call, 'install_requires')
    if value is None:
        kwargs = self._find_call_kwargs(call)
        if kwargs is None or not isinstance(kwargs, ast.Name):
            return install_requires
        variable = self._find_variable_in_body(body, kwargs.id)
        if not isinstance(variable, (ast.Dict, ast.Call)):
            return install_requires
        if isinstance(variable, ast.Call):
            if not isinstance(variable.func, ast.Name):
                return install_requires
            if variable.func.id != 'dict':
                return install_requires
            value = self._find_in_call(variable, 'install_requires')
        else:
            value = self._find_in_dict(variable, 'install_requires')
    if value is None:
        return install_requires
    if isinstance(value, ast.List):
        for el in value.elts:
            install_requires.append(el.s)
    elif isinstance(value, ast.Name):
        variable = self._find_variable_in_body(body, value.id)
        if variable is not None and isinstance(variable, ast.List):
            for el in variable.elts:
                install_requires.append(el.s)
    return install_requires","for el in value.elts:
    install_requires.append(el.s)","for i, el in enumerate(value.elts):
    install_requires.append(el.s)"
poetry,https://github.com/sheepzh/poetry/tree/master/src/poetry/utils/setup_reader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poetry/src/poetry/utils/setup_reader.py,SetupReader,"def _find_install_requires(self, call: ast.Call, body: Iterable[Any]) -> List[str]:
    install_requires = []
    value = self._find_in_call(call, 'install_requires')
    if value is None:
        kwargs = self._find_call_kwargs(call)
        if kwargs is None or not isinstance(kwargs, ast.Name):
            return install_requires
        variable = self._find_variable_in_body(body, kwargs.id)
        if not isinstance(variable, (ast.Dict, ast.Call)):
            return install_requires
        if isinstance(variable, ast.Call):
            if not isinstance(variable.func, ast.Name):
                return install_requires
            if variable.func.id != 'dict':
                return install_requires
            value = self._find_in_call(variable, 'install_requires')
        else:
            value = self._find_in_dict(variable, 'install_requires')
    if value is None:
        return install_requires
    if isinstance(value, ast.List):
        for el in value.elts:
            install_requires.append(el.s)
    elif isinstance(value, ast.Name):
        variable = self._find_variable_in_body(body, value.id)
        if variable is not None and isinstance(variable, ast.List):
            for el in variable.elts:
                install_requires.append(el.s)
    return install_requires","for el in variable.elts:
    install_requires.append(el.s)","for i, el in enumerate(variable.elts):
    install_requires.append(el.s)"
angr,https://github.com/angr/angr/tree/master/angr/procedures/msvcr/_initterm.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/procedures/msvcr/_initterm.py,_initterm,"def get_callbacks(self, fp_a, fp_z):
    callbacks = []
    table_size = fp_z - fp_a + self.state.arch.bytes
    for addr in reversed(self.state.memory.load(fp_a, table_size, endness=self.state.arch.memory_endness).chop(self.state.arch.bits)):
        addr = self.state.solver.eval(addr)
        if addr != 0:
            callbacks.append(addr)
    return callbacks","for addr in reversed(self.state.memory.load(fp_a, table_size, endness=self.state.arch.memory_endness).chop(self.state.arch.bits)):
    addr = self.state.solver.eval(addr)
    if addr != 0:
        callbacks.append(addr)","for i, addr in enumerate(reversed(self.state.memory.load(fp_a, table_size, endness=self.state.arch.memory_endness).chop(self.state.arch.bits))):
    addr = self.state.solver.eval(addr)
    if addr != 0:
        callbacks.append(addr)"
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/iot/custom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/iot/custom.py,,"def iot_hub_get_quota_metrics(client, hub_name, resource_group_name=None):
    resource_group_name = _ensure_hub_resource_group_name(client, resource_group_name, hub_name)
    iotHubQuotaMetricCollection = []
    iotHubQuotaMetricCollection.extend(client.iot_hub_resource.get_quota_metrics(resource_group_name, hub_name))
    for quotaMetric in iotHubQuotaMetricCollection:
        if quotaMetric.name == 'TotalDeviceCount':
            quotaMetric.max_value = 'Unlimited'
    return iotHubQuotaMetricCollection","for quotaMetric in iotHubQuotaMetricCollection:
    if quotaMetric.name == 'TotalDeviceCount':
        quotaMetric.max_value = 'Unlimited'","for i, quotaMetric in enumerate(iotHubQuotaMetricCollection):
    if quotaMetric.name == 'TotalDeviceCount':
        iotHubQuotaMetricCollection[i].max_value = 'Unlimited'"
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/dns_data_management.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/dns_data_management.py,IPASystemRecords,"def _get_location_dns_records_for_server(self, zone_obj, hostname, locations, roles=None, include_master_role=True, include_kerberos_realm=True):
    server = self.servers_data[hostname]
    if roles:
        eff_roles = server['roles'] & roles
    else:
        eff_roles = server['roles']
    hostname_abs = DNSName(hostname).make_absolute()
    for location in locations:
        if location == self.servers_data[hostname]['location']:
            priority = self.PRIORITY_HIGH
        else:
            priority = self.PRIORITY_LOW
        if include_kerberos_realm:
            self.__add_kerberos_txt_rec(zone_obj, location)
        if include_master_role:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
            self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
        if 'AD trust controller' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
        if 'NTP server' in eff_roles:
            self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)
    return zone_obj","for location in locations:
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)","for i, location in enumerate(locations):
    if location == self.servers_data[hostname]['location']:
        priority = self.PRIORITY_HIGH
    else:
        priority = self.PRIORITY_LOW
    if include_kerberos_realm:
        self.__add_kerberos_txt_rec(zone_obj, location)
    if include_master_role:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_SRV_REC, weight=server['weight'], priority=priority, location=location)
        self.__add_uri_records(zone_obj, hostname_abs, IPA_DEFAULT_MASTER_URI_REC, weight=server['weight'], priority=priority, location=location)
    if 'AD trust controller' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_ADTRUST_SRV_REC, weight=server['weight'], priority=priority, location=location)
    if 'NTP server' in eff_roles:
        self.__add_srv_records(zone_obj, hostname_abs, IPA_DEFAULT_NTP_SRV_REC, weight=server['weight'], priority=priority, location=location)"
swift,https://github.com/openstack/swift/tree/master/test/unit/common/middleware/s3api/test_service.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/common/middleware/s3api/test_service.py,TestS3ApiService,"def test_service_GET_with_blind_resource(self):
    buckets = (('apple', 1, 200), ('orange', 3, 430), ('apple+segment', 1, 200))
    expected = buckets[:-1]
    bucket_list = create_bucket_list_json(buckets)
    self.swift.register('GET', '/v1/AUTH_test', swob.HTTPOk, {}, bucket_list)
    req = Request.blank('/', environ={'REQUEST_METHOD': 'GET'}, headers={'Authorization': 'AWS test:tester:hmac', 'Date': self.get_date_header()})
    (status, headers, body) = self.call_s3api(req)
    self.assertEqual(status.split()[0], '200')
    elem = fromstring(body, 'ListAllMyBucketsResult')
    all_buckets = elem.find('./Buckets')
    buckets = all_buckets.iterchildren('Bucket')
    listing = list(list(buckets)[0])
    self.assertEqual(len(listing), 2)
    names = []
    for b in all_buckets.iterchildren('Bucket'):
        names.append(b.find('./Name').text)
    self.assertEqual(len(names), len(expected))
    for i in expected:
        self.assertIn(i[0], names)","for b in all_buckets.iterchildren('Bucket'):
    names.append(b.find('./Name').text)","for i,b in enumerate(all_buckets.iterchildren('Bucket')):
    names.append(b.find('./Name').text)"
swift,https://github.com/openstack/swift/tree/master/test/unit/common/middleware/s3api/test_service.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/common/middleware/s3api/test_service.py,TestS3ApiService,"def test_service_GET_with_blind_resource(self):
    buckets = (('apple', 1, 200), ('orange', 3, 430), ('apple+segment', 1, 200))
    expected = buckets[:-1]
    bucket_list = create_bucket_list_json(buckets)
    self.swift.register('GET', '/v1/AUTH_test', swob.HTTPOk, {}, bucket_list)
    req = Request.blank('/', environ={'REQUEST_METHOD': 'GET'}, headers={'Authorization': 'AWS test:tester:hmac', 'Date': self.get_date_header()})
    (status, headers, body) = self.call_s3api(req)
    self.assertEqual(status.split()[0], '200')
    elem = fromstring(body, 'ListAllMyBucketsResult')
    all_buckets = elem.find('./Buckets')
    buckets = all_buckets.iterchildren('Bucket')
    listing = list(list(buckets)[0])
    self.assertEqual(len(listing), 2)
    names = []
    for b in all_buckets.iterchildren('Bucket'):
        names.append(b.find('./Name').text)
    self.assertEqual(len(names), len(expected))
    for i in expected:
        self.assertIn(i[0], names)","for i in expected:
    self.assertIn(i[0], names)","for i, item in enumerate(expected):
    self.assertIn(item[0], names)"
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str","for index in range(service_count):
    service = services[index]
    index += 1
    commandData = ''
    if service.upper() == 'ZOOKEEPER':
        commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
    else:
        commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
    payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})","for index, service in enumerate(services):
    index += 1
    commandData = ''
    if service.upper() == 'ZOOKEEPER':
        commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
    else:
        commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
    payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})"
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_fail_saving_nonexistent_feature(self):
    features = [(FeatureType.DATA, 'nonexistent')]
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs, self.assertRaises(ValueError):
            self.eopatch.save('/', filesystem=temp_fs, features=features)","for fs_loader in self.filesystem_loaders:
    with fs_loader() as temp_fs, self.assertRaises(ValueError):
        self.eopatch.save('/', filesystem=temp_fs, features=features)","for i, fs_loader in enumerate(self.filesystem_loaders):
    with fs_loader() as temp_fs, self.assertRaises(ValueError):
        self.eopatch.save('/', filesystem=temp_fs, features=features)"
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/experiment/_main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/experiment/_main.py,Experiment,"def build_symmetrization_memory_maps(self, qubits: Sequence[int], label: str='symmetrization') -> List[Dict[str, List[float]]]:
    """"""
        Build a list of memory maps to be used in a program that is trying to perform readout
        symmetrization via parametric compilation. For example, if we have the following program:

            RX(symmetrization[0]) 0
            RX(symmetrization[1]) 1
            MEASURE 0 ro[0]
            MEASURE 1 ro[1]

        We can perform exhaustive readout symmetrization on our two qubits by providing the four
        following memory maps, and then appropriately flipping the resultant bitstrings:

            {'symmetrization': [0.0, 0.0]} -> XOR results with [0,0]
            {'symmetrization': [0.0, pi]}  -> XOR results with [0,1]
            {'symmetrization': [pi, 0.0]}  -> XOR results with [1,0]
            {'symmetrization': [pi, pi]}   -> XOR results with [1,1]

        :param qubits: List of qubits to symmetrize readout for.
        :param label: Name of the declared memory region. Defaults to ""symmetrization"".
        :return: List of memory maps that performs the desired level of symmetrization.
        """"""
    num_meas_registers = len(self.get_meas_qubits())
    symm_registers = self.get_meas_registers(qubits)
    if self.symmetrization == SymmetrizationLevel.NONE:
        return [{}]
    if self.symmetrization != SymmetrizationLevel.EXHAUSTIVE:
        raise ValueError('We only support exhaustive symmetrization for now.')
    import numpy as np
    import itertools
    assignments = itertools.product(np.array([0, np.pi]), repeat=len(symm_registers))
    memory_maps = []
    for a in assignments:
        zeros = np.zeros(num_meas_registers)
        for (idx, r) in enumerate(symm_registers):
            zeros[r] = a[idx]
        memory_maps.append({f'{label}': list(zeros)})
    return memory_maps","for a in assignments:
    zeros = np.zeros(num_meas_registers)
    for (idx, r) in enumerate(symm_registers):
        zeros[r] = a[idx]
    memory_maps.append({f'{label}': list(zeros)})","for i,a in enumerate(assignments):
    zeros = np.zeros(num_meas_registers)
    for (idx, r) in enumerate(symm_registers):
        zeros[r] = a[idx]
    memory_maps.append({f'{label}': list(zeros)})"
nova,https://github.com/openstack/nova/tree/master/nova/virt/libvirt/driver.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/libvirt/driver.py,LibvirtDriver,"def _reattach_instance_vifs(self, context, instance, network_info):
    guest = self._host.get_guest(instance)
    guest_interfaces = guest.get_interfaces()
    if len(guest_interfaces) < len(network_info):
        direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
        for vif in network_info:
            if vif['vnic_type'] in direct_vnics:
                LOG.info('Attaching vif %s to instance %s', vif['id'], instance.id)
                self.attach_interface(context, instance, instance.image_meta, vif)","for vif in network_info:
    if vif['vnic_type'] in direct_vnics:
        LOG.info('Attaching vif %s to instance %s', vif['id'], instance.id)
        self.attach_interface(context, instance, instance.image_meta, vif)","for i,vif in enumerate(network_info):
    if vif['vnic_type'] in direct_vnics:
        LOG.info('Attaching vif %s to instance %s', vif['id'], instance.id)
        self.attach_interface(context, instance, instance.image_meta, vif)"
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for (component_name, component_specs) in rte_components.items():
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","for i, (component_name, component_specs) in enumerate(rte_components.items()):
    generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
    relative_icon_data_url = component_specs['icon_data_url'][1:]
    defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
    self.assertEqual(generated_image_filepath, defined_image_filepath)
    with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
        img_data = f.read()
        (width, height) = struct.unpack('>LL', img_data[16:24])
        self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
        self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)"
heatmap,https://github.com/sethoscope/heatmap/tree/master//heatmap.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/heatmap//heatmap.py,AppendingMatrix,"def reduce(decay, values):
    """"""
        Returns a weighted sum of the values, where weight N is
        pow(decay,N).  This means the largest value counts fully, but
        additional values have diminishing contributions. decay=0 makes
        the reduction equivalent to max(), which makes each data point
        visible, but says nothing about their relative magnitude.
        decay=1 makes this like sum(), which makes the relative
        magnitude of the points more visible, but could make smaller
        values hard to see.  Experiment with values between 0 and 1.
        Values outside that range will give weird results.
        """"""
    weight = 1.0
    total = 0.0
    values.sort(reverse=True)
    for value in values:
        total += value * weight
        weight *= decay
    return total","for value in values:
    total += value * weight
    weight *= decay","for i,value in enumerate(values):
    total += value * weight
    weight *= decay"
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code","for x in term:
    if x != str2[-1]:
        str2 = str2 + x","for i,x in enumerate(term):
    if x != str2[-1]:
        str2 = str2 + x"
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code","for x in str2[1:]:
    if re.search('[^aeiou]', x):
        str3 = str3 + x","for i,x in enumerate(str2[1:]):
    if re.search('[^aeiou]', x):
        str3 = str3 + x"
videos,https://github.com/3b1b/videos/tree/master/_2017/waves.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/waves.py,EMWave,"def __init__(self, **kwargs):
    digest_config(self, kwargs)
    if not all(self.propogation_direction == RIGHT):
        self.matrix_transform = np.dot(z_to_vector(self.propogation_direction), np.linalg.inv(z_to_vector(RIGHT)))
    else:
        self.matrix_transform = None
    vector_oscillations = []
    self.E_vects = VGroup()
    self.M_vects = VGroup()
    self.A_vect = np.array(self.A_vect) / get_norm(self.A_vect)
    self.A_vect *= self.amplitude
    for alpha in np.linspace(0, 1, self.n_vectors):
        tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
        phase = -alpha * self.length * self.wave_number
        kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
        E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
        M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
        vector_oscillations += [E_ov, M_ov]
        self.E_vects.add(E_ov.vector)
        self.M_vects.add(M_ov.vector)
    ContinualAnimationGroup.__init__(self, *vector_oscillations)","for alpha in np.linspace(0, 1, self.n_vectors):
    tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
    phase = -alpha * self.length * self.wave_number
    kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
    E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
    M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
    vector_oscillations += [E_ov, M_ov]
    self.E_vects.add(E_ov.vector)
    self.M_vects.add(M_ov.vector)","for i,alpha in enumerate(np.linspace(0, 1, self.n_vectors)):
    tail = interpolate(ORIGIN, self.length * RIGHT, alpha)
    phase = -alpha * self.length * self.wave_number
    kwargs = {'phi_vect': np.array(self.phi_vect) + phase, 'frequency': self.frequency, 'tail': np.array(tail)}
    E_ov = OscillatingVector(Vector(OUT, color=E_COLOR, normal_vector=UP), A_vect=self.A_vect, **kwargs)
    M_ov = OscillatingVector(Vector(UP, color=M_COLOR, normal_vector=OUT), A_vect=rotate_vector(self.A_vect, np.pi / 2, RIGHT), **kwargs)
    vector_oscillations += [E_ov, M_ov]
    self.E_vects.add(E_ov.vector)
    self.M_vects.add(M_ov.vector)"
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr","for var in self._variables:
    if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
        if var.name is not None:
            continue
        if var.ident.startswith('iarg'):
            var.name = 'arg_%x' % var.offset
        else:
            var.name = 's_%x' % -var.offset
    elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
        if var.name is not None:
            continue
        var.name = var.ident
    elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
        if var.name is not None:
            continue
        if labels is not None and var.addr in labels:
            var.name = labels[var.addr]
            if '@@' in var.name:
                var.name = var.name[:var.name.index('@@')]
        elif isinstance(var.addr, int):
            var.name = 'g_%x' % var.addr
        elif var.ident is not None:
            var.name = var.ident
        else:
            var.name = 'g_%s' % var.addr","for i,var in enumerate(self._variables):
    if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
        if var.name is not None:
            continue
        if var.ident.startswith('iarg'):
            var.name = 'arg_%x' % var.offset
        else:
            var.name = 's_%x' % -var.offset
    elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
        if var.name is not None:
            continue
        var.name = var.ident
    elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
        if var.name is not None:
            continue
        if labels is not None and var.addr in labels:
            var.name = labels[var.addr]
            if '@@' in var.name:
                var.name = var.name[:var.name.index('@@')]
        elif isinstance(var.addr, int):
            var.name = 'g_%x' % var.addr
        elif var.ident is not None:
            var.name = var.ident
        else:
            var.name = 'g_%s' % var.addr"
pyray,https://github.com/ryu577/pyray/tree/master/pyray/shapes/twod/plot.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyray/pyray/shapes/twod/plot.py,Canvas,"def draw_grid_s(draw, r=np.eye(2), scale=64, origin=np.array([8, 8]), im_size=np.array([1024, 1024])):
    lo_range = -scale
    hi_range = im_size[0] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
        pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)
    hi_range = im_size[1] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
        pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for i in np.arange(lo_range, hi_range, scale):
    pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
    pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for idx, i in enumerate(np.arange(lo_range, hi_range, scale)):
    pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
    pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)"
pyray,https://github.com/ryu577/pyray/tree/master/pyray/shapes/twod/plot.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyray/pyray/shapes/twod/plot.py,Canvas,"def draw_grid_s(draw, r=np.eye(2), scale=64, origin=np.array([8, 8]), im_size=np.array([1024, 1024])):
    lo_range = -scale
    hi_range = im_size[0] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([i, lo_range]) - origin) + origin
        pt2 = np.dot(r, np.array([i, hi_range]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)
    hi_range = im_size[1] + 3 * scale
    for i in np.arange(lo_range, hi_range, scale):
        pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
        pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
        draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for i in np.arange(lo_range, hi_range, scale):
    pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
    pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)","for idx, i in enumerate(np.arange(lo_range, hi_range, scale)):
    pt1 = np.dot(r, np.array([lo_range, i]) - origin) + origin
    pt2 = np.dot(r, np.array([hi_range, i]) - origin) + origin
    draw.line((pt1[0], pt1[1], pt2[0], pt2[1]), fill=(120, 120, 120, 120), width=2)"
nova,https://github.com/openstack/nova/tree/master/nova/virt/hyperv/imagecache.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/hyperv/imagecache.py,ImageCache,"def _age_and_verify_cached_images(self, context, all_instances, base_dir):
    for img in self.originals:
        if img in self.used_images:
            self._update_image_timestamp(img)
        elif CONF.image_cache.remove_unused_base_images:
            self._remove_if_old_image(img)","for img in self.originals:
    if img in self.used_images:
        self._update_image_timestamp(img)
    elif CONF.image_cache.remove_unused_base_images:
        self._remove_if_old_image(img)","for i,img in enumerate(self.originals):
    if img in self.used_images:
        self._update_image_timestamp(img)
    elif CONF.image_cache.remove_unused_base_images:
        self._remove_if_old_image(img)"
MeshCNN,https://github.com/ranahanocka/MeshCNN/tree/master/util/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MeshCNN/util/util.py,,"def print_network(net):
    """"""Print the total number of parameters in the network
    Parameters:
        network
    """"""
    print('---------- Network initialized -------------')
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print('[Network] Total number of parameters : %.3f M' % (num_params / 1000000.0))
    print('-----------------------------------------------')","for param in net.parameters():
    num_params += param.numel()","for i, param in enumerate(net.parameters()):
    num_params += param.numel()"
redash,https://github.com/getredash/redash/tree/master/migrations/versions/d7d747033183_encrypt_alert_destinations.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redash/migrations/versions/d7d747033183_encrypt_alert_destinations.py,,"def upgrade():
    op.add_column('notification_destinations', sa.Column('encrypted_options', postgresql.BYTEA(), nullable=True))
    notification_destinations = table('notification_destinations', sa.Column('id', key_type('NotificationDestination'), primary_key=True), sa.Column('encrypted_options', ConfigurationContainer.as_mutable(EncryptedConfiguration(sa.Text, settings.DATASOURCE_SECRET_KEY, FernetEngine))), sa.Column('options', ConfigurationContainer.as_mutable(Configuration)))
    conn = op.get_bind()
    for dest in conn.execute(notification_destinations.select()):
        conn.execute(notification_destinations.update().where(notification_destinations.c.id == dest.id).values(encrypted_options=dest.options))
    op.drop_column('notification_destinations', 'options')
    op.alter_column('notification_destinations', 'encrypted_options', nullable=False)","for dest in conn.execute(notification_destinations.select()):
    conn.execute(notification_destinations.update().where(notification_destinations.c.id == dest.id).values(encrypted_options=dest.options))","for i, dest in enumerate(conn.execute(notification_destinations.select())):
    conn.execute(notification_destinations.update().where(notification_destinations.c.id == dest.id).values(encrypted_options=dest.options))"
PathPlanning,https://github.com/zhm-real/PathPlanning/tree/master/Sampling_based_Planning/rrt_2D/rrt_star.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPlanning/Sampling_based_Planning/rrt_2D/rrt_star.py,RrtStar,"def rewire(self, node_new, neighbor_index):
    for i in neighbor_index:
        node_neighbor = self.vertex[i]
        if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
            node_neighbor.parent = node_new","for i in neighbor_index:
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new","for index, i in enumerate(neighbor_index):
    node_neighbor = self.vertex[i]
    if self.cost(node_neighbor) > self.get_new_cost(node_new, node_neighbor):
        node_neighbor.parent = node_new"
easytrader,https://github.com/shidenggui/easytrader/tree/master/easytrader/xq_follower.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/easytrader/easytrader/xq_follower.py,XueQiuFollower,"def project_transactions(self, transactions, assets):
    for transaction in transactions:
        weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
        initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
        transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
        transaction['stock_code'] = transaction['stock_symbol'].lower()
        transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
        transaction['amount'] = int(round(initial_amount, -2))
        if transaction['action'] == 'sell' and self._adjust_sell:
            transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])","for transaction in transactions:
    weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
    initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
    transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
    transaction['stock_code'] = transaction['stock_symbol'].lower()
    transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
    transaction['amount'] = int(round(initial_amount, -2))
    if transaction['action'] == 'sell' and self._adjust_sell:
        transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])","for i, transaction in enumerate(transactions):
    weight_diff = self.none_to_zero(transaction['weight']) - self.none_to_zero(transaction['prev_weight'])
    initial_amount = abs(weight_diff) / 100 * assets / transaction['price']
    transaction['datetime'] = datetime.fromtimestamp(transaction['created_at'] // 1000)
    transaction['stock_code'] = transaction['stock_symbol'].lower()
    transaction['action'] = 'buy' if weight_diff > 0 else 'sell'
    transaction['amount'] = int(round(initial_amount, -2))
    if transaction['action'] == 'sell' and self._adjust_sell:
        transaction['amount'] = self._adjust_sell_amount(transaction['stock_code'], transaction['amount'])"
FastBERT,https://github.com/autoliuweijie/FastBERT/tree/master/pypi/fastbert/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FastBERT/pypi/fastbert/utils.py,,"def load_hyperparam(config_path, file_dir=None, args=None):
    with open(config_path, 'r', encoding='utf-8') as f:
        param = json.load(f)
        for (key, value) in param.items():
            if isinstance(key, str) and key.endswith('_path'):
                if isinstance(value, str) and value.endswith('.bin'):
                    param[key] = os.path.join(FASTBERT_HOME_DIR, value)
                else:
                    param[key] = os.path.join(file_dir, value)
    if args is None:
        args_dict = {}
    else:
        args_dict = vars(args)
    args_dict.update(param)
    args = Namespace(**args_dict)
    return args","for (key, value) in param.items():
    if isinstance(key, str) and key.endswith('_path'):
        if isinstance(value, str) and value.endswith('.bin'):
            param[key] = os.path.join(FASTBERT_HOME_DIR, value)
        else:
            param[key] = os.path.join(file_dir, value)","for i, (key, value) in enumerate(param.items()):
    if isinstance(key, str) and key.endswith('_path'):
        if isinstance(value, str) and value.endswith('.bin'):
            param[key] = os.path.join(FASTBERT_HOME_DIR, value)
        else:
            param[key] = os.path.join(file_dir, value)"
sunpy,https://github.com/sunpy/sunpy/tree/master/examples/time_series/timeseries_peak_finding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/examples/time_series/timeseries_peak_finding.py,,"def findpeaks(series, DELTA):
    """"""
    Finds extrema in a pandas series data.

    Parameters
    ----------
    series : `pandas.Series`
        The data series from which we need to find extrema.

    DELTA : `float`
        The minimum difference between data values that defines a peak.

    Returns
    -------
    minpeaks, maxpeaks : `list`
        Lists consisting of pos, val pairs for both local minima points and
        local maxima points.
    """"""
    (mn, mx) = (np.Inf, -np.Inf)
    minpeaks = []
    maxpeaks = []
    lookformax = True
    start = True
    for (time_pos, value) in series.iteritems():
        if value > mx:
            mx = value
            mxpos = time_pos
        if value < mn:
            mn = value
            mnpos = time_pos
        if lookformax:
            if value < mx - DELTA:
                maxpeaks.append((mxpos, mx))
                mn = value
                mnpos = time_pos
                lookformax = False
            elif start:
                minpeaks.append((mnpos, mn))
                mx = value
                mxpos = time_pos
                start = False
        elif value > mn + DELTA:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            lookformax = True
    if value > mn + DELTA:
        maxpeaks.append((mxpos, mx))
    elif value < mx - DELTA:
        minpeaks.append((mnpos, mn))
    return (minpeaks, maxpeaks)","for (time_pos, value) in series.iteritems():
    if value > mx:
        mx = value
        mxpos = time_pos
    if value < mn:
        mn = value
        mnpos = time_pos
    if lookformax:
        if value < mx - DELTA:
            maxpeaks.append((mxpos, mx))
            mn = value
            mnpos = time_pos
            lookformax = False
        elif start:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            start = False
    elif value > mn + DELTA:
        minpeaks.append((mnpos, mn))
        mx = value
        mxpos = time_pos
        lookformax = True","for i, (time_pos, value) in enumerate(series.iteritems()):
    if value > mx:
        mx = value
        mxpos = time_pos
    if value < mn:
        mn = value
        mnpos = time_pos
    if lookformax:
        if value < mx - DELTA:
            maxpeaks.append((mxpos, mx))
            mn = value
            mnpos = time_pos
            lookformax = False
        elif start:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            start = False
    elif value > mn + DELTA:
        minpeaks.append((mnpos, mn))
        mx = value
        mxpos = time_pos
        lookformax = True"
projects,https://github.com/explosion/projects/tree/master/tutorials/ner_fashion_brands/scripts/visualize_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/projects/tutorials/ner_fashion_brands/scripts/visualize_data.py,,"def load_data(filepath):
    examples = list(srsly.read_jsonl(filepath))
    rows = []
    n_total_ents = 0
    n_no_ents = 0
    labels = set()
    for eg in examples:
        row = {'text': eg['text'], 'ents': eg.get('spans', [])}
        n_total_ents += len(row['ents'])
        if not row['ents']:
            n_no_ents += 1
        labels.update([span['label'] for span in row['ents']])
        rows.append(row)
    return (rows, labels, n_total_ents, n_no_ents)","for eg in examples:
    row = {'text': eg['text'], 'ents': eg.get('spans', [])}
    n_total_ents += len(row['ents'])
    if not row['ents']:
        n_no_ents += 1
    labels.update([span['label'] for span in row['ents']])
    rows.append(row)","for i,eg in enumerate(examples):
    row = {'text': eg['text'], 'ents': eg.get('spans', [])}
    n_total_ents += len(row['ents'])
    if not row['ents']:
        n_no_ents += 1
    labels.update([span['label'] for span in row['ents']])
    rows.append(row)"
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)","for _ in batches:
    if self._should_stop(supervisor):
        return
    results = session.run(train_fetches, train_feed_dict)
    train_loss += results['loss']
    if self.compute_summaries:
        supervisor.summary_computed(session, results['summaries'], results['step'])","for i,_ in enumerate(batches):
    if self._should_stop(supervisor):
        return
    results = session.run(train_fetches, train_feed_dict)
    train_loss += results['loss']
    if self.compute_summaries:
        supervisor.summary_computed(session, results['summaries'], results['step'])"
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/unifi/services.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/unifi/services.py,,"def async_unload_services(hass) -> None:
    """"""Unload UniFi Network services.""""""
    for service in SUPPORTED_SERVICES:
        hass.services.async_remove(UNIFI_DOMAIN, service)","for service in SUPPORTED_SERVICES:
    hass.services.async_remove(UNIFI_DOMAIN, service)","for i, service in enumerate(SUPPORTED_SERVICES):
    hass.services.async_remove(UNIFI_DOMAIN, SUPPORTED_SERVICES[i])"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/ToolsPage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/ToolsPage.py,Tools,"def __init__(self, gcode):
    self.gcode = gcode
    self.inches = False
    self.digits = 4
    self.active = StringVar()
    self.tools = {}
    self.buttons = {}
    self.widget = {}
    self.listbox = None
    for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
        tool = cls(self)
        self.addTool(tool)
    for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
        (name, ext) = os.path.splitext(os.path.basename(f))
        try:
            exec(f'import {name}')
            tool = eval(f'{name}.Tool(self)')
            self.addTool(tool)
        except (ImportError, AttributeError):
            (typ, val, tb) = sys.exc_info()
            traceback.print_exception(typ, val, tb)","for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
    tool = cls(self)
    self.addTool(tool)","for i, cls in enumerate([Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]):
    tool = cls(self)
    self.addTool(tool)"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/ToolsPage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/ToolsPage.py,Tools,"def __init__(self, gcode):
    self.gcode = gcode
    self.inches = False
    self.digits = 4
    self.active = StringVar()
    self.tools = {}
    self.buttons = {}
    self.widget = {}
    self.listbox = None
    for cls in [Camera, Config, Font, Color, Controller, Cut, Drill, EndMill, Events, Material, Pocket, Profile, Shortcut, Stock, Tabs]:
        tool = cls(self)
        self.addTool(tool)
    for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
        (name, ext) = os.path.splitext(os.path.basename(f))
        try:
            exec(f'import {name}')
            tool = eval(f'{name}.Tool(self)')
            self.addTool(tool)
        except (ImportError, AttributeError):
            (typ, val, tb) = sys.exc_info()
            traceback.print_exception(typ, val, tb)","for f in glob.glob(f'{Utils.prgpath}/plugins/*.py'):
    (name, ext) = os.path.splitext(os.path.basename(f))
    try:
        exec(f'import {name}')
        tool = eval(f'{name}.Tool(self)')
        self.addTool(tool)
    except (ImportError, AttributeError):
        (typ, val, tb) = sys.exc_info()
        traceback.print_exception(typ, val, tb)","for i,f in enumerate(glob.glob(f'{Utils.prgpath}/plugins/*.py')):
    (name, ext) = os.path.splitext(os.path.basename(f))
    try:
        exec(f'import {name}')
        tool = eval(f'{name}.Tool(self)')
        self.addTool(tool)
    except (ImportError, AttributeError):
        (typ, val, tb) = sys.exc_info()
        traceback.print_exception(typ, val, tb)"
rewriting,https://github.com/davidbau/rewriting/tree/master/utils/tally.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rewriting/utils/tally.py,,"def tally_each(compute, dataset, sample_size=None, batch_size=10, summarize=None, cachefile=None, **kwargs):
    """"""
    Calls compute on batches of data.
    """"""
    args = dict(sample_size=sample_size)
    cached_state = load_cached_state(cachefile, args)
    if cached_state is not None:
        return TensorDict(state=cached_state).data
    loader = make_loader(dataset, sample_size, batch_size, **kwargs)
    for batch in pbar(loader):
        call_compute(compute, batch)
    if summarize is not None:
        result = summarize()
        save_cached_state(cachefile, TensorDict(data=result), args)
        return result","for batch in pbar(loader):
    call_compute(compute, batch)","for i, batch in enumerate(pbar(loader)):
    call_compute(compute, batch)"
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/neuron_view.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/neuron_view.py,,"def format_delimiters(tokens, tokenizer):
    formatted_tokens = []
    for t in tokens:
        if tokenizer.sep_token:
            t = t.replace(tokenizer.sep_token, '[SEP]')
        if tokenizer.cls_token:
            t = t.replace(tokenizer.cls_token, '[CLS]')
        formatted_tokens.append(t)
    return formatted_tokens","for t in tokens:
    if tokenizer.sep_token:
        t = t.replace(tokenizer.sep_token, '[SEP]')
    if tokenizer.cls_token:
        t = t.replace(tokenizer.cls_token, '[CLS]')
    formatted_tokens.append(t)","for i,t in enumerate(tokens):
    if tokenizer.sep_token:
        t = t.replace(tokenizer.sep_token, '[SEP]')
    if tokenizer.cls_token:
        t = t.replace(tokenizer.cls_token, '[CLS]')
    formatted_tokens.append(t)"
glance,https://github.com/openstack/glance/tree/master/glance/image_cache/drivers/sqlite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/image_cache/drivers/sqlite.py,Driver,"def delete_stalled_files(self, older_than):
    """"""
        Removes any incomplete cache entries older than a
        supplied modified time.

        :param older_than: Files written to on or before this timestamp
                           will be deleted.
        """"""
    for path in self.get_cache_files(self.incomplete_dir):
        if os.path.getmtime(path) < older_than:
            try:
                fileutils.delete_if_exists(path)
                LOG.info(_LI('Removed stalled cache file %s'), path)
            except Exception as e:
                msg = (_LW('Failed to delete file %(path)s. Got error: %(e)s'), dict(path=path, e=e))
                LOG.warn(msg)","for path in self.get_cache_files(self.incomplete_dir):
    if os.path.getmtime(path) < older_than:
        try:
            fileutils.delete_if_exists(path)
            LOG.info(_LI('Removed stalled cache file %s'), path)
        except Exception as e:
            msg = (_LW('Failed to delete file %(path)s. Got error: %(e)s'), dict(path=path, e=e))
            LOG.warn(msg)","for i, path in enumerate(self.get_cache_files(self.incomplete_dir)):
    if os.path.getmtime(path) < older_than:
        try:
            fileutils.delete_if_exists(path)
            LOG.info(_LI('Removed stalled cache file %s'), path)
        except Exception as e:
            msg = (_LW('Failed to delete file %(path)s. Got error: %(e)s'), dict(path=path, e=e))
            LOG.warn(msg)"
qutebrowser,https://github.com/qutebrowser/qutebrowser/tree/master/qutebrowser/browser/hints.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutebrowser/qutebrowser/browser/hints.py,WordHinter,"def hint(self, elems: _ElemsType) -> _HintStringsType:
    """"""Produce hint labels based on the html tags.

        Produce hint words based on the link text and random words
        from the words arg as fallback.

        Args:
            elems: The elements to get hint strings for.

        Return:
            A list of hint strings, in the same order as the elements.
        """"""
    self.ensure_initialized()
    hints = []
    used_hints: Set[str] = set()
    words = iter(self.words)
    for elem in elems:
        hint = self.new_hint_for(elem, used_hints, words)
        if not hint:
            raise HintingError('Not enough words in the dictionary.')
        used_hints.add(hint)
        hints.append(hint)
    return hints","for elem in elems:
    hint = self.new_hint_for(elem, used_hints, words)
    if not hint:
        raise HintingError('Not enough words in the dictionary.')
    used_hints.add(hint)
    hints.append(hint)","for i, elem in enumerate(elems):
    hint = self.new_hint_for(elem, used_hints, words)
    if not hint:
        raise HintingError('Not enough words in the dictionary.')
    used_hints.add(hint)
    hints.append(hint)"
docker-py,https://github.com/docker/docker-py/tree/master/tests/integration/models_images_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docker-py/tests/integration/models_images_test.py,ImageCollectionTest,"def test_save_and_load(self):
    client = docker.from_env(version=TEST_API_VERSION)
    image = client.images.get(TEST_IMG)
    with tempfile.TemporaryFile() as f:
        stream = image.save()
        for chunk in stream:
            f.write(chunk)
        f.seek(0)
        result = client.images.load(f.read())
    assert len(result) == 1
    assert result[0].id == image.id","for chunk in stream:
    f.write(chunk)","for i, chunk in enumerate(stream):
    f.write(chunk)"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_or_warn (the default; fix file if we can, warn otherwise)')
    postproc.add_option('--prefer-avconv', action='store_false', dest='prefer_ffmpeg', help='Prefer avconv over ffmpeg for running the postprocessors')
    postproc.add_option('--prefer-ffmpeg', action='store_true', dest='prefer_ffmpeg', help='Prefer ffmpeg over avconv for running the postprocessors (default)')
    postproc.add_option('--ffmpeg-location', '--avconv-location', metavar='PATH', dest='ffmpeg_location', help='Location of the ffmpeg/avconv binary; either the path to the binary or its containing directory.')
    postproc.add_option('--exec', metavar='CMD', dest='exec_cmd', help=""Execute a command on the file after downloading and post-processing, similar to find's -exec syntax. Example: --exec 'adb push {} /sdcard/Music/ && rm {}'"")
    postproc.add_option('--convert-subs', '--convert-subtitles', metavar='FORMAT', dest='convertsubtitles', default=None, help='Convert the subtitles to other format (currently supported: srt|ass|vtt|lrc)')
    parser.add_option_group(general)
    parser.add_option_group(network)
    parser.add_option_group(geo)
    parser.add_option_group(selection)
    parser.add_option_group(downloader)
    parser.add_option_group(filesystem)
    parser.add_option_group(thumbnail)
    parser.add_option_group(verbosity)
    parser.add_option_group(workarounds)
    parser.add_option_group(video_format)
    parser.add_option_group(subtitles)
    parser.add_option_group(authentication)
    parser.add_option_group(adobe_pass)
    parser.add_option_group(postproc)
    if overrideArguments is not None:
        (opts, args) = parser.parse_args(overrideArguments)
        if opts.verbose:
            write_string('[debug] Override config: ' + repr(overrideArguments) + '\n')
    else:

        def compat_conf(conf):
            if sys.version_info < (3,):
                return [a.decode(preferredencoding(), 'replace') for a in conf]
            return conf
        command_line_conf = compat_conf(sys.argv[1:])
        (opts, args) = parser.parse_args(command_line_conf)
        system_conf = user_conf = custom_conf = []
        if '--config-location' in command_line_conf:
            location = compat_expanduser(opts.config_location)
            if os.path.isdir(location):
                location = os.path.join(location, 'youtube-dl.conf')
            if not os.path.exists(location):
                parser.error('config-location %s does not exist.' % location)
            custom_conf = _readOptions(location)
        elif '--ignore-config' in command_line_conf:
            pass
        else:
            system_conf = _readOptions('/etc/youtube-dl.conf')
            if '--ignore-config' not in system_conf:
                user_conf = _readUserConf()
        argv = system_conf + user_conf + custom_conf + command_line_conf
        (opts, args) = parser.parse_args(argv)
        if opts.verbose:
            for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
                write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))
    return (parser, opts, args)","for (conf_label, conf) in (('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf)):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))","for i, (conf_label, conf) in enumerate((('System config', system_conf), ('User config', user_conf), ('Custom config', custom_conf), ('Command-line args', command_line_conf))):
    write_string('[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf))))"
onnxmltools,https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/libsvm/_parse.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnxmltools/onnxmltools/convert/libsvm/_parse.py,,"def parse_libsvm(model, initial_types=None, target_opset=None, custom_conversion_functions=None, custom_shape_calculators=None):
    raw_model_container = LibSvmModelContainer(model)
    topology = Topology(raw_model_container, default_batch_size='None', initial_types=initial_types, target_opset=target_opset, custom_conversion_functions=custom_conversion_functions, custom_shape_calculators=custom_shape_calculators)
    scope = topology.declare_scope('__root__')
    inputs = []
    for (var_name, initial_type) in initial_types:
        inputs.append(scope.declare_local_variable(var_name, initial_type))
    for variable in inputs:
        raw_model_container.add_input(variable)
    outputs = _parse_libsvm(scope, model, inputs)
    for variable in outputs:
        raw_model_container.add_output(variable)
    return topology","for (var_name, initial_type) in initial_types:
    inputs.append(scope.declare_local_variable(var_name, initial_type))","for i,(var_name, initial_type) in enumerate(initial_types):
    inputs.append(scope.declare_local_variable(var_name, initial_type))"
onnxmltools,https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/libsvm/_parse.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnxmltools/onnxmltools/convert/libsvm/_parse.py,,"def parse_libsvm(model, initial_types=None, target_opset=None, custom_conversion_functions=None, custom_shape_calculators=None):
    raw_model_container = LibSvmModelContainer(model)
    topology = Topology(raw_model_container, default_batch_size='None', initial_types=initial_types, target_opset=target_opset, custom_conversion_functions=custom_conversion_functions, custom_shape_calculators=custom_shape_calculators)
    scope = topology.declare_scope('__root__')
    inputs = []
    for (var_name, initial_type) in initial_types:
        inputs.append(scope.declare_local_variable(var_name, initial_type))
    for variable in inputs:
        raw_model_container.add_input(variable)
    outputs = _parse_libsvm(scope, model, inputs)
    for variable in outputs:
        raw_model_container.add_output(variable)
    return topology","for variable in outputs:
    raw_model_container.add_output(variable)","for i, variable in enumerate(outputs):
    raw_model_container.add_output(variable)"
ActualVim,https://github.com/lunixbochs/ActualVim/tree/master//edit.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ActualVim//edit.py,EditStep,"def resolve_args(self, view, edit):
    args = []
    for arg in self.args:
        if isinstance(arg, EditFuture):
            arg = arg.resolve(view, edit)
        args.append(arg)
    return args","for arg in self.args:
    if isinstance(arg, EditFuture):
        arg = arg.resolve(view, edit)
    args.append(arg)","for i,arg in enumerate(self.args):
    if isinstance(arg, EditFuture):
        arg = arg.resolve(view, edit)
    args.append(arg)"
blueprint,https://github.com/devstructure/blueprint/tree/master/blueprint/frontend/cfengine3.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blueprint/blueprint/frontend/cfengine3.py,Sketch,"def dumpf(self, gzip=False):
    """"""
        Generate files containing CFEngine 3 code and templates.  The directory
        structure generated is a sketch (sketch.json plus all the rest).
        """"""
    os.mkdir(self.name)
    filename = os.path.join(self.name, 'sketch.json')
    f = codecs.open(filename, 'w', encoding='utf-8')
    self._dump(f.write, inline=False)
    f.close()
    self.policy.make_content()
    for (pathname, dirname, content, meta) in self.allfiles():
        pathname = os.path.join(self.name, dirname, pathname[1:])
        try:
            os.makedirs(os.path.dirname(pathname))
        except OSError as e:
            if errno.EEXIST != e.errno:
                raise e
        if isinstance(content, unicode):
            f = codecs.open(pathname, 'w', encoding='utf-8')
        else:
            f = open(pathname, 'w')
        f.write(content)
        f.close()
    if gzip:
        filename = 'cfengine3-{0}.tar.gz'.format(self.name)
        tarball = tarfile.open(filename, 'w:gz')
        tarball.add(self.name)
        tarball.close()
        return filename
    return filename","for (pathname, dirname, content, meta) in self.allfiles():
    pathname = os.path.join(self.name, dirname, pathname[1:])
    try:
        os.makedirs(os.path.dirname(pathname))
    except OSError as e:
        if errno.EEXIST != e.errno:
            raise e
    if isinstance(content, unicode):
        f = codecs.open(pathname, 'w', encoding='utf-8')
    else:
        f = open(pathname, 'w')
    f.write(content)
    f.close()","for i, (pathname, dirname, content, meta) in enumerate(self.allfiles()):
    pathname = os.path.join(self.name, dirname, pathname[1:])
    try:
        os.makedirs(os.path.dirname(pathname))
    except OSError as e:
        if errno.EEXIST != e.errno:
            raise e
    if isinstance(content, unicode):
        f = codecs.open(pathname, 'w', encoding='utf-8')
    else:
        f = open(pathname, 'w')
    f.write(content)
    f.close()"
routersploit,https://github.com/threat9/routersploit/tree/master/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/routersploit/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,Exploit,"def run(self):
    creds = []
    print_status('Requesting {}'.format(self.get_target_url()))
    response = self.http_request(method='GET', path='/password.cgi')
    if response is None:
        print_error('Exploit failed - empty response')
        return
    tokens = [('admin', ""pwdAdmin = '(.+?)'""), ('support', ""pwdSupport = '(.+?)'""), ('user', ""pwdUser = '(.+?)'"")]
    print_status('Trying to extract credentials')
    for token in tokens:
        res = re.findall(token[1], response.text)
        if res:
            creds.append((token[0], res[0]))
    if creds:
        print_success('Credentials found')
        print_table(('Login', 'Password'), *creds)
    else:
        print_error('Exploit failed - credentials could not be found')","for token in tokens:
    res = re.findall(token[1], response.text)
    if res:
        creds.append((token[0], res[0]))","for i, token in enumerate(tokens):
    res = re.findall(token[1], response.text)
    if res:
        creds.append((token[0], res[0]))"
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for glyphName in glyphNames:
    try:
        gid = ttFont.getGlyphID(glyphName)
    except:
        assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
    vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])","for i,glyphName in enumerate(glyphNames):
    try:
        gid = ttFont.getGlyphID(glyphName)
    except:
        assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
    vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])"
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/V_O_R_G_.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/V_O_R_G_.py,table_V_O_R_G_,"def toXML(self, writer, ttFont):
    writer.simpletag('majorVersion', value=self.majorVersion)
    writer.newline()
    writer.simpletag('minorVersion', value=self.minorVersion)
    writer.newline()
    writer.simpletag('defaultVertOriginY', value=self.defaultVertOriginY)
    writer.newline()
    writer.simpletag('numVertOriginYMetrics', value=self.numVertOriginYMetrics)
    writer.newline()
    vOriginTable = []
    glyphNames = self.VOriginRecords.keys()
    for glyphName in glyphNames:
        try:
            gid = ttFont.getGlyphID(glyphName)
        except:
            assert 0, 'VORG table contains a glyph name not in ttFont.getGlyphNames(): ' + str(glyphName)
        vOriginTable.append([gid, glyphName, self.VOriginRecords[glyphName]])
    vOriginTable.sort()
    for entry in vOriginTable:
        vOriginRec = VOriginRecord(entry[1], entry[2])
        vOriginRec.toXML(writer, ttFont)","for entry in vOriginTable:
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)","for i, entry in enumerate(vOriginTable):
    vOriginRec = VOriginRecord(entry[1], entry[2])
    vOriginRec.toXML(writer, ttFont)"
maltrail,https://github.com/stamparm/maltrail/tree/master/trails/feeds/360necurs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maltrail/trails/feeds/360necurs.py,,"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
            retval[match.group(1)] = (__info__, __reference__)
    return retval","for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
    retval[match.group(1)] = (__info__, __reference__)","for i, match in enumerate(re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content)):
    retval[match.group(1)] = (__info__, __reference__)"
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","for file in files:
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {file} has {questions} questions and {answers} answers.')","for i,file in enumerate(files):
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {i} has {questions} questions and {answers} answers.')"
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
    if line.startswith('Question_ID'):
        questions += 1
    if line.startswith('Answer_to_Question_ID'):
        answers += 1","with open(data_folder / (file + '.txt'), mode='r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/dnsmasq.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/dnsmasq.py,,"def get_config(config_file='/etc/dnsmasq.conf'):
    """"""
    Dumps all options from the config file.

    config_file
        The location of the config file from which to obtain contents.
        Defaults to ``/etc/dnsmasq.conf``.

    CLI Examples:

    .. code-block:: bash

        salt '*' dnsmasq.get_config
        salt '*' dnsmasq.get_config config_file=/etc/dnsmasq.conf
    """"""
    dnsopts = _parse_dnamasq(config_file)
    if 'conf-dir' in dnsopts:
        for filename in os.listdir(dnsopts['conf-dir']):
            if filename.startswith('.'):
                continue
            if filename.endswith('~'):
                continue
            if filename.endswith('#') and filename.endswith('#'):
                continue
            dnsopts.update(_parse_dnamasq('{}/{}'.format(dnsopts['conf-dir'], filename)))
    return dnsopts","for filename in os.listdir(dnsopts['conf-dir']):
    if filename.startswith('.'):
        continue
    if filename.endswith('~'):
        continue
    if filename.endswith('#') and filename.endswith('#'):
        continue
    dnsopts.update(_parse_dnamasq('{}/{}'.format(dnsopts['conf-dir'], filename)))","for i, filename in enumerate(os.listdir(dnsopts['conf-dir'])):
    if filename.startswith('.'):
        continue
    if filename.endswith('~'):
        continue
    if filename.endswith('#') and filename.endswith('#'):
        continue
    dnsopts.update(_parse_dnamasq('{}/{}'.format(dnsopts['conf-dir'], filename)))"
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/tests/wikistats_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/tests/wikistats_tests.py,WikiStatsTestCase,"def test_sorting_order(self):
    """"""Test sorting order of languages_by_size.""""""
    family = 'wikipedia'
    ws = WikiStats()
    data = ws.get_dict(family)
    last = sys.maxsize
    last_code = ''
    for code in ws.languages_by_size(family):
        curr = int(data[code]['good'])
        self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
        last = curr
        last_code = code","for code in ws.languages_by_size(family):
    curr = int(data[code]['good'])
    self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
    last = curr
    last_code = code","for i, code in enumerate(ws.languages_by_size(family)):
    curr = int(data[code]['good'])
    self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
    last = curr
    last_code = code"
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for el in edge_lists:
    for e in el:
        self.delete(e)","for i, el in enumerate(edge_lists):
    for j, e in enumerate(el):
        self.delete(e)"
ros_comm,https://github.com/ros/ros_comm/tree/master/tools/rosgraph/src/rosgraph/impl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/tools/rosgraph/src/rosgraph/impl/graph.py,EdgeList,"def delete_all(self, node):
    """"""
        Delete all edges that start or end at node
        @param node: name of node
        @type  node: str
        """"""

    def matching(map, pref):
        return [map[k] for k in map.keys() if k.startswith(pref)]
    pref = node + '|'
    edge_lists = matching(self.edges_by_start, pref) + matching(self.edges_by_end, pref)
    for el in edge_lists:
        for e in el:
            self.delete(e)","for e in el:
    self.delete(e)","for i,e in enumerate(el):
    self.delete(e)"
qtile,https://github.com/qtile/qtile/tree/master/libqtile/widget/prompt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qtile/libqtile/widget/prompt.py,FileCompleter,"def complete(self, txt: str) -> str:
    """"""Returns the next completion for txt, or None if there is no completion""""""
    if self.lookup is None:
        self.lookup = []
        if txt == '' or txt[0] not in '~/':
            txt = '~/' + txt
        path = os.path.expanduser(txt)
        if os.path.isdir(path):
            files = glob.glob(os.path.join(path, '*'))
            prefix = txt
        else:
            files = glob.glob(path + '*')
            prefix = os.path.dirname(txt)
            prefix = prefix.rstrip('/') or '/'
        for f in files:
            display = os.path.join(prefix, os.path.basename(f))
            if os.path.isdir(f):
                display += '/'
            self.lookup.append((display, f))
            self.lookup.sort()
        self.offset = -1
        self.lookup.append((txt, txt))
    self.offset += 1
    if self.offset >= len(self.lookup):
        self.offset = 0
    ret = self.lookup[self.offset]
    self.thisfinal = ret[1]
    return ret[0]","for f in files:
    display = os.path.join(prefix, os.path.basename(f))
    if os.path.isdir(f):
        display += '/'
    self.lookup.append((display, f))
    self.lookup.sort()","for i,f in enumerate(files):
    display = os.path.join(prefix, os.path.basename(f))
    if os.path.isdir(f):
        display += '/'
    self.lookup.append((display, f))
    self.lookup.sort()"
CIFAR-ZOO,https://github.com/BIGBALLON/CIFAR-ZOO/tree/master/models/genet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CIFAR-ZOO/models/genet.py,GeResNeXt,"def __init__(self, cardinality, depth, num_classes, base_width, expansion=4):
    super(GeResNeXt, self).__init__()
    self.cardinality = cardinality
    self.depth = depth
    self.block_depth = (self.depth - 2) // 9
    self.base_width = base_width
    self.expansion = expansion
    self.num_classes = num_classes
    self.output_size = 64
    self.stages = [64, 64 * self.expansion, 128 * self.expansion, 256 * self.expansion]
    self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)
    self.bn_1 = nn.BatchNorm2d(64)
    self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 32, 1)
    self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 16, 2)
    self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 8, 2)
    self.fc = nn.Linear(self.stages[3], num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight.data)
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()","for m in self.modules():
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight.data)
    elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()","for i,m in enumerate(self.modules()):
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight.data)
    elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()"
shodan-python,https://github.com/achillean/shodan-python/tree/master/shodan/stream.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shodan-python/shodan/stream.py,Stream,"def custom(self, query, raw=False, timeout=None):
    """"""
        A filtered version of the ""banners"" stream to only return banners that match the query of interest. The query
        can vary and mix-match with different arguments (ports, tags, vulns, etc).

        :param query: A space-separated list of key:value filters query to return banner data on.
        :type query: string
        """"""
    stream = self._create_stream('/shodan/custom', query=query, timeout=timeout)
    for line in self._iter_stream(stream, raw):
        yield line","for line in self._iter_stream(stream, raw):
    yield line","for i, line in enumerate(self._iter_stream(stream, raw)):
    yield line"
salt,https://github.com/saltstack/salt/tree/master/salt/utils/data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/data.py,,"def traverse_dict_and_list(data, key, default=None, delimiter=DEFAULT_TARGET_DELIM):
    """"""
    Traverse a dict or list using a colon-delimited (or otherwise delimited,
    using the 'delimiter' param) target string. The target 'foo:bar:0' will
    return data['foo']['bar'][0] if this value exists, and will otherwise
    return the dict in the default argument.
    Function will automatically determine the target type.
    The target 'foo:bar:0' will return data['foo']['bar'][0] if data like
    {'foo':{'bar':['baz']}} , if data like {'foo':{'bar':{'0':'baz'}}}
    then return data['foo']['bar']['0']
    """"""
    ptr = data
    if isinstance(key, str):
        key = key.split(delimiter)
    if isinstance(key, int):
        key = [key]
    for each in key:
        if isinstance(ptr, list):
            try:
                idx = int(each)
            except ValueError:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[each]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    return default
            else:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[idx]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    try:
                        ptr = ptr[idx]
                    except IndexError:
                        return default
        else:
            try:
                ptr = ptr[each]
            except KeyError:
                import salt.utils.args
                try:
                    loaded_key = salt.utils.args.yamlify_arg(each)
                except Exception:
                    return default
                if loaded_key == each:
                    return default
                else:
                    try:
                        ptr = ptr[loaded_key]
                    except (KeyError, TypeError):
                        return default
            except TypeError:
                return default
    return ptr","for embedded in (x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass","for i, embedded in enumerate(x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[idx]
        embed_match = True
        break
    except KeyError:
        pass"
salt,https://github.com/saltstack/salt/tree/master/salt/utils/data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/data.py,,"def traverse_dict_and_list(data, key, default=None, delimiter=DEFAULT_TARGET_DELIM):
    """"""
    Traverse a dict or list using a colon-delimited (or otherwise delimited,
    using the 'delimiter' param) target string. The target 'foo:bar:0' will
    return data['foo']['bar'][0] if this value exists, and will otherwise
    return the dict in the default argument.
    Function will automatically determine the target type.
    The target 'foo:bar:0' will return data['foo']['bar'][0] if data like
    {'foo':{'bar':['baz']}} , if data like {'foo':{'bar':{'0':'baz'}}}
    then return data['foo']['bar']['0']
    """"""
    ptr = data
    if isinstance(key, str):
        key = key.split(delimiter)
    if isinstance(key, int):
        key = [key]
    for each in key:
        if isinstance(ptr, list):
            try:
                idx = int(each)
            except ValueError:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[each]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    return default
            else:
                embed_match = False
                for embedded in (x for x in ptr if isinstance(x, dict)):
                    try:
                        ptr = embedded[idx]
                        embed_match = True
                        break
                    except KeyError:
                        pass
                if not embed_match:
                    try:
                        ptr = ptr[idx]
                    except IndexError:
                        return default
        else:
            try:
                ptr = ptr[each]
            except KeyError:
                import salt.utils.args
                try:
                    loaded_key = salt.utils.args.yamlify_arg(each)
                except Exception:
                    return default
                if loaded_key == each:
                    return default
                else:
                    try:
                        ptr = ptr[loaded_key]
                    except (KeyError, TypeError):
                        return default
            except TypeError:
                return default
    return ptr","for embedded in (x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[each]
        embed_match = True
        break
    except KeyError:
        pass","for i, embedded in enumerate(x for x in ptr if isinstance(x, dict)):
    try:
        ptr = embedded[each]
        embed_match = True
        break
    except KeyError:
        pass"
CogView,https://github.com/THUDM/CogView/tree/master/data_utils/configure_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CogView/data_utils/configure_data.py,SplitDataset,"def __iter__(self):
    for idx in range(*self.split_range):
        yield self.wrapped_data[idx]","for idx in range(*self.split_range):
    yield self.wrapped_data[idx]","for i, idx in enumerate(range(*self.split_range)):
    yield self.wrapped_data[idx]"
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])","for i in range(RUN_STEP):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)","for i, _ in enumerate(range(RUN_STEP)):
    (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
    out_losses.append(loss[0])
    print_to_err(type(self).__name__, 'run step %d finished' % i)"
hmmlearn,https://github.com/hmmlearn/hmmlearn/tree/master/lib/hmmlearn/tests/test_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hmmlearn/lib/hmmlearn/tests/test_base.py,TestMonitor,"def test_report(self, capsys):
    n_iter = 10
    m = ConvergenceMonitor(tol=0.001, n_iter=n_iter, verbose=True)
    for i in reversed(range(n_iter)):
        m.report(-0.01 * i)
    (out, err) = capsys.readouterr()
    assert not out
    assert len(err.splitlines()) == n_iter
    assert len(m.history) == n_iter","for i in reversed(range(n_iter)):
    m.report(-0.01 * i)","for i, _ in enumerate(reversed(range(n_iter))):
    m.report(-0.01 * i)"
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/cmdline_tmpl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/cmdline_tmpl.py,CmdlineTmpl,"def cleanup(self, output_file='result.json', script_name='cmdline_test.py'):
    if os.path.exists(script_name):
        os.remove(script_name)
    if output_file:
        if type(output_file) is list:
            for f in output_file:
                os.remove(f)
        elif type(output_file) is str:
            if os.path.exists(output_file):
                if os.path.isdir(output_file):
                    shutil.rmtree(output_file)
                elif os.path.isfile(output_file):
                    os.remove(output_file)
        else:
            raise Exception('Unexpected output file argument')","for f in output_file:
    os.remove(f)","for i,f in enumerate(output_file):
    os.remove(f)"
PathPicker,https://github.com/facebook/PathPicker/tree/master/src/tests/test_parsing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPicker/src/tests/test_parsing.py,TestParseFunction,"def test_all_input_matches(self) -> None:
    for test_case in ALL_INPUT_TEST_CASES:
        result = parse.match_line(test_case.test_input, False, True)
        if not result:
            self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
            continue
        (match, _, _) = result
        self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')
    print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.')","for test_case in ALL_INPUT_TEST_CASES:
    result = parse.match_line(test_case.test_input, False, True)
    if not result:
        self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
        continue
    (match, _, _) = result
    self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')","for i,test_case in enumerate(ALL_INPUT_TEST_CASES):
    result = parse.match_line(test_case.test_input, False, True)
    if not result:
        self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
        continue
    (match, _, _) = result
    self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')"
orchest,https://github.com/orchest/orchest/tree/master/services/orchest-api/app/app/core/tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orchest/services/orchest-api/app/app/core/tasks.py,,"def delete_job_pipeline_run_directories(self, project_uuid: str, pipeline_uuid: str, job_uuid: str, pipeline_run_uuids: List[str]) -> str:
    """"""Deletes a list of job pipeline run directories given uuids.""""""
    job_dir = os.path.join('/userdir', 'jobs', project_uuid, pipeline_uuid, job_uuid)
    for uuid in pipeline_run_uuids:
        shutil.rmtree(os.path.join(job_dir, uuid), ignore_errors=True)
    return 'SUCCESS'","for uuid in pipeline_run_uuids:
    shutil.rmtree(os.path.join(job_dir, uuid), ignore_errors=True)","for i, uuid in enumerate(pipeline_run_uuids):
    shutil.rmtree(os.path.join(job_dir, uuid), ignore_errors=True)"
DeepPavlov,https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/core/common/params_search.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPavlov/deeppavlov/core/common/params_search.py,ParamsSearch,"def remove_key_from_config(config: dict, path: list) -> Tuple[dict, Any]:
    """"""
        Remove config element determined by path

        Args:
            config: dictionary
            path: list of keys and/or integers (for list)

        Returns:
            dictionary without value from path, value from path
        """"""
    config_copy = deepcopy(config)
    config_pointer = config_copy
    for el in path[:-1]:
        if isinstance(config_pointer, dict):
            config_pointer = config_pointer.setdefault(el, {})
        elif isinstance(config_pointer, list):
            config_pointer = config_pointer[el]
        else:
            pass
    value = config_pointer.pop(path[-1])
    return (config_copy, value)","for el in path[:-1]:
    if isinstance(config_pointer, dict):
        config_pointer = config_pointer.setdefault(el, {})
    elif isinstance(config_pointer, list):
        config_pointer = config_pointer[el]
    else:
        pass","for i, el in enumerate(path[:-1]):
    if isinstance(config_pointer, dict):
        config_pointer = config_pointer.setdefault(el, {})
    elif isinstance(config_pointer, list):
        config_pointer = config_pointer[el]
    else:
        pass"
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in embed_layer.collect_params().items():
    if '_embed' in k:
        arr_id = int(k[-len('_weight') - 1])
        embed_weights[arr_id] = v.data()[0].asnumpy()
        embed_grads[arr_id] = v.grad()[0].asnumpy()
    elif '_inter_proj' in k:
        arr_id = int(k[-len('_weight') - 1])
        proj_weights[arr_id] = v.data()[0].asnumpy()
        proj_grads[arr_id] = v.grad()[0].asnumpy()","for i, (k, v) in enumerate(embed_layer.collect_params().items()):
    if '_embed' in k:
        arr_id = int(k[-len('_weight') - 1])
        embed_weights[arr_id] = v.data()[0].asnumpy()
        embed_grads[arr_id] = v.grad()[0].asnumpy()
    elif '_inter_proj' in k:
        arr_id = int(k[-len('_weight') - 1])
        proj_weights[arr_id] = v.data()[0].asnumpy()
        proj_grads[arr_id] = v.grad()[0].asnumpy()"
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_proj.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for i, (k, v) in enumerate(layer_with_shared_proj.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])"
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_embed.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])","for i, (k, v) in enumerate(layer_with_shared_embed.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        with pytest.raises(AssertionError):
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])"
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/tests/test_layers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/tests/test_layers.py,,"def test_projected_adaptive_softmax(vocab_size, cutoffs, embed_size, in_units, div_val):
    layer = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer.initialize()
    layer.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    out = layer(hidden, target)
    mx.npx.waitall()
    assert out.shape == (4, 4, 4)
    embed_layer = AdaptiveEmbedding(vocab_size=vocab_size, cutoffs=cutoffs, units=in_units, embed_size=embed_size, div_val=div_val)
    layer_with_shared_proj = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj.share_parameters(embed_layer.collect_params('inter_proj'))
    layer_with_shared_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_embed.share_parameters(embed_layer.collect_params('embed'))
    layer_with_shared_proj_embed = ProjectedAdaptiveLogSoftmaxWithLoss(vocab_size=vocab_size, cutoffs=cutoffs, embed_size=embed_size, in_units=in_units, div_val=div_val)
    layer_with_shared_proj_embed.share_parameters(embed_layer.collect_params('(embed|inter_proj)'))
    embed_layer.initialize()
    embed_layer.hybridize()
    layer_with_shared_proj.initialize()
    layer_with_shared_proj.hybridize()
    layer_with_shared_embed.initialize()
    layer_with_shared_embed.hybridize()
    layer_with_shared_proj_embed.initialize()
    layer_with_shared_proj_embed.hybridize()
    hidden = mx.np.random.normal(0, 1, (4, 4, 4, 16))
    target = mx.np.random.randint(0, vocab_size, (4, 4, 4))
    with mx.autograd.record():
        loss = ((hidden - embed_layer(target)) ** 2).sum()
        loss.backward()
    assert embed_layer(target).asnumpy().shape == hidden.shape
    embed_weights = {}
    embed_grads = {}
    proj_weights = {}
    proj_grads = {}
    for (k, v) in embed_layer.collect_params().items():
        if '_embed' in k:
            arr_id = int(k[-len('_weight') - 1])
            embed_weights[arr_id] = v.data()[0].asnumpy()
            embed_grads[arr_id] = v.grad()[0].asnumpy()
        elif '_inter_proj' in k:
            arr_id = int(k[-len('_weight') - 1])
            proj_weights[arr_id] = v.data()[0].asnumpy()
            proj_grads[arr_id] = v.grad()[0].asnumpy()
    for (k, v) in layer_with_shared_proj.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])
    for (k, v) in layer_with_shared_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            with pytest.raises(AssertionError):
                assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
    for (k, v) in layer_with_shared_proj_embed.collect_params().items():
        if '_embed' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
        elif '_inter_proj' in k and '_weight' in k:
            arr_id = int(k[-len('_weight') - 1])
            assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
            assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for (k, v) in layer_with_shared_proj_embed.collect_params().items():
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])","for i, (k, v) in enumerate(layer_with_shared_proj_embed.collect_params().items()):
    if '_embed' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), embed_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), embed_grads[arr_id])
    elif '_inter_proj' in k and '_weight' in k:
        arr_id = int(k[-len('_weight') - 1])
        assert_allclose(v.data()[0].asnumpy(), proj_weights[arr_id])
        assert_allclose(v.grad()[0].asnumpy(), proj_grads[arr_id])"
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/env_dict.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/metaworld/envs/mujoco/env_dict.py,,"def create_hidden_goal_envs():
    hidden_goal_envs = {}
    for (env_name, env_cls) in ALL_V2_ENVIRONMENTS.items():
        d = {}

        def initialize(env, seed=None):
            if seed is not None:
                st0 = np.random.get_state()
                np.random.seed(seed)
            super(type(env), env).__init__()
            env._partially_observable = True
            env._freeze_rand_vec = False
            env._set_task_called = True
            env.reset()
            env._freeze_rand_vec = True
            if seed is not None:
                env.seed(seed)
                np.random.set_state(st0)
        d['__init__'] = initialize
        hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
        hg_env_name = hg_env_name.replace('-', '')
        hg_env_key = '{}-goal-hidden'.format(env_name)
        hg_env_name = '{}GoalHidden'.format(hg_env_name)
        HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
        hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls
    return OrderedDict(hidden_goal_envs)","for (env_name, env_cls) in ALL_V2_ENVIRONMENTS.items():
    d = {}

    def initialize(env, seed=None):
        if seed is not None:
            st0 = np.random.get_state()
            np.random.seed(seed)
        super(type(env), env).__init__()
        env._partially_observable = True
        env._freeze_rand_vec = False
        env._set_task_called = True
        env.reset()
        env._freeze_rand_vec = True
        if seed is not None:
            env.seed(seed)
            np.random.set_state(st0)
    d['__init__'] = initialize
    hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
    hg_env_name = hg_env_name.replace('-', '')
    hg_env_key = '{}-goal-hidden'.format(env_name)
    hg_env_name = '{}GoalHidden'.format(hg_env_name)
    HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
    hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls","for i, (env_name, env_cls) in enumerate(ALL_V2_ENVIRONMENTS.items()):
    d = {}

    def initialize(env, seed=None):
        if seed is not None:
            st0 = np.random.get_state()
            np.random.seed(seed)
        super(type(env), env).__init__()
        env._partially_observable = True
        env._freeze_rand_vec = False
        env._set_task_called = True
        env.reset()
        env._freeze_rand_vec = True
        if seed is not None:
            env.seed(seed)
            np.random.set_state(st0)
    d['__init__'] = initialize
    hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
    hg_env_name = hg_env_name.replace('-', '')
    hg_env_key = '{}-goal-hidden'.format(env_name)
    hg_env_name = '{}GoalHidden'.format(hg_env_name)
    HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
    hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls"
dlrm,https://github.com/facebookresearch/dlrm/tree/master//dlrm_data_pytorch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dlrm//dlrm_data_pytorch.py,,"def generate_dist_input_batch(m_den, ln_emb, n, num_indices_per_lookup, num_indices_per_lookup_fixed, rand_data_dist, rand_data_min, rand_data_max, rand_data_mu, rand_data_sigma):
    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))
    lS_emb_offsets = []
    lS_emb_indices = []
    for size in ln_emb:
        lS_batch_offsets = []
        lS_batch_indices = []
        offset = 0
        for _ in range(n):
            if num_indices_per_lookup_fixed:
                sparse_group_size = np.int64(num_indices_per_lookup)
            else:
                r = ra.random(1)
                sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
            if rand_data_dist == 'gaussian':
                if rand_data_mu == -1:
                    rand_data_mu = (rand_data_max + rand_data_min) / 2.0
                r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
                sparse_group = np.clip(r, rand_data_min, rand_data_max)
                sparse_group = np.unique(sparse_group).astype(np.int64)
            elif rand_data_dist == 'uniform':
                r = ra.random(sparse_group_size)
                sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
            else:
                raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
            sparse_group_size = np.int64(sparse_group.size)
            lS_batch_offsets += [offset]
            lS_batch_indices += sparse_group.tolist()
            offset += sparse_group_size
        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
        lS_emb_indices.append(torch.tensor(lS_batch_indices))
    return (Xt, lS_emb_offsets, lS_emb_indices)","for size in ln_emb:
    lS_batch_offsets = []
    lS_batch_indices = []
    offset = 0
    for _ in range(n):
        if num_indices_per_lookup_fixed:
            sparse_group_size = np.int64(num_indices_per_lookup)
        else:
            r = ra.random(1)
            sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
        if rand_data_dist == 'gaussian':
            if rand_data_mu == -1:
                rand_data_mu = (rand_data_max + rand_data_min) / 2.0
            r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
            sparse_group = np.clip(r, rand_data_min, rand_data_max)
            sparse_group = np.unique(sparse_group).astype(np.int64)
        elif rand_data_dist == 'uniform':
            r = ra.random(sparse_group_size)
            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
        else:
            raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
        sparse_group_size = np.int64(sparse_group.size)
        lS_batch_offsets += [offset]
        lS_batch_indices += sparse_group.tolist()
        offset += sparse_group_size
    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
    lS_emb_indices.append(torch.tensor(lS_batch_indices))","for i,size in enumerate(ln_emb):
    lS_batch_offsets = []
    lS_batch_indices = []
    offset = 0
    for _ in range(n):
        if num_indices_per_lookup_fixed:
            sparse_group_size = np.int64(num_indices_per_lookup)
        else:
            r = ra.random(1)
            sparse_group_size = np.int64(np.round(max([1.0], r * min(size, num_indices_per_lookup))))
        if rand_data_dist == 'gaussian':
            if rand_data_mu == -1:
                rand_data_mu = (rand_data_max + rand_data_min) / 2.0
            r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)
            sparse_group = np.clip(r, rand_data_min, rand_data_max)
            sparse_group = np.unique(sparse_group).astype(np.int64)
        elif rand_data_dist == 'uniform':
            r = ra.random(sparse_group_size)
            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))
        else:
            raise (rand_data_dist, 'distribution is not supported.                      please select uniform or gaussian')
        sparse_group_size = np.int64(sparse_group.size)
        lS_batch_offsets += [offset]
        lS_batch_indices += sparse_group.tolist()
        offset += sparse_group_size
    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))
    lS_emb_indices.append(torch.tensor(lS_batch_indices))"
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for rname in recorder_names:
    if not self.recorders.get(rname):
        raise NoSuchConfigurationRecorderException(rname)
    recorders.append(self.recorders[rname].to_dict())","for i,rname in enumerate(recorder_names):
    if not self.recorders.get(rname):
        raise NoSuchConfigurationRecorderException(rname)
    recorders.append(self.recorders[rname].to_dict())"
moto,https://github.com/spulec/moto/tree/master/moto/config/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,ConfigBackend,"def describe_configuration_recorders(self, recorder_names):
    recorders = []
    if recorder_names:
        for rname in recorder_names:
            if not self.recorders.get(rname):
                raise NoSuchConfigurationRecorderException(rname)
            recorders.append(self.recorders[rname].to_dict())
    else:
        for recorder in self.recorders.values():
            recorders.append(recorder.to_dict())
    return recorders","for recorder in self.recorders.values():
    recorders.append(recorder.to_dict())","for i, recorder in enumerate(self.recorders.values()):
    recorders.append(recorder.to_dict())"
tern,https://github.com/tern-tools/tern/tree/master/tern/formats/default/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tern/tern/formats/default/generator.py,,"def print_layer_report(layer):
    """"""Generate a report for a given layer""""""
    notes = get_layer_notices(layer)
    (filelicenses, pkgs_table) = get_layer_info_list(layer)
    notes += formats.layer_file_licenses_list.format(list=filelicenses)
    if pkgs_table:
        notes += formats.layer_packages_header.format('\n')
        for line in pkgs_table.splitlines():
            notes += '\t' + line + '\n'
    else:
        notes += formats.layer_packages_header.format('None\n')
    return notes","for line in pkgs_table.splitlines():
    notes += '\t' + line + '\n'","for i,line in enumerate(pkgs_table.splitlines()):
    notes += '\t' + line + '\n'"
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for nonterminal in nonterminals:
    if nonterminal not in first_plans:
        _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)","for i, nonterminal in enumerate(nonterminals):
    if nonterminal not in first_plans:
        _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)"
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for dfas in nonterminal_to_dfas.values():
    for dfa_state in dfas:
        transitions = dfa_state.transitions
        for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
            for (transition, pushes) in first_plans[nonterminal].items():
                if transition in transitions:
                    prev_plan = transitions[transition]
                    choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                    raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                transitions[transition] = DFAPlan(next_dfa, pushes)","for i, dfas in enumerate(nonterminal_to_dfas.values()):
    for dfa_state in dfas:
        transitions = dfa_state.transitions
        for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
            for (transition, pushes) in first_plans[nonterminal].items():
                if transition in transitions:
                    prev_plan = transitions[transition]
                    choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                    raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                transitions[transition] = DFAPlan(next_dfa, pushes)"
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for dfa_state in dfas:
    transitions = dfa_state.transitions
    for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
        for (transition, pushes) in first_plans[nonterminal].items():
            if transition in transitions:
                prev_plan = transitions[transition]
                choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
            transitions[transition] = DFAPlan(next_dfa, pushes)","for i, dfa_state in enumerate(dfas):
    transitions = dfa_state.transitions
    for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
        for (transition, pushes) in first_plans[nonterminal].items():
            if transition in transitions:
                prev_plan = transitions[transition]
                choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
            transitions[transition] = DFAPlan(next_dfa, pushes)"
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","for (transition, pushes) in first_plans[nonterminal].items():
    if transition in transitions:
        prev_plan = transitions[transition]
        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
    transitions[transition] = DFAPlan(next_dfa, pushes)","for i, (transition, pushes) in enumerate(first_plans[nonterminal].items()):
    if transition in transitions:
        prev_plan = transitions[transition]
        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
    transitions[transition] = DFAPlan(next_dfa, pushes)"
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/os/operation_siren.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/os/operation_siren.py,OperationSiren,"def _os_explore(self):
    """"""
        Explore all dangerous zones at the beginning of month.
        """"""

    def end():
        logger.info('OS explore finished, delay to next reset')
        next_reset = get_os_next_reset()
        logger.attr('OpsiNextReset', next_reset)
        logger.info('To run again, clear OpsiExplore.Scheduler.NextRun and set OpsiExplore.OpsiExplore.LastZone=0')
        with self.config.multi_set():
            self.config.OpsiExplore_LastZone = 0
            self.config.task_delay(target=next_reset)
            self.config.task_call('OpsiDaily', force_call=False)
            self.config.task_call('OpsiShop', force_call=False)
        self.config.task_stop()
    logger.hr('OS explore', level=1)
    order = [int(f.strip(' \t\r\n')) for f in self.config.OS_EXPLORE_FILTER.split('>')]
    try:
        last_zone = self.name_to_zone(self.config.OpsiExplore_LastZone).zone_id
    except ScriptError:
        logger.warning(f'Invalid OpsiExplore_LastZone={self.config.OpsiExplore_LastZone}, re-explore')
        last_zone = 0
    if last_zone in order:
        order = order[order.index(last_zone) + 1:]
        logger.info(f'Last zone: {self.name_to_zone(last_zone)}, next zone: {order[:1]}')
    elif last_zone == 0:
        logger.info(f'First run, next zone: {order[:1]}')
    else:
        raise ScriptError(f'Invalid last_zone: {last_zone}')
    if not len(order):
        end()
    for zone in order:
        if not self.globe_goto(zone, stop_if_safe=True):
            logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
            self.config.OpsiExplore_LastZone = zone
            continue
        logger.hr(f'OS explore {zone}', level=1)
        if not self.config.OpsiExplore_SpecialRadar:
            self.tuning_sample_use()
        self.fleet_set(self.config.OpsiFleet_Fleet)
        self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
        self._os_explore_task_delay()
        self.run_auto_search()
        self.config.OpsiExplore_LastZone = zone
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.handle_after_auto_search()
        self.config.check_task_switch()
        if zone == order[-1]:
            end()","for zone in order:
    if not self.globe_goto(zone, stop_if_safe=True):
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.config.OpsiExplore_LastZone = zone
        continue
    logger.hr(f'OS explore {zone}', level=1)
    if not self.config.OpsiExplore_SpecialRadar:
        self.tuning_sample_use()
    self.fleet_set(self.config.OpsiFleet_Fleet)
    self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
    self._os_explore_task_delay()
    self.run_auto_search()
    self.config.OpsiExplore_LastZone = zone
    logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
    self.handle_after_auto_search()
    self.config.check_task_switch()
    if zone == order[-1]:
        end()","for i, zone in enumerate(order):
    if not self.globe_goto(zone, stop_if_safe=True):
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.config.OpsiExplore_LastZone = zone
        continue
    logger.hr(f'OS explore {zone}', level=1)
    if not self.config.OpsiExplore_SpecialRadar:
        self.tuning_sample_use()
    self.fleet_set(self.config.OpsiFleet_Fleet)
    self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
    self._os_explore_task_delay()
    self.run_auto_search()
    self.config.OpsiExplore_LastZone = zone
    logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
    self.handle_after_auto_search()
    self.config.check_task_switch()
    if zone == order[-1]:
        end()"
rasa,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/core/domain.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rasa/rasa/shared/core/domain.py,Domain,"def _add_categorical_slot_default_value(self) -> None:
    """"""Add a default value to all categorical slots.

        All unseen values found for the slot will be mapped to this default value
        for featurization.
        """"""
    for slot in [s for s in self.slots if isinstance(s, CategoricalSlot)]:
        slot.add_default_value()","for slot in [s for s in self.slots if isinstance(s, CategoricalSlot)]:
    slot.add_default_value()","for i, slot in enumerate([s for s in self.slots if isinstance(s, CategoricalSlot)]):
    slot.add_default_value()"
pytext,https://github.com/facebookresearch/pytext/tree/master/pytext/metric_reporters/disjoint_multitask_metric_reporter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytext/pytext/metric_reporters/disjoint_multitask_metric_reporter.py,DisjointMultitaskMetricReporter,"def add_channel(self, channel):
    for reporter in self.reporters.values():
        reporter.add_channel(channel)","for reporter in self.reporters.values():
    reporter.add_channel(channel)","for i, reporter in enumerate(self.reporters.values()):
    reporter.add_channel(channel)"
cloud189,https://github.com/Aruelius/cloud189/tree/master/cloud189/cli/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud189/cloud189/cli/utils.py,,"def parsing_up_params(arg: str, follow, force, mkdir) -> (bool, bool, bool, bool):
    """"""
    :param str arg: 
    :param bool follow: 
    :param bool force: 
    :param bool mkdir: 
    :return: follow, force, mkdir, match( arg)
    """"""
    match = False
    if len(arg) > 1:
        if arg.startswith('--'):
            if arg == '--follow':
                follow = True
                match = True
            elif arg == '--force':
                force = True
                match = True
            elif arg == '--nodir':
                mkdir = False
                match = True
        elif arg.startswith('-'):
            for i in arg[1:]:
                if i == 'f':
                    follow = True
                    match = True
                elif i == 'F':
                    force = True
                    match = True
                elif i == 'n':
                    mkdir = False
                    match = True
    return (follow, force, mkdir, match)","for i in arg[1:]:
    if i == 'f':
        follow = True
        match = True
    elif i == 'F':
        force = True
        match = True
    elif i == 'n':
        mkdir = False
        match = True","for index, i in enumerate(arg[1:]):
    if i == 'f':
        follow = True
        match = True
    elif i == 'F':
        force = True
        match = True
    elif i == 'n':
        mkdir = False
        match = True"
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/test_streams.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/tests/test_streams.py,,"def test_all_linked_stream_parameters_owners():
    """"""Test to ensure operations can accept parameters in streams dictionary""""""
    stream_classes = param.concrete_descendents(LinkedStream)
    for stream_class in stream_classes.values():
        for (name, p) in stream_class.param.params().items():
            if name != 'name' and p.owner != stream_class:
                msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
                raise Exception(msg % (stream_class, name, p.owner))","for stream_class in stream_classes.values():
    for (name, p) in stream_class.param.params().items():
        if name != 'name' and p.owner != stream_class:
            msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
            raise Exception(msg % (stream_class, name, p.owner))","for i, stream_class in enumerate(stream_classes.values()):
    for (name, p) in stream_class.param.params().items():
        if name != 'name' and p.owner != stream_class:
            msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
            raise Exception(msg % (stream_class, name, p.owner))"
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/test_streams.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/holoviews/holoviews/tests/test_streams.py,,"def test_all_linked_stream_parameters_owners():
    """"""Test to ensure operations can accept parameters in streams dictionary""""""
    stream_classes = param.concrete_descendents(LinkedStream)
    for stream_class in stream_classes.values():
        for (name, p) in stream_class.param.params().items():
            if name != 'name' and p.owner != stream_class:
                msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
                raise Exception(msg % (stream_class, name, p.owner))","for (name, p) in stream_class.param.params().items():
    if name != 'name' and p.owner != stream_class:
        msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
        raise Exception(msg % (stream_class, name, p.owner))","for i, (name, p) in enumerate(stream_class.param.params().items()):
    if name != 'name' and p.owner != stream_class:
        msg = 'Linked stream %r has parameter %r which is inherited from %s. Parameter needs to be redeclared in the class definition of this linked stream.'
        raise Exception(msg % (stream_class, name, p.owner))"
vision,https://github.com/pytorch/vision/tree/master/test/test_datasets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vision/test/test_datasets.py,CarlaStereoTestCase,"def test_train_splits(self):
    with self.create_dataset() as (dataset, _):
        for (left, right, disparity) in dataset:
            datasets_utils.shape_test_for_stereo(left, right, disparity)","for (left, right, disparity) in dataset:
    datasets_utils.shape_test_for_stereo(left, right, disparity)","for i,(left, right, disparity) in enumerate(dataset):
    datasets_utils.shape_test_for_stereo(left, right, disparity)"
DFDNet,https://github.com/csxmli2016/DFDNet/tree/master/models/base_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DFDNet/models/base_model.py,BaseModel,"def print_networks(self, verbose):
    for name in self.model_names:
        if isinstance(name, str):
            net = getattr(self, 'net' + name)
            num_params = 0
            for param in net.parameters():
                num_params += param.numel()","for name in self.model_names:
    if isinstance(name, str):
        net = getattr(self, 'net' + name)
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()","for i,name in enumerate(self.model_names):
    if isinstance(name, str):
        net = getattr(self, 'net' + name)
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()"
DFDNet,https://github.com/csxmli2016/DFDNet/tree/master/models/base_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DFDNet/models/base_model.py,BaseModel,"def print_networks(self, verbose):
    for name in self.model_names:
        if isinstance(name, str):
            net = getattr(self, 'net' + name)
            num_params = 0
            for param in net.parameters():
                num_params += param.numel()","for param in net.parameters():
    num_params += param.numel()","for i, param in enumerate(net.parameters()):
    num_params += param.numel()"
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})","for file_id in ids:
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})","for i,file_id in enumerate(ids):
    if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
        image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
        image.shops.add(shop_id)
        added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/core/torch_generator_agent.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/core/torch_generator_agent.py,TopKSampling,"def select_paths(self, logprobs, prior_scores, current_length) -> _PathSelection:
    (values, indices) = logprobs.topk(self.k, dim=-1)
    probs = torch.softmax(values, dim=-1)
    choices = torch.multinomial(probs, 1)[:, 0]
    hyp_ids = torch.arange(logprobs.size(0)).to(logprobs.device)
    tok_ids = indices[hyp_ids, choices]
    scores = values[hyp_ids, choices]
    best_scores = prior_scores.expand_as(scores) + scores
    token_details: Optional[List[_PathSelectionTokenDetails]] = None
    if self.verbose:
        tok_logprobs = probs[hyp_ids, choices].log().view(-1).cpu().numpy()
        tok_ranks = choices.view(-1).cpu().numpy()
        token_details = []
        for (tok_logprob, tok_rank) in zip(tok_logprobs, tok_ranks):
            token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})
    return _PathSelection(hypothesis_ids=hyp_ids, token_ids=tok_ids, scores=best_scores, token_details=token_details)","for (tok_logprob, tok_rank) in zip(tok_logprobs, tok_ranks):
    token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})","for i, (tok_logprob, tok_rank) in enumerate(zip(tok_logprobs, tok_ranks)):
    token_details.append({'token_logprob': tok_logprob, 'token_rank': int(tok_rank)})"
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)","for cell_block in mesh.cells:
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))","for i, cell_block in enumerate(mesh.cells):
    if cell_block.type not in ['line', 'triangle', 'quad']:
        continue
    if cell_block.type == 'line':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
    elif cell_block.type == 'triangle':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    elif cell_block.type == 'quad':
        fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
    for cell in cell_block.data:
        ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))"
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)","for cell in cell_block.data:
    ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))","for i, cell in enumerate(cell_block.data):
    ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))"
mars,https://github.com/mars-project/mars/tree/master/mars/lib/filesystem/oss.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/lib/filesystem/oss.py,OSSFileSystem,"def ls(self, path: path_type) -> List[path_type]:
    file_list = []
    file_entry = oc.OSSFileEntry(path)
    if not file_entry.is_dir():
        raise OSError('ls for file is not supported')
    else:
        (bucket, key, access_key_id, access_key_secret, end_point) = oc.parse_osspath(path)
        oss_bucket = oss2.Bucket(auth=oss2.Auth(access_key_id=access_key_id, access_key_secret=access_key_secret), endpoint=end_point, bucket_name=bucket, connect_timeout=_oss_time_out)
        for obj in oss2.ObjectIteratorV2(oss_bucket, prefix=key):
            if obj.key.endswith('/'):
                continue
            obj_path = f'oss://{bucket}/{obj.key}'
            file_list.append(obj_path)
    return file_list","for obj in oss2.ObjectIteratorV2(oss_bucket, prefix=key):
    if obj.key.endswith('/'):
        continue
    obj_path = f'oss://{bucket}/{obj.key}'
    file_list.append(obj_path)","for i,obj in enumerate(oss2.ObjectIteratorV2(oss_bucket, prefix=key)):
    if obj.key.endswith('/'):
        continue
    obj_path = f'oss://{bucket}/{obj.key}'
    file_list.append(obj_path)"
buildbot,https://github.com/buildbot/buildbot/tree/master/master/buildbot/data/test_result_sets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/buildbot/master/buildbot/data/test_result_sets.py,TestResultSetsEndpoint,"def get(self, resultSpec, kwargs):
    complete = resultSpec.popBooleanFilter('complete')
    if 'stepid' in kwargs:
        step_dbdict = (yield self.master.db.steps.getStep(kwargs['stepid']))
        build_dbdict = (yield self.master.db.builds.getBuild(step_dbdict['buildid']))
        sets = (yield self.master.db.test_result_sets.getTestResultSets(build_dbdict['builderid'], buildid=step_dbdict['buildid'], stepid=kwargs['stepid'], complete=complete, result_spec=resultSpec))
    elif 'buildid' in kwargs:
        build_dbdict = (yield self.master.db.builds.getBuild(kwargs['buildid']))
        sets = (yield self.master.db.test_result_sets.getTestResultSets(build_dbdict['builderid'], buildid=kwargs['buildid'], complete=complete, result_spec=resultSpec))
    else:
        builderid = (yield self.getBuilderId(kwargs))
        sets = (yield self.master.db.test_result_sets.getTestResultSets(builderid, complete=complete, result_spec=resultSpec))
    results = []
    for dbdict in sets:
        results.append((yield self.db2data(dbdict)))
    return results","for dbdict in sets:
    results.append((yield self.db2data(dbdict)))","for i, dbdict in enumerate(sets):
    results.append((yield self.db2data(dbdict))))"
h,https://github.com/hypothesis/h/tree/master/tests/h/views/activity_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/h/tests/h/views/activity_test.py,TestGroupSearchController,"def test_search_returns_group_moderators_faceted_by(self, controller, pyramid_request, test_user, test_group):
    pyramid_request.params = {'q': 'user:does_not_matter'}
    result = controller.search()
    for moderator in result['group_users_args'][1]:
        assert not moderator['faceted_by']","for moderator in result['group_users_args'][1]:
    assert not moderator['faceted_by']","for i, moderator in enumerate(result['group_users_args'][1]):
    assert not moderator['faceted_by']"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/status.py,,"def linux_w():
    """"""
        Linux specific implementation for w
        """"""
    user_list = []
    users = __salt__['cmd.run']('w -fh').splitlines()
    for row in users:
        if not row:
            continue
        comps = row.split()
        rec = {'idle': comps[3], 'jcpu': comps[4], 'login': comps[2], 'pcpu': comps[5], 'tty': comps[1], 'user': comps[0], 'what': ' '.join(comps[6:])}
        user_list.append(rec)
    return user_list","for row in users:
    if not row:
        continue
    comps = row.split()
    rec = {'idle': comps[3], 'jcpu': comps[4], 'login': comps[2], 'pcpu': comps[5], 'tty': comps[1], 'user': comps[0], 'what': ' '.join(comps[6:])}
    user_list.append(rec)","for i,row in enumerate(users):
    if not row:
        continue
    comps = row.split()
    rec = {'idle': comps[3], 'jcpu': comps[4], 'login': comps[2], 'pcpu': comps[5], 'tty': comps[1], 'user': comps[0], 'what': ' '.join(comps[6:])}
    user_list.append(rec)"
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/jobs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/jobs.py,DeleteJobHandler,"def post(self):
    """"""Handle a post request.""""""
    key = helpers.get_integer_key(request)
    job = ndb.Key(data_types.Job, key).get()
    if not job:
        raise helpers.EarlyExitException('Job not found.', 400)
    for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
        if job.name in fuzzer.jobs:
            fuzzer.jobs.remove(job.name)
            fuzzer.put()
    query = data_types.FuzzerJob.query()
    query = query.filter(data_types.FuzzerJob.job == job.name)
    for mapping in ndb_utils.get_all_from_query(query):
        mapping.key.delete()
    job.key.delete()
    helpers.log('Deleted job %s' % job.name, helpers.MODIFY_OPERATION)
    return self.redirect('/jobs')","for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
    if job.name in fuzzer.jobs:
        fuzzer.jobs.remove(job.name)
        fuzzer.put()","for i,fuzzer in enumerate(ndb_utils.get_all_from_model(data_types.Fuzzer)):
    if job.name in fuzzer.jobs:
        fuzzer.jobs.remove(job.name)
        fuzzer.put()"
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/jobs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/jobs.py,DeleteJobHandler,"def post(self):
    """"""Handle a post request.""""""
    key = helpers.get_integer_key(request)
    job = ndb.Key(data_types.Job, key).get()
    if not job:
        raise helpers.EarlyExitException('Job not found.', 400)
    for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
        if job.name in fuzzer.jobs:
            fuzzer.jobs.remove(job.name)
            fuzzer.put()
    query = data_types.FuzzerJob.query()
    query = query.filter(data_types.FuzzerJob.job == job.name)
    for mapping in ndb_utils.get_all_from_query(query):
        mapping.key.delete()
    job.key.delete()
    helpers.log('Deleted job %s' % job.name, helpers.MODIFY_OPERATION)
    return self.redirect('/jobs')","for mapping in ndb_utils.get_all_from_query(query):
    mapping.key.delete()","for i,mapping in enumerate(ndb_utils.get_all_from_query(query)):
    mapping.key.delete()"
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for epoch in range(hp.epochs):
    dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
    pbar = tqdm(dataloader)
    for (i, data) in enumerate(pbar):
        pbar.set_description('Processing at epoch %d' % epoch)
        global_step += 1
        if global_step < 400000:
            adjust_learning_rate(optimizer, global_step)
        (character, mel, mel_input, pos_text, pos_mel, _) = data
        stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
        character = character.cuda()
        mel = mel.cuda()
        mel_input = mel_input.cuda()
        pos_text = pos_text.cuda()
        pos_mel = pos_mel.cuda()
        (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
        mel_loss = nn.L1Loss()(mel_pred, mel)
        post_mel_loss = nn.L1Loss()(postnet_pred, mel)
        loss = mel_loss + post_mel_loss
        writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
        writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
        if global_step % hp.image_step == 1:
            for (i, prob) in enumerate(attn_probs):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_enc):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_dec):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        optimizer.step()
        if global_step % hp.save_step == 0:
            t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))","for epoch in range(hp.epochs):
    dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
    pbar = tqdm(dataloader)
    for (i, data) in enumerate(pbar):
        pbar.set_description('Processing at epoch %d' % epoch)
        global_step += 1
        if global_step < 400000:
            adjust_learning_rate(optimizer, global_step)
        (character, mel, mel_input, pos_text, pos_mel, _) = data
        stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
        character = character.cuda()
        mel = mel.cuda()
        mel_input = mel_input.cuda()
        pos_text = pos_text.cuda()
        pos_mel = pos_mel.cuda()
        (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
        mel_loss = nn.L1Loss()(mel_pred, mel)
        post_mel_loss = nn.L1Loss()(postnet_pred, mel)
        loss = mel_loss + post_mel_loss
        writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
        writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
        if global_step % hp.image_step == 1:
            for (i, prob) in enumerate(attn_probs):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_enc):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
            for (i, prob) in enumerate(attns_dec):
                num_h = prob.size(0)
                for j in range(4):
                    x = vutils.make_grid(prob[j * 16] * 255)
                    writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        optimizer.step()
        if global_step % hp.save_step == 0:
            t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))"
sentinelsat,https://github.com/sentinelsat/sentinelsat/tree/master/tests/test_docs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentinelsat/tests/test_docs.py,,"def test_rst(rst_file):
    with open(rst_file) as input_file:
        contents = input_file.read()
    all_errors = []
    errors = rstcheck.check(contents, report_level=2, ignore={'languages': ['python', 'bash']})
    for (line_number, error) in errors:
        if 'Title underline too short' in error:
            continue
        m = re.search('Unknown interpreted text role ""([^""]+)""', error)
        if m and m.group(1) in ['program', 'paramref']:
            continue
        m = re.search('Unknown directive type ""([^""]+)""', error)
        if m and m.group(1) in ['automodule']:
            continue
        all_errors.append((line_number, error))
    assert len(all_errors) == 0","for (line_number, error) in errors:
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))","for i, (line_number, error) in enumerate(errors):
    if 'Title underline too short' in error:
        continue
    m = re.search('Unknown interpreted text role ""([^""]+)""', error)
    if m and m.group(1) in ['program', 'paramref']:
        continue
    m = re.search('Unknown directive type ""([^""]+)""', error)
    if m and m.group(1) in ['automodule']:
        continue
    all_errors.append((line_number, error))"
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-en/d2l/tensorflow.py,,"def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):
    """"""Defined in :numref:`sec_minibatches`""""""
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01), trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)
    (net, loss) = (lambda X: d2l.linreg(X, w, b), d2l.squared_loss)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35])
    (n, timer) = (0, d2l.Timer())
    for _ in range(num_epochs):
        for (X, y) in data_iter:
            with tf.GradientTape() as g:
                l = tf.math.reduce_mean(loss(net(X), y))
            (dw, db) = g.gradient(l, [w, b])
            trainer_fn([w, b], [dw, db], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n / X.shape[0]
                q = p / tf.data.experimental.cardinality(data_iter).numpy()
                r = (d2l.evaluate_loss(net, data_iter, loss),)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch')
    return (timer.cumsum(), animator.Y[0])","for _ in range(num_epochs):
    for (X, y) in data_iter:
        with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))
        (dw, db) = g.gradient(l, [w, b])
        trainer_fn([w, b], [dw, db], states, hyperparams)
        n += X.shape[0]
        if n % 200 == 0:
            timer.stop()
            p = n / X.shape[0]
            q = p / tf.data.experimental.cardinality(data_iter).numpy()
            r = (d2l.evaluate_loss(net, data_iter, loss),)
            animator.add(q, r)
            timer.start()","for epoch, _ in enumerate(range(num_epochs)):
    for (X, y) in data_iter:
        with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))
        (dw, db) = g.gradient(l, [w, b])
        trainer_fn([w, b], [dw, db], states, hyperparams)
        n += X.shape[0]
        if n % 200 == 0:
            timer.stop()
            p = n / X.shape[0]
            q = p / tf.data.experimental.cardinality(data_iter).numpy()
            r = (d2l.evaluate_loss(net, data_iter, loss),)
            animator.add(q, r)
            timer.start()"
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-en/d2l/tensorflow.py,,"def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):
    """"""Defined in :numref:`sec_minibatches`""""""
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01), trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)
    (net, loss) = (lambda X: d2l.linreg(X, w, b), d2l.squared_loss)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35])
    (n, timer) = (0, d2l.Timer())
    for _ in range(num_epochs):
        for (X, y) in data_iter:
            with tf.GradientTape() as g:
                l = tf.math.reduce_mean(loss(net(X), y))
            (dw, db) = g.gradient(l, [w, b])
            trainer_fn([w, b], [dw, db], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n / X.shape[0]
                q = p / tf.data.experimental.cardinality(data_iter).numpy()
                r = (d2l.evaluate_loss(net, data_iter, loss),)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch')
    return (timer.cumsum(), animator.Y[0])","for (X, y) in data_iter:
    with tf.GradientTape() as g:
        l = tf.math.reduce_mean(loss(net(X), y))
    (dw, db) = g.gradient(l, [w, b])
    trainer_fn([w, b], [dw, db], states, hyperparams)
    n += X.shape[0]
    if n % 200 == 0:
        timer.stop()
        p = n / X.shape[0]
        q = p / tf.data.experimental.cardinality(data_iter).numpy()
        r = (d2l.evaluate_loss(net, data_iter, loss),)
        animator.add(q, r)
        timer.start()","for i, (X, y) in enumerate(data_iter):
    with tf.GradientTape() as g:
        l = tf.math.reduce_mean(loss(net(X), y))
    (dw, db) = g.gradient(l, [w, b])
    trainer_fn([w, b], [dw, db], states, hyperparams)
    n += X.shape[0]
    if n % 200 == 0:
        timer.stop()
        p = n / X.shape[0]
        q = p / tf.data.experimental.cardinality(data_iter).numpy()
        r = (d2l.evaluate_loss(net, data_iter, loss),)
        animator.add(q, r)
        timer.start()"
WatchAD,https://github.com/Qianlitp/WatchAD/tree/master/models/Log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WatchAD/models/Log.py,TargetInfo,"def __init__(self, event_data):
    self.domain_name = None
    self.user_name = None
    self.user_sid = None
    self.logon_id = None
    self.info = None
    self.server_name = None
    self.sid = None
    self.full_user_name = None
    self._field_map = {'TargetDomainName': 'domain_name', 'TargetUserName': 'user_name', 'TargetUserSid': 'user_sid', 'TargetSid': 'sid', 'TargetLogonId': 'logon_id', 'TargetInfo': 'info', 'TargetServerName': 'server_name'}
    for (key, value) in self._field_map.items():
        if key not in event_data:
            continue
        if key == 'TargetUserName':
            if '@' in event_data[key]:
                user_name = event_data[key].split('@')[0]
                self.__dict__.update({value: user_name})
            else:
                self.__dict__.update({value: event_data[key]})
            self.__dict__.update({'full_user_name': event_data[key]})
        elif key in event_data:
            self.__dict__.update({value: event_data[key]})","for (key, value) in self._field_map.items():
    if key not in event_data:
        continue
    if key == 'TargetUserName':
        if '@' in event_data[key]:
            user_name = event_data[key].split('@')[0]
            self.__dict__.update({value: user_name})
        else:
            self.__dict__.update({value: event_data[key]})
        self.__dict__.update({'full_user_name': event_data[key]})
    elif key in event_data:
        self.__dict__.update({value: event_data[key]})","for i, (key, value) in enumerate(self._field_map.items()):
    if key not in event_data:
        continue
    if key == 'TargetUserName':
        if '@' in event_data[key]:
            user_name = event_data[key].split('@')[0]
            self.__dict__.update({value: user_name})
        else:
            self.__dict__.update({value: event_data[key]})
        self.__dict__.update({'full_user_name': event_data[key]})
    elif key in event_data:
        self.__dict__.update({value: event_data[key]})"
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/trainer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/fairseq/trainer.py,Trainer,"def valid_step(self, sample, raise_oom=False):
    """"""Do forward pass in evaluation mode.""""""
    if self.tpu:
        import torch_xla.core.xla_model as xm
        xm.rendezvous('valid_step')
    extra_kwargs = {}
    if self.cfg.ema.store_ema and getattr(self.task, 'uses_ema', False):
        extra_kwargs['ema_model'] = self.ema.get_model()
    with torch.no_grad():
        self.model.eval()
        self.criterion.eval()
        (sample, is_dummy_batch) = self._prepare_sample(sample)
        try:
            (_loss, sample_size, logging_output) = self.task.valid_step(sample, self.model, self.criterion, **extra_kwargs)
        except RuntimeError as e:
            if 'out of memory' in str(e):
                self._log_oom(e)
                if not raise_oom:
                    logger.warning('ran out of memory in validation step, retrying batch')
                    for p in self.model.parameters():
                        if p.grad is not None:
                            p.grad = None
                    if self.cuda:
                        torch.cuda.empty_cache()
                    return self.valid_step(sample, raise_oom=True)
            raise e
        logging_outputs = [logging_output]
        if is_dummy_batch:
            if torch.is_tensor(sample_size):
                sample_size.zero_()
            else:
                sample_size *= 0.0
    if self.data_parallel_world_size > 1:
        (logging_outputs, (sample_size,)) = self._aggregate_logging_outputs(logging_outputs, sample_size, ignore=is_dummy_batch)
    if self.tpu:
        logging_outputs = self._xla_markstep_and_send_to_cpu(logging_outputs)
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
    return logging_output","for p in self.model.parameters():
    if p.grad is not None:
        p.grad = None","for i,p in enumerate(self.model.parameters()):
    if p.grad is not None:
        p.grad = None"
DeepPrivacy,https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/dataset/places2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPrivacy/deep_privacy/dataset/places2.py,Places2Dataset,"def _load_impaths(self):
    relevant_suffixes = ['.png', '.jpg', '.jpeg']
    image_dir = self.dirpath
    image_paths = []
    for (dirpath, dirnames, filenames) in os.walk(image_dir):
        for filename in filenames:
            path = pathlib.Path(dirpath, filename)
            if path.suffix in relevant_suffixes:
                assert path.is_file()
                image_paths.append(path)
    image_paths.sort(key=lambda x: int(x.stem.split('_')[-1]))
    return image_paths","for (dirpath, dirnames, filenames) in os.walk(image_dir):
    for filename in filenames:
        path = pathlib.Path(dirpath, filename)
        if path.suffix in relevant_suffixes:
            assert path.is_file()
            image_paths.append(path)","for i, (dirpath, dirnames, filenames) in enumerate(os.walk(image_dir)):
    for filename in filenames:
        path = pathlib.Path(dirpath, filename)
        if path.suffix in relevant_suffixes:
            assert path.is_file()
            image_paths.append(path)"
DeepPrivacy,https://github.com/hukkelas/DeepPrivacy/tree/master/deep_privacy/dataset/places2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepPrivacy/deep_privacy/dataset/places2.py,Places2Dataset,"def _load_impaths(self):
    relevant_suffixes = ['.png', '.jpg', '.jpeg']
    image_dir = self.dirpath
    image_paths = []
    for (dirpath, dirnames, filenames) in os.walk(image_dir):
        for filename in filenames:
            path = pathlib.Path(dirpath, filename)
            if path.suffix in relevant_suffixes:
                assert path.is_file()
                image_paths.append(path)
    image_paths.sort(key=lambda x: int(x.stem.split('_')[-1]))
    return image_paths","for filename in filenames:
    path = pathlib.Path(dirpath, filename)
    if path.suffix in relevant_suffixes:
        assert path.is_file()
        image_paths.append(path)","for i,filename in enumerate(filenames):
    path = pathlib.Path(dirpath, filename)
    if path.suffix in relevant_suffixes:
        assert path.is_file()
        image_paths.append(path)"
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for subdir in os.listdir(dir1):
    if osp.isdir(osp.join(dir1, subdir)):
        if check_keywords(subdir, keywords):
            dir_path1 = osp.join(dir1, subdir)
            for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                img_list = list()
                for dir in dir_list:
                    dir_path = dir_path1.replace(dir1, dir)
                    img_path = osp.join(dir_path, img_name)
                    img = cv2.imread(img_path)
                    img_list.append(img)
                    if img_path.find(dir1) >= 0:
                        res_img_path = img_path.replace(dir1, res_dir)
                if any([img is None for img in img_list]):
                    continue
                res_img = np.concatenate(img_list, axis=0)
                (h, w) = res_img.shape[:2]
                res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                res_img_path = res_img_path.replace('.png', '.jpg')
                ry_utils.make_subdir(res_img_path)
                cv2.imwrite(res_img_path, res_img)
                print(res_img_path)","for i, subdir in enumerate(os.listdir(dir1)):
    if osp.isdir(osp.join(dir1, subdir)):
        if check_keywords(subdir, keywords):
            dir_path1 = osp.join(dir1, subdir)
            for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                img_list = list()
                for j, dir in enumerate(dir_list):
                    dir_path = dir_path1.replace(dir1, dir)
                    img_path = osp.join(dir_path, img_name)
                    img = cv2.imread(img_path)
                    img_list.append(img)
                    if img_path.find(dir1) >= 0:
                        res_img_path = img_path.replace(dir1, res_dir)
                if any([img is None for img in img_list]):
                    continue
                res_img = np.concatenate(img_list, axis=0)
                (h, w) = res_img.shape[:2]
                res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                res_img_path = res_img_path.replace('.png', '.jpg')
                ry_utils.make_subdir(res_img_path)
                cv2.imwrite(res_img_path, res_img)
                print(res_img_path)"
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
    img_list = list()
    for dir in dir_list:
        dir_path = dir_path1.replace(dir1, dir)
        img_path = osp.join(dir_path, img_name)
        img = cv2.imread(img_path)
        img_list.append(img)
        if img_path.find(dir1) >= 0:
            res_img_path = img_path.replace(dir1, res_dir)
    if any([img is None for img in img_list]):
        continue
    res_img = np.concatenate(img_list, axis=0)
    (h, w) = res_img.shape[:2]
    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
    res_img_path = res_img_path.replace('.png', '.jpg')
    ry_utils.make_subdir(res_img_path)
    cv2.imwrite(res_img_path, res_img)
    print(res_img_path)","for i,img_name in enumerate(ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only')):
    img_list = list()
    for dir in dir_list:
        dir_path = dir_path1.replace(dir1, dir)
        img_path = osp.join(dir_path, img_name)
        img = cv2.imread(img_path)
        img_list.append(img)
        if img_path.find(dir1) >= 0:
            res_img_path = img_path.replace(dir1, res_dir)
    if any([img is None for img in img_list]):
        continue
    res_img = np.concatenate(img_list, axis=0)
    (h, w) = res_img.shape[:2]
    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
    res_img_path = res_img_path.replace('.png', '.jpg')
    ry_utils.make_subdir(res_img_path)
    cv2.imwrite(res_img_path, res_img)
    print(res_img_path)"
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","for dir in dir_list:
    dir_path = dir_path1.replace(dir1, dir)
    img_path = osp.join(dir_path, img_name)
    img = cv2.imread(img_path)
    img_list.append(img)
    if img_path.find(dir1) >= 0:
        res_img_path = img_path.replace(dir1, res_dir)","for i, dir in enumerate(dir_list):
    dir_path = dir_path1.replace(dir1, dir)
    img_path = osp.join(dir_path, img_name)
    img = cv2.imread(img_path)
    img_list.append(img)
    if img_path.find(dir1) >= 0:
        res_img_path = img_path.replace(dir1, res_dir)"
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)","for inv_y in (False, True):
    for (vc, result) in results.items():
        net = pp.create_empty_network(sn_mva=16)
        add_network(net, vc)
        try:
            sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
        except Exception as e:
            raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
        check_results(net, vc, result)","for i, inv_y in enumerate((False, True)):
    for (vc, result) in results.items():
        net = pp.create_empty_network(sn_mva=16)
        add_network(net, vc)
        try:
            sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
        except Exception as e:
            raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
        check_results(net, vc, result)"
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)","for (vc, result) in results.items():
    net = pp.create_empty_network(sn_mva=16)
    add_network(net, vc)
    try:
        sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
    except Exception as e:
        raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
    check_results(net, vc, result)","for i, (vc, result) in enumerate(results.items()):
    net = pp.create_empty_network(sn_mva=16)
    add_network(net, vc)
    try:
        sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
    except Exception as e:
        raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
    check_results(net, vc, result)"
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)","for collider in collrates:
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table","for i,collider in enumerate(collrates):
    collname = collider_ids[collider]
    coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
    coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
    coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
    coll_tables[collname] = coll_table"
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch12/tx.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/programmingbitcoin/code-ch12/tx.py,Tx,"def fee(self):
    """"""Returns the fee of this transaction in satoshi""""""
    (input_sum, output_sum) = (0, 0)
    for tx_in in self.tx_ins:
        input_sum += tx_in.value(self.testnet)
    for tx_out in self.tx_outs:
        output_sum += tx_out.amount
    return input_sum - output_sum","for tx_in in self.tx_ins:
    input_sum += tx_in.value(self.testnet)","for i, tx_in in enumerate(self.tx_ins):
    input_sum += tx_in.value(self.testnet)"
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch12/tx.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/programmingbitcoin/code-ch12/tx.py,Tx,"def fee(self):
    """"""Returns the fee of this transaction in satoshi""""""
    (input_sum, output_sum) = (0, 0)
    for tx_in in self.tx_ins:
        input_sum += tx_in.value(self.testnet)
    for tx_out in self.tx_outs:
        output_sum += tx_out.amount
    return input_sum - output_sum","for tx_out in self.tx_outs:
    output_sum += tx_out.amount","for i, tx_out in enumerate(self.tx_outs):
    output_sum += tx_out.amount"
FakeNewsNet,https://github.com/KaiDMML/FakeNewsNet/tree/master/code/tweet_collection.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FakeNewsNet/code/tweet_collection.py,TweetCollector,"def collect_data(self, choices):
    for choice in choices:
        news_list = self.load_news_file(choice)
        collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for choice in choices:
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)","for i, choice in enumerate(choices):
    news_list = self.load_news_file(choice)
    collect_tweets(news_list, choice['news_source'], choice['label'], self.config)"
FARM,https://github.com/deepset-ai/FARM/tree/master/farm/modeling/biadaptive_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FARM/farm/modeling/biadaptive_model.py,BaseBiAdaptiveModel,"def connect_heads_with_processor(self, tasks, require_labels=True):
    """"""
        Populates prediction head with information coming from tasks.

        :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)
        :param require_labels: If True, an error will be thrown when a task is not supplied with labels)
        :return:
        """"""
    for head in self.prediction_heads:
        head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
        label_list = tasks[head.task_name]['label_list']
        if not label_list and require_labels:
            raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
        label_list = tasks[head.task_name]['label_list']
        head.label_list = label_list
        num_labels = len(label_list)
        head.metric = tasks[head.task_name]['metric']","for head in self.prediction_heads:
    head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
    label_list = tasks[head.task_name]['label_list']
    if not label_list and require_labels:
        raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
    label_list = tasks[head.task_name]['label_list']
    head.label_list = label_list
    num_labels = len(label_list)
    head.metric = tasks[head.task_name]['metric']","for i, head in enumerate(self.prediction_heads):
    head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
    label_list = tasks[head.task_name]['label_list']
    if not label_list and require_labels:
        raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
    label_list = tasks[head.task_name]['label_list']
    head.label_list = label_list
    num_labels = len(label_list)
    head.metric = tasks[head.task_name]['metric']"
heamy,https://github.com/rushter/heamy/tree/master/heamy/pipeline.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/heamy/heamy/pipeline.py,ModelsPipeline,"def stack(self, k=5, stratify=False, shuffle=True, seed=100, full_test=True, add_diff=False):
    """"""Stacks sequence of models.

        Parameters
        ----------

        k : int, default 5
            Number of folds.
        stratify : bool, default False
        shuffle : bool, default True
        seed : int, default 100
        full_test : bool, default True
            If True then evaluate test dataset on the full data otherwise take the mean of every fold.
        add_diff : bool, default False

        Returns
        -------
        `DataFrame`

        Examples
        --------
        >>> pipeline = ModelsPipeline(model_rf,model_lr)
        >>> stack_ds = pipeline.stack(k=10, seed=111)
        """"""
    result_train = []
    result_test = []
    y = None
    for model in self.models:
        result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
        train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
        test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
        result_train.append(train_df)
        result_test.append(test_df)
        if y is None:
            y = result.y_train
    result_train = pd.concat(result_train, axis=1)
    result_test = pd.concat(result_test, axis=1)
    if add_diff:
        result_train = feature_combiner(result_train)
        result_test = feature_combiner(result_test)
    ds = Dataset(X_train=result_train, y_train=y, X_test=result_test)
    return ds","for model in self.models:
    result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
    train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
    test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
    result_train.append(train_df)
    result_test.append(test_df)
    if y is None:
        y = result.y_train","for i,model in enumerate(self.models):
    result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)
    train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))
    test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))
    result_train.append(train_df)
    result_test.append(test_df)
    if y is None:
        y = result.y_train"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (name, cloud) in clouds.items():
    for (resource_name, resource) in cloud.resources.items():
        issues = issues.union(check_deprecations(resource))
        for (fname, f) in resource.filter_registry.items():
            if fname in ('and', 'or', 'not'):
                continue
            issues = issues.union(check_deprecations(f))
        for (aname, a) in resource.action_registry.items():
            issues = issues.union(check_deprecations(a))","for i, (name, cloud) in enumerate(clouds.items()):
    for (resource_name, resource) in cloud.resources.items():
        issues = issues.union(check_deprecations(resource))
        for (fname, f) in resource.filter_registry.items():
            if fname in ('and', 'or', 'not'):
                continue
            issues = issues.union(check_deprecations(f))
        for (aname, a) in resource.action_registry.items():
            issues = issues.union(check_deprecations(a))"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for dep in getattr(source, 'deprecations', ()):
    when = dep.removed_after
    if when is not None:
        name = f'{source.__module__}.{source.__name__}'
        if not isinstance(when, str):
            issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
            continue
        try:
            datetime.strptime(when, '%Y-%m-%d')
        except ValueError:
            issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")","for i, dep in enumerate(getattr(source, 'deprecations', ())):
    when = dep.removed_after
    if when is not None:
        name = f'{source.__module__}.{source.__name__}'
        if not isinstance(when, str):
            issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
            continue
        try:
            datetime.strptime(when, '%Y-%m-%d')
        except ValueError:
            issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (resource_name, resource) in cloud.resources.items():
    issues = issues.union(check_deprecations(resource))
    for (fname, f) in resource.filter_registry.items():
        if fname in ('and', 'or', 'not'):
            continue
        issues = issues.union(check_deprecations(f))
    for (aname, a) in resource.action_registry.items():
        issues = issues.union(check_deprecations(a))","for i, (resource_name, resource) in enumerate(cloud.resources.items()):
    issues = issues.union(check_deprecations(resource))
    for j, (fname, f) in enumerate(resource.filter_registry.items()):
        if fname in ('and', 'or', 'not'):
            continue
        issues = issues.union(check_deprecations(f))
    for k, (aname, a) in enumerate(resource.action_registry.items()):
        issues = issues.union(check_deprecations(a))"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","for (fname, f) in resource.filter_registry.items():
    if fname in ('and', 'or', 'not'):
        continue
    issues = issues.union(check_deprecations(f))","for i, (fname, f) in enumerate(resource.filter_registry.items()):
    if fname in ('and', 'or', 'not'):
        continue
    issues = issues.union(check_deprecations(f))"
swift,https://github.com/openstack/swift/tree/master/test/unit/obj/test_diskfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/obj/test_diskfile.py,BaseDiskFileTestMixin,"def setUp(self):
    skip_if_no_xattrs()
    self.tmpdir = mkdtemp()
    self.testdir = os.path.join(self.tmpdir, 'tmp_test_obj_server_DiskFile')
    self.existing_device = 'sda1'
    self.existing_device2 = 'sda2'
    for policy in POLICIES:
        mkdirs(os.path.join(self.testdir, self.existing_device, diskfile.get_tmp_dir(policy)))
        mkdirs(os.path.join(self.testdir, self.existing_device2, diskfile.get_tmp_dir(policy)))
    self._orig_tpool_exc = tpool.execute
    tpool.execute = lambda f, *args, **kwargs: f(*args, **kwargs)
    self.conf = dict(devices=self.testdir, mount_check='false', keep_cache_size=2 * 1024, mb_per_sync=1)
    self.logger = debug_logger('test-' + self.__class__.__name__)
    self.df_mgr = self.mgr_cls(self.conf, self.logger)
    self.df_router = diskfile.DiskFileRouter(self.conf, self.logger)
    self._ts_iter = (Timestamp(t) for t in itertools.count(int(time())))","for policy in POLICIES:
    mkdirs(os.path.join(self.testdir, self.existing_device, diskfile.get_tmp_dir(policy)))
    mkdirs(os.path.join(self.testdir, self.existing_device2, diskfile.get_tmp_dir(policy)))","for i,policy in enumerate(POLICIES):
    mkdirs(os.path.join(self.testdir, self.existing_device, diskfile.get_tmp_dir(policy)))
    mkdirs(os.path.join(self.testdir, self.existing_device2, diskfile.get_tmp_dir(policy)))"
taiga-back,https://github.com/taigaio/taiga-back/tree/master/taiga/importers/trello/importer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/taiga/importers/trello/importer.py,TrelloImporter,"def list_projects(self):
    projects_data = self._client.get('/members/me/boards', {'fields': 'id,name,desc,prefs,idOrganization', 'organization': 'true', 'organization_fields': 'prefs'})
    projects = []
    for project in projects_data:
        is_private = False
        if project['prefs']['permissionLevel'] == 'private':
            is_private = True
        if project['prefs']['permissionLevel'] == 'org':
            if 'organization' not in project:
                is_private = True
            elif 'prefs' not in project['organization']:
                is_private = True
            elif project['organization']['prefs']['permissionLevel'] == 'private':
                is_private = True
        projects.append({'id': project['id'], 'name': project['name'], 'description': project['desc'], 'is_private': is_private})
    return projects","for project in projects_data:
    is_private = False
    if project['prefs']['permissionLevel'] == 'private':
        is_private = True
    if project['prefs']['permissionLevel'] == 'org':
        if 'organization' not in project:
            is_private = True
        elif 'prefs' not in project['organization']:
            is_private = True
        elif project['organization']['prefs']['permissionLevel'] == 'private':
            is_private = True
    projects.append({'id': project['id'], 'name': project['name'], 'description': project['desc'], 'is_private': is_private})","for i, project in enumerate(projects_data):
    is_private = False
    if project['prefs']['permissionLevel'] == 'private':
        is_private = True
    if project['prefs']['permissionLevel'] == 'org':
        if 'organization' not in project:
            is_private = True
        elif 'prefs' not in project['organization']:
            is_private = True
        elif project['organization']['prefs']['permissionLevel'] == 'private':
            is_private = True
    projects.append({'id': project['id'], 'name': project['name'], 'description': project['desc'], 'is_private': is_private})"
leetCode,https://github.com/HuberTRoy/leetCode/tree/master/Array/NumberOfIslands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/leetCode/Array/NumberOfIslands.py,Solution,"def helper(x, y):
    Xy = self.makeXY(x, y)
    for i in Xy:
        try:
            if i[1] < 0 or i[0] < 0:
                continue
            if court[i[1]][i[0]] == '1':
                court[i[1]][i[0]] = '0'
                t = helper(i[0], i[1])
        except IndexError:
            continue
    else:
        return 1","for i in Xy:
    try:
        if i[1] < 0 or i[0] < 0:
            continue
        if court[i[1]][i[0]] == '1':
            court[i[1]][i[0]] = '0'
            t = helper(i[0], i[1])
    except IndexError:
        continue
else:
    return 1","for i, xy in enumerate(Xy):
    try:
        if xy[1] < 0 or xy[0] < 0:
            continue
        if court[xy[1]][xy[0]] == '1':
            court[xy[1]][xy[0]] = '0'
            t = helper(xy[0], xy[1])
    except IndexError:
        continue
else:
    return 1"
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for node in nodes:
    walk.append([node])","for i,node in enumerate(nodes):
    walk.append([node])"
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for l in range(max_depth):
    outdegree = self.outdegree(cur_nodes)
    mask = outdegree != 0
    if np.any(mask):
        cur_walk_ids = cur_walk_ids[mask]
        cur_nodes = cur_nodes[mask]
        outdegree = outdegree[mask]
    else:
        break
    succ = self.successor(cur_nodes)
    sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
    nxt_cur_nodes = []
    for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
        walk[walk_id].append(s[ind])
        nxt_cur_nodes.append(s[ind])
    cur_nodes = np.array(nxt_cur_nodes)","for l, _ in enumerate(range(max_depth)):
    outdegree = self.outdegree(cur_nodes)
    mask = outdegree != 0
    if np.any(mask):
        cur_walk_ids = cur_walk_ids[mask]
        cur_nodes = cur_nodes[mask]
        outdegree = outdegree[mask]
    else:
        break
    succ = self.successor(cur_nodes)
    sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
    nxt_cur_nodes = []
    for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
        walk[walk_id].append(s[ind])
        nxt_cur_nodes.append(s[ind])
    cur_nodes = np.array(nxt_cur_nodes)"
PGL,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/graph.py,Graph,"def random_walk(self, nodes, max_depth):
    """"""Implement of random walk.

        This function get random walks path for given nodes and depth.

        Args:
            nodes: Walk starting from nodes
            max_depth: Max walking depth

        Return:
            A list of walks.
        """"""
    walk = []
    for node in nodes:
        walk.append([node])
    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes)
    for l in range(max_depth):
        outdegree = self.outdegree(cur_nodes)
        mask = outdegree != 0
        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            outdegree = outdegree[mask]
        else:
            break
        succ = self.successor(cur_nodes)
        sample_index = np.floor(np.random.rand(outdegree.shape[0]) * outdegree).astype('int64')
        nxt_cur_nodes = []
        for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes)
    return walk","for (s, ind, walk_id) in zip(succ, sample_index, cur_walk_ids):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])","for i, (s, ind, walk_id) in enumerate(zip(succ, sample_index, cur_walk_ids)):
    walk[walk_id].append(s[ind])
    nxt_cur_nodes.append(s[ind])"
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for res_type in result_types:
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}","for i,res_type in enumerate(result_types):
    result_file = result_files[res_type]
    if result_file.endswith('.json'):
        coco_dets = coco.loadRes(result_file)
        gt_img_ids = coco.getImgIds()
        det_img_ids = coco_dets.getImgIds()
        iou_type = 'bbox' if res_type == 'proposal' else res_type
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        tgt_ids = gt_img_ids if not single_result else det_img_ids
        if single_result:
            res_dict = dict()
            for id_i in tgt_ids:
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = [id_i]
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
        cocoEval = COCOeval(coco, coco_dets, iou_type)
        if res_type == 'proposal':
            cocoEval.params.useCats = 0
            cocoEval.params.maxDets = list(max_dets)
        cocoEval.params.imgIds = tgt_ids
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}"
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","for id_i in tgt_ids:
    cocoEval = COCOeval(coco, coco_dets, iou_type)
    if res_type == 'proposal':
        cocoEval.params.useCats = 0
        cocoEval.params.maxDets = list(max_dets)
    cocoEval.params.imgIds = [id_i]
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
    res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})","for i, id_i in enumerate(tgt_ids):
    cocoEval = COCOeval(coco, coco_dets, iou_type)
    if res_type == 'proposal':
        cocoEval.params.useCats = 0
        cocoEval.params.maxDets = list(max_dets)
    cocoEval.params.imgIds = [id_i]
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
    res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})"
numpy,https://github.com/numpy/numpy/tree/master/numpy/distutils/command/install.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/distutils/command/install.py,install,"def run(self):
    if not have_setuptools:
        r = old_install.run(self)
    else:
        r = self.setuptools_run()
    if self.record:
        with open(self.record, 'r') as f:
            lines = []
            need_rewrite = False
            for l in f:
                l = l.rstrip()
                if ' ' in l:
                    need_rewrite = True
                    l = '""%s""' % l
                lines.append(l)
        if need_rewrite:
            self.execute(write_file, (self.record, lines), ""re-writing list of installed files to '%s'"" % self.record)
    return r","for l in f:
    l = l.rstrip()
    if ' ' in l:
        need_rewrite = True
        l = '""%s""' % l
    lines.append(l)","for i,l in enumerate(f):
    l = l.rstrip()
    if ' ' in l:
        need_rewrite = True
        l = '""%s""' % l
    lines.append(l)"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/once.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/once.py,OnceIE,"def _extract_once_formats(self, url, http_formats_preference=None):
    (domain_id, application_id, media_item_id) = re.match(OnceIE._VALID_URL, url).groups()
    formats = self._extract_m3u8_formats(self.ADAPTIVE_URL_TEMPLATE % (domain_id, application_id, media_item_id), media_item_id, 'mp4', m3u8_id='hls', fatal=False)
    progressive_formats = []
    for adaptive_format in formats:
        adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
        rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
        if rendition_id:
            progressive_format = adaptive_format.copy()
            progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
            progressive_formats.append(progressive_format)
    self._check_formats(progressive_formats, media_item_id)
    formats.extend(progressive_formats)
    return formats","for adaptive_format in formats:
    adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
    rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
    if rendition_id:
        progressive_format = adaptive_format.copy()
        progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
        progressive_formats.append(progressive_format)","for i, adaptive_format in enumerate(formats):
    adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
    rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
    if rendition_id:
        progressive_format = adaptive_format.copy()
        progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
        progressive_formats.append(progressive_format)"
HUNT,https://github.com/bugcrowd/HUNT/tree/master/Burp/lib/issues.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HUNT/Burp/lib/issues.py,Issues,"def create_scanner_issues(self, view, callbacks, helpers, vuln_parameters, request_response):
    issues = self.issues
    json = self.json
    for vuln_parameter in vuln_parameters:
        issue_name = vuln_parameter['vuln_name']
        vuln_param = vuln_parameter['vuln_param']
        param_name = vuln_parameter['param']
        param_value = vuln_parameter['value']
        url = helpers.analyzeRequest(request_response).getUrl()
        url = urlparse.urlsplit(str(url))
        hostname = url.hostname
        path = url.path
        url = url.scheme + '://' + url.hostname + url.path
        http_service = request_response.getHttpService()
        http_messages = [callbacks.applyMarkers(request_response, None, None)]
        detail = json['issues'][issue_name]['detail']
        severity = 'Medium'
        scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
        is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
        if is_scanner_issue_dupe:
            continue
        else:
            self.set_scanner_issues(scanner_issue)
        issue_count = self.set_issue_count(issue_name, vuln_param)
        total_count = self.total_count[issue_name]
        view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
        view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)","for vuln_parameter in vuln_parameters:
    issue_name = vuln_parameter['vuln_name']
    vuln_param = vuln_parameter['vuln_param']
    param_name = vuln_parameter['param']
    param_value = vuln_parameter['value']
    url = helpers.analyzeRequest(request_response).getUrl()
    url = urlparse.urlsplit(str(url))
    hostname = url.hostname
    path = url.path
    url = url.scheme + '://' + url.hostname + url.path
    http_service = request_response.getHttpService()
    http_messages = [callbacks.applyMarkers(request_response, None, None)]
    detail = json['issues'][issue_name]['detail']
    severity = 'Medium'
    scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
    is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
    if is_scanner_issue_dupe:
        continue
    else:
        self.set_scanner_issues(scanner_issue)
    issue_count = self.set_issue_count(issue_name, vuln_param)
    total_count = self.total_count[issue_name]
    view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
    view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)","for i,vuln_parameter in enumerate(vuln_parameters):
    issue_name = vuln_parameter['vuln_name']
    vuln_param = vuln_parameter['vuln_param']
    param_name = vuln_parameter['param']
    param_value = vuln_parameter['value']
    url = helpers.analyzeRequest(request_response).getUrl()
    url = urlparse.urlsplit(str(url))
    hostname = url.hostname
    path = url.path
    url = url.scheme + '://' + url.hostname + url.path
    http_service = request_response.getHttpService()
    http_messages = [callbacks.applyMarkers(request_response, None, None)]
    detail = json['issues'][issue_name]['detail']
    severity = 'Medium'
    scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
    is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
    if is_scanner_issue_dupe:
        continue
    else:
        self.set_scanner_issues(scanner_issue)
    issue_count = self.set_issue_count(issue_name, vuln_param)
    total_count = self.total_count[issue_name]
    view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
    view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)"
data-driven-web-apps-with-flask,https://github.com/talkpython/data-driven-web-apps-with-flask/tree/master/app/ch12-forms/starter/pypi_org/bin/load_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-driven-web-apps-with-flask/app/ch12-forms/starter/pypi_org/bin/load_data.py,,"def do_import_languages(file_data: List[dict]):
    imported = set()
    print('Importing languages ... ', flush=True)
    with progressbar.ProgressBar(max_value=len(file_data)) as bar:
        for (idx, p) in enumerate(file_data):
            info = p.get('info')
            classifiers = info.get('classifiers')
            for c in classifiers:
                if 'Programming Language' not in c:
                    continue
                original = c
                c = c.replace('Implementation ::', '').replace('::', ':')
                text = c
                parts = c.split(':')
                if len(parts) > 1:
                    text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
                if text not in imported:
                    imported.add(text)
                    session = db_session.create_session()
                    lang = ProgrammingLanguage()
                    lang.description = original
                    lang.id = text
                    session.add(lang)
                    session.commit()
            bar.update(idx)
    sys.stderr.flush()
    sys.stdout.flush()","for c in classifiers:
    if 'Programming Language' not in c:
        continue
    original = c
    c = c.replace('Implementation ::', '').replace('::', ':')
    text = c
    parts = c.split(':')
    if len(parts) > 1:
        text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
    if text not in imported:
        imported.add(text)
        session = db_session.create_session()
        lang = ProgrammingLanguage()
        lang.description = original
        lang.id = text
        session.add(lang)
        session.commit()","for i,c in enumerate(classifiers):
    if 'Programming Language' not in c:
        continue
    original = c
    c = c.replace('Implementation ::', '').replace('::', ':')
    text = c
    parts = c.split(':')
    if len(parts) > 1:
        text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
    if text not in imported:
        imported.add(text)
        session = db_session.create_session()
        lang = ProgrammingLanguage()
        lang.description = original
        lang.id = text
        session.add(lang)
        session.commit()"
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/test/training/test_synthetic_reward_training.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/test/training/test_synthetic_reward_training.py,,"def create_sequence_data(state_dim, action_dim, seq_len, batch_size, num_batches):
    SCALE = 2
    weight = SCALE * torch.randn(state_dim + action_dim)
    data = [None for _ in range(num_batches)]
    for i in range(num_batches):
        state = SCALE * torch.randn(seq_len, batch_size, state_dim)
        action = SCALE * torch.randn(seq_len, batch_size, action_dim)
        valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
        feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
        feature_mask = (feature_mask >= seq_len - valid_step).float()
        assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
        feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
        assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
        feature = torch.cat((state, action), dim=2)
        masked_feature = feature * feature_mask
        left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
        right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
        reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
        mask = torch.arange(seq_len).repeat(batch_size, 1)
        mask = (mask >= seq_len - valid_step).float()
        reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
        data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))
    return (weight, data)","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))","for i in range(num_batches):
    state = SCALE * torch.randn(seq_len, batch_size, state_dim)
    action = SCALE * torch.randn(seq_len, batch_size, action_dim)
    valid_step = torch.randint(1, seq_len + 1, (batch_size, 1))
    feature_mask = torch.arange(seq_len).repeat(batch_size, 1)
    feature_mask = (feature_mask >= seq_len - valid_step).float()
    assert feature_mask.shape == (batch_size, seq_len), feature_mask.shape
    feature_mask = feature_mask.transpose(0, 1).unsqueeze(-1)
    assert feature_mask.shape == (seq_len, batch_size, 1), feature_mask.shape
    feature = torch.cat((state, action), dim=2)
    masked_feature = feature * feature_mask
    left_shifted = torch.cat((masked_feature.narrow(0, 1, seq_len - 1), torch.zeros(1, batch_size, state_dim + action_dim)), dim=0)
    right_shifted = torch.cat((torch.zeros(1, batch_size, state_dim + action_dim), masked_feature.narrow(0, 0, seq_len - 1)), dim=0)
    reward_matrix = torch.matmul(left_shifted + right_shifted, weight).transpose(0, 1)
    mask = torch.arange(seq_len).repeat(batch_size, 1)
    mask = (mask >= seq_len - valid_step).float()
    reward = (reward_matrix * mask).sum(dim=1).reshape(-1, 1)
    data[i] = rlt.MemoryNetworkInput(state=rlt.FeatureData(state), action=rlt.FeatureData(action), valid_step=valid_step, reward=reward, next_state=torch.tensor([]), step=torch.tensor([]), not_terminal=torch.tensor([]), time_diff=torch.tensor([]))"
fgmk,https://github.com/ericoporto/fgmk/tree/master/fgmk/actions_wdgt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fgmk/fgmk/actions_wdgt.py,tinyActionsWdgt,"def removeAction(self):
    if self.ssettings == {}:
        return
    if len(self.ActionList.selectedItems()) < 1:
        return
    previous_actions = self.getValue()
    for item in self.ActionList.selectedItems():
        itemIndex = self.ActionList.row(item)
        self.ActionList.takeItem(itemIndex)
    current_actions = self.getValue()
    self.somethingChanged.emit(previous_actions, current_actions, 'remove', 'remove action')","for item in self.ActionList.selectedItems():
    itemIndex = self.ActionList.row(item)
    self.ActionList.takeItem(itemIndex)","for i, item in enumerate(self.ActionList.selectedItems()):
    itemIndex = self.ActionList.row(item)
    self.ActionList.takeItem(itemIndex)"
contrastive-unpaired-translation,https://github.com/taesungp/contrastive-unpaired-translation/tree/master/datasets/make_dataset_aligned.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/contrastive-unpaired-translation/datasets/make_dataset_aligned.py,,"def align_images(a_file_paths, b_file_paths, target_path):
    if not os.path.exists(target_path):
        os.makedirs(target_path)
    for i in range(len(a_file_paths)):
        img_a = Image.open(a_file_paths[i])
        img_b = Image.open(b_file_paths[i])
        assert img_a.size == img_b.size
        aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
        aligned_image.paste(img_a, (0, 0))
        aligned_image.paste(img_b, (img_a.size[0], 0))
        aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))","for i in range(len(a_file_paths)):
    img_a = Image.open(a_file_paths[i])
    img_b = Image.open(b_file_paths[i])
    assert img_a.size == img_b.size
    aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
    aligned_image.paste(img_a, (0, 0))
    aligned_image.paste(img_b, (img_a.size[0], 0))
    aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))","for i, file_path in enumerate(a_file_paths):
    img_a = Image.open(file_path)
    img_b = Image.open(b_file_paths[i])
    assert img_a.size == img_b.size
    aligned_image = Image.new('RGB', (img_a.size[0] * 2, img_a.size[1]))
    aligned_image.paste(img_a, (0, 0))
    aligned_image.paste(img_b, (img_a.size[0], 0))
    aligned_image.save(os.path.join(target_path, '{:04d}.jpg'.format(i)))"
transformers,https://github.com/huggingface/transformers/tree/master/tests/test_modeling_tf_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/tests/test_modeling_tf_common.py,TFModelTesterMixin,"def test_numpy_arrays_inputs(self):
    (config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()

    def prepare_numpy_arrays(inputs_dict):
        inputs_np_dict = {}
        for (k, v) in inputs_dict.items():
            if tf.is_tensor(v):
                inputs_np_dict[k] = v.numpy()
            else:
                inputs_np_dict[k] = np.array(k)
        return inputs_np_dict
    for model_class in self.all_model_classes:
        model = model_class(config)
        inputs = self._prepare_for_class(inputs_dict, model_class)
        inputs_np = prepare_numpy_arrays(inputs)
        output_for_dict_input = model(inputs_np)
        output_for_kw_input = model(**inputs_np)
        self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for model_class in self.all_model_classes:
    model = model_class(config)
    inputs = self._prepare_for_class(inputs_dict, model_class)
    inputs_np = prepare_numpy_arrays(inputs)
    output_for_dict_input = model(inputs_np)
    output_for_kw_input = model(**inputs_np)
    self.assert_outputs_same(output_for_dict_input, output_for_kw_input)","for i,model_class in enumerate(self.all_model_classes):
    model = model_class(config)
    inputs = self._prepare_for_class(inputs_dict, model_class)
    inputs_np = prepare_numpy_arrays(inputs)
    output_for_dict_input = model(inputs_np)
    output_for_kw_input = model(**inputs_np)
    self.assert_outputs_same(output_for_dict_input, output_for_kw_input)"
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","for ci in qs.iterator():
    if ci.position:
        try:
            ia = ci.position.order.invoice_address
        except InvoiceAddress.DoesNotExist:
            ia = InvoiceAddress()
    yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","for i, ci in enumerate(qs.iterator()):
    if ci.position:
        try:
            ia = ci.position.order.invoice_address
        except InvoiceAddress.DoesNotExist:
            ia = InvoiceAddress()
    yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for j in range(self['ny']):
    x = 0.0
    for i in range(self['nx']):
        if i == 0 and j == 0:
            x += dx
            continue
        undoinfo = []
        newblocks = []
        for bid in blocks:
            undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
            newblocks.append((pos, None))
            pos += 1
        app.addUndo(undoinfo)
        app.gcode.moveLines(newblocks, x, y)
        x += dx
    y += dy","for j in range(self['ny']):
    x = 0.0
    for i, _ in enumerate(range(self['nx'])):
        if i == 0 and j == 0:
            x += dx
            continue
        undoinfo = []
        newblocks = []
        for bid in blocks:
            undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
            newblocks.append((pos, None))
            pos += 1
        app.addUndo(undoinfo)
        app.gcode.moveLines(newblocks, x, y)
        x += dx
    y += dy"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for i in range(self['nx']):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx","for i, _ in enumerate(range(self['nx'])):
    if i == 0 and j == 0:
        x += dx
        continue
    undoinfo = []
    newblocks = []
    for bid in blocks:
        undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
        newblocks.append((pos, None))
        pos += 1
    app.addUndo(undoinfo)
    app.gcode.moveLines(newblocks, x, y)
    x += dx"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/tile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/tile.py,Tool,"def execute(self, app):
    blocks = app.editor.getSelectedBlocks()
    if not blocks:
        app.editor.selectAll()
        blocks = app.editor.getSelectedBlocks()
    if not blocks:
        messagebox.showerror(_('Tile error'), _('No g-code blocks selected'))
        return
    try:
        dx = self.fromMm('dx')
    except Exception:
        dx = 0.0
    try:
        dy = self.fromMm('dy')
    except Exception:
        dy = 0.0
    pos = blocks[-1]
    y = 0.0
    pos += 1
    for j in range(self['ny']):
        x = 0.0
        for i in range(self['nx']):
            if i == 0 and j == 0:
                x += dx
                continue
            undoinfo = []
            newblocks = []
            for bid in blocks:
                undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
                newblocks.append((pos, None))
                pos += 1
            app.addUndo(undoinfo)
            app.gcode.moveLines(newblocks, x, y)
            x += dx
        y += dy
    app.refresh()
    app.setStatus(_('Tiled selected blocks'))","for bid in blocks:
    undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
    newblocks.append((pos, None))
    pos += 1","for i,bid in enumerate(blocks):
    undoinfo.append(app.gcode.cloneBlockUndo(bid, pos))
    newblocks.append((pos, None))
    pos += 1"
robosuite,https://github.com/ARISE-Initiative/robosuite/tree/master/robosuite/demos/demo_collect_and_playback_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/robosuite/robosuite/demos/demo_collect_and_playback_data.py,,"def collect_random_trajectory(env, timesteps=1000):
    """"""Run a random policy to collect trajectories.

    The rollout trajectory is saved to files in npz format.
    Modify the DataCollectionWrapper wrapper to add new fields or change data formats.

    Args:
        env (MujocoEnv): environment instance to collect trajectories from
        timesteps(int): how many environment timesteps to run for a given trajectory
    """"""
    env.reset()
    dof = env.action_dim
    for t in range(timesteps):
        action = np.random.randn(dof)
        env.step(action)
        env.render()
        if t % 100 == 0:
            print(t)","for t in range(timesteps):
    action = np.random.randn(dof)
    env.step(action)
    env.render()
    if t % 100 == 0:
        print(t)","for i,t in enumerate(range(timesteps)):
    action = np.random.randn(dof)
    env.step(action)
    env.render()
    if i % 100 == 0:
        print(i)"
mars,https://github.com/mars-project/mars/tree/master/mars/dataframe/arithmetic/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/dataframe/arithmetic/core.py,DataFrameBinOpMixin,"def _tile_dataframe_series(cls, op):
    (left, right) = (op.lhs, op.rhs)
    df = op.outputs[0]
    (nsplits, out_shape, left_chunks, right_chunks) = align_dataframe_series(left, right, axis=op.axis)
    out_chunk_indexes = itertools.product(*(range(s) for s in out_shape))
    out_chunks = []
    for (out_idx, df_chunk) in zip(out_chunk_indexes, left_chunks):
        if op.axis == 'columns' or op.axis == 1:
            series_chunk = right_chunks[out_idx[1]]
            kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
        else:
            series_chunk = right_chunks[out_idx[0]]
            kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
        out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
        out_chunks.append(out_chunk)
    new_op = op.copy()
    return new_op.new_dataframes(op.inputs, df.shape, nsplits=tuple((tuple(ns) for ns in nsplits)), chunks=out_chunks, dtypes=df.dtypes, index_value=df.index_value, columns_value=df.columns_value)","for (out_idx, df_chunk) in zip(out_chunk_indexes, left_chunks):
    if op.axis == 'columns' or op.axis == 1:
        series_chunk = right_chunks[out_idx[1]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
    else:
        series_chunk = right_chunks[out_idx[0]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
    out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
    out_chunks.append(out_chunk)","for i, (out_idx, df_chunk) in enumerate(zip(out_chunk_indexes, left_chunks)):
    if op.axis == 'columns' or op.axis == 1:
        series_chunk = right_chunks[out_idx[1]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'index_value': df_chunk.index_value, 'dtypes_value': df_chunk.dtypes_value}
    else:
        series_chunk = right_chunks[out_idx[0]]
        kw = {'shape': (nsplits[0][out_idx[0]], nsplits[1][out_idx[1]]), 'columns_value': df_chunk.columns_value, 'dtypes_value': df_chunk.dtypes_value}
    out_chunk = op.copy().reset_key().new_chunk([df_chunk, series_chunk], index=out_idx, **kw)
    out_chunks.append(out_chunk)"
espresso,https://github.com/freewym/espresso/tree/master/fairseq/data/multilingual/multilingual_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/data/multilingual/multilingual_utils.py,,"def augment_dictionary(dictionary: Dictionary, language_list: List[str], lang_tok_style: str, langtoks_specs: Sequence[str]=(LangTokSpec.main.value,), extra_data: Optional[Dict[str, str]]=None) -> None:
    for spec in langtoks_specs:
        for language in language_list:
            dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))
    if lang_tok_style == LangTokStyle.mbart.value or (extra_data is not None and LangTokSpec.mono_dae.value in extra_data):
        dictionary.add_symbol('<mask>')","for spec in langtoks_specs:
    for language in language_list:
        dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))","for i, spec in enumerate(langtoks_specs):
    for language in language_list:
        dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))"
espresso,https://github.com/freewym/espresso/tree/master/fairseq/data/multilingual/multilingual_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/data/multilingual/multilingual_utils.py,,"def augment_dictionary(dictionary: Dictionary, language_list: List[str], lang_tok_style: str, langtoks_specs: Sequence[str]=(LangTokSpec.main.value,), extra_data: Optional[Dict[str, str]]=None) -> None:
    for spec in langtoks_specs:
        for language in language_list:
            dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))
    if lang_tok_style == LangTokStyle.mbart.value or (extra_data is not None and LangTokSpec.mono_dae.value in extra_data):
        dictionary.add_symbol('<mask>')","for language in language_list:
    dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))","for i, language in enumerate(language_list):
    dictionary.add_symbol(get_lang_tok(lang=language, lang_tok_style=lang_tok_style, spec=spec))"
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/core/HydrusData.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/core/HydrusData.py,,"def GetSiblingProcessPorts(db_path, instance):
    path = os.path.join(db_path, instance + '_running')
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            file_text = f.read()
            try:
                (pid, create_time) = HydrusText.DeserialiseNewlinedTexts(file_text)
                pid = int(pid)
                create_time = float(create_time)
            except ValueError:
                return None
            try:
                if psutil.pid_exists(pid):
                    ports = []
                    p = psutil.Process(pid)
                    for conn in p.connections():
                        if conn.status == 'LISTEN':
                            ports.append(int(conn.laddr[1]))
                    return ports
            except psutil.Error:
                return None
    return None","for conn in p.connections():
    if conn.status == 'LISTEN':
        ports.append(int(conn.laddr[1]))","for i, conn in enumerate(p.connections()):
    if conn.status == 'LISTEN':
        ports.append(int(conn.laddr[1]))"
python-miio,https://github.com/rytilahti/python-miio/tree/master/miio/click_common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-miio/miio/click_common.py,DeviceGroupMeta,"def _get_commands_for_namespace(namespace):
    commands = {}
    for (_, val) in namespace.items():
        if not callable(val):
            continue
        device_group_command = getattr(val, '_device_group_command', None)
        if device_group_command is None:
            continue
        commands[device_group_command.command_name] = device_group_command
    return commands","for (_, val) in namespace.items():
    if not callable(val):
        continue
    device_group_command = getattr(val, '_device_group_command', None)
    if device_group_command is None:
        continue
    commands[device_group_command.command_name] = device_group_command","for i, (_, val) in enumerate(namespace.items()):
    if not callable(val):
        continue
    device_group_command = getattr(val, '_device_group_command', None)
    if device_group_command is None:
        continue
    commands[device_group_command.command_name] = device_group_command"
keras-transformer,https://github.com/kpot/keras-transformer/tree/master/example/run_gpt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-transformer/example/run_gpt.py,,"def stream_bpe_tokens():
    for line in training_set.splitlines():
        clean_line = line.strip()
        if not clean_line:
            continue
        id_word_pairs = encoder(clean_line)
        yield from id_word_pairs","for line in training_set.splitlines():
    clean_line = line.strip()
    if not clean_line:
        continue
    id_word_pairs = encoder(clean_line)
    yield from id_word_pairs","for i, line in enumerate(training_set.splitlines()):
    clean_line = line.strip()
    if not clean_line:
        continue
    id_word_pairs = encoder(clean_line)
    yield from id_word_pairs"
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for (arn, role_entry_j) in all_roles.items():
    role_entry = json.loads(role_entry_j)
    if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
        roles_to_delete_from_cache.append(arn)","for i, (arn, role_entry_j) in enumerate(all_roles.items()):
    role_entry = json.loads(role_entry_j)
    if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
        roles_to_delete_from_cache.append(arn)"
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for account_id in accounts_d.keys():
    if config.get('environment') in ['prod', 'dev']:
        tasks.append(cache_iam_resources_for_account.s(account_id))
    else:
        log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
        if account_id in config.get('celery.test_account_ids', []):
            tasks.append(cache_iam_resources_for_account.s(account_id))","for i, account_id in enumerate(accounts_d.keys()):
    if config.get('environment') in ['prod', 'dev']:
        tasks.append(cache_iam_resources_for_account.s(account_id))
    else:
        log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
        if account_id in config.get('celery.test_account_ids', []):
            tasks.append(cache_iam_resources_for_account.s(account_id))"
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for role_entry in roles:
    _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)","for i, role_entry in enumerate(roles):
    _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)"
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","for arn in roles_to_delete_from_cache:
    all_roles.pop(arn, None)","for i, arn in enumerate(roles_to_delete_from_cache):
    all_roles.pop(arn, None)"
fairseq,https://github.com/pytorch/fairseq/tree/master/fairseq/models/lstm.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/fairseq/models/lstm.py,LSTMDecoder,"def extract_features(self, prev_output_tokens, encoder_out: Optional[Tuple[Tensor, Tensor, Tensor, Tensor]]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):
    """"""
        Similar to *forward* but only return features.
        """"""
    if encoder_out is not None:
        encoder_outs = encoder_out[0]
        encoder_hiddens = encoder_out[1]
        encoder_cells = encoder_out[2]
        encoder_padding_mask = encoder_out[3]
    else:
        encoder_outs = torch.empty(0)
        encoder_hiddens = torch.empty(0)
        encoder_cells = torch.empty(0)
        encoder_padding_mask = torch.empty(0)
    srclen = encoder_outs.size(0)
    if incremental_state is not None and len(incremental_state) > 0:
        prev_output_tokens = prev_output_tokens[:, -1:]
    (bsz, seqlen) = prev_output_tokens.size()
    x = self.embed_tokens(prev_output_tokens)
    x = self.dropout_in_module(x)
    x = x.transpose(0, 1)
    if incremental_state is not None and len(incremental_state) > 0:
        (prev_hiddens, prev_cells, input_feed) = self.get_cached_state(incremental_state)
    elif encoder_out is not None:
        prev_hiddens = [encoder_hiddens[i] for i in range(self.num_layers)]
        prev_cells = [encoder_cells[i] for i in range(self.num_layers)]
        if self.encoder_hidden_proj is not None:
            prev_hiddens = [self.encoder_hidden_proj(y) for y in prev_hiddens]
            prev_cells = [self.encoder_cell_proj(y) for y in prev_cells]
        input_feed = x.new_zeros(bsz, self.hidden_size)
    else:
        zero_state = x.new_zeros(bsz, self.hidden_size)
        prev_hiddens = [zero_state for i in range(self.num_layers)]
        prev_cells = [zero_state for i in range(self.num_layers)]
        input_feed = None
    assert srclen > 0 or self.attention is None, 'attention is not supported if there are no encoder outputs'
    attn_scores: Optional[Tensor] = x.new_zeros(srclen, seqlen, bsz) if self.attention is not None else None
    outs = []
    for j in range(seqlen):
        if input_feed is not None:
            input = torch.cat((x[j, :, :], input_feed), dim=1)
        else:
            input = x[j]
        for (i, rnn) in enumerate(self.layers):
            (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
            input = self.dropout_out_module(hidden)
            if self.residuals:
                input = input + prev_hiddens[i]
            prev_hiddens[i] = hidden
            prev_cells[i] = cell
        if self.attention is not None:
            assert attn_scores is not None
            (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
        else:
            out = hidden
        out = self.dropout_out_module(out)
        if input_feed is not None:
            input_feed = out
        outs.append(out)
    prev_hiddens_tensor = torch.stack(prev_hiddens)
    prev_cells_tensor = torch.stack(prev_cells)
    cache_state = torch.jit.annotate(Dict[str, Optional[Tensor]], {'prev_hiddens': prev_hiddens_tensor, 'prev_cells': prev_cells_tensor, 'input_feed': input_feed})
    self.set_incremental_state(incremental_state, 'cached_state', cache_state)
    x = torch.cat(outs, dim=0).view(seqlen, bsz, self.hidden_size)
    x = x.transpose(1, 0)
    if hasattr(self, 'additional_fc') and self.adaptive_softmax is None:
        x = self.additional_fc(x)
        x = self.dropout_out_module(x)
    if not self.training and self.need_attn and (self.attention is not None):
        assert attn_scores is not None
        attn_scores = attn_scores.transpose(0, 2)
    else:
        attn_scores = None
    return (x, attn_scores)","for j in range(seqlen):
    if input_feed is not None:
        input = torch.cat((x[j, :, :], input_feed), dim=1)
    else:
        input = x[j]
    for (i, rnn) in enumerate(self.layers):
        (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
        input = self.dropout_out_module(hidden)
        if self.residuals:
            input = input + prev_hiddens[i]
        prev_hiddens[i] = hidden
        prev_cells[i] = cell
    if self.attention is not None:
        assert attn_scores is not None
        (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
    else:
        out = hidden
    out = self.dropout_out_module(out)
    if input_feed is not None:
        input_feed = out
    outs.append(out)","for j, _ in enumerate(range(seqlen)):
    if input_feed is not None:
        input = torch.cat((x[j, :, :], input_feed), dim=1)
    else:
        input = x[j]
    for (i, rnn) in enumerate(self.layers):
        (hidden, cell) = rnn(input, (prev_hiddens[i], prev_cells[i]))
        input = self.dropout_out_module(hidden)
        if self.residuals:
            input = input + prev_hiddens[i]
        prev_hiddens[i] = hidden
        prev_cells[i] = cell
    if self.attention is not None:
        assert attn_scores is not None
        (out, attn_scores[:, j, :]) = self.attention(hidden, encoder_outs, encoder_padding_mask)
    else:
        out = hidden
    out = self.dropout_out_module(out)
    if input_feed is not None:
        input_feed = out
    outs.append(out)"
hydra,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/defaults_list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/defaults_list.py,,"def _update_overrides(defaults_list: List[InputDefault], overrides: Overrides, parent: InputDefault, interpolated_subtree: bool) -> None:
    seen_override = False
    last_override_seen = None
    for d in defaults_list:
        if d.is_self():
            continue
        d.update_parent(parent.get_group_path(), parent.get_final_package())
        legacy_hydra_override = False
        if isinstance(d, GroupDefault):
            assert d.group is not None
            if not version.base_at_least('1.2'):
                legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
        if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
            assert isinstance(last_override_seen, GroupDefault)
            pcp = parent.get_config_path()
            okey = last_override_seen.get_override_key()
            oval = last_override_seen.get_name()
            raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
        if isinstance(d, GroupDefault):
            if legacy_hydra_override:
                d.override = True
                url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
                msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
                deprecation_warning(msg)
            if d.override:
                if not legacy_hydra_override:
                    seen_override = True
                last_override_seen = d
                if interpolated_subtree:
                    raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
                overrides.add_override(parent.get_config_path(), d)","for d in defaults_list:
    if d.is_self():
        continue
    d.update_parent(parent.get_group_path(), parent.get_final_package())
    legacy_hydra_override = False
    if isinstance(d, GroupDefault):
        assert d.group is not None
        if not version.base_at_least('1.2'):
            legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
    if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
        assert isinstance(last_override_seen, GroupDefault)
        pcp = parent.get_config_path()
        okey = last_override_seen.get_override_key()
        oval = last_override_seen.get_name()
        raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
    if isinstance(d, GroupDefault):
        if legacy_hydra_override:
            d.override = True
            url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
            msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
            deprecation_warning(msg)
        if d.override:
            if not legacy_hydra_override:
                seen_override = True
            last_override_seen = d
            if interpolated_subtree:
                raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
            overrides.add_override(parent.get_config_path(), d)","for i,d in enumerate(defaults_list):
    if d.is_self():
        continue
    d.update_parent(parent.get_group_path(), parent.get_final_package())
    legacy_hydra_override = False
    if isinstance(d, GroupDefault):
        assert d.group is not None
        if not version.base_at_least('1.2'):
            legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
    if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
        assert isinstance(last_override_seen, GroupDefault)
        pcp = parent.get_config_path()
        okey = last_override_seen.get_override_key()
        oval = last_override_seen.get_name()
        raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
    if isinstance(d, GroupDefault):
        if legacy_hydra_override:
            d.override = True
            url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
            msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
            deprecation_warning(msg)
        if d.override:
            if not legacy_hydra_override:
                seen_override = True
            last_override_seen = d
            if interpolated_subtree:
                raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
            overrides.add_override(parent.get_config_path(), d)"
kube-janitor,https://github.com/hjacobs/kube-janitor/tree/master/kube_janitor/resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kube-janitor/kube_janitor/resources.py,,"def get_namespaced_resource_types(api):
    for (api_version, resource) in discover_namespaced_api_resources(api):
        clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
        yield clazz","for (api_version, resource) in discover_namespaced_api_resources(api):
    clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
    yield clazz","for i, (api_version, resource) in enumerate(discover_namespaced_api_resources(api)):
    clazz = namespaced_object_factory(resource['kind'], resource['name'], api_version)
    yield clazz"
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_imperative_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_imperative_optimizer.py,TestOptimizerLearningRate,"def func_test_set_lr(self):
    with fluid.dygraph.guard():
        a = np.random.uniform(-0.1, 0.1, [10, 10]).astype('float32')
        linear = paddle.nn.Linear(10, 10)
        a = fluid.dygraph.to_variable(a)
        b = linear(a)
        loss = paddle.mean(b)
        adam = fluid.optimizer.Adam(0.1, parameter_list=linear.parameters())
        lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
        for i in range(5):
            adam.set_lr(lr_list[i])
            adam.minimize(loss)
            lr = adam.current_step_lr()
            np.testing.assert_allclose(lr, lr_list[i], rtol=1e-06, atol=0.0)
        lr_var = fluid.layers.create_global_var(shape=[1], value=0.7, dtype='float32')
        adam.set_lr(lr_var)
        adam.minimize(loss)
        lr = adam.current_step_lr()
        np.testing.assert_allclose(lr, 0.7, rtol=1e-06, atol=0.0)
        with self.assertRaises(RuntimeError):
            adam = fluid.optimizer.Adam(fluid.dygraph.NaturalExpDecay(learning_rate=0.1, decay_steps=3, decay_rate=0.5, staircase=True), parameter_list=linear.parameters())
            adam.set_lr(0.01)","for i in range(5):
    adam.set_lr(lr_list[i])
    adam.minimize(loss)
    lr = adam.current_step_lr()
    np.testing.assert_allclose(lr, lr_list[i], rtol=1e-06, atol=0.0)","for i, _ in enumerate(range(5)):
    adam.set_lr(lr_list[i])
    adam.minimize(loss)
    lr = adam.current_step_lr()
    np.testing.assert_allclose(lr, lr_list[i], rtol=1e-06, atol=0.0)"
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/build_gen.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/build_gen.py,,"def _build_gen_ppc(net, ppc):
    """"""
    Takes the empty ppc network and fills it with the gen values. The gen
    datatype will be float afterwards.

    **INPUT**:
        **net** -The pandapower format network

        **ppc** - The PYPOWER format network to fill in values
    """"""
    mode = net['_options']['mode']
    distributed_slack = net['_options']['distributed_slack']
    if mode == 'estimate':
        return
    _is_elements = net['_is_elements']
    gen_order = dict()
    f = 0
    for element in ['ext_grid', 'gen']:
        f = add_gen_order(gen_order, element, _is_elements, f)
    if mode == 'opf':
        if len(net.dcline) > 0:
            ppc['dcline'] = net.dcline[['loss_mw', 'loss_percent']].values
        for element in ['sgen_controllable', 'load_controllable', 'storage_controllable']:
            f = add_gen_order(gen_order, element, _is_elements, f)
    f = add_gen_order(gen_order, 'xward', _is_elements, f)
    _init_ppc_gen(net, ppc, f)
    for (element, (f, t)) in gen_order.items():
        add_element_to_gen(net, ppc, element, f, t)
    net._gen_order = gen_order
    if distributed_slack:
        xward_pq_buses = _get_xward_pq_buses(net, ppc)
        (gen_mask, xward_mask) = _gen_xward_mask(net, ppc)
        _normalise_slack_weights(ppc, gen_mask, xward_mask, xward_pq_buses)","for element in ['ext_grid', 'gen']:
    f = add_gen_order(gen_order, element, _is_elements, f)","for i, element in enumerate(['ext_grid', 'gen']):
    f = add_gen_order(gen_order, element, _is_elements, f)"
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/build_gen.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/build_gen.py,,"def _build_gen_ppc(net, ppc):
    """"""
    Takes the empty ppc network and fills it with the gen values. The gen
    datatype will be float afterwards.

    **INPUT**:
        **net** -The pandapower format network

        **ppc** - The PYPOWER format network to fill in values
    """"""
    mode = net['_options']['mode']
    distributed_slack = net['_options']['distributed_slack']
    if mode == 'estimate':
        return
    _is_elements = net['_is_elements']
    gen_order = dict()
    f = 0
    for element in ['ext_grid', 'gen']:
        f = add_gen_order(gen_order, element, _is_elements, f)
    if mode == 'opf':
        if len(net.dcline) > 0:
            ppc['dcline'] = net.dcline[['loss_mw', 'loss_percent']].values
        for element in ['sgen_controllable', 'load_controllable', 'storage_controllable']:
            f = add_gen_order(gen_order, element, _is_elements, f)
    f = add_gen_order(gen_order, 'xward', _is_elements, f)
    _init_ppc_gen(net, ppc, f)
    for (element, (f, t)) in gen_order.items():
        add_element_to_gen(net, ppc, element, f, t)
    net._gen_order = gen_order
    if distributed_slack:
        xward_pq_buses = _get_xward_pq_buses(net, ppc)
        (gen_mask, xward_mask) = _gen_xward_mask(net, ppc)
        _normalise_slack_weights(ppc, gen_mask, xward_mask, xward_pq_buses)","for element in ['sgen_controllable', 'load_controllable', 'storage_controllable']:
    f = add_gen_order(gen_order, element, _is_elements, f)","for i, element in enumerate(['sgen_controllable', 'load_controllable', 'storage_controllable']):
    f = add_gen_order(gen_order, element, _is_elements, f)"
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/modeling/data_handler/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/haystack/haystack/modeling/data_handler/processor.py,TextClassificationProcessor,"def convert_labels(self, dictionary: Dict):
    ret: Dict = {}
    for (task_name, task) in self.tasks.items():
        label_name = task['label_name']
        label_raw = dictionary[label_name]
        label_list = task['label_list']
        if task['task_type'] == 'classification':
            label_ids = [label_list.index(label_raw)]
        elif task['task_type'] == 'multilabel_classification':
            label_ids = [0] * len(label_list)
            for l in label_raw.split(','):
                if l != '':
                    label_ids[label_list.index(l)] = 1
        ret[task['label_tensor_name']] = label_ids
    return ret","for (task_name, task) in self.tasks.items():
    label_name = task['label_name']
    label_raw = dictionary[label_name]
    label_list = task['label_list']
    if task['task_type'] == 'classification':
        label_ids = [label_list.index(label_raw)]
    elif task['task_type'] == 'multilabel_classification':
        label_ids = [0] * len(label_list)
        for l in label_raw.split(','):
            if l != '':
                label_ids[label_list.index(l)] = 1
    ret[task['label_tensor_name']] = label_ids","for i, (task_name, task) in enumerate(self.tasks.items()):
    label_name = task['label_name']
    label_raw = dictionary[label_name]
    label_list = task['label_list']
    if task['task_type'] == 'classification':
        label_ids = [label_list.index(label_raw)]
    elif task['task_type'] == 'multilabel_classification':
        label_ids = [0] * len(label_list)
        for l in label_raw.split(','):
            if l != '':
                label_ids[label_list.index(l)] = 1
    ret[task['label_tensor_name']] = label_ids"
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/modeling/data_handler/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/haystack/haystack/modeling/data_handler/processor.py,TextClassificationProcessor,"def convert_labels(self, dictionary: Dict):
    ret: Dict = {}
    for (task_name, task) in self.tasks.items():
        label_name = task['label_name']
        label_raw = dictionary[label_name]
        label_list = task['label_list']
        if task['task_type'] == 'classification':
            label_ids = [label_list.index(label_raw)]
        elif task['task_type'] == 'multilabel_classification':
            label_ids = [0] * len(label_list)
            for l in label_raw.split(','):
                if l != '':
                    label_ids[label_list.index(l)] = 1
        ret[task['label_tensor_name']] = label_ids
    return ret","for l in label_raw.split(','):
    if l != '':
        label_ids[label_list.index(l)] = 1","for i,l in enumerate(label_raw.split(',')):
    if l != '':
        label_ids[label_list.index(l)] = 1"
mypy,https://github.com/python/mypy/tree/master/mypy/join.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/join.py,,"def combine_arg_names(t: CallableType, s: CallableType) -> List[Optional[str]]:
    """"""Produces a list of argument names compatible with both callables.

    For example, suppose 't' and 's' have the following signatures:

    - t: (a: int, b: str, X: str) -> None
    - s: (a: int, b: str, Y: str) -> None

    This function would return [""a"", ""b"", None]. This information
    is then used above to compute the join of t and s, which results
    in a signature of (a: int, b: str, str) -> None.

    Note that the third argument's name is omitted and 't' and 's'
    are both valid subtypes of this inferred signature.

    Precondition: is_similar_types(t, s) is true.
    """"""
    num_args = len(t.arg_types)
    new_names = []
    for i in range(num_args):
        t_name = t.arg_names[i]
        s_name = s.arg_names[i]
        if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
            new_names.append(t_name)
        else:
            new_names.append(None)
    return new_names","for i in range(num_args):
    t_name = t.arg_names[i]
    s_name = s.arg_names[i]
    if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
        new_names.append(t_name)
    else:
        new_names.append(None)","for i, (t_name, s_name) in enumerate(zip(t.arg_names, s.arg_names)):
    if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
        new_names.append(t_name)
    else:
        new_names.append(None)"
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/addons/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mitmproxy/mitmproxy/addons/core.py,Core,"def revert(self, flows: typing.Sequence[flow.Flow]) -> None:
    """"""
            Revert flow changes.
        """"""
    updated = []
    for f in flows:
        if f.modified():
            f.revert()
            updated.append(f)
    ctx.log.alert('Reverted %s flows.' % len(updated))
    ctx.master.addons.trigger(hooks.UpdateHook(updated))","for f in flows:
    if f.modified():
        f.revert()
        updated.append(f)","for i,f in enumerate(flows):
    if f.modified():
        f.revert()
        updated.append(f)"
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for f in filenames:
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))","for i,f in enumerate(filenames):
    with open(f, 'r') as input_file:
        json_data = json.load(input_file)
        for item in json_data:
            print(json.dumps(item))"
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for line in input_file:
    filenames.append(line.rstrip())","for i, line in enumerate(input_file):
    filenames.append(line.rstrip())"
pshtt,https://github.com/cisagov/pshtt/tree/master/gce-scripts/combine_shards.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pshtt/gce-scripts/combine_shards.py,,"def main():
    if len(sys.argv) < 2:
        print('you need a filename!')
        exit(1)
    master_file = sys.argv[1]
    filenames = []
    with open(master_file, 'r') as input_file:
        for line in input_file:
            filenames.append(line.rstrip())
    for f in filenames:
        with open(f, 'r') as input_file:
            json_data = json.load(input_file)
            for item in json_data:
                print(json.dumps(item))","for item in json_data:
    print(json.dumps(item))","for i,item in enumerate(json_data):
    print(json.dumps(item))"
cmake_format,https://github.com/cheshirekow/cmake_format/tree/master/cmakelang/tools/bump_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cmake_format/cmakelang/tools/bump_version.py,,"def main():
    fields = ['major', 'minor', 'patch', 'dev', 'drop-dev']
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('field', nargs='?', choices=fields, default='dev')
    args = parser.parse_args()
    thisdir = os.path.dirname(os.path.realpath(__file__))
    rootdir = os.path.dirname(os.path.dirname(thisdir))
    init_path = os.path.join(rootdir, 'cmakelang/__init__.py')
    current_version = get_current_version(init_path)
    if args.field == 'drop-dev':
        new_version = current_version[:3]
    else:
        field_idx = fields.index(args.field)
        new_version = list(current_version)
        new_version[field_idx] += 1
        for idx in range(field_idx + 1, len(new_version)):
            new_version[idx] = 0
    process_init(init_path, new_version)
    process_installation_rst(os.path.join(rootdir, 'cmakelang/doc/installation.rst'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package.json'), new_version)
    process_json(os.path.join(rootdir, 'cmakelang/vscode_extension/package-lock.json'), new_version)
    return 0","for idx in range(field_idx + 1, len(new_version)):
    new_version[idx] = 0","for idx, _ in enumerate(new_version[field_idx + 1:], start=field_idx + 1):
    new_version[idx] = 0"
cozmo-python-sdk,https://github.com/anki/cozmo-python-sdk/tree/master/examples/apps/quizmaster_cozmo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cozmo-python-sdk/examples/apps/quizmaster_cozmo.py,CozmoQuizMaster,"def turn_player_lights_on(self):
    for player in self._players:
        player.turn_light_on()","for player in self._players:
    player.turn_light_on()","for i, player in enumerate(self._players):
    player.turn_light_on()"
aliyun-openapi-python-sdk,https://github.com/aliyun/aliyun-openapi-python-sdk/tree/master/aliyun-python-sdk-core/aliyunsdkcore/vendored/requests/packages/urllib3/_collections.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aliyun-openapi-python-sdk/aliyun-python-sdk-core/aliyunsdkcore/vendored/requests/packages/urllib3/_collections.py,HTTPHeaderDict,"def __iter__(self):
    for vals in self._container.values():
        yield vals[0]","for vals in self._container.values():
    yield vals[0]","for i, vals in enumerate(self._container.values()):
    yield vals[0]"
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
    (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
    self.writer.add_summary(summary, i)
    (accuracy, _) = self.accuracy(predictions, labels)
    if i % 50 == 0:
        print('Minibatch loss at step %d: %f' % (i, l))
        print('Minibatch accuracy: %.1f%%' % accuracy)","for i, (samples, labels) in enumerate(data_iterator(train_samples, train_labels, self.train_batch_size)):
    (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
    self.writer.add_summary(summary, i)
    (accuracy, _) = self.accuracy(predictions, labels)
    if i % 50 == 0:
        print('Minibatch loss at step %d: %f' % (i, l))
        print('Minibatch accuracy: %.1f%%' % accuracy)"
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
    print('samples shape', samples.shape)
    (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
    self.writer.add_summary(summary, i)
    (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
    accuracies.append(accuracy)
    confusionMatrices.append(cm)
    print('Test Accuracy: %.1f%%' % accuracy)","for i, (samples, labels) in enumerate(data_iterator(test_samples, test_labels, self.test_batch_size)):
    print('samples shape', samples.shape)
    (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
    self.writer.add_summary(summary, i)
    (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
    accuracies.append(accuracy)
    confusionMatrices.append(cm)
    print('Test Accuracy: %.1f%%' % accuracy)"
vid2vid,https://github.com/NVIDIA/vid2vid/tree/master/models/networks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vid2vid/models/networks.py,MultiscaleDiscriminator,"def forward(self, input):
    num_D = self.num_D
    result = []
    input_downsampled = input
    for i in range(num_D):
        if self.getIntermFeat:
            model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
        else:
            model = getattr(self, 'layer' + str(num_D - 1 - i))
        result.append(self.singleD_forward(model, input_downsampled))
        if i != num_D - 1:
            input_downsampled = self.downsample(input_downsampled)
    return result","for i in range(num_D):
    if self.getIntermFeat:
        model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
    else:
        model = getattr(self, 'layer' + str(num_D - 1 - i))
    result.append(self.singleD_forward(model, input_downsampled))
    if i != num_D - 1:
        input_downsampled = self.downsample(input_downsampled)","for i in range(num_D):
    if self.getIntermFeat:
        model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in range(self.n_layers + 2)]
    else:
        model = getattr(self, 'layer' + str(num_D - 1 - i))
    result.append(self.singleD_forward(model, input_downsampled))
    if i != num_D - 1:
        input_downsampled = self.downsample(input_downsampled)"
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/database/mssql/mssql_db.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/database/mssql/mssql_db.py,,"def db_import(conn, cursor, module, db, target):
    if os.path.isfile(target):
        backup = open(target, 'r')
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
        return (0, 'import successful', '')
    else:
        return (1, 'cannot find target file', 'cannot find target file')","for line in backup:
    if line is None:
        break
    elif line.startswith('GO'):
        cursor.execute(sqlQuery)
        sqlQuery = 'USE [%s]\n' % db
    else:
        sqlQuery += line","for i,line in enumerate(backup):
    if line is None:
        break
    elif line.startswith('GO'):
        cursor.execute(sqlQuery)
        sqlQuery = 'USE [%s]\n' % db
    else:
        sqlQuery += line"
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape != subspace.shape:
        all_non_vectorized = False
        break"
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')","for (key, subspace) in observation_space.spaces.items():
    if observation[key].shape[1:] != subspace.shape:
        all_vectorized = False
        break","for i, (key, subspace) in enumerate(observation_space.spaces.items()):
    if observation[key].shape[1:] != subspace.shape:
        all_vectorized = False
        break"
agents,https://github.com/tensorflow/agents/tree/master/tf_agents/eval/metric_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/agents/tf_agents/eval/metric_utils.py,,"def compute(metrics, environment, policy, num_episodes=1):
    """"""Compute metrics using `policy` on the `environment`.

  Args:
    metrics: List of metrics to compute.
    environment: py_environment instance.
    policy: py_policy instance used to step the environment. A tf_policy can be
      used in_eager_mode.
    num_episodes: Number of episodes to compute the metrics over.

  Returns:
    A dictionary of results {metric_name: metric_value}
  """"""
    for metric in metrics:
        metric.reset()
    time_step = environment.reset()
    policy_state = policy.get_initial_state(environment.batch_size)
    driver = py_driver.PyDriver(environment, policy, observers=metrics, max_steps=None, max_episodes=num_episodes)
    driver.run(time_step, policy_state)
    results = [(metric.name, metric.result()) for metric in metrics]
    return collections.OrderedDict(results)","for metric in metrics:
    metric.reset()","for i, metric in enumerate(metrics):
    metric.reset()"
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","for (model, classes) in models:
    net = model(pretrained=pretrained)
    ctx = mx.cpu()
    if not pretrained:
        net.initialize(ctx=ctx)
    net_params = net.collect_params()
    weight_count = 0
    for param in net_params.values():
        if param.shape is None or not param._differentiable:
            continue
        weight_count += np.prod(param.shape)
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resdropresnet20_cifar10 or weight_count == 272474
    assert model != resdropresnet20_cifar100 or weight_count == 278324
    assert model != resdropresnet20_svhn or weight_count == 272474
    x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
    y = net(x)
    assert y.shape == (14, classes)","for i, (model, classes) in enumerate(models):
    net = model(pretrained=pretrained)
    ctx = mx.cpu()
    if not pretrained:
        net.initialize(ctx=ctx)
    net_params = net.collect_params()
    weight_count = 0
    for param in net_params.values():
        if param.shape is None or not param._differentiable:
            continue
        weight_count += np.prod(param.shape)
    print('m={}, {}'.format(model.__name__, weight_count))
    assert model != resdropresnet20_cifar10 or weight_count == 272474
    assert model != resdropresnet20_cifar100 or weight_count == 278324
    assert model != resdropresnet20_svhn or weight_count == 272474
    x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
    y = net(x)
    assert y.shape == (14, classes)"
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","for param in net_params.values():
    if param.shape is None or not param._differentiable:
        continue
    weight_count += np.prod(param.shape)","for i,param in enumerate(net_params.values()):
    if param.shape is None or not param._differentiable:
        continue
    weight_count += np.prod(param.shape)"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for item in self.data.trainSet_i:
    if len(self.data.trainSet_i[item]) > 1:
        self.itemNet[item] = self.data.trainSet_i[item]","for i, item in enumerate(self.data.trainSet_i):
    if len(self.data.trainSet_i[item]) > 1:
        self.itemNet[item] = self.data.trainSet_i[item]"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for item in self.itemNet:
    for user in self.itemNet[item]:
        if self.itemNet[item][user] >= 1:
            self.filteredRatings[user].append(item)","for i,item in enumerate(self.itemNet):
    for user in self.itemNet[item]:
        if self.itemNet[item][user] >= 1:
            self.filteredRatings[user].append(self.itemNet[i])"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.filteredRatings:
    s1 = set(self.filteredRatings[user1])
    for user2 in self.filteredRatings:
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight","for i,user1 in enumerate(self.filteredRatings):
    s1 = set(self.filteredRatings[user1])
    for j,user2 in enumerate(self.filteredRatings):
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.CUNet:
    sims = []
    u1 = self.data.user[user1]
    self.W[u1] = model.wv[user1]
    for user2 in self.CUNet:
        if user1 != user2:
            u2 = self.data.user[user2]
            self.W[u2] = model.wv[user2]
            sims.append((user2, cosine(self.W[u1], self.W[u2])))
    self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
    i += 1
    if i % 200 == 0:
        print('progress:', i, '/', len(self.CUNet))","for i, user1 in enumerate(self.CUNet):
    sims = []
    u1 = self.data.user[user1]
    self.W[u1] = model.wv[user1]
    for j, user2 in enumerate(self.CUNet):
        if i != j:
            u2 = self.data.user[user2]
            self.W[u2] = model.wv[user2]
            sims.append((user2, cosine(self.W[u1], self.W[u2])))
    self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
    if i % 200 == 0:
        print('progress:', i, '/', len(self.CUNet))"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user in self.itemNet[item]:
    if self.itemNet[item][user] >= 1:
        self.filteredRatings[user].append(item)","for i,user in enumerate(self.itemNet[item]):
    if self.itemNet[item][user] >= 1:
        self.filteredRatings[user].append(item)"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user2 in self.filteredRatings:
    if user1 != user2:
        s2 = set(self.filteredRatings[user2])
        weight = len(s1.intersection(s2))
        if weight > 0:
            self.CUNet[user1] += [user2] * weight","for i,user2 in enumerate(self.filteredRatings):
    if user1 != user2:
        s2 = set(self.filteredRatings[user2])
        weight = len(s1.intersection(s2))
        if weight > 0:
            self.CUNet[user1] += [user2] * weight"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for t in range(self.walkCount):
    path = [user]
    lastNode = user
    for i in range(1, self.walkLength):
        nextNode = choice(self.CUNet[lastNode])
        count = 0
        while nextNode in self.visited[lastNode]:
            nextNode = choice(self.CUNet[lastNode])
            count += 1
            if count == 10:
                break
        path.append(nextNode)
        self.visited[user][nextNode] = 1
        lastNode = nextNode
    self.walks.append(path)","for t in range(self.walkCount):
    path = [user]
    lastNode = user
    for i, _ in enumerate(range(1, self.walkLength)):
        nextNode = choice(self.CUNet[lastNode])
        count = 0
        while nextNode in self.visited[lastNode]:
            nextNode = choice(self.CUNet[lastNode])
            count += 1
            if count == 10:
                break
        path.append(nextNode)
        self.visited[user][nextNode] = 1
        lastNode = nextNode
    self.walks.append(path)"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user2 in self.CUNet:
    if user1 != user2:
        u2 = self.data.user[user2]
        self.W[u2] = model.wv[user2]
        sims.append((user2, cosine(self.W[u1], self.W[u2])))","for i,user2 in enumerate(self.CUNet):
    if user1 != user2:
        u2 = self.data.user[user2]
        self.W[u2] = model.wv[user2]
        sims.append((user2, cosine(self.W[u1], self.W[u2]))))"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for entry in self.data.trainingData:
    (user, item, rating) = entry
    u = self.data.user[user]
    i = self.data.item[item]
    error = rating - self.P[u].dot(self.Q[i])
    self.loss += error ** 2
    p = self.P[u]
    q = self.Q[i]
    self.P[u] += self.lRate * (error * q - self.regU * p)
    self.Q[i] += self.lRate * (error * p - self.regI * q)","for idx, entry in enumerate(self.data.trainingData):
    (user, item, rating) = entry
    u = self.data.user[user]
    i = self.data.item[item]
    error = rating - self.P[u].dot(self.Q[i])
    self.loss += error ** 2
    p = self.P[u]
    q = self.Q[i]
    self.P[u] += self.lRate * (error * q - self.regU * p)
    self.Q[i] += self.lRate * (error * p - self.regI * q)"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user in self.CUNet:
    u = self.data.user[user]
    friends = self.topKSim[user]
    for friend in friends:
        uf = self.data.user[friend[0]]
        self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
        self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])","for i,user in enumerate(self.CUNet):
    u = self.data.user[user]
    friends = self.topKSim[user]
    for j,friend in enumerate(friends):
        uf = self.data.user[friend[0]]
        self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
        self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])"
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for friend in friends:
    uf = self.data.user[friend[0]]
    self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
    self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])","for i, friend in enumerate(friends):
    uf = self.data.user[friend[0]]
    self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
    self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])"
py3status,https://github.com/ultrabug/py3status/tree/master/py3status/modules/net_iplist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/py3status/py3status/modules/net_iplist.py,Py3status,"def _get_data(self):
    txt = self.py3.command_output(['ip', 'address', 'show']).splitlines()
    data = {}
    for line in txt:
        iface = self.iface_re.match(line)
        if iface:
            cur_iface = iface.group('iface')
            if not self.remove_empty:
                data[cur_iface] = {}
            continue
        ip4 = self.ip_re.match(line)
        if ip4:
            data.setdefault(cur_iface, {}).setdefault('ip4', []).append(ip4.group('ip4'))
            continue
        ip6 = self.ip6_re.match(line)
        if ip6:
            data.setdefault(cur_iface, {}).setdefault('ip6', []).append(ip6.group('ip6'))
            continue
    return data","for line in txt:
    iface = self.iface_re.match(line)
    if iface:
        cur_iface = iface.group('iface')
        if not self.remove_empty:
            data[cur_iface] = {}
        continue
    ip4 = self.ip_re.match(line)
    if ip4:
        data.setdefault(cur_iface, {}).setdefault('ip4', []).append(ip4.group('ip4'))
        continue
    ip6 = self.ip6_re.match(line)
    if ip6:
        data.setdefault(cur_iface, {}).setdefault('ip6', []).append(ip6.group('ip6'))
        continue","for i,line in enumerate(txt):
    iface = self.iface_re.match(line)
    if iface:
        cur_iface = iface.group('iface')
        if not self.remove_empty:
            data[cur_iface] = {}
        continue
    ip4 = self.ip_re.match(line)
    if ip4:
        data.setdefault(cur_iface, {}).setdefault('ip4', []).append(ip4.group('ip4'))
        continue
    ip6 = self.ip6_re.match(line)
    if ip6:
        data.setdefault(cur_iface, {}).setdefault('ip6', []).append(ip6.group('ip6'))
        continue"
pylearn2,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/models/dbm/ising.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pylearn2/pylearn2/models/dbm/ising.py,IsingHidden,"def mf_update(self, state_below, state_above, layer_above=None, double_weights=False, iter_name=None):
    """"""
        .. todo::

            WRITEME
        """"""
    self.input_space.validate(state_below)
    if self.requires_reformat:
        if not isinstance(state_below, tuple):
            for sb in get_debug_values(state_below):
                if sb.shape[0] != self.dbm.batch_size:
                    raise ValueError('self.dbm.batch_size is %d but got ' + 'shape of %d' % (self.dbm.batch_size, sb.shape[0]))
                assert reduce(operator.mul, sb.shape[1:]) == self.input_dim
        state_below = self.input_space.format_as(state_below, self.desired_space)
    if iter_name is None:
        iter_name = 'anon'
    if state_above is not None:
        assert layer_above is not None
        msg = layer_above.downward_message(state_above)
        msg.name = 'msg_from_' + layer_above.layer_name + '_to_' + self.layer_name + '[' + iter_name + ']'
    else:
        msg = None
    if double_weights:
        state_below = 2.0 * state_below
        state_below.name = self.layer_name + '_' + iter_name + '_2state'
    z = self.transformer.lmul(state_below) + self.b
    if self.layer_name is not None and iter_name is not None:
        z.name = self.layer_name + '_' + iter_name + '_z'
    if msg is not None:
        z = z + msg
    h = T.tanh(self.beta * z)
    return h","for sb in get_debug_values(state_below):
    if sb.shape[0] != self.dbm.batch_size:
        raise ValueError('self.dbm.batch_size is %d but got ' + 'shape of %d' % (self.dbm.batch_size, sb.shape[0]))
    assert reduce(operator.mul, sb.shape[1:]) == self.input_dim","for i, sb in enumerate(get_debug_values(state_below)):
    if sb.shape[0] != self.dbm.batch_size:
        raise ValueError('self.dbm.batch_size is %d but got ' + 'shape of %d' % (self.dbm.batch_size, sb.shape[0]))
    assert reduce(operator.mul, sb.shape[1:]) == self.input_dim"
hummingbird,https://github.com/microsoft/hummingbird/tree/master/hummingbird/ml/operator_converters/onnx/imputer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbird/hummingbird/ml/operator_converters/onnx/imputer.py,,"def convert_onnx_imputer(operator, device=None, extra_config={}):
    """"""
    Converter for `ai.onnx.ml.Imputer`

    Args:
        operator: An operator wrapping a `ai.onnx.ml.Imputer` model
        device: String defining the type of device the converted operator should be run on
        extra_config: Extra configuration used to select the best conversion strategy

    Returns:
        A PyTorch model
    """"""
    stats = missing = None
    for attr in operator.raw_operator.origin.attribute:
        if attr.name == 'imputed_value_floats':
            stats = np.array(attr.floats).astype('float64')
        elif attr.name == 'replaced_value_float':
            missing = attr.f
    if any((v is None for v in [stats, missing])):
        raise RuntimeError('Error parsing Imputer, found unexpected None. stats: {}, missing: {}', stats, missing)
    return SimpleImputer(operator, device, statistics=stats, missing=missing, strategy='constant')","for attr in operator.raw_operator.origin.attribute:
    if attr.name == 'imputed_value_floats':
        stats = np.array(attr.floats).astype('float64')
    elif attr.name == 'replaced_value_float':
        missing = attr.f","for i, attr in enumerate(operator.raw_operator.origin.attribute):
    if attr.name == 'imputed_value_floats':
        stats = np.array(attr.floats).astype('float64')
    elif attr.name == 'replaced_value_float':
        missing = attr.f"
zvt,https://github.com/zvtvz/zvt/tree/master/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,ChinaETFDayKdataRecorder,"def on_finish_entity(self, entity):
    kdatas = get_kdata(entity_id=entity.id, level=IntervalLevel.LEVEL_1DAY.value, order=Etf1dKdata.timestamp.asc(), return_type='domain', session=self.session, filters=[Etf1dKdata.cumulative_net_value.is_(None)])
    if kdatas and len(kdatas) > 0:
        start = kdatas[0].timestamp
        end = kdatas[-1].timestamp
        df = self.fetch_cumulative_net_value(entity, start, end)
        if df is not None and (not df.empty):
            for kdata in kdatas:
                if kdata.timestamp in df.index:
                    kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
                    kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']
            self.session.commit()
            self.logger.info(f'{entity.code} - {entity.name}...')","for kdata in kdatas:
    if kdata.timestamp in df.index:
        kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
        kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']","for i,kdata in enumerate(kdatas):
    if kdata.timestamp in df.index:
        kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
        kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']"
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for _index in range(len(domain_segments)):
    full_domain = '.'.join(domain_segments)
    potential_domain = Domain.query.filter(Domain.name == full_domain).first()
    if potential_domain in domains:
        domain = potential_domain
        break
    domain_segments.pop(0)","for _index, segment in enumerate(domain_segments):
    full_domain = '.'.join(domain_segments)
    potential_domain = Domain.query.filter(Domain.name == full_domain).first()
    if potential_domain in domains:
        domain = potential_domain
        break
    domain_segments.pop(0)"
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)","for ip in myip_addr or remote_addr:
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()","for i,ip in enumerate(myip_addr or remote_addr):
    if isinstance(ip, ipaddress.IPv4Address):
        rtype = 'A'
    else:
        rtype = 'AAAA'
    r = Record(name=hostname, type=rtype)
    if r.exists(domain.name) and r.is_allowed_edit():
        if r.data == str(ip):
            history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
            history.add()
        else:
            oldip = r.data
            result = r.update(domain.name, str(ip))
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
            else:
                response = '911'
                break
    elif r.is_allowed_edit():
        ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
        if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
            rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
            rrset = {'rrsets': rrset_data}
            result = Record().add(domain.name, rrset)
            if result['status'] == 'ok':
                history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                history.add()
                response = 'good'
    else:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()"
moto,https://github.com/spulec/moto/tree/master/moto/s3/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/s3/models.py,FakeMultipart,"def list_parts(self, part_number_marker, max_parts):
    max_marker = part_number_marker + max_parts
    for part_id in self.partlist[part_number_marker:max_marker]:
        yield self.parts[part_id]","for part_id in self.partlist[part_number_marker:max_marker]:
    yield self.parts[part_id]","for i, part_id in enumerate(self.partlist[part_number_marker:max_marker]):
    yield self.parts[part_id]"
chainer,https://github.com/chainer/chainer/tree/master/examples/chainermn/seq2seq/seq2seq_mp1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chainer/examples/chainermn/seq2seq/seq2seq_mp1.py,Decoder,"def translate(self, xs, max_length=100):
    batch = len(xs)
    with chainer.no_backprop_mode():
        with chainer.using_config('train', False):
            result = []
            ys = self.xp.zeros(batch, self.xp.int32)
            eys = self.embed_y(ys)
            eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
            (h, c, ys, _) = self.mn_decoder(eys)
            cys = chainer.functions.concat(ys, axis=0)
            wy = self.W(cys)
            ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
            result.append(ys)
            for i in range(1, max_length):
                eys = self.embed_y(ys)
                eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
                (h, c, ys) = self.mn_decoder.actual_rnn(h, c, eys)
                cys = chainer.functions.concat(ys, axis=0)
                wy = self.W(cys)
                ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
                result.append(ys)
    result = cuda.to_cpu(self.xp.stack(result).T)
    outs = []
    for y in result:
        inds = numpy.argwhere(y == 0)
        if len(inds) > 0:
            y = y[:inds[0, 0]]
        outs.append(y)
    return outs","for i in range(1, max_length):
    eys = self.embed_y(ys)
    eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
    (h, c, ys) = self.mn_decoder.actual_rnn(h, c, eys)
    cys = chainer.functions.concat(ys, axis=0)
    wy = self.W(cys)
    ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
    result.append(ys)","for i, _ in enumerate(range(1, max_length)):
    eys = self.embed_y(ys)
    eys = chainer.functions.split_axis(eys, batch, 0, force_tuple=True)
    (h, c, ys) = self.mn_decoder.actual_rnn(h, c, eys)
    cys = chainer.functions.concat(ys, axis=0)
    wy = self.W(cys)
    ys = self.xp.argmax(wy.data, axis=1).astype(self.xp.int32)
    result.append(ys)"
TSD,https://github.com/Sense-X/TSD/tree/master/mmdet/models/detectors/reppoints_detector.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/detectors/reppoints_detector.py,RepPointsDetector,"def merge_aug_results(self, aug_bboxes, aug_scores, img_metas):
    """"""Merge augmented detection bboxes and scores.

        Args:
            aug_bboxes (list[Tensor]): shape (n, 4*#class)
            aug_scores (list[Tensor] or None): shape (n, #class)
            img_shapes (list[Tensor]): shape (3, ).

        Returns:
            tuple: (bboxes, scores)
        """"""
    recovered_bboxes = []
    for (bboxes, img_info) in zip(aug_bboxes, img_metas):
        img_shape = img_info[0]['img_shape']
        scale_factor = img_info[0]['scale_factor']
        flip = img_info[0]['flip']
        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
        recovered_bboxes.append(bboxes)
    bboxes = torch.cat(recovered_bboxes, dim=0)
    if aug_scores is None:
        return bboxes
    else:
        scores = torch.cat(aug_scores, dim=0)
        return (bboxes, scores)","for (bboxes, img_info) in zip(aug_bboxes, img_metas):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)","for i, (bboxes, img_info) in enumerate(zip(aug_bboxes, img_metas)):
    img_shape = img_info[0]['img_shape']
    scale_factor = img_info[0]['scale_factor']
    flip = img_info[0]['flip']
    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)
    recovered_bboxes.append(bboxes)"
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)","for (img_path, annotation_path) in tqdm(filename_pairs):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)","for i, (img_path, annotation_path) in enumerate(tqdm(filename_pairs)):
    img = misc.imread(img_path)
    img = misc.imresize(img, SIZE)
    imgs.append(img)
    annotation = misc.imread(annotation_path)
    annotation[annotation <= 128] = 0
    annotation[annotation > 128] = 1
    annotation = misc.imresize(annotation, SIZE, 'nearest')
    labels.append(annotation)"
videoflow,https://github.com/videoflow/videoflow/tree/master/tests/test_release_resources.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videoflow/tests/test_release_resources.py,,"def test_bboxannotator_resources():
    for datasetid in BoundingBoxAnnotator.supported_datasets:
        filename = f'labels_{datasetid}.pbtxt'
        url_path = BASE_URL_DETECTION + filename
        get_file(filename, url_path)","for datasetid in BoundingBoxAnnotator.supported_datasets:
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)","for i, datasetid in enumerate(BoundingBoxAnnotator.supported_datasets):
    filename = f'labels_{datasetid}.pbtxt'
    url_path = BASE_URL_DETECTION + filename
    get_file(filename, url_path)"
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for marker in self.tracker.markers:
    marker_type = marker.marker_type
    if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
        continue
    centroid = marker.centroid()
    if marker.uid in surface.registered_markers_dist.keys():
        active_markers = active_markers_by_type.get(marker_type, [])
        active_markers.append(centroid)
        active_markers_by_type[marker_type] = active_markers
    else:
        inactive_markers = inactive_markers_by_type.get(marker_type, [])
        inactive_markers.append(centroid)
        inactive_markers_by_type[marker_type] = inactive_markers","for i, marker in enumerate(self.tracker.markers):
    marker_type = marker.marker_type
    if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
        continue
    centroid = marker.centroid()
    if marker.uid in surface.registered_markers_dist.keys():
        active_markers = active_markers_by_type.get(marker_type, [])
        active_markers.append(centroid)
        active_markers_by_type[marker_type] = active_markers
    else:
        inactive_markers = inactive_markers_by_type.get(marker_type, [])
        inactive_markers.append(centroid)
        inactive_markers_by_type[marker_type] = inactive_markers"
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for (marker_type, inactive_markers) in inactive_markers_by_type.items():
    color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in inactive_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, (marker_type, inactive_markers) in enumerate(inactive_markers_by_type.items()):
    color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in inactive_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))"
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for (marker_type, active_markers) in active_markers_by_type.items():
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, (marker_type, active_markers) in enumerate(active_markers_by_type.items()):
    color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
    color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
    for pt in active_markers:
        self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))"
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/surface_tracker/gui.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pupil/pupil_src/shared_modules/surface_tracker/gui.py,GUI,"def _draw_marker_toggles(self, surface):
    active_markers_by_type = {}
    inactive_markers_by_type = {}
    for marker in self.tracker.markers:
        marker_type = marker.marker_type
        if marker_type == Surface_Marker_Type.SQUARE and marker.perimeter < self.tracker.marker_detector.marker_min_perimeter:
            continue
        centroid = marker.centroid()
        if marker.uid in surface.registered_markers_dist.keys():
            active_markers = active_markers_by_type.get(marker_type, [])
            active_markers.append(centroid)
            active_markers_by_type[marker_type] = active_markers
        else:
            inactive_markers = inactive_markers_by_type.get(marker_type, [])
            inactive_markers.append(centroid)
            inactive_markers_by_type[marker_type] = inactive_markers
    for (marker_type, inactive_markers) in inactive_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_INACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in inactive_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))
    for (marker_type, active_markers) in active_markers_by_type.items():
        color_rgb = SURFACE_MARKER_TOGGLE_ACTIVE_COLOR_RGB_BY_TYPE[marker_type]
        color_rgba = rgb_to_rgba(color_rgb, alpha=0.8)
        for pt in active_markers:
            self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for pt in inactive_markers:
    self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))","for i, pt in enumerate(inactive_markers):
    self._draw_circle_filled(tuple(pt), size=20 / 2, color=pyglui_utils.RGBA(*color_rgba))"
kaggle_ndsb2017,https://github.com/juliandewit/kaggle_ndsb2017/tree/master//helpers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kaggle_ndsb2017//helpers.py,,"def get_segmented_lungs(im, plot=False):
    binary = im < -400
    cleared = clear_border(binary)
    label_image = label(cleared)
    areas = [r.area for r in regionprops(label_image)]
    areas.sort()
    if len(areas) > 2:
        for region in regionprops(label_image):
            if region.area < areas[-2]:
                for coordinates in region.coords:
                    label_image[coordinates[0], coordinates[1]] = 0
    binary = label_image > 0
    selem = disk(2)
    binary = binary_erosion(binary, selem)
    selem = disk(10)
    binary = binary_closing(binary, selem)
    edges = roberts(binary)
    binary = ndi.binary_fill_holes(edges)
    get_high_vals = binary == 0
    im[get_high_vals] = -2000
    return (im, binary)","for coordinates in region.coords:
    label_image[coordinates[0], coordinates[1]] = 0","for i, coordinates in enumerate(region.coords):
    label_image[coordinates[0], coordinates[1]] = 0"
bitarray,https://github.com/ilanschnell/bitarray/tree/master/bitarray/test_bitarray.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bitarray/bitarray/test_bitarray.py,CreateObjectTests,"def foo():
    for x in lst:
        yield x","for x in lst:
    yield x","for i,x in enumerate(lst):
    yield x"
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","for message in messages:
    signal_declarations = []
    for signal in message.signals:
        signal_declaration = ''
        if floating_point_numbers:
            signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declarations.append(signal_declaration)
    declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
    if signal_declarations:
        declaration += '\n' + '\n'.join(signal_declarations)
    declarations.append(declaration)","for i, message in enumerate(messages):
    signal_declarations = []
    for signal in message.signals:
        signal_declaration = ''
        if floating_point_numbers:
            signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
        signal_declarations.append(signal_declaration)
    declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
    if signal_declarations:
        declaration += '\n' + '\n'.join(signal_declarations)
    declarations.append(declaration)"
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","for signal in message.signals:
    signal_declaration = ''
    if floating_point_numbers:
        signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declarations.append(signal_declaration)","for i, signal in enumerate(message.signals):
    signal_declaration = ''
    if floating_point_numbers:
        signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
    signal_declarations.append(signal_declaration)"
PyQt5-Apps,https://github.com/taseikyo/PyQt5-Apps/tree/master/hust-lib/src/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/hust-lib/src/main.py,MWin,"def resolveDataDone(self, data):
    self.statusBar.showMessage('...', 1000)
    if self.first_kw:
        self.total_pages = int(data[0])
        self.jump_page.setMaximum(self.total_pages)
        self.page_count.setText(f' {self.total_pages} ')
        self.first_kw = False
    self.current_page_index.setText(f' {self.cur_page} ')
    self.jump_page.setValue(self.cur_page)
    self.pre = data[1]
    self.suf = data[2]
    self.request.pre = self.pre
    self.request.suf = self.suf
    books = data[-1]
    if not self.table_has_header:
        self.table.setColumnCount(2)
        self.table.setHorizontalHeaderLabels(['', ''])
        self.table_has_header = True
    else:
        self.table.clearContents()
        self.table.setRowCount(0)
    lists = []
    for x in books:
        tl = x.find('span', {'class', 'briefcitTitle'}).a
        title = tl.text
        link = tl['href']
        detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
        lists.append([title, detail, link])
        row = self.table.rowCount()
        self.table.insertRow(row)
        self.table.setItem(row, 0, QTableWidgetItem(title))
        self.table.setItem(row, 1, QTableWidgetItem(detail))
    self.book_lists = lists","for x in books:
    tl = x.find('span', {'class', 'briefcitTitle'}).a
    title = tl.text
    link = tl['href']
    detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
    lists.append([title, detail, link])
    row = self.table.rowCount()
    self.table.insertRow(row)
    self.table.setItem(row, 0, QTableWidgetItem(title))
    self.table.setItem(row, 1, QTableWidgetItem(detail))","for i,x in enumerate(books):
    tl = x.find('span', {'class', 'briefcitTitle'}).a
    title = tl.text
    link = tl['href']
    detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
    lists.append([title, detail, link])
    row = self.table.rowCount()
    self.table.insertRow(row)
    self.table.setItem(row, 0, QTableWidgetItem(title))
    self.table.setItem(row, 1, QTableWidgetItem(detail))"
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/syndication_tests/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v22/tests/syndication_tests/tests.py,SyndicationFeedTest,"def test_rss2_single_enclosure(self):
    response = self.client.get('/syndication/rss2/single-enclosure/')
    doc = minidom.parseString(response.content)
    chan = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0]
    items = chan.getElementsByTagName('item')
    for item in items:
        enclosures = item.getElementsByTagName('enclosure')
        self.assertEqual(len(enclosures), 1)","for item in items:
    enclosures = item.getElementsByTagName('enclosure')
    self.assertEqual(len(enclosures), 1)","for i,item in enumerate(items):
    enclosures = item.getElementsByTagName('enclosure')
    self.assertEqual(len(enclosures), 1)"
qutebrowser,https://github.com/qutebrowser/qutebrowser/tree/master/tests/unit/completion/test_completionwidget.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutebrowser/tests/unit/completion/test_completionwidget.py,,"def test_completion_item_focus(which, tree, expected, completionview, model, qtbot):
    """"""Test that on_next_prev_item moves the selection properly.

    Args:
        which: the direction in which to move the selection.
        tree: Each list represents a completion category, with each string
              being an item under that category.
        expected: expected argument from on_selection_changed for each
                  successive movement. None implies no signal should be
                  emitted.
    """"""
    for catdata in tree:
        cat = listcategory.ListCategory('', ((x,) for x in catdata))
        model.add_category(cat)
    completionview.set_model(model)
    for entry in expected:
        if entry is None:
            with qtbot.assert_not_emitted(completionview.selection_changed):
                completionview.completion_item_focus(which)
        else:
            with qtbot.wait_signal(completionview.selection_changed) as sig:
                completionview.completion_item_focus(which)
                assert sig.args == [entry]","for catdata in tree:
    cat = listcategory.ListCategory('', ((x,) for x in catdata))
    model.add_category(cat)","for i, catdata in enumerate(tree):
    cat = listcategory.ListCategory('', ((x,) for x in catdata))
    model.add_category(cat)"
espnet,https://github.com/espnet/espnet/tree/master/test/test_e2e_asr.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/test/test_e2e_asr.py,,"def test_torch_save_and_load():
    args = make_arg()
    model = th_asr.E2E(10, 5, args)
    for p in model.parameters():
        p.data.uniform_()
    if not os.path.exists('.pytest_cache'):
        os.makedirs('.pytest_cache')
    tmppath = tempfile.mktemp()
    asr_utils.torch_save(tmppath, model)
    p_saved = [p.data.numpy() for p in model.parameters()]
    for p in model.parameters():
        p.data.zero_()
    asr_utils.torch_load(tmppath, model)
    for (p1, p2) in zip(p_saved, model.parameters()):
        np.testing.assert_array_equal(p1, p2.data.numpy())
    if os.path.exists(tmppath):
        os.remove(tmppath)","for p in model.parameters():
    p.data.uniform_()","for i, p in enumerate(model.parameters()):
    p.data.uniform_()"
espnet,https://github.com/espnet/espnet/tree/master/test/test_e2e_asr.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/test/test_e2e_asr.py,,"def test_torch_save_and_load():
    args = make_arg()
    model = th_asr.E2E(10, 5, args)
    for p in model.parameters():
        p.data.uniform_()
    if not os.path.exists('.pytest_cache'):
        os.makedirs('.pytest_cache')
    tmppath = tempfile.mktemp()
    asr_utils.torch_save(tmppath, model)
    p_saved = [p.data.numpy() for p in model.parameters()]
    for p in model.parameters():
        p.data.zero_()
    asr_utils.torch_load(tmppath, model)
    for (p1, p2) in zip(p_saved, model.parameters()):
        np.testing.assert_array_equal(p1, p2.data.numpy())
    if os.path.exists(tmppath):
        os.remove(tmppath)","for p in model.parameters():
    p.data.zero_()","for i, p in enumerate(model.parameters()):
    p.data.zero_()"
espnet,https://github.com/espnet/espnet/tree/master/test/test_e2e_asr.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/test/test_e2e_asr.py,,"def test_torch_save_and_load():
    args = make_arg()
    model = th_asr.E2E(10, 5, args)
    for p in model.parameters():
        p.data.uniform_()
    if not os.path.exists('.pytest_cache'):
        os.makedirs('.pytest_cache')
    tmppath = tempfile.mktemp()
    asr_utils.torch_save(tmppath, model)
    p_saved = [p.data.numpy() for p in model.parameters()]
    for p in model.parameters():
        p.data.zero_()
    asr_utils.torch_load(tmppath, model)
    for (p1, p2) in zip(p_saved, model.parameters()):
        np.testing.assert_array_equal(p1, p2.data.numpy())
    if os.path.exists(tmppath):
        os.remove(tmppath)","for (p1, p2) in zip(p_saved, model.parameters()):
    np.testing.assert_array_equal(p1, p2.data.numpy())","for i, (p1, p2) in enumerate(zip(p_saved, model.parameters())):
    np.testing.assert_array_equal(p1, p2.data.numpy())"
DataStructure_Algorithm_ZJU,https://github.com/CYBruce/DataStructure_Algorithm_ZJU/tree/master//04-4 .py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DataStructure_Algorithm_ZJU//04-4 .py,,"def get_input(N, L):
    global same
    LIST = []
    real = input().split()
    for i in range(N):
        real[i] = int(real[i])
    for i in range(L):
        test = input().split()
        for j in range(N):
            test[j] = int(test[j])
        same = 0
        IsBST(real, test)
        if same == 0:
            LIST.append('Yes')
        else:
            LIST.append('No')
    for i in range(len(LIST)):
        print(LIST[i])","for i in range(len(LIST)):
    print(LIST[i])","for i, item in enumerate(LIST):
    print(item)"
hyperpose,https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pifpaf/eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyperpose/hyperpose/Model/pifpaf/eval.py,,"def visualize(img, img_id, processed_img, pd_pif_maps, pd_paf_maps, humans, stride=8, save_dir='./save_dir'):
    print(f'{len(humans)} human found!')
    print('visualizing...')
    os.makedirs(save_dir, exist_ok=True)
    ori_img = np.clip(img * 255.0, 0.0, 255.0).astype(np.uint8)
    processed_img = np.clip(processed_img * 255.0, 0.0, 255.0).astype(np.uint8)
    vis_img = ori_img.copy()
    for human in humans:
        vis_img = human.draw_human(vis_img)
    (pd_pif_conf, pd_pif_vec, _, pd_pif_scale) = pd_pif_maps
    (pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, _, _, _, _) = pd_paf_maps
    pd_pif_conf_show = np.amax(pd_pif_conf, axis=0)
    pd_pif_hr_conf_show = np.amax(get_hr_conf(pd_pif_conf, pd_pif_vec, pd_pif_scale, stride=stride, thresh=0.1), axis=0)
    pd_paf_conf_show = np.amax(pd_paf_conf, axis=0)
    pd_paf_vec_show = np.zeros(shape=(pd_pif_hr_conf_show.shape[0], pd_pif_hr_conf_show.shape[1], 3)).astype(np.int8)
    pd_paf_vec_show = get_arrow_map(pd_paf_vec_show, pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, thresh=0.1)
    fig = plt.figure(figsize=(12, 12))
    a = fig.add_subplot(3, 3, 1)
    a.set_title('input image')
    plt.imshow(ori_img)
    a = fig.add_subplot(3, 3, 3)
    a.set_title('output result')
    plt.imshow(vis_img)
    a = fig.add_subplot(3, 3, 4)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 5)
    a.set_title('pif_conf_map')
    plt.imshow(pd_pif_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 6)
    a.set_title('pif_hr_conf_map')
    plt.imshow(pd_pif_hr_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 7)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 8)
    a.set_title('paf_conf_map')
    plt.imshow(pd_paf_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 9)
    a.set_title('paf_vec_map')
    plt.imshow(pd_paf_vec_show, alpha=0.8)
    plt.colorbar()
    plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))
    plt.close()","for human in humans:
    vis_img = human.draw_human(vis_img)","for i,human in enumerate(humans):
    vis_img = human.draw_human(vis_img)"
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for b in range(value.shape[0]):
    b_kernel = kernel[min(b, kernel.shape[0] - 1)]
    for o in range(kernel.shape[1]):
        for i in range(value.shape[1]):
            result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for b, _ in enumerate(range(value.shape[0])):
    b_kernel = kernel[min(b, kernel.shape[0] - 1)]
    for o in range(kernel.shape[1]):
        for i in range(value.shape[1]):
            result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)"
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for o in range(kernel.shape[1]):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for o, _ in enumerate(range(kernel.shape[1])):
    for i in range(value.shape[1]):
        result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)"
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/math/backend/_numpy_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PhiFlow/phi/math/backend/_numpy_backend.py,NumPyBackend,"def conv(self, value, kernel, zero_padding=True):
    assert kernel.shape[0] in (1, value.shape[0])
    assert value.shape[1] == kernel.shape[2], f'value has {value.shape[1]} channels but kernel has {kernel.shape[2]}'
    assert value.ndim + 1 == kernel.ndim
    if zero_padding:
        result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
    else:
        valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
        result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
    mode = 'same' if zero_padding else 'valid'
    for b in range(value.shape[0]):
        b_kernel = kernel[min(b, kernel.shape[0] - 1)]
        for o in range(kernel.shape[1]):
            for i in range(value.shape[1]):
                result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
    return result","for i in range(value.shape[1]):
    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)","for i, _ in enumerate(range(value.shape[1])):
    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)"
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/kfserving/transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kale/backend/kale/kfserving/transformer.py,KaleTransformer,"def _load_transformer_assets(self):
    marshal.set_data_dir(serveutils.TRANSFORMER_ASSETS_DIR)
    log.info('Loading transformer function...')
    _fn = marshal.load(serveutils.TRANSFORMER_FN_ASSET_NAME)
    self.fn = types.FunctionType(_fn.__code__, globals(), _fn.__name__, _fn.__defaults__, _fn.__closure__)
    log.info('Processing source notebook for imports and functions...')
    processor = NotebookProcessor(nb_path=os.path.join(serveutils.TRANSFORMER_ASSETS_DIR, serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME), skip_validation=True)
    self.init_code = processor.get_imports_and_functions()
    log.info('Initialization code:\n%s' % self.init_code)
    log.info('Running initialization code...')
    exec(self.init_code, globals())
    log.info(""Loading transformer's assets..."")
    for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
        if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
            continue
        basename = os.path.splitext(file)[0]
        self.assets[basename] = marshal.load(basename)
    log.info('Assets successfully loaded: %s' % self.assets.keys())
    log.info('Initializing assets...')
    for (asset_name, asset_value) in self.assets.items():
        globals()[asset_name] = asset_value","for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
    if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
        continue
    basename = os.path.splitext(file)[0]
    self.assets[basename] = marshal.load(basename)","for i,file in enumerate(os.listdir(serveutils.TRANSFORMER_ASSETS_DIR)):
    if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
        continue
    basename = os.path.splitext(file)[0]
    self.assets[basename] = marshal.load(basename)"
MAML-Pytorch,https://github.com/dragen1860/MAML-Pytorch/tree/master/backup/csmlv0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MAML-Pytorch/backup/csmlv0.py,,"def inner_train(K, gpuidx, support_x, support_y, query_x, query_y, concepts, Q):
    """"""
    inner-loop train function.
    :param K: train iterations
    :param gpuidx: which gpu to train
    :param support_x:   [b, setsz, c_, h, w]
    :param support_y:   []
    :param query_x:     [b, querysz]
    :param query_y:
    :param concepts:    concepts network
    :param Q:           Queue to receive result
    :return:
    """"""
    assert support_x.size(0) == query_x.size(0)
    support_x = support_x.cuda(gpuidx)
    support_y = support_y.cuda(gpuidx)
    query_x = query_x.cuda(gpuidx)
    query_y = query_y.cuda(gpuidx)
    support_db = TensorDataset(support_x, support_y)
    query_db = TensorDataset(query_x, query_y)
    outlayer = OutLayer().cuda(gpuidx)
    criteon = nn.CrossEntropyLoss().cuda(gpuidx)
    optimizer = optim.Adam(outlayer.parameters(), lr=0.001)
    right = Variable(torch.zeros(1).cuda(gpuidx))
    loss = Variable(torch.zeros(1).cuda(gpuidx))
    for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
        for i in range(K):
            x = concepts[gpuidx](support_xb)
            x = x.detach()
            logits = outlayer(x)
            loss = criteon(logits, support_yb)
            outlayer.zero_grad()
            loss.backward()
            optimizer.step()
        x = concepts[gpuidx](query_xb)
        logits = outlayer(x)
        (_, idx) = logits.max(1)
        pred = idx.long()
        right += torch.eq(pred, query_yb).sum().float()
        loss += criteon(logits, query_yb)
    accuracy = right.data[0] / np.array(query_y.size()).prod()
    print(gpuidx, loss.data[0], accuracy)
    Q.put([gpuidx, loss.data[0], accuracy])
    del outlayer, criteon
    print('removed outlayer and criteon.')","for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
    for i in range(K):
        x = concepts[gpuidx](support_xb)
        x = x.detach()
        logits = outlayer(x)
        loss = criteon(logits, support_yb)
        outlayer.zero_grad()
        loss.backward()
        optimizer.step()
    x = concepts[gpuidx](query_xb)
    logits = outlayer(x)
    (_, idx) = logits.max(1)
    pred = idx.long()
    right += torch.eq(pred, query_yb).sum().float()
    loss += criteon(logits, query_yb)","for i, ((support_xb, support_yb), (query_xb, query_yb)) in enumerate(zip(support_db, query_db)):
    for j in range(K):
        x = concepts[gpuidx](support_xb)
        x = x.detach()
        logits = outlayer(x)
        loss = criteon(logits, support_yb)
        outlayer.zero_grad()
        loss.backward()
        optimizer.step()
    x = concepts[gpuidx](query_xb)
    logits = outlayer(x)
    (_, idx) = logits.max(1)
    pred = idx.long()
    right += torch.eq(pred, query_yb).sum().float()
    loss += criteon(logits, query_yb)"
MAML-Pytorch,https://github.com/dragen1860/MAML-Pytorch/tree/master/backup/csmlv0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MAML-Pytorch/backup/csmlv0.py,,"def inner_train(K, gpuidx, support_x, support_y, query_x, query_y, concepts, Q):
    """"""
    inner-loop train function.
    :param K: train iterations
    :param gpuidx: which gpu to train
    :param support_x:   [b, setsz, c_, h, w]
    :param support_y:   []
    :param query_x:     [b, querysz]
    :param query_y:
    :param concepts:    concepts network
    :param Q:           Queue to receive result
    :return:
    """"""
    assert support_x.size(0) == query_x.size(0)
    support_x = support_x.cuda(gpuidx)
    support_y = support_y.cuda(gpuidx)
    query_x = query_x.cuda(gpuidx)
    query_y = query_y.cuda(gpuidx)
    support_db = TensorDataset(support_x, support_y)
    query_db = TensorDataset(query_x, query_y)
    outlayer = OutLayer().cuda(gpuidx)
    criteon = nn.CrossEntropyLoss().cuda(gpuidx)
    optimizer = optim.Adam(outlayer.parameters(), lr=0.001)
    right = Variable(torch.zeros(1).cuda(gpuidx))
    loss = Variable(torch.zeros(1).cuda(gpuidx))
    for ((support_xb, support_yb), (query_xb, query_yb)) in zip(support_db, query_db):
        for i in range(K):
            x = concepts[gpuidx](support_xb)
            x = x.detach()
            logits = outlayer(x)
            loss = criteon(logits, support_yb)
            outlayer.zero_grad()
            loss.backward()
            optimizer.step()
        x = concepts[gpuidx](query_xb)
        logits = outlayer(x)
        (_, idx) = logits.max(1)
        pred = idx.long()
        right += torch.eq(pred, query_yb).sum().float()
        loss += criteon(logits, query_yb)
    accuracy = right.data[0] / np.array(query_y.size()).prod()
    print(gpuidx, loss.data[0], accuracy)
    Q.put([gpuidx, loss.data[0], accuracy])
    del outlayer, criteon
    print('removed outlayer and criteon.')","for i in range(K):
    x = concepts[gpuidx](support_xb)
    x = x.detach()
    logits = outlayer(x)
    loss = criteon(logits, support_yb)
    outlayer.zero_grad()
    loss.backward()
    optimizer.step()","for i, _ in enumerate(range(K)):
    x = concepts[gpuidx](support_xb)
    x = x.detach()
    logits = outlayer(x)
    loss = criteon(logits, support_yb)
    outlayer.zero_grad()
    loss.backward()
    optimizer.step()"
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/utils/addrwidth.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/utils/addrwidth.py,,"def gen_addrs():
    for (block_type, top_bottom, cfg_row, cfg_col, frame_count) in bitstream.gen_part_base_addrs():
        yield (bitstream.addr_bits2word(block_type, top_bottom, cfg_row, cfg_col, 0), frame_count)","for (block_type, top_bottom, cfg_row, cfg_col, frame_count) in bitstream.gen_part_base_addrs():
    yield (bitstream.addr_bits2word(block_type, top_bottom, cfg_row, cfg_col, 0), frame_count)","for i, (block_type, top_bottom, cfg_row, cfg_col, frame_count) in enumerate(bitstream.gen_part_base_addrs()):
    yield (bitstream.addr_bits2word(block_type, top_bottom, cfg_row, cfg_col, 0), frame_count)"
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/modnas/estim/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/modnas/estim/base.py,,"def build_criterions_all(crit_configs, device_ids=None):
    """"""Build Criterions from configs.""""""
    crits_all = []
    crits_train = []
    crits_eval = []
    crits_valid = []
    for crit_conf in streamline_spec(crit_configs):
        crit = backend.get_criterion(crit_conf, device_ids=device_ids)
        crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
        if not isinstance(crit_mode, list):
            crit_mode = [crit_mode]
        if 'all' in crit_mode:
            crits_all.append(crit)
        if 'train' in crit_mode:
            crits_train.append(crit)
        if 'eval' in crit_mode:
            crits_eval.append(crit)
        if 'valid' in crit_mode:
            crits_valid.append(crit)
    return (crits_all, crits_train, crits_eval, crits_valid)","for crit_conf in streamline_spec(crit_configs):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)","for i,crit_conf in enumerate(streamline_spec(crit_configs)):
    crit = backend.get_criterion(crit_conf, device_ids=device_ids)
    crit_mode = crit_conf['mode'] if isinstance(crit_conf, dict) and 'mode' in crit_conf else 'all'
    if not isinstance(crit_mode, list):
        crit_mode = [crit_mode]
    if 'all' in crit_mode:
        crits_all.append(crit)
    if 'train' in crit_mode:
        crits_train.append(crit)
    if 'eval' in crit_mode:
        crits_eval.append(crit)
    if 'valid' in crit_mode:
        crits_valid.append(crit)"
ivre,https://github.com/ivre/ivre/tree/master/ivre/db/tiny.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ivre/ivre/db/tiny.py,TinyDBNmap,"def remove(self, rec):
    """"""Removes the record from the active column. `rec` must be the record
        as returned by `.get()` or the record id.

        """"""
    q = Query()
    if isinstance(rec, dict):
        scanids = rec.get('scanid', [])
    else:
        try:
            scanids = self.get(q._id == rec)[0].get('scanid', [])
        except IndexError:
            scanids = []
    super().remove(rec)
    for scanid in scanids:
        if not self.db.get(q.scanid.any([scanid])):
            self.db_scans.remove(cond=Query()._id == scanid)","for scanid in scanids:
    if not self.db.get(q.scanid.any([scanid])):
        self.db_scans.remove(cond=Query()._id == scanid)","for i, scanid in enumerate(scanids):
    if not self.db.get(q.scanid.any([scanid])):
        self.db_scans.remove(cond=Query()._id == scanid)"
zao-,https://github.com/qiucheng025/zao-/tree/master/lib/gui/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zao-/lib/gui/stats.py,SessionsSummary,"def format_stats(compiled_stats):
    """""" Format for display """"""
    logger.debug('Formatting stats')
    for summary in compiled_stats:
        (hrs, mins, secs) = convert_time(summary['elapsed'])
        summary['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
        summary['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
        summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
        summary['rate'] = '{0:.1f}'.format(summary['rate'])
    return compiled_stats","for summary in compiled_stats:
    (hrs, mins, secs) = convert_time(summary['elapsed'])
    summary['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
    summary['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
    summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
    summary['rate'] = '{0:.1f}'.format(summary['rate'])","for i, summary in enumerate(compiled_stats):
    (hrs, mins, secs) = convert_time(summary['elapsed'])
    compiled_stats[i]['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
    compiled_stats[i]['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
    compiled_stats[i]['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
    compiled_stats[i]['rate'] = '{0:.1f}'.format(summary['rate'])"
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","for market in markets:
    if len(market.strip()) == 0:
        return 'Invalid assets. The given entry contains an empty market.'
    tokens = market.strip().split('-')
    if len(tokens) >= 2:
        return f'Invalid asset. {market} contain more than 1 asset.'
    for token in tokens:
        if len(token.strip()) == 0:
            return f'Invalid market. Ticker {token} has an invalid length.'
        if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
            return f'Invalid market. Ticker {token} contains invalid characters.'
        if token in tokens_list:
            return f'Duplicate market {token}.'
        tokens_list.append(token)","for i, market in enumerate(markets):
    if len(market.strip()) == 0:
        return 'Invalid assets. The given entry contains an empty market.'
    tokens = market.strip().split('-')
    if len(tokens) >= 2:
        return f'Invalid asset. {market} contain more than 1 asset.'
    for token in tokens:
        if len(token.strip()) == 0:
            return f'Invalid market. Ticker {token} has an invalid length.'
        if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
            return f'Invalid market. Ticker {token} contains invalid characters.'
        if token in tokens_list:
            return f'Duplicate market {token}.'
        tokens_list.append(token)"
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","for token in tokens:
    if len(token.strip()) == 0:
        return f'Invalid market. Ticker {token} has an invalid length.'
    if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
        return f'Invalid market. Ticker {token} contains invalid characters.'
    if token in tokens_list:
        return f'Duplicate market {token}.'
    tokens_list.append(token)","for i, token in enumerate(tokens):
    if len(token.strip()) == 0:
        return f'Invalid market. Ticker {token} has an invalid length.'
    if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
        return f'Invalid market. Ticker {token} contains invalid characters.'
    if token in tokens_list:
        return f'Duplicate market {token}.'
    tokens_list.append(token)"
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/mbart/configuration_mbart.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/mbart/configuration_mbart.py,MBartOnnxConfig,"def _generate_dummy_inputs_for_default_and_seq2seq_lm(self, tokenizer: PreTrainedTokenizer, batch_size: int=-1, seq_length: int=-1, is_pair: bool=False, framework: Optional[TensorType]=None) -> Mapping[str, Any]:
    encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(tokenizer, batch_size, seq_length, is_pair, framework)
    decoder_seq_length = seq_length if not self.use_past else 1
    decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(tokenizer, batch_size, decoder_seq_length, is_pair, framework)
    decoder_inputs = {f'decoder_{name}': tensor for (name, tensor) in decoder_inputs.items()}
    common_inputs = dict(**encoder_inputs, **decoder_inputs)
    if self.use_past:
        if not is_torch_available():
            raise ValueError('Cannot generate dummy past_keys inputs without PyTorch installed.')
        else:
            import torch
        (batch, encoder_seq_length) = common_inputs['input_ids'].shape
        decoder_seq_length = common_inputs['decoder_input_ids'].shape[1]
        (num_encoder_attention_heads, num_decoder_attention_heads) = self.num_attention_heads
        encoder_shape = (batch, num_encoder_attention_heads, encoder_seq_length, self._config.hidden_size // num_encoder_attention_heads)
        decoder_past_length = decoder_seq_length + 3
        decoder_shape = (batch, num_decoder_attention_heads, decoder_past_length, self._config.hidden_size // num_decoder_attention_heads)
        common_inputs['decoder_attention_mask'] = torch.cat([common_inputs['decoder_attention_mask'], torch.ones(batch, decoder_past_length)], dim=1)
        common_inputs['past_key_values'] = []
        (num_encoder_layers, num_decoder_layers) = self.num_layers
        min_num_layers = min(num_encoder_layers, num_decoder_layers)
        max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers
        remaining_side_name = 'encoder' if num_encoder_layers > num_decoder_layers else 'decoder'
        for _ in range(min_num_layers):
            common_inputs['past_key_values'].append((torch.zeros(decoder_shape), torch.zeros(decoder_shape), torch.zeros(encoder_shape), torch.zeros(encoder_shape)))
        shape = encoder_shape if remaining_side_name == 'encoder' else decoder_shape
        for _ in range(min_num_layers, max_num_layers):
            common_inputs['past_key_values'].append((torch.zeros(shape), torch.zeros(shape)))
    return common_inputs","for _ in range(min_num_layers, max_num_layers):
    common_inputs['past_key_values'].append((torch.zeros(shape), torch.zeros(shape)))","for i in range(min_num_layers, max_num_layers):
    common_inputs['past_key_values'].append((torch.zeros(shape), torch.zeros(shape)))"
brasil.io,https://github.com/turicas/brasil.io/tree/master/core/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brasil.io/core/models.py,DynamicTableConfig,"def get_dynamic_table_customization(cls, dataset_slug, table_name):
    CustomConfig = None
    for subclass in subclasses(cls):
        valid_implementation_conditions = [getattr(subclass, 'dataset_slug', None) == dataset_slug, getattr(subclass, 'table_name', None) == table_name]
        if all(valid_implementation_conditions):
            CustomConfig = subclass
            break
    return None if not CustomConfig else CustomConfig()","for subclass in subclasses(cls):
    valid_implementation_conditions = [getattr(subclass, 'dataset_slug', None) == dataset_slug, getattr(subclass, 'table_name', None) == table_name]
    if all(valid_implementation_conditions):
        CustomConfig = subclass
        break","for i, subclass in enumerate(subclasses(cls)):
    valid_implementation_conditions = [getattr(subclass, 'dataset_slug', None) == dataset_slug, getattr(subclass, 'table_name', None) == table_name]
    if all(valid_implementation_conditions):
        CustomConfig = subclass
        break"
pootle,https://github.com/translate/pootle/tree/master/tests/views/admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pootle/tests/views/admin.py,,"def test_admin_view_project_delete_tp(english, client, admin):
    user = admin
    project = Project.objects.get(code='project0')
    tp = TranslationProject.objects.create(language=english, project=project)
    project.config['pootle.core.lang_mapping'] = {tp.language.code: 'foo'}
    client.login(username=user.username, password=TEST_USERS['admin']['password'])
    get_response = _admin_view_get(client, project)
    post_data = {}
    formset = get_response.context['formset']
    forms = formset.forms + formset.extra_forms + [formset.management_form]
    for form in forms:
        for field in form.fields:
            post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')
    tp_pk = post_data['form-0-id']
    post_data['form-0-DELETE'] = 'true'
    response = _admin_view_post(client, project, **post_data)
    assert tp_pk not in project.translationproject_set.values_list('pk', flat=True)
    _test_admin_view(response, project)
    assert project.config['pootle.core.lang_mapping'] == {}","for field in form.fields:
    post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')","for i, field in enumerate(form.fields):
    post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')"
amundsen,https://github.com/amundsen-io/amundsen/tree/master/common/tests/unit/utils/test_atlas_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/amundsen/common/tests/unit/utils/test_atlas_utils.py,TestAtlasColumnKey,"def test_table_column_key_details_from_qualified_name(self) -> None:
    params = [('db_name.table_name.column_name@cluster_name', dict(cluster='cluster_name', schema='db_name', table='table_name', column='column_name')), ('db_name.table_name.column_name.dot@cluster_name.dot', dict(cluster='cluster_name.dot', schema='db_name', table='table_name', column='column_name.dot'))]
    for (qn, details) in params:
        with self.subTest(f'Test extract details from qualified name: {qn}'):
            result = AtlasColumnKey(qn)
            self.assertEqual(details, result.get_details())","for (qn, details) in params:
    with self.subTest(f'Test extract details from qualified name: {qn}'):
        result = AtlasColumnKey(qn)
        self.assertEqual(details, result.get_details())","for i, (qn, details) in enumerate(params):
    with self.subTest(f'Test extract details from qualified name: {qn}'):
        result = AtlasColumnKey(qn)
        self.assertEqual(details, result.get_details())"
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/tools/_paga.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scanpy/scanpy/tools/_paga.py,PAGA,"def _compute_connectivities_v1_2(self):
    import igraph
    ones = self._neighbors.distances.copy()
    ones.data = np.ones(len(ones.data))
    g = _utils.get_igraph_from_adjacency(ones, directed=True)
    vc = igraph.VertexClustering(g, membership=self._adata.obs[self._groups_key].cat.codes.values)
    ns = vc.sizes()
    n = sum(ns)
    es_inner_cluster = [vc.subgraph(i).ecount() for i in range(len(ns))]
    cg = vc.cluster_graph(combine_edges='sum')
    inter_es = _utils.get_sparse_from_igraph(cg, weight_attr='weight')
    es = np.array(es_inner_cluster) + inter_es.sum(axis=1).A1
    inter_es = inter_es + inter_es.T
    connectivities = inter_es.copy()
    expected_n_edges = inter_es.copy()
    inter_es = inter_es.tocoo()
    for (i, j, v) in zip(inter_es.row, inter_es.col, inter_es.data):
        expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
        if expected_random_null != 0:
            scaled_value = v / expected_random_null
        else:
            scaled_value = 1
        if scaled_value > 1:
            scaled_value = 1
        connectivities[i, j] = scaled_value
        expected_n_edges[i, j] = expected_random_null
    self.ns = ns
    self.expected_n_edges_random = expected_n_edges
    self.connectivities = connectivities
    self.connectivities_tree = self._get_connectivities_tree_v1_2()
    return (inter_es.tocsr(), connectivities)","for (i, j, v) in zip(inter_es.row, inter_es.col, inter_es.data):
    expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
    if expected_random_null != 0:
        scaled_value = v / expected_random_null
    else:
        scaled_value = 1
    if scaled_value > 1:
        scaled_value = 1
    connectivities[i, j] = scaled_value
    expected_n_edges[i, j] = expected_random_null","for idx, (i, j, v) in enumerate(zip(inter_es.row, inter_es.col, inter_es.data)):
    expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
    if expected_random_null != 0:
        scaled_value = v / expected_random_null
    else:
        scaled_value = 1
    if scaled_value > 1:
        scaled_value = 1
    connectivities[i, j] = scaled_value
    expected_n_edges[i, j] = expected_random_null"
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug","for (param, expection) in expections.items():
    url = reverse('milestones-list') + '?' + param
    response = client.get(url)
    number_of_milestones = len(response.data)
    assert response.status_code == 200
    assert number_of_milestones == expection, param
    if number_of_milestones > 0:
        assert response.data[0]['slug'] == milestone.slug","for i, (param, expection) in enumerate(expections.items()):
    url = reverse('milestones-list') + '?' + param
    response = client.get(url)
    number_of_milestones = len(response.data)
    assert response.status_code == 200
    assert number_of_milestones == expection, param
    if number_of_milestones > 0:
        assert response.data[0]['slug'] == milestone.slug"
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()","for stat_set in stat_sets:
    for stat in self.stats[stat_set]:
        self.stats[stat_set][stat].reset()","for i, stat_set in enumerate(stat_sets):
    for stat in self.stats[stat_set]:
        self.stats[stat_set][stat].reset()"
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()","for stat in self.stats[stat_set]:
    self.stats[stat_set][stat].reset()","for i, stat in enumerate(self.stats[stat_set]):
    self.stats[stat_set][stat].reset()"
ezdxf,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/entities/ltype.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/entities/ltype.py,Linetype,"def setup_pattern(self, pattern: Union[Sequence[float], str], length: float=0) -> None:
    complex_line_type = True if isinstance(pattern, str) else False
    if complex_line_type:
        tags = self._setup_complex_pattern(pattern, length)
    else:
        tags = Tags([DXFTag(72, 65), DXFTag(73, len(pattern) - 1), DXFTag(40, float(pattern[0]))])
        for element in pattern[1:]:
            tags.append(DXFTag(49, float(element)))
            tags.append(DXFTag(74, 0))
    self.pattern_tags = LinetypePattern(tags)","for element in pattern[1:]:
    tags.append(DXFTag(49, float(element)))
    tags.append(DXFTag(74, 0))","for i, element in enumerate(pattern[1:]):
    tags.append(DXFTag(49, float(element)))
    tags.append(DXFTag(74, 0))"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/bpath.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/bpath.py,Path,"def intersectPath(self, path, setinside=None):
    points = []

    def addPoint(i, P):
        if eq(P, self[i].A, EPS):
            return
        if eq(P, self[i].B, EPS):
            return
        oi = self[i].order(P)
        points.append((i, oi, P))
    for (i, si) in enumerate(self):
        for cut in path:
            (P1, P2) = si.intersect(cut)
            if P1 is not None and P2 is not None and eq(P1, P2, EPS):
                P2 = None
            if P1:
                addPoint(i, P1)
            if P2:
                addPoint(i, P2)
    points.sort(key=itemgetter(0, 1))
    for (i, o, P) in reversed(points):
        split = self[i].split(P)
        if not isinstance(split, int):
            self.insert(i + 1, split)
            self[i]._cross = True
    if setinside is not None:
        self.markInside(path, setinside)
    return points","for (i, o, P) in reversed(points):
    split = self[i].split(P)
    if not isinstance(split, int):
        self.insert(i + 1, split)
        self[i]._cross = True","for i, (o, P) in enumerate(reversed(points)):
    split = self[o].split(P)
    if not isinstance(split, int):
        self.insert(o + 1, split)
        self[o]._cross = True"
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/bpath.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/bpath.py,Path,"def intersectPath(self, path, setinside=None):
    points = []

    def addPoint(i, P):
        if eq(P, self[i].A, EPS):
            return
        if eq(P, self[i].B, EPS):
            return
        oi = self[i].order(P)
        points.append((i, oi, P))
    for (i, si) in enumerate(self):
        for cut in path:
            (P1, P2) = si.intersect(cut)
            if P1 is not None and P2 is not None and eq(P1, P2, EPS):
                P2 = None
            if P1:
                addPoint(i, P1)
            if P2:
                addPoint(i, P2)
    points.sort(key=itemgetter(0, 1))
    for (i, o, P) in reversed(points):
        split = self[i].split(P)
        if not isinstance(split, int):
            self.insert(i + 1, split)
            self[i]._cross = True
    if setinside is not None:
        self.markInside(path, setinside)
    return points","for cut in path:
    (P1, P2) = si.intersect(cut)
    if P1 is not None and P2 is not None and eq(P1, P2, EPS):
        P2 = None
    if P1:
        addPoint(i, P1)
    if P2:
        addPoint(i, P2)","for i,cut in enumerate(path):
    (P1, P2) = si.intersect(cut)
    if P1 is not None and P2 is not None and eq(P1, P2, EPS):
        P2 = None
    if P1:
        addPoint(i, P1)
    if P2:
        addPoint(i, P2)"
edx-platform,https://github.com/edx/edx-platform/tree/master/scripts/xblock/xblock_counts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/scripts/xblock/xblock_counts.py,,"def write_block_summary_report(course_data):
    """"""
    Generate a CSV file containing a summary of the xBlock usage

    Arguments:
        course_data (list of dicts): a list of course_data objects

    Returns:
        Nothing
    """"""
    (block_summary_counts, unique_course_counts) = _get_block_summary_totals(course_data)
    with open('xblock_summary_counts.csv', 'wb') as csvfile:
        summary_writer = csv.writer(csvfile, delimiter=',', quotechar='""', quoting=csv.QUOTE_MINIMAL)
        summary_writer.writerow(['XBLOCK_NAME', 'UNIQUE_COURSES', 'NUM_TOTAL_INSTANCES'])
        for block_type in sorted(block_summary_counts):
            block_count = block_summary_counts.get(block_type)
            summary_writer.writerow([block_type, str(unique_course_counts[block_type]), str(block_count)])
        csvfile.close()","for block_type in sorted(block_summary_counts):
    block_count = block_summary_counts.get(block_type)
    summary_writer.writerow([block_type, str(unique_course_counts[block_type]), str(block_count)])","for i, block_type in enumerate(sorted(block_summary_counts)):
    block_count = block_summary_counts.get(block_type)
    summary_writer.writerow([block_type, str(unique_course_counts[block_type]), str(block_count)])"
neutron,https://github.com/openstack/neutron/tree/master/neutron/services/trunk/drivers/openvswitch/driver.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/services/trunk/drivers/openvswitch/driver.py,OVSDriver,"def _update_subport_binding(self, context, trunk_id):
    """"""Update the subport binding host""""""
    trunk_obj = self._get_trunk(context, trunk_id)
    trunk_port = self.core_plugin.get_port(context, trunk_obj.port_id)
    trunk_host = trunk_port.get(portbindings.HOST_ID)
    for subport in trunk_obj.sub_ports:
        port = self.core_plugin.update_port(context, subport.port_id, {'port': {portbindings.HOST_ID: trunk_host, 'device_owner': trunk_consts.TRUNK_SUBPORT_OWNER}})
        vif_type = port.get(portbindings.VIF_TYPE)
        if vif_type == portbindings.VIF_TYPE_BINDING_FAILED:
            raise trunk_exc.SubPortBindingError(port_id=subport.port_id, trunk_id=trunk_obj.id)","for subport in trunk_obj.sub_ports:
    port = self.core_plugin.update_port(context, subport.port_id, {'port': {portbindings.HOST_ID: trunk_host, 'device_owner': trunk_consts.TRUNK_SUBPORT_OWNER}})
    vif_type = port.get(portbindings.VIF_TYPE)
    if vif_type == portbindings.VIF_TYPE_BINDING_FAILED:
        raise trunk_exc.SubPortBindingError(port_id=subport.port_id, trunk_id=trunk_obj.id)","for i, subport in enumerate(trunk_obj.sub_ports):
    port = self.core_plugin.update_port(context, subport.port_id, {'port': {portbindings.HOST_ID: trunk_host, 'device_owner': trunk_consts.TRUNK_SUBPORT_OWNER}})
    vif_type = port.get(portbindings.VIF_TYPE)
    if vif_type == portbindings.VIF_TYPE_BINDING_FAILED:
        raise trunk_exc.SubPortBindingError(port_id=subport.port_id, trunk_id=trunk_obj.id)"
justpy,https://github.com/elimintz/justpy/tree/master/justpy/htmlcomponents.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/justpy/justpy/htmlcomponents.py,Div,"def to_html(self, indent=0, indent_step=0, format=True):
    block_indent = ' ' * indent
    if format:
        ws = '\n'
    else:
        ws = ''
    s = f'{block_indent}<{self.html_tag} '
    d = self.convert_object_to_dict()
    for (attr, value) in d['attrs'].items():
        if value:
            s = f'{s}{attr}=""{value}"" '
    if self.style:
        s = f'{s}style=""{self.style}""'
    if self.classes:
        s = f'{s}class=""{self.classes}"">{ws}'
    else:
        s = f'{s}>{ws}'
    if self.inner_html:
        s = f'{s}{self.inner_html}</{self.html_tag}>{ws}'
        return s
    try:
        s = f'{s}{self.text}{ws}'
    except:
        pass
    for c in self.components:
        s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'
    s = f'{s}{block_indent}</{self.html_tag}>{ws}'
    return s","for c in self.components:
    s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'","for i,c in enumerate(self.components):
    s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'"
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_dragonbones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_dragonbones.py,,"def get_bone_index(self, armature, bone_name):
    armature_bones = []
    for bone in armature.data.bones:
        armature_bones.append(bone)
    for (i, bone) in enumerate(armature_bones):
        if bone_name == bone.name:
            return i","for bone in armature.data.bones:
    armature_bones.append(bone)","for i,bone in enumerate(armature.data.bones):
    armature_bones.append(bone)"
configuration,https://github.com/edx/configuration/tree/master/util/jenkins/helm_update_checker/helm_update_checker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/configuration/util/jenkins/helm_update_checker/helm_update_checker.py,,"def get_repo_name(repo_url):
    try:
        get_repo_cmd = 'helm repo list -o json'
        repositories = subprocess.check_output(get_repo_cmd, shell=True).strip()
        repo_list = json.loads(repositories.decode())
        for repo in repo_list:
            if repo['url'] == repo_url:
                return repo['name']
    except subprocess.CalledProcessError as e:
        print(e.output)","for repo in repo_list:
    if repo['url'] == repo_url:
        return repo['name']","for i, repo in enumerate(repo_list):
    if repo['url'] == repo_url:
        return repo['name']"
checkov,https://github.com/bridgecrewio/checkov/tree/master/checkov/terraform/parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkov/checkov/terraform/parser.py,,"def is_acceptable_module_param(value: Any) -> bool:
    """"""
    This function determines if a value should be passed to a module as a parameter. We don't want to pass
    unresolved var, local or module references because they can't be resolved from the module, so they need
    to be resolved prior to being passed down.
    """"""
    value_type = type(value)
    if value_type is dict:
        for (k, v) in value.items():
            if not is_acceptable_module_param(v) or not is_acceptable_module_param(k):
                return False
        return True
    if value_type is set or value_type is list:
        for v in value:
            if not is_acceptable_module_param(v):
                return False
        return True
    if value_type is not str:
        return True
    for vbm in find_var_blocks(value):
        if vbm.is_simple_var():
            return False
    return True","for v in value:
    if not is_acceptable_module_param(v):
        return False","for i,v in enumerate(value):
    if not is_acceptable_module_param(v):
        return False"
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/examples/pointnet2_segmentation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_geometric/examples/pointnet2_segmentation.py,,"def test(loader):
    model.eval()
    y_mask = loader.dataset.y_mask
    ious = [[] for _ in range(len(loader.dataset.categories))]
    for data in loader:
        data = data.to(device)
        pred = model(data).argmax(dim=1)
        (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
        iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
        iou[torch.isnan(iou)] = 1
        for (iou, category) in zip(iou.unbind(), data.category.unbind()):
            ious[category.item()].append(iou[y_mask[category]])
    ious = [torch.stack(iou).mean(0).mean(0) for iou in ious]
    return torch.tensor(ious).mean().item()","for data in loader:
    data = data.to(device)
    pred = model(data).argmax(dim=1)
    (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
    iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
    iou[torch.isnan(iou)] = 1
    for (iou, category) in zip(iou.unbind(), data.category.unbind()):
        ious[category.item()].append(iou[y_mask[category]])","for i, data in enumerate(loader):
    data = data.to(device)
    pred = model(data).argmax(dim=1)
    (i, u) = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
    iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
    iou[torch.isnan(iou)] = 1
    for (iou, category) in zip(iou.unbind(), data.category.unbind()):
        ious[category.item()].append(iou[y_mask[category]])"
bumblebee-status,https://github.com/tobi-wan-kenobi/bumblebee-status/tree/master/bumblebee_status/modules/contrib/sensors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bumblebee-status/bumblebee_status/modules/contrib/sensors.py,Module,"def _get_temp_from_sensors(self):
    if self._json == True:
        try:
            output = json.loads(util.cli.execute('sensors -j'))
            for key in self.parameter('path').split('/'):
                output = output[key]
            return int(float(output))
        except Exception as e:
            logging.error('unable to read sensors: {}'.format(str(e)))
            return 'unknown'
    else:
        output = util.cli.execute('sensors -u')
        if self._match_pattern:
            temp_pattern = self.parameter('match', 'temp1_input')
            match = re.search('{}.+{}:\\s*([\\d.]+)$'.format(self._match_pattern, temp_pattern), output.replace('\n', ''))
            if match:
                return int(float(match.group(1)))
            else:
                return 'unknown'
        match = self._pattern.findall(output)
        if match:
            return int(float(match[self._match_number]))
    return 'unknown'","for key in self.parameter('path').split('/'):
    output = output[key]","keys = self.parameter('path').split('/')
for i, key in enumerate(keys):
    output = output[key]"
mayavi,https://github.com/enthought/mayavi/tree/master/tvtk/indenter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/tvtk/indenter.py,VTKDocMassager,"def _rename_methods(self, doc):
    lines = doc.split('\n')
    nl = []
    for line in lines:
        words = line.split(' ')
        nw = []
        for word in words:
            if word[:3] == 'vtk':
                nw.append(word)
            else:
                nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
        nl.append(' '.join(nw))
    return '\n'.join(nl)","for line in lines:
    words = line.split(' ')
    nw = []
    for word in words:
        if word[:3] == 'vtk':
            nw.append(word)
        else:
            nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
    nl.append(' '.join(nw))","for i,line in enumerate(lines):
    words = line.split(' ')
    nw = []
    for word in words:
        if word[:3] == 'vtk':
            nw.append(word)
        else:
            nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
    nl.append(' '.join(nw))"
mayavi,https://github.com/enthought/mayavi/tree/master/tvtk/indenter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mayavi/tvtk/indenter.py,VTKDocMassager,"def _rename_methods(self, doc):
    lines = doc.split('\n')
    nl = []
    for line in lines:
        words = line.split(' ')
        nw = []
        for word in words:
            if word[:3] == 'vtk':
                nw.append(word)
            else:
                nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))
        nl.append(' '.join(nw))
    return '\n'.join(nl)","for word in words:
    if word[:3] == 'vtk':
        nw.append(word)
    else:
        nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))","for i, word in enumerate(words):
    if word[:3] == 'vtk':
        nw.append(word)
    else:
        nw.append(self.func_re.sub(lambda mo: camel2enthought(mo.group()), word))"
astropy,https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_groups.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/table/tests/test_groups.py,,"def test_group_mixins():
    """"""
    Test grouping a table with mixin columns
    """"""
    idx = np.arange(4)
    x = np.array([3.0, 1.0, 2.0, 1.0])
    q = x * u.m
    lon = coordinates.Longitude(x * u.deg)
    lat = coordinates.Latitude(x * u.deg)
    tm = time.Time(2000, format='jyear') + time.TimeDelta(x * 1e-10, format='sec')
    sc = coordinates.SkyCoord(ra=lon, dec=lat)
    aw = table_helpers.ArrayWrapper(x)
    nd = np.array([(3, 'c'), (1, 'a'), (2, 'b'), (1, 'a')], dtype='<i4,|S1').view(NdarrayMixin)
    qt = QTable([idx, x, q, lon, lat, tm, sc, aw, nd], names=['idx', 'x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd'])
    mixin_keys = ['x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd']
    for key in mixin_keys:
        qtg = qt.group_by(key)
        assert np.all(qtg['idx'] == [1, 3, 2, 0])
        for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
            assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
            assert np.all(qt[name][[2]] == qtg.groups[1][name])
            assert np.all(qt[name][[0]] == qtg.groups[2][name])
    uqt = unique(qt, keys=mixin_keys)
    assert len(uqt) == 3
    assert np.all(uqt['idx'] == [1, 2, 0])
    assert np.all(uqt['x'] == [1.0, 2.0, 3.0])
    idxg = qt['idx'].group_by(qt[mixin_keys])
    assert np.all(idxg == [1, 3, 2, 0])","for key in mixin_keys:
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])","for i,key in enumerate(mixin_keys):
    qtg = qt.group_by(key)
    assert np.all(qtg['idx'] == [1, 3, 2, 0])
    for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
        assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
        assert np.all(qt[name][[2]] == qtg.groups[1][name])
        assert np.all(qt[name][[0]] == qtg.groups[2][name])"
astropy,https://github.com/astropy/astropy/tree/master/astropy/table/tests/test_groups.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/table/tests/test_groups.py,,"def test_group_mixins():
    """"""
    Test grouping a table with mixin columns
    """"""
    idx = np.arange(4)
    x = np.array([3.0, 1.0, 2.0, 1.0])
    q = x * u.m
    lon = coordinates.Longitude(x * u.deg)
    lat = coordinates.Latitude(x * u.deg)
    tm = time.Time(2000, format='jyear') + time.TimeDelta(x * 1e-10, format='sec')
    sc = coordinates.SkyCoord(ra=lon, dec=lat)
    aw = table_helpers.ArrayWrapper(x)
    nd = np.array([(3, 'c'), (1, 'a'), (2, 'b'), (1, 'a')], dtype='<i4,|S1').view(NdarrayMixin)
    qt = QTable([idx, x, q, lon, lat, tm, sc, aw, nd], names=['idx', 'x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd'])
    mixin_keys = ['x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd']
    for key in mixin_keys:
        qtg = qt.group_by(key)
        assert np.all(qtg['idx'] == [1, 3, 2, 0])
        for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
            assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
            assert np.all(qt[name][[2]] == qtg.groups[1][name])
            assert np.all(qt[name][[0]] == qtg.groups[2][name])
    uqt = unique(qt, keys=mixin_keys)
    assert len(uqt) == 3
    assert np.all(uqt['idx'] == [1, 2, 0])
    assert np.all(uqt['x'] == [1.0, 2.0, 3.0])
    idxg = qt['idx'].group_by(qt[mixin_keys])
    assert np.all(idxg == [1, 3, 2, 0])","for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
    assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
    assert np.all(qt[name][[2]] == qtg.groups[1][name])
    assert np.all(qt[name][[0]] == qtg.groups[2][name])","for i,name in enumerate(['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']):
    assert np.all(qt[name][[1, 3]] == qtg.groups[0][name])
    assert np.all(qt[name][[2]] == qtg.groups[1][name])
    assert np.all(qt[name][[0]] == qtg.groups[2][name])"
zentral,https://github.com/zentralopensource/zentral/tree/master/tests/inventory/test_metrics_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zentral/tests/inventory/test_metrics_views.py,PrometheusViewsTestCase,"def test_prometheus_metrics_osx_apps_bundle_names(self):
    old_config = settings._collection['apps']['zentral.contrib.inventory'].pop('metrics_options', None)
    settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = ConfigDict({'osx_apps': {'sources': ['zentral tests'], 'bundle_names': ['Baller']}})
    response = self.client.get(reverse('inventory_metrics:all'), HTTP_AUTHORIZATION='Bearer CHANGE ME!!!')
    self.assertEqual(response.status_code, 200)
    seen = False
    for family in text_string_to_metric_families(response.content.decode('utf-8')):
        if family.name == 'zentral_inventory_active_machines_bucket':
            continue
        self.assertEqual(len(family.samples), 7)
        for sample in family.samples:
            self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
            le = sample.labels['le']
            self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
            if le == '1':
                self.assertEqual(sample.value, 0)
            else:
                self.assertEqual(sample.value, 1)
        self.assertFalse(seen)
        seen = True
    self.assertTrue(seen)
    if old_config:
        settings._collection['apps']['zentral.contrib.inventory']['metrics_options'] = old_config","for family in text_string_to_metric_families(response.content.decode('utf-8')):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True","for i,family in enumerate(text_string_to_metric_families(response.content.decode('utf-8'))):
    if family.name == 'zentral_inventory_active_machines_bucket':
        continue
    self.assertEqual(len(family.samples), 7)
    for sample in family.samples:
        self.assertEqual(sample.name, 'zentral_inventory_osx_apps_bucket')
        le = sample.labels['le']
        self.assertEqual(sample.labels, {'name': 'Baller', 'source_name': self.ms.source.name, 'source_id': str(self.ms.source.pk), 'version': '1.2.3', 'le': le})
        if le == '1':
            self.assertEqual(sample.value, 0)
        else:
            self.assertEqual(sample.value, 1)
    self.assertFalse(seen)
    seen = True"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/sr_evaluator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/sr_evaluator.py,ModelOVModel,"def fit_to_input(self, input_data):
    fitted = {}
    for (name, info) in self.inputs.items():
        data = input_data[self._name_to_idx[name]]
        data = np.expand_dims(data, axis=0)
        data = np.transpose(data, [0, 3, 1, 2])
        if not info.get_partial_shape.is_dynamic:
            assert parse_partial_shape(info.input_data.shape) == np.shape(data)
        fitted[name] = data
    return fitted","for (name, info) in self.inputs.items():
    data = input_data[self._name_to_idx[name]]
    data = np.expand_dims(data, axis=0)
    data = np.transpose(data, [0, 3, 1, 2])
    if not info.get_partial_shape.is_dynamic:
        assert parse_partial_shape(info.input_data.shape) == np.shape(data)
    fitted[name] = data","for i, (name, info) in enumerate(self.inputs.items()):
    data = input_data[self._name_to_idx[name]]
    data = np.expand_dims(data, axis=0)
    data = np.transpose(data, [0, 3, 1, 2])
    if not info.get_partial_shape.is_dynamic:
        assert parse_partial_shape(info.input_data.shape) == np.shape(data)
    fitted[name] = data"
SDV,https://github.com/sdv-dev/SDV/tree/master/sdv/timeseries/deepecho.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV/sdv/timeseries/deepecho.py,DeepEchoModel,"def _fit(self, timeseries_data):
    self._model = self._build_model()
    if self._sequence_index:
        timeseries_data = timeseries_data.rename(columns={self._sequence_index + '.value': self._sequence_index})
    self._output_columns = list(timeseries_data.columns)
    self._data_columns = [column for column in timeseries_data.columns if column not in self._entity_columns + self._context_columns]
    sequences = assemble_sequences(timeseries_data, self._entity_columns, self._context_columns, self._segment_size, self._sequence_index, drop_sequence_index=False)
    data_types = list()
    context_types = list()
    for field in self._output_columns:
        dtype = timeseries_data[field].dtype
        kind = dtype.kind
        if kind in ('i', 'f'):
            data_type = 'continuous'
        elif kind in ('O', 'b'):
            data_type = 'categorical'
        else:
            raise ValueError(f'Unsupported dtype {dtype}')
        if field in self._data_columns:
            data_types.append(data_type)
        elif field in self._context_columns:
            context_types.append(data_type)
    if self._sequence_index:
        self._transform_sequence_index(sequences)
        data_types.append('continuous')
    self._model.fit_sequences(sequences, context_types, data_types)","for field in self._output_columns:
    dtype = timeseries_data[field].dtype
    kind = dtype.kind
    if kind in ('i', 'f'):
        data_type = 'continuous'
    elif kind in ('O', 'b'):
        data_type = 'categorical'
    else:
        raise ValueError(f'Unsupported dtype {dtype}')
    if field in self._data_columns:
        data_types.append(data_type)
    elif field in self._context_columns:
        context_types.append(data_type)","for i, field in enumerate(self._output_columns):
    dtype = timeseries_data[field].dtype
    kind = dtype.kind
    if kind in ('i', 'f'):
        data_type = 'continuous'
    elif kind in ('O', 'b'):
        data_type = 'categorical'
    else:
        raise ValueError(f'Unsupported dtype {dtype}')
    if field in self._data_columns:
        data_types.append(data_type)
    elif field in self._context_columns:
        context_types.append(data_type)"
manuskript,https://github.com/olivierkes/manuskript/tree/master/manuskript/functions/spellchecker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/manuskript/manuskript/functions/spellchecker.py,Spellchecker,"def getDictionary(dictionary):
    if not dictionary:
        dictionary = Spellchecker.getDefaultDictionary()
    if not dictionary:
        return None
    values = dictionary.split(':', 1)
    if len(values) == 1:
        (lib, name) = (Spellchecker.implementations[0].getLibraryName(), dictionary)
        dictionary = Spellchecker.normalizeDictName(lib, name)
    else:
        (lib, name) = values
    try:
        d = Spellchecker.dictionaries.get(dictionary, None)
        if d == None:
            for impl in Spellchecker.implementations:
                if impl.isInstalled() and lib == impl.getLibraryName():
                    d = impl(name)
                    Spellchecker.dictionaries[dictionary] = d
                    break
        return d
    except Exception as e:
        pass
    return None","for impl in Spellchecker.implementations:
    if impl.isInstalled() and lib == impl.getLibraryName():
        d = impl(name)
        Spellchecker.dictionaries[dictionary] = d
        break","for i, impl in enumerate(Spellchecker.implementations):
    if impl.isInstalled() and lib == impl.getLibraryName():
        d = impl(name)
        Spellchecker.dictionaries[dictionary] = d
        break"
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_saving_in_empty_folder(self):
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs:
            if isinstance(temp_fs, TempFS):
                self.eopatch.save(temp_fs.root_path)
            else:
                self.eopatch.save('/', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
            subfolder = 'new-subfolder'
            self.eopatch.save('new-subfolder', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for fs_loader in self.filesystem_loaders:
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))","for i, fs_loader in enumerate(self.filesystem_loaders):
    with fs_loader() as temp_fs:
        if isinstance(temp_fs, TempFS):
            self.eopatch.save(temp_fs.root_path)
        else:
            self.eopatch.save('/', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
        subfolder = 'new-subfolder'
        self.eopatch.save('new-subfolder', filesystem=temp_fs)
        self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))"
vpn-at-home,https://github.com/ezaquarii/vpn-at-home/tree/master/backend/vpnathome/apps/management/tests/test_blocked_hosts_lists.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vpn-at-home/backend/vpnathome/apps/management/tests/test_blocked_hosts_lists.py,EnableHostsSource,"def test_enable_and_disable_sources(self):
    response = self.admin_client.put(self.url, self.update_data)
    self.assertResponseOk(response)
    for reference_item in self.update_data:
        database_item = BlockListUrl.objects.get(id=reference_item['id'])
        self.assertEquals(reference_item['enabled'], database_item.enabled)","for reference_item in self.update_data:
    database_item = BlockListUrl.objects.get(id=reference_item['id'])
    self.assertEquals(reference_item['enabled'], database_item.enabled)","for i, reference_item in enumerate(self.update_data):
    database_item = BlockListUrl.objects.get(id=reference_item['id'])
    self.assertEquals(reference_item['enabled'], database_item.enabled)"
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for size in self.photo_sizes.all():
    size.clear_cache()","for i,size in enumerate(self.photo_sizes.all()):
    size.clear_cache()"
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
    for obj in getattr(self, prop).all():
        obj.clear_cache()
        obj.pre_cache()","for i, prop in enumerate([prop for prop in dir(self) if prop[-8:] == '_related']):
    for obj in getattr(self, prop).all():
        obj.clear_cache()
        obj.pre_cache()"
django-photologue,https://github.com/richardbarran/django-photologue/tree/master/photologue/models.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-photologue/photologue/models.py,BaseEffect,"def save(self, *args, **kwargs):
    try:
        default_storage.delete(self.sample_filename())
    except:
        pass
    models.Model.save(self, *args, **kwargs)
    self.create_sample()
    for size in self.photo_sizes.all():
        size.clear_cache()
    for prop in [prop for prop in dir(self) if prop[-8:] == '_related']:
        for obj in getattr(self, prop).all():
            obj.clear_cache()
            obj.pre_cache()","for obj in getattr(self, prop).all():
    obj.clear_cache()
    obj.pre_cache()","for i,obj in enumerate(getattr(self, prop).all()):
    obj.clear_cache()
    obj.pre_cache()"
youtube-dl,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/rtve.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/rtve.py,RTVEALaCartaIE,"def _extract_png_formats(self, video_id):
    png = self._download_webpage('http://www.rtve.es/ztnr/movil/thumbnail/%s/videos/%s.png' % (self._manager, video_id), video_id, 'Downloading url information', query={'q': 'v2'})
    q = qualities(['Media', 'Alta', 'HQ', 'HD_READY', 'HD_FULL'])
    formats = []
    for (quality, video_url) in self._decrypt_url(png):
        ext = determine_ext(video_url)
        if ext == 'm3u8':
            formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
        elif ext == 'mpd':
            formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
        else:
            formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})
    self._sort_formats(formats)
    return formats","for (quality, video_url) in self._decrypt_url(png):
    ext = determine_ext(video_url)
    if ext == 'm3u8':
        formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
    elif ext == 'mpd':
        formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
    else:
        formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})","for i, (quality, video_url) in enumerate(self._decrypt_url(png)):
    ext = determine_ext(video_url)
    if ext == 'm3u8':
        formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
    elif ext == 'mpd':
        formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
    else:
        formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})"
nnFormer,https://github.com/282857341/nnFormer/tree/master/nnformer/preprocessing/preprocessing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/preprocessing/preprocessing.py,GenericPreprocessor,"def resample_and_normalize(self, data, target_spacing, properties, seg=None, force_separate_z=None):
    """"""
        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values
        (spacing etc)
        :param data:
        :param target_spacing:
        :param properties:
        :param seg:
        :param force_separate_z:
        :return:
        """"""
    original_spacing_transposed = np.array(properties['original_spacing'])[self.transpose_forward]
    before = {'spacing': properties['original_spacing'], 'spacing_transposed': original_spacing_transposed, 'data.shape (data is transposed)': data.shape}
    data[np.isnan(data)] = 0
    (data, seg) = resample_patient(data, seg, np.array(original_spacing_transposed), target_spacing, 3, 1, force_separate_z=force_separate_z, order_z_data=0, order_z_seg=0, separate_z_anisotropy_threshold=self.resample_separate_z_anisotropy_threshold)
    after = {'spacing': target_spacing, 'data.shape (data is resampled)': data.shape}
    print('before:', before, '\nafter: ', after, '\n')
    if seg is not None:
        seg[seg < -1] = 0
    properties['size_after_resampling'] = data[0].shape
    properties['spacing_after_resampling'] = target_spacing
    use_nonzero_mask = self.use_nonzero_mask
    assert len(self.normalization_scheme_per_modality) == len(data), 'self.normalization_scheme_per_modality must have as many entries as data has modalities'
    assert len(self.use_nonzero_mask) == len(data), 'self.use_nonzero_mask must have as many entries as data has modalities'
    for c in range(len(data)):
        scheme = self.normalization_scheme_per_modality[c]
        if scheme == 'CT':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            mean_intensity = self.intensityproperties[c]['mean']
            std_intensity = self.intensityproperties[c]['sd']
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            data[c] = (data[c] - mean_intensity) / std_intensity
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        elif scheme == 'CT2':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            mask = (data[c] > lower_bound) & (data[c] < upper_bound)
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            mn = data[c][mask].mean()
            sd = data[c][mask].std()
            data[c] = (data[c] - mn) / sd
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        else:
            if use_nonzero_mask[c]:
                mask = seg[-1] >= 0
            else:
                mask = np.ones(seg.shape[1:], dtype=bool)
            data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
            data[c][mask == 0] = 0
    return (data, seg, properties)","for c in range(len(data)):
    scheme = self.normalization_scheme_per_modality[c]
    if scheme == 'CT':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        mean_intensity = self.intensityproperties[c]['mean']
        std_intensity = self.intensityproperties[c]['sd']
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        data[c] = (data[c] - mean_intensity) / std_intensity
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    elif scheme == 'CT2':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        mask = (data[c] > lower_bound) & (data[c] < upper_bound)
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        mn = data[c][mask].mean()
        sd = data[c][mask].std()
        data[c] = (data[c] - mn) / sd
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    else:
        if use_nonzero_mask[c]:
            mask = seg[-1] >= 0
        else:
            mask = np.ones(seg.shape[1:], dtype=bool)
        data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
        data[c][mask == 0] = 0","for c, _ in enumerate(range(len(data))):
    scheme = self.normalization_scheme_per_modality[c]
    if scheme == 'CT':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        mean_intensity = self.intensityproperties[c]['mean']
        std_intensity = self.intensityproperties[c]['sd']
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        data[c] = (data[c] - mean_intensity) / std_intensity
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    elif scheme == 'CT2':
        assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
        lower_bound = self.intensityproperties[c]['percentile_00_5']
        upper_bound = self.intensityproperties[c]['percentile_99_5']
        mask = (data[c] > lower_bound) & (data[c] < upper_bound)
        data[c] = np.clip(data[c], lower_bound, upper_bound)
        mn = data[c][mask].mean()
        sd = data[c][mask].std()
        data[c] = (data[c] - mn) / sd
        if use_nonzero_mask[c]:
            data[c][seg[-1] < 0] = 0
    else:
        if use_nonzero_mask[c]:
            mask = seg[-1] >= 0
        else:
            mask = np.ones(seg.shape[1:], dtype=bool)
        data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
        data[c][mask == 0] = 0"
espresso,https://github.com/freewym/espresso/tree/master/fairseq/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/utils.py,,"def clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:

    def grad_exists(p):
        return p is not None and getattr(p, 'grad', None) is not None
    if isinstance(params, torch.Tensor):
        params = [params]
    params = list(params)
    grads = [p.grad.detach() for p in params if grad_exists(p) and (not hasattr(p, 'expert'))]
    expert_grads = [p.grad.detach() for p in params if grad_exists(p) and hasattr(p, 'expert')]
    if len(grads) == 0:
        if len(params) > 0:
            return params[0].new_tensor(0.0)
        else:
            return torch.tensor(0.0)
    if len(grads) == 1:
        total_norm = torch.norm(grads[0], p=2, dtype=torch.float32)
    elif multi_tensor_l2norm_available:
        total_norm = multi_tensor_total_norm(grads)
    else:
        if torch.cuda.is_available():
            warnings.warn(""amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library"")
            device = torch.cuda.current_device()
        elif grads[0].device.type == 'xla':
            device = grads[0].device
        else:
            device = torch.device('cpu')
        total_norm = torch.norm(torch.stack([torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]))
    if aggregate_norm_fn is not None:
        total_norm = aggregate_norm_fn(total_norm)
    if max_norm > 0:
        max_norm = float(max_norm)
        clip_coef = (max_norm / (total_norm + 1e-06)).clamp_(max=1)
        for g in grads + expert_grads:
            g.mul_(clip_coef)
    return total_norm","for g in grads + expert_grads:
    g.mul_(clip_coef)","for i,g in enumerate(grads + expert_grads):
    g.mul_(clip_coef)"
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","for index in self._ops.keys():
    term_strs.append('%s%s' % (self[index], index))","for i, index in enumerate(self._ops.keys()):
    term_strs.append('%s%s' % (self[index], index))"
glance,https://github.com/openstack/glance/tree/master/glance/db/simple/api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/db/simple/api.py,,"def metadef_tag_update(context, namespace_name, id, values):
    """"""Update a metadef tag""""""
    global DATA
    namespace = metadef_namespace_get(context, namespace_name)
    _check_namespace_visibility(context, namespace, namespace_name)
    tag = metadef_tag_get_by_id(context, namespace_name, id)
    if tag['name'] != values['name']:
        for db_tag in DATA['metadef_tags']:
            if db_tag['name'] == values['name'] and db_tag['namespace_id'] == namespace['id']:
                LOG.debug('Invalid update. It would result in a duplicate metadata definition tag with same name=%(name)s  in namespace=%(namespace_name)s.', {'name': tag['name'], 'namespace_name': namespace_name})
                raise exception.MetadefDuplicateTag(name=tag['name'], namespace_name=namespace_name)
    DATA['metadef_tags'].remove(tag)
    tag.update(values)
    tag['updated_at'] = timeutils.utcnow()
    DATA['metadef_tags'].append(tag)
    return tag","for db_tag in DATA['metadef_tags']:
    if db_tag['name'] == values['name'] and db_tag['namespace_id'] == namespace['id']:
        LOG.debug('Invalid update. It would result in a duplicate metadata definition tag with same name=%(name)s  in namespace=%(namespace_name)s.', {'name': tag['name'], 'namespace_name': namespace_name})
        raise exception.MetadefDuplicateTag(name=tag['name'], namespace_name=namespace_name)","for i, db_tag in enumerate(DATA['metadef_tags']):
    if db_tag['name'] == values['name'] and db_tag['namespace_id'] == namespace['id']:
        LOG.debug('Invalid update. It would result in a duplicate metadata definition tag with same name=%(name)s  in namespace=%(namespace_name)s.', {'name': tag['name'], 'namespace_name': namespace_name})
        raise exception.MetadefDuplicateTag(name=tag['name'], namespace_name=namespace_name)"
numpy,https://github.com/numpy/numpy/tree/master/tools/refguide_check.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/tools/refguide_check.py,,"def check_documentation(base_path, results, args, dots):
    """"""
    Check examples in any *.rst located inside `base_path`.
    Add the output to `results`.

    See Also
    --------
    check_doctests_testfile
    """"""
    for filename in iter_included_files(base_path, args.verbose):
        if dots:
            sys.stderr.write(filename + ' ')
            sys.stderr.flush()
        tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

        def scratch():
            pass
        scratch.__name__ = filename
        results.append((scratch, tut_results))
        if dots:
            sys.stderr.write('\n')
            sys.stderr.flush()","for filename in iter_included_files(base_path, args.verbose):
    if dots:
        sys.stderr.write(filename + ' ')
        sys.stderr.flush()
    tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

    def scratch():
        pass
    scratch.__name__ = filename
    results.append((scratch, tut_results))
    if dots:
        sys.stderr.write('\n')
        sys.stderr.flush()","for i, filename in enumerate(iter_included_files(base_path, args.verbose)):
    if dots:
        sys.stderr.write(filename + ' ')
        sys.stderr.flush()
    tut_results = check_doctests_testfile(filename, args.verbose >= 2, dots=dots, doctest_warnings=args.doctest_warnings)

    def scratch():
        pass
    scratch.__name__ = filename
    results.append((scratch, tut_results))
    if dots:
        sys.stderr.write('\n')
        sys.stderr.flush()"
git-imerge,https://github.com/mhagger/git-imerge/tree/master//gitimerge.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/git-imerge//gitimerge.py,MergeState,"def find_index(self, commit):
    """"""Return (i1,i2) for the specified commit.

        Raise CommitNotFoundError if it is not known.""""""
    for i2 in range(0, self.len2):
        for i1 in range(0, self.len1):
            if (i1, i2) in self:
                record = self[i1, i2]
                if record.sha1 == commit:
                    return (i1, i2)
    raise CommitNotFoundError(commit)","for i2 in range(0, self.len2):
    for i1 in range(0, self.len1):
        if (i1, i2) in self:
            record = self[i1, i2]
            if record.sha1 == commit:
                return (i1, i2)","for i2, _ in enumerate(range(0, self.len2)):
    for i1 in range(0, self.len1):
        if (i1, i2) in self:
            record = self[i1, i2]
            if record.sha1 == commit:
                return (i1, i2)"
DeepRobust,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/defense/gcn_preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRobust/deeprobust/graph/defense/gcn_preprocess.py,,"def dropedge_both(A, iA, jA, features, threshold1=2.5, threshold2=0.01):
    removed_cnt = 0
    for row in range(len(iA) - 1):
        for i in range(iA[row], iA[row + 1]):
            n1 = row
            n2 = jA[i]
            C1 = np.linalg.norm(features[n1] - features[n2])
            (a, b) = (features[n1], features[n2])
            inner_product = (a * b).sum()
            C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
            if C1 > threshold1 or threshold2 < 0:
                A[i] = 0
                removed_cnt += 1
    return removed_cnt","for row in range(len(iA) - 1):
    for i in range(iA[row], iA[row + 1]):
        n1 = row
        n2 = jA[i]
        C1 = np.linalg.norm(features[n1] - features[n2])
        (a, b) = (features[n1], features[n2])
        inner_product = (a * b).sum()
        C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
        if C1 > threshold1 or threshold2 < 0:
            A[i] = 0
            removed_cnt += 1","for row, _ in enumerate(iA[:-1]):
    for i in range(iA[row], iA[row + 1]):
        n1 = row
        n2 = jA[i]
        C1 = np.linalg.norm(features[n1] - features[n2])
        (a, b) = (features[n1], features[n2])
        inner_product = (a * b).sum()
        C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
        if C1 > threshold1 or threshold2 < 0:
            A[i] = 0
            removed_cnt += 1"
DeepRobust,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/defense/gcn_preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRobust/deeprobust/graph/defense/gcn_preprocess.py,,"def dropedge_both(A, iA, jA, features, threshold1=2.5, threshold2=0.01):
    removed_cnt = 0
    for row in range(len(iA) - 1):
        for i in range(iA[row], iA[row + 1]):
            n1 = row
            n2 = jA[i]
            C1 = np.linalg.norm(features[n1] - features[n2])
            (a, b) = (features[n1], features[n2])
            inner_product = (a * b).sum()
            C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
            if C1 > threshold1 or threshold2 < 0:
                A[i] = 0
                removed_cnt += 1
    return removed_cnt","for i in range(iA[row], iA[row + 1]):
    n1 = row
    n2 = jA[i]
    C1 = np.linalg.norm(features[n1] - features[n2])
    (a, b) = (features[n1], features[n2])
    inner_product = (a * b).sum()
    C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
    if C1 > threshold1 or threshold2 < 0:
        A[i] = 0
        removed_cnt += 1","for i, j in enumerate(range(iA[row], iA[row + 1])):
    n1 = row
    n2 = jA[j]
    C1 = np.linalg.norm(features[n1] - features[n2])
    (a, b) = (features[n1], features[n2])
    inner_product = (a * b).sum()
    C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
    if C1 > threshold1 or threshold2 < 0:
        A[j] = 0
        removed_cnt += 1"
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/distributions/gaussian_scale_mixture.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyro/pyro/distributions/gaussian_scale_mixture.py,GaussianScaleMixture,"def _compute_coeffs(self):
    """"""
        These coefficients are used internally in the backward call.
        """"""
    dimov2 = int(self.dim / 2)
    coeffs = torch.ones(dimov2)
    for k in range(dimov2 - 1):
        coeffs[k + 1:] *= self.dim - 2 * (k + 1)
    return coeffs","for k in range(dimov2 - 1):
    coeffs[k + 1:] *= self.dim - 2 * (k + 1)","for k, _ in enumerate(range(dimov2 - 1)):
    coeffs[k + 1:] *= self.dim - 2 * (k + 1)"
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for i in range(len(olist) // 2):
    olist[i * 2] = F.softmax(olist[i * 2], dim=1)","for i,_ in enumerate(range(len(olist) // 2)):
    olist[i * 2] = F.softmax(olist[i * 2], dim=1)"
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for i in range(len(olist) // 2):
    (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
    (FB, FC, FH, FW) = ocls.size()
    stride = 2 ** (i + 2)
    anchor = stride * 4
    poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
    for (Iindex, hindex, windex) in poss:
        (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
        score = ocls[0, 1, hindex, windex]
        loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
        priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
        variances = [0.1, 0.2]
        box = decode(loc, priors, variances)
        (x1, y1, x2, y2) = box[0]
        bboxlist.append([x1, y1, x2, y2, score])","for i, _ in enumerate(range(len(olist) // 2)):
    (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
    (FB, FC, FH, FW) = ocls.size()
    stride = 2 ** (i + 2)
    anchor = stride * 4
    poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
    for (Iindex, hindex, windex) in poss:
        (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
        score = ocls[0, 1, hindex, windex]
        loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
        priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
        variances = [0.1, 0.2]
        box = decode(loc, priors, variances)
        (x1, y1, x2, y2) = box[0]
        bboxlist.append([x1, y1, x2, y2, score])"
VTuber_Unity,https://github.com/kwea123/VTuber_Unity/tree/master/face_alignment/detection/sfd/detect.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/face_alignment/detection/sfd/detect.py,,"def detect(net, img, device):
    img = img - np.array([104, 117, 123])
    img = img.transpose(2, 0, 1)
    img = img[np.newaxis]
    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True
    img = torch.FloatTensor(img).to(device)
    (BB, CC, HH, WW) = img.size()
    with torch.no_grad():
        olist = net(img)
    bboxlist = []
    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)
    olist = [oelem.cpu() for oelem in olist]
    for i in range(len(olist) // 2):
        (ocls, oreg) = (olist[i * 2], olist[i * 2 + 1])
        (FB, FC, FH, FW) = ocls.size()
        stride = 2 ** (i + 2)
        anchor = stride * 4
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for (Iindex, hindex, windex) in poss:
            (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
            score = ocls[0, 1, hindex, windex]
            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
            priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
            variances = [0.1, 0.2]
            box = decode(loc, priors, variances)
            (x1, y1, x2, y2) = box[0]
            bboxlist.append([x1, y1, x2, y2, score])
    bboxlist = np.array(bboxlist)
    return bboxlist","for (Iindex, hindex, windex) in poss:
    (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
    score = ocls[0, 1, hindex, windex]
    loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
    priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
    variances = [0.1, 0.2]
    box = decode(loc, priors, variances)
    (x1, y1, x2, y2) = box[0]
    bboxlist.append([x1, y1, x2, y2, score])","for (Iindex, hindex, windex), poss_item in enumerate(poss):
    (axc, ayc) = (stride / 2 + windex * stride, stride / 2 + hindex * stride)
    score = ocls[0, 1, hindex, windex]
    loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)
    priors = torch.Tensor([[axc, ayc, stride * 4, stride * 4]])
    variances = [0.1, 0.2]
    box = decode(loc, priors, variances)
    (x1, y1, x2, y2) = box[0]
    bboxlist.append([x1, y1, x2, y2, score])"
doit,https://github.com/pydoit/doit/tree/master/doit/task.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/doit/doit/task.py,Task,"def clean(self, outstream, dryrun):
    """"""Execute task's clean
        @ivar outstream: 'write' output into this stream
        @ivar dryrun (bool): if True clean tasks are not executed
                             (just print out what would be executed)
        """"""
    self.init_options()
    if self._remove_targets is True:
        clean_targets(self, dryrun)
    else:
        for action in self.clean_actions:
            msg = ""%s - executing '%s'\n""
            outstream.write(msg % (self.name, action))
            execute_on_dryrun = False
            if isinstance(action, PythonAction):
                action_sig = inspect.signature(action.py_callable)
                if 'dryrun' in action_sig.parameters:
                    execute_on_dryrun = True
                    action.kwargs['dryrun'] = dryrun
            if not dryrun or execute_on_dryrun:
                result = action.execute(out=outstream)
                if isinstance(result, CatchedException):
                    sys.stderr.write(str(result))","for action in self.clean_actions:
    msg = ""%s - executing '%s'\n""
    outstream.write(msg % (self.name, action))
    execute_on_dryrun = False
    if isinstance(action, PythonAction):
        action_sig = inspect.signature(action.py_callable)
        if 'dryrun' in action_sig.parameters:
            execute_on_dryrun = True
            action.kwargs['dryrun'] = dryrun
    if not dryrun or execute_on_dryrun:
        result = action.execute(out=outstream)
        if isinstance(result, CatchedException):
            sys.stderr.write(str(result))","for i, action in enumerate(self.clean_actions):
    msg = ""%s - executing '%s'\n""
    outstream.write(msg % (self.name, action))
    execute_on_dryrun = False
    if isinstance(action, PythonAction):
        action_sig = inspect.signature(action.py_callable)
        if 'dryrun' in action_sig.parameters:
            execute_on_dryrun = True
            action.kwargs['dryrun'] = dryrun
    if not dryrun or execute_on_dryrun:
        result = action.execute(out=outstream)
        if isinstance(result, CatchedException):
            sys.stderr.write(str(result))"
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","for i in range(bbox.shape[0]):
    if easy_view and label[i] not in g_easy_view_labels:
        continue
    if exclude_table and label[i] == g_classes.index('table'):
        continue
    length = bbox[i, 3:6] - bbox[i, 0:3]
    a = length[0]
    b = length[1]
    c = length[2]
    x = bbox[i, 0]
    y = bbox[i, 1]
    z = bbox[i, 2]
    color = np.array(g_label2color[label[i]], dtype=float) / 255.0
    material = 'material%d' % ins_cnt
    fout_obj.write('usemtl %s\n' % material)
    fout_obj.write('v %f %f %f\n' % (x, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y, z))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z))
    fout_obj.write('g default\n')
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
    fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
    fout_obj.write('\n')
    fout_mtl.write('newmtl %s\n' % material)
    fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
    fout_mtl.write('\n')
    v_cnt += 8
    ins_cnt += 1","for i in range(bbox.shape[0]):
    if easy_view and label[i] not in g_easy_view_labels:
        continue
    if exclude_table and label[i] == g_classes.index('table'):
        continue
    length = bbox[i, 3:6] - bbox[i, 0:3]
    a = length[0]
    b = length[1]
    c = length[2]
    x = bbox[i, 0]
    y = bbox[i, 1]
    z = bbox[i, 2]
    color = np.array(g_label2color[label[i]], dtype=float) / 255.0
    material = 'material%d' % ins_cnt
    fout_obj.write('usemtl %s\n' % material)
    fout_obj.write('v %f %f %f\n' % (x, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
    fout_obj.write('v %f %f %f\n' % (x, y, z))
    fout_obj.write('v %f %f %f\n' % (x, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
    fout_obj.write('v %f %f %f\n' % (x + a, y, z))
    fout_obj.write('g default\n')
    fout_obj.write('f %d %d %d %d\n' % (4 + i*8, 3 + i*8, 2 + i*8, 1 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (1 + i*8, 2 + i*8, 6 + i*8, 5 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (7 + i*8, 6 + i*8, 2 + i*8, 3 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (4 + i*8, 8 + i*8, 7 + i*8, 3 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (5 + i*8, 8 + i*8, 4 + i*8, 1 + i*8))
    fout_obj.write('f %d %d %d %d\n' % (5 + i*8, 6 + i*8, 7 + i*8, 8 + i*8))
    fout_obj.write('\n')
    fout_mtl.write('newmtl %s\n' % material)
    fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
    fout_mtl.write('\n')
    ins_cnt += 1"
evennia,https://github.com/evennia/evennia/tree/master/evennia/prototypes/menus.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evennia/evennia/prototypes/menus.py,,"def _keep_diff(caller, **kwargs):
    """"""Change to KEEP setting for a given section of a diff""""""
    path = kwargs['path']
    diff = kwargs['diff']
    tmp = diff
    for key in path[:-1]:
        tmp = tmp[key]
    tmp[path[-1]] = tuple(list(tmp[path[-1]][:-1]) + ['KEEP'])","for key in path[:-1]:
    tmp = tmp[key]","for i,key in enumerate(path[:-1]):
    tmp = tmp[key]"
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/test_client_regress/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v22/tests/test_client_regress/tests.py,AssertFormsetErrorTests,"def test_no_error_field(self):
    """"""An assertion is raised if the field doesn't have any errors""""""
    for (prefix, kwargs) in self.msg_prefixes:
        msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
        with self.assertRaisesMessage(AssertionError, msg):
            self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)","for (prefix, kwargs) in self.msg_prefixes:
    msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
    with self.assertRaisesMessage(AssertionError, msg):
        self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)","for i, (prefix, kwargs) in enumerate(self.msg_prefixes):
    msg = prefix + ""The field 'value' on formset 'my_formset', form 1 in context 0 contains no errors""
    with self.assertRaisesMessage(AssertionError, msg):
        self.assertFormsetError(self.response_form_errors, 'my_formset', 1, 'value', 'Some error.', **kwargs)"
model-optimization,https://github.com/tensorflow/model-optimization/tree/master/tensorflow_model_optimization/python/core/clustering/keras/cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-optimization/tensorflow_model_optimization/python/core/clustering/keras/cluster.py,,"def _wrap_list(layers):
    output = []
    for layer in layers:
        output.append(_add_clustering_wrapper(layer))
    return output","for layer in layers:
    output.append(_add_clustering_wrapper(layer))","for i, layer in enumerate(layers):
    output.append(_add_clustering_wrapper(layer))"
Pillow,https://github.com/python-pillow/Pillow/tree/master/src/PIL/ImageShow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pillow/src/PIL/ImageShow.py,,"def show(image, title=None, **options):
    """"""
    Display a given image.

    :param image: An image object.
    :param title: Optional title. Not all viewers can display the title.
    :param \\**options: Additional viewer options.
    :returns: ``True`` if a suitable viewer was found, ``False`` otherwise.
    """"""
    for viewer in _viewers:
        if viewer.show(image, title=title, **options):
            return True
    return False","for viewer in _viewers:
    if viewer.show(image, title=title, **options):
        return True","for i, viewer in enumerate(_viewers):
    if viewer.show(image, title=title, **options):
        return True"
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/s3/_read_parquet.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-data-wrangler/awswrangler/s3/_read_parquet.py,,"def _read_parquet(path: str, version_id: Optional[str], columns: Optional[List[str]], categories: Optional[List[str]], safe: bool, map_types: bool, boto3_session: Union[boto3.Session, _utils.Boto3PrimitivesType], dataset: bool, validate_schema: Optional[bool], path_root: Optional[str], s3_additional_kwargs: Optional[Dict[str, str]], use_threads: Union[bool, int], pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:
    pyarrow_args = _set_default_pyarrow_additional_kwargs(pyarrow_additional_kwargs)
    boto3_session = _utils.ensure_session(boto3_session)
    df: pd.DataFrame = _arrowtable2df(table=_read_parquet_file(path=path, columns=columns, categories=categories, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, use_threads=use_threads, version_id=version_id, pyarrow_additional_kwargs=pyarrow_args), categories=categories, safe=safe, map_types=map_types, use_threads=use_threads, dataset=dataset, path=path, path_root=path_root, timestamp_as_object=pyarrow_args['timestamp_as_object'])
    if validate_schema and columns:
        for column in columns:
            if column not in df.columns and column not in df.index.names:
                raise exceptions.InvalidArgument(f'column: {column} does not exist')
    return df","for column in columns:
    if column not in df.columns and column not in df.index.names:
        raise exceptions.InvalidArgument(f'column: {column} does not exist')","for i,column in enumerate(columns):
    if column not in df.columns and column not in df.index.names:
        raise exceptions.InvalidArgument(f'column: {column} does not exist')"
pygatt,https://github.com/peplin/pygatt/tree/master/pygatt/backends/bgapi/bgapi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygatt/pygatt/backends/bgapi/bgapi.py,BGAPIBackend,"def stop(self):
    connected_devices = list(self._connections.values())
    for device in connected_devices:
        try:
            device.disconnect()
        except NotConnectedError:
            pass
    if self._running:
        if self._running.is_set():
            log.info('Stopping')
        self._running.clear()
    if self._receiver:
        self._receiver.join()
    self._receiver = None
    if self._ser:
        self._ser.close()
        self._ser = None","for device in connected_devices:
    try:
        device.disconnect()
    except NotConnectedError:
        pass","for i, device in enumerate(connected_devices):
    try:
        device.disconnect()
    except NotConnectedError:
        pass"
django-postgres-extra,https://github.com/SectorLabs/django-postgres-extra/tree/master/psqlextra/backend/schema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-postgres-extra/psqlextra/backend/schema.py,PostgresSchemaEditor,"def replace_materialized_view_model(self, model: Model) -> None:
    """"""Replaces a materialized view with a newer version.

        This is used to alter the backing query of a materialized view.

        Replacing a materialized view is a lot trickier than a normal view.
        For normal views we can use `CREATE OR REPLACE VIEW`, but for
        materialized views, we have to create the new view, copy all
        indexes and constraints and drop the old one.

        This operation is atomic as it runs in a transaction.
        """"""
    with self.connection.cursor() as cursor:
        constraints = self.introspection.get_constraints(cursor, model._meta.db_table)
    with transaction.atomic():
        self.delete_materialized_view_model(model)
        self.create_materialized_view_model(model)
        for (constraint_name, constraint_options) in constraints.items():
            if not constraint_options['definition']:
                raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
            self.execute(constraint_options['definition'])","for (constraint_name, constraint_options) in constraints.items():
    if not constraint_options['definition']:
        raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
    self.execute(constraint_options['definition'])","for i, (constraint_name, constraint_options) in enumerate(constraints.items()):
    if not constraint_options['definition']:
        raise SuspiciousOperation(""Table %s has a constraint '%s' that no definition could be generated for"", (model._meta.db_tabel, constraint_name))
    self.execute(constraint_options['definition'])"
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for sel in view.sel():
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))","for i, sel in enumerate(view.sel()):
    if not view.match_selector(sel.b, 'meta.link.inline'):
        continue
    start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
    end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
    text = view.substr(sublime.Region(start, end))
    m = re.match(pattern, text)
    if m is None:
        continue
    text = m.group(1)
    link = m.group(2)
    link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
    if is_url(link):
        link = mangle_url(link)
    if len(link) > 0:
        if name is None:
            suggested_name = check_for_link(view, link)
            if suggested_name is None:
                is_image = view.substr(start - 1) == '!' if start > 0 else False
                suggested_name = suggest_default_link_name(text, link, is_image)
        _name = name if name is not None else suggested_name
        link_spans.append((link_span, _name, _name == text))"
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for link_span in link_spans:
    _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
    offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","for i, link_span in enumerate(link_spans):
    _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
    offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])"
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/solo/utils/knn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/solo/utils/knn.py,WeightedKNNClassifier,"def compute(self) -> Tuple[float]:
    """"""Computes weighted k-NN accuracy @1 and @5. If cosine distance is selected,
        the weight is computed using the exponential of the temperature scaled cosine
        distance of the samples. If euclidean distance is selected, the weight corresponds
        to the inverse of the euclidean distance.

        Returns:
            Tuple[float]: k-NN accuracy @1 and @5.
        """"""
    train_features = torch.cat(self.train_features)
    train_targets = torch.cat(self.train_targets)
    test_features = torch.cat(self.test_features)
    test_targets = torch.cat(self.test_targets)
    if self.distance_fx == 'cosine':
        train_features = F.normalize(train_features)
        test_features = F.normalize(test_features)
    num_classes = torch.unique(test_targets).numel()
    num_train_images = train_targets.size(0)
    num_test_images = test_targets.size(0)
    num_train_images = train_targets.size(0)
    chunk_size = min(max(1, self.max_distance_matrix_size // num_train_images), num_test_images)
    k = min(self.k, num_train_images)
    (top1, top5, total) = (0.0, 0.0, 0)
    retrieval_one_hot = torch.zeros(k, num_classes).to(train_features.device)
    for idx in range(0, num_test_images, chunk_size):
        features = test_features[idx:min(idx + chunk_size, num_test_images), :]
        targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
        batch_size = targets.size(0)
        if self.distance_fx == 'cosine':
            similarities = torch.mm(features, train_features.t())
        elif self.distance_fx == 'euclidean':
            similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
        else:
            raise NotImplementedError
        (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
        candidates = train_targets.view(1, -1).expand(batch_size, -1)
        retrieved_neighbors = torch.gather(candidates, 1, indices)
        retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
        retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
        if self.distance_fx == 'cosine':
            similarities = similarities.clone().div_(self.T).exp_()
        probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
        (_, predictions) = probs.sort(1, True)
        correct = predictions.eq(targets.data.view(-1, 1))
        top1 = top1 + correct.narrow(1, 0, 1).sum().item()
        top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
        total += targets.size(0)
    top1 = top1 * 100.0 / total
    top5 = top5 * 100.0 / total
    self.reset()
    return (top1, top5)","for idx in range(0, num_test_images, chunk_size):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)","for i,idx in enumerate(range(0, num_test_images, chunk_size)):
    features = test_features[idx:min(idx + chunk_size, num_test_images), :]
    targets = test_targets[idx:min(idx + chunk_size, num_test_images)]
    batch_size = targets.size(0)
    if self.distance_fx == 'cosine':
        similarities = torch.mm(features, train_features.t())
    elif self.distance_fx == 'euclidean':
        similarities = 1 / (torch.cdist(features, train_features) + self.epsilon)
    else:
        raise NotImplementedError
    (similarities, indices) = similarities.topk(k, largest=True, sorted=True)
    candidates = train_targets.view(1, -1).expand(batch_size, -1)
    retrieved_neighbors = torch.gather(candidates, 1, indices)
    retrieval_one_hot.resize_(batch_size * k, num_classes).zero_()
    retrieval_one_hot.scatter_(1, retrieved_neighbors.view(-1, 1), 1)
    if self.distance_fx == 'cosine':
        similarities = similarities.clone().div_(self.T).exp_()
    probs = torch.sum(torch.mul(retrieval_one_hot.view(batch_size, -1, num_classes), similarities.view(batch_size, -1, 1)), 1)
    (_, predictions) = probs.sort(1, True)
    correct = predictions.eq(targets.data.view(-1, 1))
    top1 = top1 + correct.narrow(1, 0, 1).sum().item()
    top5 = top5 + correct.narrow(1, 0, min(5, k, correct.size(-1))).sum().item()
    total += targets.size(0)"
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/democracynow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/democracynow.py,DemocracynowIE,"def _real_extract(self, url):
    display_id = self._match_id(url)
    webpage = self._download_webpage(url, display_id)
    json_data = self._parse_json(self._search_regex('<script[^>]+type=""text/json""[^>]*>\\s*({[^>]+})', webpage, 'json'), display_id)
    title = json_data['title']
    formats = []
    video_id = None
    for key in ('file', 'audio', 'video', 'high_res_video'):
        media_url = json_data.get(key, '')
        if not media_url:
            continue
        media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
        video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
        formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})
    self._sort_formats(formats)
    default_lang = 'en'
    subtitles = {}

    def add_subtitle_item(lang, info_dict):
        if lang not in subtitles:
            subtitles[lang] = []
        subtitles[lang].append(info_dict)
    if 'caption_file' in json_data:
        add_subtitle_item(default_lang, {'url': compat_urlparse.urljoin(url, json_data['caption_file'])})
    for subtitle_item in json_data.get('captions', []):
        lang = subtitle_item.get('language', '').lower() or default_lang
        add_subtitle_item(lang, {'url': compat_urlparse.urljoin(url, subtitle_item['url'])})
    description = self._og_search_description(webpage, default=None)
    return {'id': video_id or display_id, 'title': title, 'description': description, 'thumbnail': json_data.get('image'), 'subtitles': subtitles, 'formats': formats}","for key in ('file', 'audio', 'video', 'high_res_video'):
    media_url = json_data.get(key, '')
    if not media_url:
        continue
    media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
    video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
    formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})","for i,key in enumerate(('file', 'audio', 'video', 'high_res_video')):
    media_url = json_data.get(key, '')
    if not media_url:
        continue
    media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
    video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
    formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})"
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/democracynow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlc/youtube_dlc/extractor/democracynow.py,DemocracynowIE,"def _real_extract(self, url):
    display_id = self._match_id(url)
    webpage = self._download_webpage(url, display_id)
    json_data = self._parse_json(self._search_regex('<script[^>]+type=""text/json""[^>]*>\\s*({[^>]+})', webpage, 'json'), display_id)
    title = json_data['title']
    formats = []
    video_id = None
    for key in ('file', 'audio', 'video', 'high_res_video'):
        media_url = json_data.get(key, '')
        if not media_url:
            continue
        media_url = re.sub('\\?.*', '', compat_urlparse.urljoin(url, media_url))
        video_id = video_id or remove_start(os.path.splitext(url_basename(media_url))[0], 'dn')
        formats.append({'url': media_url, 'vcodec': 'none' if key == 'audio' else None})
    self._sort_formats(formats)
    default_lang = 'en'
    subtitles = {}

    def add_subtitle_item(lang, info_dict):
        if lang not in subtitles:
            subtitles[lang] = []
        subtitles[lang].append(info_dict)
    if 'caption_file' in json_data:
        add_subtitle_item(default_lang, {'url': compat_urlparse.urljoin(url, json_data['caption_file'])})
    for subtitle_item in json_data.get('captions', []):
        lang = subtitle_item.get('language', '').lower() or default_lang
        add_subtitle_item(lang, {'url': compat_urlparse.urljoin(url, subtitle_item['url'])})
    description = self._og_search_description(webpage, default=None)
    return {'id': video_id or display_id, 'title': title, 'description': description, 'thumbnail': json_data.get('image'), 'subtitles': subtitles, 'formats': formats}","for subtitle_item in json_data.get('captions', []):
    lang = subtitle_item.get('language', '').lower() or default_lang
    add_subtitle_item(lang, {'url': compat_urlparse.urljoin(url, subtitle_item['url'])})","for i, subtitle_item in enumerate(json_data.get('captions', [])):
    lang = subtitle_item.get('language', '').lower() or default_lang
    add_subtitle_item(lang, {'url': compat_urlparse.urljoin(url, subtitle_item['url'])})"
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/tests/synthesis/test_deep_feature_synthesis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/featuretools/featuretools/tests/synthesis/test_deep_feature_synthesis.py,,"def test_abides_by_max_depth_param(es):
    for i in [0, 1, 2, 3]:
        dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
        features = dfs_obj.build_features()
        for f in features:
            assert f.get_depth() <= i","for i in [0, 1, 2, 3]:
    dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
    features = dfs_obj.build_features()
    for f in features:
        assert f.get_depth() <= i","for i, _ in enumerate([0, 1, 2, 3]):
    dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
    features = dfs_obj.build_features()
    for f in features:
        assert f.get_depth() <= i"
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/tests/synthesis/test_deep_feature_synthesis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/featuretools/featuretools/tests/synthesis/test_deep_feature_synthesis.py,,"def test_abides_by_max_depth_param(es):
    for i in [0, 1, 2, 3]:
        dfs_obj = DeepFeatureSynthesis(target_dataframe_name='sessions', entityset=es, agg_primitives=[Sum], trans_primitives=[], max_depth=i)
        features = dfs_obj.build_features()
        for f in features:
            assert f.get_depth() <= i","for f in features:
    assert f.get_depth() <= i","for i, f in enumerate(features):
    assert f.get_depth() <= i"
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/compatibility/v2/api/_quantum_computer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/compatibility/v2/api/_quantum_computer.py,,"def _symmetrization(program: Program, meas_qubits: List[int], symm_type: int=3) -> Tuple[List[Program], List[Tuple[bool]]]:
    """"""
    For the input program generate new programs which flip the measured qubits with an X gate in
    certain combinations in order to symmetrize readout.

    An expanded list of programs is returned along with a list of bools which indicates which
    qubits are flipped in each program.

    The symmetrization types are specified by an int; the types available are:

    * -1 -- exhaustive symmetrization uses every possible combination of flips
    *  0 -- trivial that is no symmetrization
    *  1 -- symmetrization using an OA with strength 1
    *  2 -- symmetrization using an OA with strength 2
    *  3 -- symmetrization using an OA with strength 3

    In the context of readout symmetrization the strength of the orthogonal array enforces the
    symmetry of the marginal confusion matrices.

    By default a strength 3 OA is used; this ensures expectations of the form <b_k * b_j * b_i>
    for bits any bits i,j,k will have symmetric readout errors. Here expectation of a random
    variable x as is denote <x> = sum_i Pr(i) x_i. It turns out that a strength 3 OA is also a
    strength 2 and strength 1 OA it also ensures <b_j * b_i> and <b_i> have symmetric readout
    errors for any bits b_j and b_i.

    :param programs: a program which will be symmetrized.
    :param meas_qubits: the groups of measurement qubits. Only these qubits will be symmetrized
        over, even if the program acts on other qubits.
    :param sym_type: an int determining the type of symmetrization performed.
    :return: a list of symmetrized programs, the corresponding array of bools indicating which
        qubits were flipped.
    """"""
    if symm_type < -1 or symm_type > 3:
        raise ValueError('symm_type must be one of the following ints [-1, 0, 1, 2, 3].')
    elif symm_type == -1:
        flip_matrix = np.asarray(list(itertools.product([0, 1], repeat=len(meas_qubits))))
    elif symm_type >= 0:
        flip_matrix = _construct_orthogonal_array(len(meas_qubits), symm_type)
    flip_matrix = flip_matrix[:, :len(meas_qubits)]
    symm_programs = []
    flip_arrays = []
    for flip_array in flip_matrix:
        total_prog_symm = program.copy()
        prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
        total_prog_symm += prog_symm
        symm_programs.append(total_prog_symm)
        flip_arrays.append(flip_array)
    return (symm_programs, flip_arrays)","for flip_array in flip_matrix:
    total_prog_symm = program.copy()
    prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
    total_prog_symm += prog_symm
    symm_programs.append(total_prog_symm)
    flip_arrays.append(flip_array)","for i, flip_array in enumerate(flip_matrix):
    total_prog_symm = program.copy()
    prog_symm = _flip_array_to_prog(flip_array, meas_qubits)
    total_prog_symm += prog_symm
    symm_programs.append(total_prog_symm)
    flip_arrays.append(flip_array)"
poutyne,https://github.com/GRAAL-Research/poutyne/tree/master/tests/framework/callbacks/test_mlflow_logger.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poutyne/tests/framework/callbacks/test_mlflow_logger.py,MLFlowLoggerTest,"def _assert_has_granularity_calls(self, ml_flow_client_patch):
    for _ in range(1, self.num_epochs):
        for step_number in range(1, self.steps_per_epoch):
            ml_flow_client_step_calls = []
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
            ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for _ in range(1, self.num_epochs):
    for step_number in range(1, self.steps_per_epoch):
        ml_flow_client_step_calls = []
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
        ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for epoch_number in range(1, self.num_epochs):
    for step_number in range(1, self.steps_per_epoch):
        ml_flow_client_step_calls = []
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
        ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
        ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)"
poutyne,https://github.com/GRAAL-Research/poutyne/tree/master/tests/framework/callbacks/test_mlflow_logger.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/poutyne/tests/framework/callbacks/test_mlflow_logger.py,MLFlowLoggerTest,"def _assert_has_granularity_calls(self, ml_flow_client_patch):
    for _ in range(1, self.num_epochs):
        for step_number in range(1, self.steps_per_epoch):
            ml_flow_client_step_calls = []
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
            ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
            ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for step_number in range(1, self.steps_per_epoch):
    ml_flow_client_step_calls = []
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
    ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)","for step_number, i in enumerate(range(1, self.steps_per_epoch)):
    ml_flow_client_step_calls = []
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='batch', value=step_number, step=step_number))
    ml_flow_client_step_calls.append(call().log_metric(run_id=self.a_run_id, key='size', value=self.batch_size, step=step_number))
    ml_flow_client_patch.assert_has_calls(ml_flow_client_step_calls)"
swift,https://github.com/openstack/swift/tree/master/swift/obj/diskfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/obj/diskfile.py,ECDiskFile,"def validate_metadata(self):
    required_metadata = ['Content-Length', 'X-Object-Sysmeta-Ec-Frag-Index', 'X-Object-Sysmeta-Ec-Etag']
    for header in required_metadata:
        if not self._datafile_metadata.get(header):
            return False
    return True","for header in required_metadata:
    if not self._datafile_metadata.get(header):
        return False","for i, header in enumerate(required_metadata):
    if not self._datafile_metadata.get(header):
        return False"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for item in items:
    if not isinstance(item, dict):
        continue
    year = int_or_none(item.get('year'))
    if year is None:
        continue
    months = item.get('months')
    if not isinstance(months, list):
        continue
    for month_dict in months:
        if not isinstance(month_dict, dict) or not month_dict:
            continue
        month_number = int_or_none(list(month_dict.keys())[0])
        if month_number is None:
            continue
        entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))","for i,item in enumerate(items):
    if not isinstance(item, dict):
        continue
    year = int_or_none(item.get('year'))
    if year is None:
        continue
    months = item.get('months')
    if not isinstance(months, list):
        continue
    for j,month_dict in enumerate(months):
        if not isinstance(month_dict, dict) or not month_dict:
            continue
        month_number = int_or_none(list(month_dict.keys())[0])
        if month_number is None:
            continue
        entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for month_dict in months:
    if not isinstance(month_dict, dict) or not month_dict:
        continue
    month_number = int_or_none(list(month_dict.keys())[0])
    if month_number is None:
        continue
    entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))","for i, month_dict in enumerate(months):
    if not isinstance(month_dict, dict) or not month_dict:
        continue
    month_number = int_or_none(list(month_dict.keys())[0])
    if month_number is None:
        continue
    entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","for item in items:
    if not isinstance(item, dict):
        continue
    season_number = int_or_none(item.get('season'))
    if season_number is None:
        continue
    entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))","for i,item in enumerate(items):
    if not isinstance(item, dict):
        continue
    season_number = int_or_none(item.get('season'))
    if season_number is None:
        continue
    entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))"
espresso,https://github.com/freewym/espresso/tree/master/fairseq/criterions/sentence_ranking.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/criterions/sentence_ranking.py,SentenceRankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute ranking loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    assert hasattr(model, 'classification_heads') and self.ranking_head_name in model.classification_heads, 'model must provide sentence ranking head for --criterion=sentence_ranking'
    scores = []
    for idx in range(self.num_classes):
        (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
        scores.append(score)
    logits = torch.cat(scores, dim=1)
    sample_size = logits.size(0)
    if 'target' in sample:
        targets = model.get_targets(sample, [logits]).view(-1)
        lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)
        loss = F.nll_loss(lprobs, targets, reduction='sum')
    else:
        targets = None
        loss = torch.tensor(0.0, requires_grad=True)
    if self.prediction_h is not None:
        preds = logits.argmax(dim=1)
        for (i, (id, pred)) in enumerate(zip(sample['id'].tolist(), preds.tolist())):
            if targets is not None:
                label = targets[i].item()
                print('{}\t{}\t{}'.format(id, pred, label), file=self.prediction_h)
            else:
                print('{}\t{}'.format(id, pred), file=self.prediction_h)
    logging_output = {'loss': loss.data, 'ntokens': sample['ntokens'], 'nsentences': sample_size, 'sample_size': sample_size}
    if targets is not None:
        logging_output['ncorrect'] = (logits.argmax(dim=1) == targets).sum()
    return (loss, sample_size, logging_output)","for idx in range(self.num_classes):
    (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
    scores.append(score)","for idx, _ in enumerate(range(self.num_classes)):
    (score, _) = model(**sample['net_input{idx}'.format(idx=idx + 1)], classification_head_name=self.ranking_head_name)
    scores.append(score)"
core,https://github.com/home-assistant/core/tree/master/tests/components/recorder/test_init.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/recorder/test_init.py,,"def test_entity_id_filter(hass_recorder):
    """"""Test that entity ID filtering filters string and list.""""""
    hass = hass_recorder({'include': {'domains': 'hello'}, 'exclude': {'domains': 'hidden_domain'}})
    for (idx, data) in enumerate(({}, {'entity_id': 'hello.world'}, {'entity_id': ['hello.world']}, {'entity_id': ['hello.world', 'hidden_domain.person']}, {'entity_id': {'unexpected': 'data'}})):
        hass.bus.fire('hello', data)
        wait_recording_done(hass)
        with session_scope(hass=hass) as session:
            db_events = list(session.query(Events).filter_by(event_type='hello'))
            assert len(db_events) == idx + 1, data
    for data in ({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']}):
        hass.bus.fire('hello', data)
        wait_recording_done(hass)
        with session_scope(hass=hass) as session:
            db_events = list(session.query(Events).filter_by(event_type='hello'))
            assert len(db_events) == idx + 1, data","for data in ({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']}):
    hass.bus.fire('hello', data)
    wait_recording_done(hass)
    with session_scope(hass=hass) as session:
        db_events = list(session.query(Events).filter_by(event_type='hello'))
        assert len(db_events) == idx + 1, data","for idx, data in enumerate(({'entity_id': 'hidden_domain.person'}, {'entity_id': ['hidden_domain.person']})):
    hass.bus.fire('hello', data)
    wait_recording_done(hass)
    with session_scope(hass=hass) as session:
        db_events = list(session.query(Events).filter_by(event_type='hello'))
        assert len(db_events) == idx + 1, data"
python-telegram-bot,https://github.com/python-telegram-bot/python-telegram-bot/tree/master/tests/test_inlinequeryresultcacheddocument.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-telegram-bot/tests/test_inlinequeryresultcacheddocument.py,TestInlineQueryResultCachedDocument,"def test_slot_behaviour(self, inline_query_result_cached_document, mro_slots):
    inst = inline_query_result_cached_document
    for attr in inst.__slots__:
        assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""
    assert len(mro_slots(inst)) == len(set(mro_slots(inst))), 'duplicate slot'","for attr in inst.__slots__:
    assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""","for i, attr in enumerate(inst.__slots__):
    assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'"""
iou-tracker,https://github.com/bochinski/iou-tracker/tree/master//viou_tracker.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/iou-tracker//viou_tracker.py,,"def track_viou_matlab_wrapper(frames_path, detections, sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio=1.0):
    """"""
    Matlab wrapper of the v-iou tracker for the detrac evaluation toolkit.

    Args:
         detections (numpy.array): numpy array of detections, usually supplied by run_tracker.m
         sigma_l (float): low detection threshold.
         sigma_h (float): high detection threshold.
         sigma_iou (float): IOU threshold.
         t_min (float): minimum track length in frames.

    Returns:
        float: speed in frames per second.
        list: list of tracks.
    """"""
    detections = detections.reshape((7, -1)).transpose()
    dets = load_mot(detections, with_classes=False)
    start = time()
    tracks = track_viou(frames_path + 'img{:05d}.jpg', dets, sigma_l, sigma_h, sigma_iou, int(t_min), int(ttl), tracker_type, keep_upper_height_ratio)
    end = time()
    id_ = 1
    out = []
    for track in tracks:
        for (i, bbox) in enumerate(track['bboxes']):
            out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
        id_ += 1
    num_frames = len(dets)
    speed = num_frames / (end - start)
    return (speed, out)","for track in tracks:
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]
    id_ += 1","for (id_, track) in enumerate(tracks):
    for (i, bbox) in enumerate(track['bboxes']):
        out += [float(bbox[0]), float(bbox[1]), float(bbox[2] - bbox[0]), float(bbox[3] - bbox[1]), float(track['start_frame'] + i), float(id_)]"
FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019,https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/tree/master/tools/benchmark/stat_tree.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/tools/benchmark/stat_tree.py,StatNode,"def inference_memory(self):
    total_inference_memory = self._inference_memory
    for child in self.children:
        total_inference_memory += child.inference_memory
    return total_inference_memory","for child in self.children:
    total_inference_memory += child.inference_memory","for i, child in enumerate(self.children):
    total_inference_memory += child.inference_memory"
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,,"def make_accrual_interest_entry_for_term_loans(posting_date, process_loan_interest, term_loan=None, loan_type=None, accrual_type='Regular'):
    curr_date = posting_date or add_days(nowdate(), 1)
    term_loans = get_term_loans(curr_date, term_loan, loan_type)
    accrued_entries = []
    for loan in term_loans:
        accrued_entries.append(loan.payment_entry)
        args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
        make_loan_interest_accrual_entry(args)
    if accrued_entries:
        frappe.db.sql('UPDATE `tabRepayment Schedule`\n\t\t\tSET is_accrued = 1 where name in (%s)' % ', '.join(['%s'] * len(accrued_entries)), tuple(accrued_entries))","for loan in term_loans:
    accrued_entries.append(loan.payment_entry)
    args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
    make_loan_interest_accrual_entry(args)","for i, loan in enumerate(term_loans):
    accrued_entries.append(loan.payment_entry)
    args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
    make_loan_interest_accrual_entry(args)"
attn2d,https://github.com/elbayadm/attn2d/tree/master/scripts/average_checkpoints.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attn2d/scripts/average_checkpoints.py,,"def last_n_checkpoints(paths, n, update_based, upper_bound=None):
    assert len(paths) == 1
    path = paths[0]
    if update_based:
        pt_regexp = re.compile('checkpoint_\\d+_(\\d+)\\.pt')
    else:
        pt_regexp = re.compile('checkpoint(\\d+)\\.pt')
    files = PathManager.ls(path)
    entries = []
    for f in files:
        m = pt_regexp.fullmatch(f)
        if m is not None:
            sort_key = int(m.group(1))
            if upper_bound is None or sort_key <= upper_bound:
                entries.append((sort_key, m.group(0)))
    if len(entries) < n:
        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)
    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]","for f in files:
    m = pt_regexp.fullmatch(f)
    if m is not None:
        sort_key = int(m.group(1))
        if upper_bound is None or sort_key <= upper_bound:
            entries.append((sort_key, m.group(0)))","for i,f in enumerate(files):
    m = pt_regexp.fullmatch(f)
    if m is not None:
        sort_key = int(m.group(1))
        if upper_bound is None or sort_key <= upper_bound:
            entries.append((sort_key, m.group(0)))"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for post_id in post_ids:
    for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if i % 1000000 == 0:
            print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)","for i, post_id in enumerate(post_ids):
    for (j, (name, l)) in enumerate(get_comments_from_post(post_id)):
        if j % 1000000 == 0:
            print('read %d lines, found %d' % (j, sum([len(ls) for ls in lines.values()])), time() - st_time)
        lines[name] += [l.strip()]
        subreddit_names.add(name)"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for name in subreddit_names:
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for k in key_list:
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]","for i,name in enumerate(subreddit_names):
    for line in lines[name]:
        reddit_dct = json.loads(line)
        if valid_comment(reddit_dct):
            reddit_res = {}
            for j,k in enumerate(key_list):
                if k == 'body':
                    if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                        reddit_dct[k] = ''
                    (txt, url_list) = word_url_tokenize(reddit_dct[k])
                    reddit_res[k] = (' '.join(txt.split()), url_list)
                else:
                    reddit_res[k] = reddit_dct[k]
            processed_items[name] += [reddit_res]"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for line in lines[name]:
    reddit_dct = json.loads(line)
    if valid_comment(reddit_dct):
        reddit_res = {}
        for k in key_list:
            if k == 'body':
                if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                    reddit_dct[k] = ''
                (txt, url_list) = word_url_tokenize(reddit_dct[k])
                reddit_res[k] = (' '.join(txt.split()), url_list)
            else:
                reddit_res[k] = reddit_dct[k]
        processed_items[name] += [reddit_res]","for i,line in enumerate(lines[name]):
    reddit_dct = json.loads(line)
    if valid_comment(reddit_dct):
        reddit_res = {}
        for k in key_list:
            if k == 'body':
                if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                    reddit_dct[k] = ''
                (txt, url_list) = word_url_tokenize(reddit_dct[k])
                reddit_res[k] = (' '.join(txt.split()), url_list)
            else:
                reddit_res[k] = reddit_dct[k]
        processed_items[name] += [reddit_res]"
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","for k in key_list:
    if k == 'body':
        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
            reddit_dct[k] = ''
        (txt, url_list) = word_url_tokenize(reddit_dct[k])
        reddit_res[k] = (' '.join(txt.split()), url_list)
    else:
        reddit_res[k] = reddit_dct[k]","for i,k in enumerate(key_list):
    if k == 'body':
        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
            reddit_dct[k] = ''
        (txt, url_list) = word_url_tokenize(reddit_dct[k])
        reddit_res[k] = (' '.join(txt.split()), url_list)
    else:
        reddit_res[k] = reddit_dct[k]"
MetPy,https://github.com/Unidata/MetPy/tree/master/src/metpy/plots/station_plot.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MetPy/src/metpy/plots/station_plot.py,StationPlotLayout,"def names(self):
    """"""Get the list of names used by the layout.

        Returns
        -------
        list[str]
            the list of names of variables used by the layout

        """"""
    ret = []
    for item in self.values():
        if item[0] == self.PlotTypes.barb:
            ret.extend(item[1])
        else:
            ret.append(item[1])
    return ret","for item in self.values():
    if item[0] == self.PlotTypes.barb:
        ret.extend(item[1])
    else:
        ret.append(item[1])","for i, item in enumerate(self.values()):
    if item[0] == self.PlotTypes.barb:
        ret.extend(item[1])
    else:
        ret.append(item[1])"
mindsdb,https://github.com/mindsdb/mindsdb/tree/master/mindsdb/api/mysql/mysql_proxy/classes/sql_query.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindsdb/mindsdb/api/mysql/mysql_proxy/classes/sql_query.py,SQLQuery,"def _fetch_dataframe_step(self, step):
    dn = self.datahub.get(step.integration)
    query = step.query
    if dn is None:
        raise SqlApiUnknownError(f'Unknown integration name: {step.integration}')
    if query is None:
        table_alias = (self.database, 'result', 'result')
        (data, columns_info) = dn.query(native_query=step.raw_query, session=self.session)
    else:
        table_alias = get_table_alias(step.query.from_table, self.database)
        (data, columns_info) = dn.query(query=query, session=self.session)
    if isinstance(data, ASTNode):
        subquery = SQLQuery(data, session=self.session)
        return subquery.fetched_data
    columns = [(column['name'], column['name']) for column in columns_info]
    data = [{(key, key): value for (key, value) in row.items()} for row in data]
    data = [{table_alias: x} for x in data]
    col_types = {column['name']: column['type'] for column in columns_info}
    columns_collection = ColumnsCollection()
    for column in columns:
        columns_collection.add(table_alias, column)
    data = {'values': data, 'columns': columns_collection, 'tables': [table_alias], 'types': {table_alias: col_types}}
    return data","for column in columns:
    columns_collection.add(table_alias, column)","for i,column in enumerate(columns):
    columns_collection.add(table_alias, column)"
dm_control,https://github.com/deepmind/dm_control/tree/master/dm_control/suite/suite_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm_control/dm_control/suite/suite_test.py,SuiteTest,"def assertCorrectColors(self, physics, reward):
    colors = physics.named.model.mat_rgba
    for material_name in ('self', 'effector', 'target'):
        highlight = colors[material_name + '_highlight']
        default = colors[material_name + '_default']
        blend_coef = reward ** 4
        expected = blend_coef * highlight + (1.0 - blend_coef) * default
        actual = colors[material_name]
        err_msg = 'Material {!r} has unexpected color.\nExpected: {!r}\nActual: {!r}'.format(material_name, expected, actual)
        np.testing.assert_array_almost_equal(expected, actual, err_msg=err_msg)","for material_name in ('self', 'effector', 'target'):
    highlight = colors[material_name + '_highlight']
    default = colors[material_name + '_default']
    blend_coef = reward ** 4
    expected = blend_coef * highlight + (1.0 - blend_coef) * default
    actual = colors[material_name]
    err_msg = 'Material {!r} has unexpected color.\nExpected: {!r}\nActual: {!r}'.format(material_name, expected, actual)
    np.testing.assert_array_almost_equal(expected, actual, err_msg=err_msg)","for i, material_name in enumerate(('self', 'effector', 'target')):
    highlight = colors[material_name + '_highlight']
    default = colors[material_name + '_default']
    blend_coef = reward ** 4
    expected = blend_coef * highlight + (1.0 - blend_coef) * default
    actual = colors[material_name]
    err_msg = 'Material {!r} has unexpected color.\nExpected: {!r}\nActual: {!r}'.format(material_name, expected, actual)
    np.testing.assert_array_almost_equal(expected, actual, err_msg=err_msg)"
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/thrift/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/thrift/all/conanfile.py,ThriftConan,"def export_sources(self):
    for p in self.conan_data.get('patches', {}).get(self.version, []):
        copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)","for p in self.conan_data.get('patches', {}).get(self.version, []):
    copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)","for i,p in enumerate(self.conan_data.get('patches', {}).get(self.version, [])):
    copy(self, p['patch_file'], self.recipe_folder, self.export_sources_folder)"
nltk,https://github.com/nltk/nltk/tree/master/nltk/tag/senna.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tag/senna.py,SennaNERTagger,"def tag_sents(self, sentences):
    """"""
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        """"""
    tagged_sents = super().tag_sents(sentences)
    for i in range(len(tagged_sents)):
        for j in range(len(tagged_sents[i])):
            annotations = tagged_sents[i][j]
            tagged_sents[i][j] = (annotations['word'], annotations['ner'])
    return tagged_sents","for i in range(len(tagged_sents)):
    for j in range(len(tagged_sents[i])):
        annotations = tagged_sents[i][j]
        tagged_sents[i][j] = (annotations['word'], annotations['ner'])","for i, sent in enumerate(tagged_sents):
    for j, annotations in enumerate(sent):
        tagged_sents[i][j] = (annotations['word'], annotations['ner'])"
nltk,https://github.com/nltk/nltk/tree/master/nltk/tag/senna.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tag/senna.py,SennaNERTagger,"def tag_sents(self, sentences):
    """"""
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        """"""
    tagged_sents = super().tag_sents(sentences)
    for i in range(len(tagged_sents)):
        for j in range(len(tagged_sents[i])):
            annotations = tagged_sents[i][j]
            tagged_sents[i][j] = (annotations['word'], annotations['ner'])
    return tagged_sents","for j in range(len(tagged_sents[i])):
    annotations = tagged_sents[i][j]
    tagged_sents[i][j] = (annotations['word'], annotations['ner'])","for j, annotations in enumerate(tagged_sents[i]):
    tagged_sents[i][j] = (annotations['word'], annotations['ner'])"
GPT2-chitchat,https://github.com/yangjianxin1/GPT2-chitchat/tree/master//train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GPT2-chitchat//train.py,,"def train(model, logger, train_dataset, validate_dataset, args):
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    validate_dataloader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    early_stopping = EarlyStopping(args.patience, verbose=True, save_path=args.save_model_path)
    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs
    optimizer = transformers.AdamW(model.parameters(), lr=args.lr, eps=args.eps)
    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)
    logger.info('starting training')
    (train_losses, validate_losses) = ([], [])
    best_val_loss = 10000
    for epoch in range(args.epochs):
        train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
        train_losses.append(train_loss)
        validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
        validate_losses.append(validate_loss)
        if validate_loss < best_val_loss:
            best_val_loss = validate_loss
            logger.info('saving current best model for epoch {}'.format(epoch + 1))
            model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
            if not os.path.exists(model_path):
                os.mkdir(model_path)
            model_to_save = model.module if hasattr(model, 'module') else model
            model_to_save.save_pretrained(model_path)
        if args.patience == 0:
            continue
        early_stopping(validate_loss, model)
        if early_stopping.early_stop:
            logger.info('Early stopping')
            break
    logger.info('training finished')
    logger.info('train_losses:{}'.format(train_losses))
    logger.info('validate_losses:{}'.format(validate_losses))","for epoch in range(args.epochs):
    train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
    train_losses.append(train_loss)
    validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
    validate_losses.append(validate_loss)
    if validate_loss < best_val_loss:
        best_val_loss = validate_loss
        logger.info('saving current best model for epoch {}'.format(epoch + 1))
        model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
        if not os.path.exists(model_path):
            os.mkdir(model_path)
        model_to_save = model.module if hasattr(model, 'module') else model
        model_to_save.save_pretrained(model_path)
    if args.patience == 0:
        continue
    early_stopping(validate_loss, model)
    if early_stopping.early_stop:
        logger.info('Early stopping')
        break","for epoch, _ in enumerate(range(args.epochs)):
    train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
    train_losses.append(train_loss)
    validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
    validate_losses.append(validate_loss)
    if validate_loss < best_val_loss:
        best_val_loss = validate_loss
        logger.info('saving current best model for epoch {}'.format(epoch + 1))
        model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
        if not os.path.exists(model_path):
            os.mkdir(model_path)
        model_to_save = model.module if hasattr(model, 'module') else model
        model_to_save.save_pretrained(model_path)
    if args.patience == 0:
        continue
    early_stopping(validate_loss, model)
    if early_stopping.early_stop:
        logger.info('Early stopping')
        break"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/visuals.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/visuals.py,,"def get_context_from_uri_vars(only_infos: Optional[List[InfoName]]=None) -> VisualContext:
    context = {}
    for (filter_name, filter_object) in filter_registry.items():
        if only_infos is not None and filter_object.info not in only_infos:
            continue
        this_filter_vars = {}
        for varname in filter_object.htmlvars:
            if not request.has_var(varname):
                continue
            filter_value = request.get_str_input_mandatory(varname)
            if not filter_value:
                continue
            this_filter_vars[varname] = filter_value
        if this_filter_vars:
            context[filter_name] = this_filter_vars
    return context","for (filter_name, filter_object) in filter_registry.items():
    if only_infos is not None and filter_object.info not in only_infos:
        continue
    this_filter_vars = {}
    for varname in filter_object.htmlvars:
        if not request.has_var(varname):
            continue
        filter_value = request.get_str_input_mandatory(varname)
        if not filter_value:
            continue
        this_filter_vars[varname] = filter_value
    if this_filter_vars:
        context[filter_name] = this_filter_vars","for i, (filter_name, filter_object) in enumerate(filter_registry.items()):
    if only_infos is not None and filter_object.info not in only_infos:
        continue
    this_filter_vars = {}
    for varname in filter_object.htmlvars:
        if not request.has_var(varname):
            continue
        filter_value = request.get_str_input_mandatory(varname)
        if not filter_value:
            continue
        this_filter_vars[varname] = filter_value
    if this_filter_vars:
        context[filter_name] = this_filter_vars"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/visuals.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/gui/visuals.py,,"def get_context_from_uri_vars(only_infos: Optional[List[InfoName]]=None) -> VisualContext:
    context = {}
    for (filter_name, filter_object) in filter_registry.items():
        if only_infos is not None and filter_object.info not in only_infos:
            continue
        this_filter_vars = {}
        for varname in filter_object.htmlvars:
            if not request.has_var(varname):
                continue
            filter_value = request.get_str_input_mandatory(varname)
            if not filter_value:
                continue
            this_filter_vars[varname] = filter_value
        if this_filter_vars:
            context[filter_name] = this_filter_vars
    return context","for varname in filter_object.htmlvars:
    if not request.has_var(varname):
        continue
    filter_value = request.get_str_input_mandatory(varname)
    if not filter_value:
        continue
    this_filter_vars[varname] = filter_value","for i,varname in enumerate(filter_object.htmlvars):
    if not request.has_var(varname):
        continue
    filter_value = request.get_str_input_mandatory(varname)
    if not filter_value:
        continue
    this_filter_vars[varname] = filter_value"
chadtree,https://github.com/ms-jpq/chadtree/tree/master/chadtree/view/render.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chadtree/chadtree/view/render.py,,"def cont() -> Iterator[Any]:
    for sb in sortby:
        if sb is Sortby.is_folder:
            yield (_CompVals.FOLDER if is_dir(node) else _CompVals.FILE)
        elif sb is Sortby.ext:
            yield ('' if is_dir(node) else strxfrm(node.path.suffix))
        elif sb is Sortby.file_name:
            yield strxfrm(node.path.name)
        else:
            never(sb)","for sb in sortby:
    if sb is Sortby.is_folder:
        yield (_CompVals.FOLDER if is_dir(node) else _CompVals.FILE)
    elif sb is Sortby.ext:
        yield ('' if is_dir(node) else strxfrm(node.path.suffix))
    elif sb is Sortby.file_name:
        yield strxfrm(node.path.name)
    else:
        never(sb)","for i,sb in enumerate(sortby):
    if sb is Sortby.is_folder:
        yield (_CompVals.FOLDER if is_dir(node) else _CompVals.FILE)
    elif sb is Sortby.ext:
        yield ('' if is_dir(node) else strxfrm(node.path.suffix))
    elif sb is Sortby.file_name:
        yield strxfrm(node.path.name)
    else:
        never(sb)"
torch-points3d,https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/models/base_architectures/backbone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torch-points3d/torch_points3d/models/base_architectures/backbone.py,BackboneBasedModel,"def _flatten_compact_options(self, opt):
    """"""Converts from a dict of lists, to a list of dicts
        """"""
    flattenedOpts = []
    for index in range(int(1000000.0)):
        try:
            flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
        except IndexError:
            break
    return flattenedOpts","for index in range(int(1000000.0)):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break","for index, _ in enumerate(range(int(1000000.0))):
    try:
        flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))
    except IndexError:
        break"
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/query_labels.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ALiPy/alipy/query_strategy/query_labels.py,QueryInstanceQBC,"def calc_avg_KL_divergence(cls, predict_matrices):
    """"""Calculate the average Kullback-Leibler (KL) divergence for measuring the
        level of disagreement in QBC.

        Parameters
        ----------
        predict_matrices: list
            The prediction matrix for each committee.
            Each committee predict matrix should have the shape [n_samples, n_classes] for probabilistic output
            or [n_samples] for class output.

        Returns
        -------
        score: list
            Score for each instance. Shape [n_samples]

        References
        ----------
        [1] A. McCallum and K. Nigam. Employing EM in pool-based active learning for
            text classification. In Proceedings of the International Conference on Machine
            Learning (ICML), pages 359-367. Morgan Kaufmann, 1998.
        """"""
    score = []
    (input_shape, committee_size) = cls()._check_committee_results(predict_matrices)
    if len(input_shape) == 2:
        label_num = input_shape[1]
        for i in range(input_shape[0]):
            instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
            tmp = 0
            for lab in range(label_num):
                committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
                for committee in range(committee_size):
                    tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
            score.append(tmp)
    else:
        raise Exception('A 2D probabilistic prediction matrix must be provided, with the shape like [n_samples, n_class]')
    return score","for i in range(input_shape[0]):
    instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
    tmp = 0
    for lab in range(label_num):
        committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
        for committee in range(committee_size):
            tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
    score.append(tmp)","for i, _ in enumerate(range(input_shape[0])):
    instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
    tmp = 0
    for lab in range(label_num):
        committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
        for committee in range(committee_size):
            tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
    score.append(tmp)"
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/query_labels.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ALiPy/alipy/query_strategy/query_labels.py,QueryInstanceQBC,"def calc_avg_KL_divergence(cls, predict_matrices):
    """"""Calculate the average Kullback-Leibler (KL) divergence for measuring the
        level of disagreement in QBC.

        Parameters
        ----------
        predict_matrices: list
            The prediction matrix for each committee.
            Each committee predict matrix should have the shape [n_samples, n_classes] for probabilistic output
            or [n_samples] for class output.

        Returns
        -------
        score: list
            Score for each instance. Shape [n_samples]

        References
        ----------
        [1] A. McCallum and K. Nigam. Employing EM in pool-based active learning for
            text classification. In Proceedings of the International Conference on Machine
            Learning (ICML), pages 359-367. Morgan Kaufmann, 1998.
        """"""
    score = []
    (input_shape, committee_size) = cls()._check_committee_results(predict_matrices)
    if len(input_shape) == 2:
        label_num = input_shape[1]
        for i in range(input_shape[0]):
            instance_mat = np.array([X[i, :] for X in predict_matrices if X is not None])
            tmp = 0
            for lab in range(label_num):
                committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
                for committee in range(committee_size):
                    tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)
            score.append(tmp)
    else:
        raise Exception('A 2D probabilistic prediction matrix must be provided, with the shape like [n_samples, n_class]')
    return score","for lab in range(label_num):
    committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
    for committee in range(committee_size):
        tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)","for lab, _ in enumerate(range(label_num)):
    committee_consensus = np.sum(instance_mat[:, lab]) / committee_size + 1e-09
    for committee, _ in enumerate(range(committee_size)):
        tmp += instance_mat[committee, lab] * np.log((instance_mat[committee, lab] + 1e-09) / committee_consensus)"
nltk,https://github.com/nltk/nltk/tree/master/nltk/tokenize/punkt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tokenize/punkt.py,PunktBaseClass,"def _tokenize_words(self, plaintext):
    """"""
        Divide the given text into tokens, using the punkt word
        segmentation regular expression, and generate the resulting list
        of tokens augmented as three-tuples with two boolean values for whether
        the given token occurs at the start of a paragraph or a new line,
        respectively.
        """"""
    parastart = False
    for line in plaintext.split('\n'):
        if line.strip():
            line_toks = iter(self._lang_vars.word_tokenize(line))
            try:
                tok = next(line_toks)
            except StopIteration:
                continue
            yield self._Token(tok, parastart=parastart, linestart=True)
            parastart = False
            for tok in line_toks:
                yield self._Token(tok)
        else:
            parastart = True","for line in plaintext.split('\n'):
    if line.strip():
        line_toks = iter(self._lang_vars.word_tokenize(line))
        try:
            tok = next(line_toks)
        except StopIteration:
            continue
        yield self._Token(tok, parastart=parastart, linestart=True)
        parastart = False
        for tok in line_toks:
            yield self._Token(tok)
    else:
        parastart = True","for i,line in enumerate(plaintext.split('\n')):
    if line.strip():
        line_toks = iter(self._lang_vars.word_tokenize(line))
        try:
            tok = next(line_toks)
        except StopIteration:
            continue
        yield self._Token(tok, parastart=parastart, linestart=True)
        parastart = False
        for tok in line_toks:
            yield self._Token(tok)
    else:
        parastart = True"
nltk,https://github.com/nltk/nltk/tree/master/nltk/tokenize/punkt.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/tokenize/punkt.py,PunktBaseClass,"def _tokenize_words(self, plaintext):
    """"""
        Divide the given text into tokens, using the punkt word
        segmentation regular expression, and generate the resulting list
        of tokens augmented as three-tuples with two boolean values for whether
        the given token occurs at the start of a paragraph or a new line,
        respectively.
        """"""
    parastart = False
    for line in plaintext.split('\n'):
        if line.strip():
            line_toks = iter(self._lang_vars.word_tokenize(line))
            try:
                tok = next(line_toks)
            except StopIteration:
                continue
            yield self._Token(tok, parastart=parastart, linestart=True)
            parastart = False
            for tok in line_toks:
                yield self._Token(tok)
        else:
            parastart = True","for tok in line_toks:
    yield self._Token(tok)","for i,tok in enumerate(line_toks):
    yield self._Token(tok)"
Kats,https://github.com/facebookresearch/Kats/tree/master/kats/tests/test_consts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kats/kats/tests/test_consts.py,TimeSeriesDataOpsTest,"def test_get_item(self) -> None:
    self.assertEqual(self.ts_date_transform_concat_univ[:len(self.ts_univ_1)], self.ts_univ_1)
    self.assertEqual(self.ts_date_transform_concat_multi[:len(self.ts_multi_1)], self.ts_multi_1)
    for col in self.ts_date_transform_concat_multi.value.columns:
        ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
        self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)
    self.assertEqual(self.ts_date_transform_concat_multi[MULTIVAR_VALUE_DF_COLS], self.ts_date_transform_concat_multi)
    self.assertEqual(self.ts_univ_1[:], self.ts_univ_1)
    self.assertEqual(self.ts_univ_1[0:0], TimeSeriesData(time=pd.Series(name=TIME_COL_NAME), value=pd.Series(name=VALUE_COL_NAME), time_col_name=TIME_COL_NAME))","for col in self.ts_date_transform_concat_multi.value.columns:
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)","for i, col in enumerate(self.ts_date_transform_concat_multi.value.columns):
    ts_univ = TimeSeriesData(time=self.ts_date_transform_concat_multi.time, value=self.ts_date_transform_concat_multi.value[col], time_col_name=self.ts_date_transform_concat_multi.time_col_name)
    self.assertEqual(self.ts_date_transform_concat_multi[col], ts_univ)"
SimSiam,https://github.com/PatrickHua/SimSiam/tree/master/augmentations/gaussian_blur.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SimSiam/augmentations/gaussian_blur.py,,"def gaussian_blur(img: Tensor, kernel_size: List[int], sigma: Optional[List[float]]=None) -> Tensor:
    """"""Performs Gaussian blurring on the img by given kernel.
    The image can be a PIL Image or a Tensor, in which case it is expected
    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions

    Args:
        img (PIL Image or Tensor): Image to be blurred
        kernel_size (sequence of ints or int): Gaussian kernel size. Can be a sequence of integers
            like ``(kx, ky)`` or a single integer for square kernels.
            In torchscript mode kernel_size as single int is not supported, use a tuple or
            list of length 1: ``[ksize, ]``.
        sigma (sequence of floats or float, optional): Gaussian kernel standard deviation. Can be a
            sequence of floats like ``(sigma_x, sigma_y)`` or a single float to define the
            same sigma in both X/Y directions. If None, then it is computed using
            ``kernel_size`` as ``sigma = 0.3 * ((kernel_size - 1) * 0.5 - 1) + 0.8``.
            Default, None. In torchscript mode sigma as single float is
            not supported, use a tuple or list of length 1: ``[sigma, ]``.

    Returns:
        PIL Image or Tensor: Gaussian Blurred version of the image.
    """"""
    if not isinstance(kernel_size, (int, list, tuple)):
        raise TypeError('kernel_size should be int or a sequence of integers. Got {}'.format(type(kernel_size)))
    if isinstance(kernel_size, int):
        kernel_size = [kernel_size, kernel_size]
    if len(kernel_size) != 2:
        raise ValueError('If kernel_size is a sequence its length should be 2. Got {}'.format(len(kernel_size)))
    for ksize in kernel_size:
        if ksize % 2 == 0 or ksize < 0:
            raise ValueError('kernel_size should have odd and positive integers. Got {}'.format(kernel_size))
    if sigma is None:
        sigma = [ksize * 0.15 + 0.35 for ksize in kernel_size]
    if sigma is not None and (not isinstance(sigma, (int, float, list, tuple))):
        raise TypeError('sigma should be either float or sequence of floats. Got {}'.format(type(sigma)))
    if isinstance(sigma, (int, float)):
        sigma = [float(sigma), float(sigma)]
    if isinstance(sigma, (list, tuple)) and len(sigma) == 1:
        sigma = [sigma[0], sigma[0]]
    if len(sigma) != 2:
        raise ValueError('If sigma is a sequence, its length should be 2. Got {}'.format(len(sigma)))
    for s in sigma:
        if s <= 0.0:
            raise ValueError('sigma should have positive values. Got {}'.format(sigma))
    t_img = img
    if not isinstance(img, torch.Tensor):
        if not _is_pil_image(img):
            raise TypeError('img should be PIL Image or Tensor. Got {}'.format(type(img)))
        t_img = to_tensor(img)
    output = _gaussian_blur(t_img, kernel_size, sigma)
    if not isinstance(img, torch.Tensor):
        output = to_pil_image(output)
    return output","for ksize in kernel_size:
    if ksize % 2 == 0 or ksize < 0:
        raise ValueError('kernel_size should have odd and positive integers. Got {}'.format(kernel_size))","for i,ksize in enumerate(kernel_size):
    if ksize % 2 == 0 or ksize < 0:
        raise ValueError('kernel_size should have odd and positive integers. Got {}'.format(kernel_size))"
SimSiam,https://github.com/PatrickHua/SimSiam/tree/master/augmentations/gaussian_blur.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SimSiam/augmentations/gaussian_blur.py,,"def gaussian_blur(img: Tensor, kernel_size: List[int], sigma: Optional[List[float]]=None) -> Tensor:
    """"""Performs Gaussian blurring on the img by given kernel.
    The image can be a PIL Image or a Tensor, in which case it is expected
    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions

    Args:
        img (PIL Image or Tensor): Image to be blurred
        kernel_size (sequence of ints or int): Gaussian kernel size. Can be a sequence of integers
            like ``(kx, ky)`` or a single integer for square kernels.
            In torchscript mode kernel_size as single int is not supported, use a tuple or
            list of length 1: ``[ksize, ]``.
        sigma (sequence of floats or float, optional): Gaussian kernel standard deviation. Can be a
            sequence of floats like ``(sigma_x, sigma_y)`` or a single float to define the
            same sigma in both X/Y directions. If None, then it is computed using
            ``kernel_size`` as ``sigma = 0.3 * ((kernel_size - 1) * 0.5 - 1) + 0.8``.
            Default, None. In torchscript mode sigma as single float is
            not supported, use a tuple or list of length 1: ``[sigma, ]``.

    Returns:
        PIL Image or Tensor: Gaussian Blurred version of the image.
    """"""
    if not isinstance(kernel_size, (int, list, tuple)):
        raise TypeError('kernel_size should be int or a sequence of integers. Got {}'.format(type(kernel_size)))
    if isinstance(kernel_size, int):
        kernel_size = [kernel_size, kernel_size]
    if len(kernel_size) != 2:
        raise ValueError('If kernel_size is a sequence its length should be 2. Got {}'.format(len(kernel_size)))
    for ksize in kernel_size:
        if ksize % 2 == 0 or ksize < 0:
            raise ValueError('kernel_size should have odd and positive integers. Got {}'.format(kernel_size))
    if sigma is None:
        sigma = [ksize * 0.15 + 0.35 for ksize in kernel_size]
    if sigma is not None and (not isinstance(sigma, (int, float, list, tuple))):
        raise TypeError('sigma should be either float or sequence of floats. Got {}'.format(type(sigma)))
    if isinstance(sigma, (int, float)):
        sigma = [float(sigma), float(sigma)]
    if isinstance(sigma, (list, tuple)) and len(sigma) == 1:
        sigma = [sigma[0], sigma[0]]
    if len(sigma) != 2:
        raise ValueError('If sigma is a sequence, its length should be 2. Got {}'.format(len(sigma)))
    for s in sigma:
        if s <= 0.0:
            raise ValueError('sigma should have positive values. Got {}'.format(sigma))
    t_img = img
    if not isinstance(img, torch.Tensor):
        if not _is_pil_image(img):
            raise TypeError('img should be PIL Image or Tensor. Got {}'.format(type(img)))
        t_img = to_tensor(img)
    output = _gaussian_blur(t_img, kernel_size, sigma)
    if not isinstance(img, torch.Tensor):
        output = to_pil_image(output)
    return output","for s in sigma:
    if s <= 0.0:
        raise ValueError('sigma should have positive values. Got {}'.format(sigma))","for i,s in enumerate(sigma):
    if s <= 0.0:
        raise ValueError('sigma should have positive values. Got {} at index {}'.format(sigma, i))"
pyhanlp,https://github.com/hankcs/pyhanlp/tree/master/tests/book/ch10/demo_clustering_f.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyhanlp/tests/book/ch10/demo_clustering_f.py,,"if __name__ == '__main__':
    for algorithm in ('kmeans', 'repeated bisection'):
        print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","for algorithm in ('kmeans', 'repeated bisection'):
    print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","for i, algorithm in enumerate(('kmeans', 'repeated bisection')):
    print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))"
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","for i in range(batch_size):
    width_begin = map_width * i
    width_end = map_width * (i + 1)
    image = dataset.reverse_transform(batch_images[i])
    grid_image[:, width_begin:width_end, :] = image
    if 'semantic' in target_keys:
        gt_sem = batch_targets['semantic'][i].cpu().numpy()
        gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
        grid_target[:map_height, width_begin:width_end, :] = gt_sem
    if 'center' in target_keys:
        gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
        gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        gt_ctr = gt_ctr.clip(0, 255)
        grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
    if 'offset' in target_keys:
        gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
        gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
        grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
    if 'semantic_weights' in target_keys:
        gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
    if 'center_weights' in target_keys:
        gt_ign = batch_targets['center_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
    if 'offset_weights' in target_keys:
        gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
        gt_ign = gt_ign[:, :, None] * 255
        gt_ign = np.tile(gt_ign, (1, 1, 3))
        grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
    if 'foreground' in target_keys:
        gt_fg = batch_targets['foreground'][i].cpu().numpy()
        gt_fg = gt_fg[:, :, None] * 255
        grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
    if 'semantic' in output_keys:
        pred_sem = semantic_pred[i].cpu().numpy()
        pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
        grid_output[:map_height, width_begin:width_end, :] = pred_sem
    if 'center' in output_keys:
        pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
        pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
        pred_ctr = pred_ctr.clip(0, 255)
        grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
    if 'offset' in output_keys:
        pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
        pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
        grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
    if 'foreground' in output_keys:
        if foreground_pred is not None:
            pred_fg = foreground_pred[i].cpu().numpy()
            pred_fg = pred_fg[:, :, None] * 255
            grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg","for i in range(batch_size):
    width_begin = map_width * i
    width_end = map_width * (i + 1)
    image = dataset.reverse_transform(batch_images[i])
    grid_image[:, width_begin:width_end, :] = image
    for j, target_key in enumerate(target_keys):
        if target_key == 'semantic':
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        elif target_key == 'center':
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        elif target_key == 'offset':
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        elif target_key == 'semantic_weights':
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        elif target_key == 'center_weights':
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        elif target_key == 'offset_weights':
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        elif target_key == 'foreground':
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
    for k, output_key in enumerate(output_keys):
        if output_key == 'semantic':
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        elif output_key == 'center':
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        elif output_key == 'offset':
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        elif output_key == 'foreground':
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg"
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]","for (s, p) in zip(scores, points):
    if s > heap_items[0][0] and p not in in_heap:
        pop = heapq.heapreplace(heap_items, (s, p))
        in_heap.remove(pop[1])
        in_heap.add(p)","for i, (s, p) in enumerate(zip(scores, points)):
    if s > heap_items[0][0] and p not in in_heap:
        pop = heapq.heapreplace(heap_items, (s, p))
        in_heap.remove(pop[1])
        in_heap.add(p)"
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]","for (s, p) in zip(new_scores, new_points):
    if s > heap_items[0][0] and p not in in_heap:
        pop = heapq.heapreplace(heap_items, (s, p))
        in_heap.remove(pop[1])
        in_heap.add(p)
        k_last_modify = k","for i, (s, p) in enumerate(zip(new_scores, new_points)):
    if s > heap_items[0][0] and p not in in_heap:
        pop = heapq.heapreplace(heap_items, (s, p))
        in_heap.remove(pop[1])
        in_heap.add(p)
        k_last_modify = k"
checkmk,https://github.com/tribe29/checkmk/tree/master/omd/packages/omd/omdlib/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/omd/packages/omd/omdlib/main.py,,"def clear_environment() -> None:
    keep = ['TERM']
    for key in os.environ:
        if key not in keep:
            del os.environ[key]","for key in os.environ:
    if key not in keep:
        del os.environ[key]","for i,key in enumerate(os.environ):
    if key not in keep:
        del os.environ[key]"
python-iptables,https://github.com/ldx/python-iptables/tree/master/tests/test_targets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-iptables/tests/test_targets.py,TestXTNotrackTarget,"def tearDown(self):
    for r in self.chain.rules:
        self.chain.delete_rule(r)
    self.chain.flush()
    self.chain.delete()","for r in self.chain.rules:
    self.chain.delete_rule(r)","for i,r in enumerate(self.chain.rules):
    self.chain.delete_rule(r)"
cubes,https://github.com/DataBrewery/cubes/tree/master/cubes/query/cells.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cubes/cubes/query/cells.py,,"def cuts_from_string(cube, string, member_converters=None, role_member_converters=None):
    """"""Return list of cuts specified in `string`. You can use this function to
    parse cuts encoded in a URL.

    Arguments:

    * `string`  string containing the cut descritption (see below)
    * `cube`  cube for which the cuts are being created
    * `member_converters`  callables converting single-item values into paths.
      Keys are dimension names.
    * `role_member_converters`  callables converting single-item values into
      paths. Keys are dimension role names (`Dimension.role`).

    Examples::

        date:2004
        date:2004,1
        date:2004,1|class=5
        date:2004,1,1|category:5,10,12|class:5

    Ranges are in form ``from-to`` with possibility of open range::

        date:2004-2010
        date:2004,5-2010,3
        date:2004,5-2010
        date:2004,5-
        date:-2010

    Sets are in form ``path1;path2;path3`` (none of the paths should be
    empty)::

        date:2004;2010
        date:2004;2005,1;2010,10

    Grammar::

        <list> ::= <cut> | <cut> '|' <list>
        <cut> ::= <dimension> ':' <path>
        <dimension> ::= <identifier>
        <path> ::= <value> | <value> ',' <path>

    The characters '|', ':' and ',' are configured in `CUT_STRING_SEPARATOR`,
    `DIMENSION_STRING_SEPARATOR`, `PATH_STRING_SEPARATOR` respectively.
    """"""
    if not string:
        return []
    cuts = []
    dim_cuts = CUT_STRING_SEPARATOR.split(string)
    for dim_cut in dim_cuts:
        cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
        cuts.append(cut)
    return cuts","for dim_cut in dim_cuts:
    cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
    cuts.append(cut)","for i,dim_cut in enumerate(dim_cuts):
    cut = cut_from_string(dim_cut, cube, member_converters, role_member_converters)
    cuts.append(cut)"
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/host.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/host.py,host,"def get_managed_hosts(self, dn):
    host_filter = 'managedBy=%s' % dn
    host_attrs = ['fqdn']
    ldap = self.api.Backend.ldap2
    managed_hosts = []
    try:
        (hosts, _truncated) = ldap.find_entries(base_dn=DN(self.container_dn, api.env.basedn), filter=host_filter, attrs_list=host_attrs)
        for host in hosts:
            managed_hosts.append(host.dn)
    except errors.NotFound:
        return []
    return managed_hosts","for host in hosts:
    managed_hosts.append(host.dn)","for i, host in enumerate(hosts):
    managed_hosts.append(host.dn)"
Project_CodeNet,https://github.com/IBM/Project_CodeNet/tree/master/model-experiments/masked-language-model/infer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Project_CodeNet/model-experiments/masked-language-model/infer.py,,"def encode(text):
    R = [0] * config.MAX_LEN
    text = tokenize(text)
    for i in range(len(text)):
        w = text[i]
        if w in token2id:
            R[i] = token2id[w]
        else:
            R[i] = 1
    return np.array(R)","for i in range(len(text)):
    w = text[i]
    if w in token2id:
        R[i] = token2id[w]
    else:
        R[i] = 1","for i, w in enumerate(text):
    if w in token2id:
        R[i] = token2id[w]
    else:
        R[i] = 1"
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)","for item in builder.widgets.items():
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)","for i, item in enumerate(builder.widgets.items()):
    (widget_name, widget) = item
    signal_ids = []
    try:
        widget_type = type(widget)
        while widget_type:
            signal_ids.extend(GObject.signal_list_ids(widget_type))
            widget_type = GObject.type_parent(widget_type)
    except RuntimeError:
        pass
    signal_names = [GObject.signal_name(sid) for sid in signal_ids]
    for sig in signal_names:
        sig = sig.replace('-', '_')
        handler_names = ['on_%s_%s' % (widget_name, sig)]
        if widget is callback_obj:
            handler_names.append('on_%s' % sig)
        do_connect((i, item), sig, handler_names, callback_handler_dict, builder.connections)"
Tuxemon,https://github.com/Tuxemon/Tuxemon/tree/master/tuxemon/cli/processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tuxemon/tuxemon/cli/processor.py,CommandProcessor,"def collect_commands(self, folder: str) -> Iterable[CLICommand]:
    """"""
        Use plugins to load CLICommand classes for commands.

        Parameters:
            folder: Folder to search.

        """"""
    pm = PluginManager()
    pm.setPluginPlaces([folder])
    pm.include_patterns = ['commands']
    pm.exclude_classes = ['CLICommand']
    pm.collectPlugins()
    for cmd_class in get_available_classes(pm, interface=CLICommand):
        if cmd_class.usable_from_root:
            yield cmd_class()","for cmd_class in get_available_classes(pm, interface=CLICommand):
    if cmd_class.usable_from_root:
        yield cmd_class()","for i, cmd_class in enumerate(get_available_classes(pm, interface=CLICommand)):
    if cmd_class.usable_from_root:
        yield cmd_class()"
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    runner = self._get_fextractor_instance(fextractor_type)
    runner.run(parallelize=self.parallelize, processes=self.processes)
    results = runner.results
    self.type2results_dict[fextractor_type] = results","for i,fextractor_type in enumerate(self.feature_dict):
    runner = self._get_fextractor_instance(fextractor_type)
    runner.run(parallelize=self.parallelize, processes=self.processes)
    results = runner.results
    self.type2results_dict[fextractor_type] = results"
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for fextractor_type in self.feature_dict:
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]","for i,fextractor_type in enumerate(self.feature_dict):
    assert fextractor_type in self.type2results_dict
    for atom_feature in self._get_atom_features(fextractor_type):
        scores_key = self._get_scores_key(fextractor_type, atom_feature)
        for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
            try:
                result_dicts[result_index][scores_key] = result[scores_key]
            except KeyError:
                scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                result_dicts[result_index][scores_key] = result[scores_key_alt]"
vmaf,https://github.com/Netflix/vmaf/tree/master/python/vmaf/core/feature_assembler.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vmaf/python/vmaf/core/feature_assembler.py,FeatureAssembler,"def run(self):
    """"""
        Do all the calculation here.
        :return:
        """"""
    for fextractor_type in self.feature_dict:
        runner = self._get_fextractor_instance(fextractor_type)
        runner.run(parallelize=self.parallelize, processes=self.processes)
        results = runner.results
        self.type2results_dict[fextractor_type] = results
    result_dicts = list(map(lambda x: dict(), self.assets))
    for fextractor_type in self.feature_dict:
        assert fextractor_type in self.type2results_dict
        for atom_feature in self._get_atom_features(fextractor_type):
            scores_key = self._get_scores_key(fextractor_type, atom_feature)
            for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
                try:
                    result_dicts[result_index][scores_key] = result[scores_key]
                except KeyError:
                    scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
                    result_dicts[result_index][scores_key] = result[scores_key_alt]
    self.results = list(map(lambda tasset: BasicResult(tasset[0], tasset[1]), zip(self.assets, result_dicts)))","for atom_feature in self._get_atom_features(fextractor_type):
    scores_key = self._get_scores_key(fextractor_type, atom_feature)
    for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
        try:
            result_dicts[result_index][scores_key] = result[scores_key]
        except KeyError:
            scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
            result_dicts[result_index][scores_key] = result[scores_key_alt]","for (i, atom_feature) in enumerate(self._get_atom_features(fextractor_type)):
    scores_key = self._get_scores_key(fextractor_type, atom_feature)
    for (result_index, result) in enumerate(self.type2results_dict[fextractor_type]):
        try:
            result_dicts[result_index][scores_key] = result[scores_key]
        except KeyError:
            scores_key_alt = BasicResult.scores_key_wildcard_match(result.result_dict, scores_key)
            result_dicts[result_index][scores_key] = result[scores_key_alt]"
quodlibet,https://github.com/quodlibet/quodlibet/tree/master/tests/test_browsers__base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/tests/test_browsers__base.py,TBrowser,"def test_can_filter(self):
    for key in ['foo', 'title', 'fake~key', '~woobar', '~#huh']:
        self.failIf(self.browser.can_filter(key))","for key in ['foo', 'title', 'fake~key', '~woobar', '~#huh']:
    self.failIf(self.browser.can_filter(key))","for i,key in enumerate(['foo', 'title', 'fake~key', '~woobar', '~#huh']):
    self.failIf(self.browser.can_filter(key))"
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/loom/loom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/loom/loom.py,Loom,"def _setup_metadata(self):
    """"""Construct the serialized metadata about this loom for the scheduler.""""""
    loom_metadata = loom_pb2.LoomMetadata()
    loom_metadata.max_depth = self._max_depth
    for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
        type_shape_metadata = loom_metadata.type_shape_metadata.add()
        type_shape_metadata.dtype = ts.dtype_enum
        type_shape_metadata.shape.extend(ts.shape)
        type_shape_metadata.tag = ts.tag
        type_shape_metadata.name = str(ts)
        type_shape_metadata.tensor_names.extend(tensor_names)
        type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict
    for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
        op_metadata = loom_metadata.op_metadata.add()
        op_metadata.name = op_name
        op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
        op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))
    self._loom_metadata_str = loom_metadata.SerializeToString()","for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
    type_shape_metadata = loom_metadata.type_shape_metadata.add()
    type_shape_metadata.dtype = ts.dtype_enum
    type_shape_metadata.shape.extend(ts.shape)
    type_shape_metadata.tag = ts.tag
    type_shape_metadata.name = str(ts)
    type_shape_metadata.tensor_names.extend(tensor_names)
    type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict","for i, (ts, tensor_names) in enumerate(zip(self._type_shapes, self._ts_idx_to_tensor_names)):
    type_shape_metadata = loom_metadata.type_shape_metadata.add()
    type_shape_metadata.dtype = ts.dtype_enum
    type_shape_metadata.shape.extend(ts.shape)
    type_shape_metadata.tag = ts.tag
    type_shape_metadata.name = str(ts)
    type_shape_metadata.tensor_names.extend(tensor_names)
    type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict"
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/loom/loom.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/loom/loom.py,Loom,"def _setup_metadata(self):
    """"""Construct the serialized metadata about this loom for the scheduler.""""""
    loom_metadata = loom_pb2.LoomMetadata()
    loom_metadata.max_depth = self._max_depth
    for (ts, tensor_names) in zip(self._type_shapes, self._ts_idx_to_tensor_names):
        type_shape_metadata = loom_metadata.type_shape_metadata.add()
        type_shape_metadata.dtype = ts.dtype_enum
        type_shape_metadata.shape.extend(ts.shape)
        type_shape_metadata.tag = ts.tag
        type_shape_metadata.name = str(ts)
        type_shape_metadata.tensor_names.extend(tensor_names)
        type_shape_metadata.is_batch_input = ts in self._batch_inputs or self._direct_feed_dict
    for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
        op_metadata = loom_metadata.op_metadata.add()
        op_metadata.name = op_name
        op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
        op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))
    self._loom_metadata_str = loom_metadata.SerializeToString()","for (op_name, op) in zip(self._loom_op_names, self._loom_ops):
    op_metadata = loom_metadata.op_metadata.add()
    op_metadata.name = op_name
    op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
    op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))","for i, (op_name, op) in enumerate(zip(self._loom_op_names, self._loom_ops)):
    op_metadata = loom_metadata.op_metadata.add()
    op_metadata.name = op_name
    op_metadata.input_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.input_type_shapes))
    op_metadata.output_ts_idx.extend((self._type_shape_to_idx[ts] for ts in op.output_type_shapes))"
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial","for file in vbox:
    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f.readlines():
            res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
            if res:
                serial.append(f'127.0.0.1:{res.group(1)}')","for i,file in enumerate(vbox):
    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f.readlines():
            res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
            if res:
                serial.append(f'127.0.0.1:{res.group(1)}')"
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial","for file in files:
    if re.match(self.vbox_name, file):
        file = os.path.join(path, file)
        vbox.append(file)","for i,file in enumerate(files):
    if re.match(self.vbox_name, file):
        file = os.path.join(path, file)
        vbox.append(file)"
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial","for line in f.readlines():
    res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
    if res:
        serial.append(f'127.0.0.1:{res.group(1)}')","for i,line in enumerate(f.readlines()):
    res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
    if res:
        serial.append(f'127.0.0.1:{res.group(1)}')"
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ctci-solutions/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,,"def show(placement):
    parts = ['\n+-----------------+\n']
    for row in xrange(8):
        parts.append('| ')
        for col in xrange(8):
            bit = 1 << row * 8 + col
            if bit & placement:
                parts.append('Q ')
            else:
                parts.append('  ')
        parts.append('|\n')
    parts.append('+-----------------+\n')
    return ''.join(parts)","for row in xrange(8):
    parts.append('| ')
    for col in xrange(8):
        bit = 1 << row * 8 + col
        if bit & placement:
            parts.append('Q ')
        else:
            parts.append('  ')
    parts.append('|\n')","for row, _ in enumerate(xrange(8)):
    parts.append('| ')
    for col in xrange(8):
        bit = 1 << row * 8 + col
        if bit & placement:
            parts.append('Q ')
        else:
            parts.append('  ')
    parts.append('|\n')"
speechbrain,https://github.com/speechbrain/speechbrain/tree/master/recipes/KsponSpeech/ASR/transformer/train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/speechbrain/recipes/KsponSpeech/ASR/transformer/train.py,,"if __name__ == '__main__':
    (hparams_file, run_opts, overrides) = sb.parse_arguments(sys.argv[1:])
    with open(hparams_file) as fin:
        hparams = load_hyperpyyaml(fin, overrides)
    sb.utils.distributed.ddp_init_group(run_opts)
    from ksponspeech_prepare import prepare_ksponspeech
    sb.create_experiment_directory(experiment_directory=hparams['output_folder'], hyperparams_to_save=hparams_file, overrides=overrides)
    run_on_main(prepare_ksponspeech, kwargs={'data_folder': hparams['data_folder'], 'tr_splits': hparams['train_splits'], 'dev_splits': hparams['dev_splits'], 'te_splits': hparams['test_splits'], 'save_folder': hparams['data_folder'], 'merge_lst': hparams['train_splits'], 'merge_name': hparams['train_csv'], 'skip_prep': hparams['skip_prep']})
    (train_data, valid_data, test_datasets, tokenizer) = dataio_prepare(hparams)
    run_on_main(hparams['pretrainer'].collect_files)
    hparams['pretrainer'].load_collected(device=run_opts['device'])
    asr_brain = ASR(modules=hparams['modules'], opt_class=hparams['Adam'], hparams=hparams, run_opts=run_opts, checkpointer=hparams['checkpointer'])
    asr_brain.tokenizer = hparams['tokenizer']
    asr_brain.fit(asr_brain.hparams.epoch_counter, train_data, valid_data, train_loader_kwargs=hparams['train_dataloader_opts'], valid_loader_kwargs=hparams['valid_dataloader_opts'])
    for k in test_datasets.keys():
        asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
        asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])","for k in test_datasets.keys():
    asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
    asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])","for i,k in enumerate(test_datasets.keys()):
    asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
    asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])"
indico,https://github.com/indico/indico/tree/master/indico/vendor/django_mail/backends/smtp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/indico/indico/vendor/django_mail/backends/smtp.py,EmailBackend,"def send_messages(self, email_messages):
    """"""
        Send one or more EmailMessage objects and return the number of email
        messages sent.
        """"""
    if not email_messages:
        return 0
    with self._lock:
        new_conn_created = self.open()
        if not self.connection or new_conn_created is None:
            return 0
        num_sent = 0
        for message in email_messages:
            sent = self._send(message)
            if sent:
                num_sent += 1
        if new_conn_created:
            self.close()
    return num_sent","for message in email_messages:
    sent = self._send(message)
    if sent:
        num_sent += 1","for i, message in enumerate(email_messages):
    sent = self._send(message)
    if sent:
        num_sent += 1"
jittor,https://github.com/Jittor/jittor/tree/master/python/jittor/test/test_transform.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jittor/python/jittor/test/test_transform.py,Tester,"def test_1_channel_ndarray_to_pil_image(self):
    img_data_float = np.random.rand(4, 4, 1).astype(np.float32)
    img_data_byte = np.random.randint(0, 255, (4, 4, 1)).astype(np.uint8)
    img_data_short = np.random.randint(0, 32767, (4, 4, 1)).astype(np.int16)
    img_data_int = np.random.randint(0, 2147483647, (4, 4, 1)).astype(np.int32)
    inputs = [img_data_float, img_data_byte, img_data_short, img_data_int]
    expected_modes = ['F', 'L', 'I;16', 'I']
    for (img_data, mode) in zip(inputs, expected_modes):
        for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
            img = t(img_data)
            self.assertEqual(img.mode, mode)
            self.assertTrue(np.allclose(img_data[:, :, 0], img))","for (img_data, mode) in zip(inputs, expected_modes):
    for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
        img = t(img_data)
        self.assertEqual(img.mode, mode)
        self.assertTrue(np.allclose(img_data[:, :, 0], img))","for i, (img_data, mode) in enumerate(zip(inputs, expected_modes)):
    for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
        img = t(img_data)
        self.assertEqual(img.mode, mode)
        self.assertTrue(np.allclose(img_data[:, :, 0], img))"
jittor,https://github.com/Jittor/jittor/tree/master/python/jittor/test/test_transform.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jittor/python/jittor/test/test_transform.py,Tester,"def test_1_channel_ndarray_to_pil_image(self):
    img_data_float = np.random.rand(4, 4, 1).astype(np.float32)
    img_data_byte = np.random.randint(0, 255, (4, 4, 1)).astype(np.uint8)
    img_data_short = np.random.randint(0, 32767, (4, 4, 1)).astype(np.int16)
    img_data_int = np.random.randint(0, 2147483647, (4, 4, 1)).astype(np.int32)
    inputs = [img_data_float, img_data_byte, img_data_short, img_data_int]
    expected_modes = ['F', 'L', 'I;16', 'I']
    for (img_data, mode) in zip(inputs, expected_modes):
        for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
            img = t(img_data)
            self.assertEqual(img.mode, mode)
            self.assertTrue(np.allclose(img_data[:, :, 0], img))","for t in [transform.ToPILImage(), transform.ToPILImage(mode=mode)]:
    img = t(img_data)
    self.assertEqual(img.mode, mode)
    self.assertTrue(np.allclose(img_data[:, :, 0], img))","for i,t in enumerate([transform.ToPILImage(), transform.ToPILImage(mode=mode)]):
    img = t(img_data)
    self.assertEqual(img.mode, mode)
    self.assertTrue(np.allclose(img_data[:, :, 0], img))"
pyrsistent,https://github.com/tobgu/pyrsistent/tree/master/pyrsistent/_pbag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyrsistent/pyrsistent/_pbag.py,PBag,"def __iter__(self):
    """"""
        Return an iterator of all elements, including duplicates.

        >>> list(pbag([1, 1, 2]))
        [1, 1, 2]
        >>> list(pbag([1, 2]))
        [1, 2]
        """"""
    for (elt, count) in self._counts.iteritems():
        for i in range(count):
            yield elt","for (elt, count) in self._counts.iteritems():
    for i in range(count):
        yield elt","for (i, (elt, count)) in enumerate(self._counts.iteritems()):
    for j in range(count):
        yield elt"
SDV,https://github.com/sdv-dev/SDV/tree/master//tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV//tasks.py,,"def install_minimum(c):
    with open('setup.py', 'r') as setup_py:
        lines = setup_py.read().splitlines()
    versions = []
    started = False
    for line in lines:
        if started:
            if line == ']':
                started = False
                continue
            line = line.strip()
            if _validate_python_version(line):
                requirement = re.match('[^>]*', line).group(0)
                requirement = re.sub('[\'"",]', '', requirement)
                version = re.search('>=?[^(,|#)]*', line).group(0)
                if version:
                    version = re.sub('>=?', '==', version)
                    version = re.sub('[\'"",]', '', version)
                    requirement += version
                versions.append(requirement)
        elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
            started = True
    c.run(f""python -m pip install {' '.join(versions)}"")","for line in lines:
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True","for i,line in enumerate(lines):
    if started:
        if line == ']':
            started = False
            continue
        line = line.strip()
        if _validate_python_version(line):
            requirement = re.match('[^>]*', line).group(0)
            requirement = re.sub('[\'"",]', '', requirement)
            version = re.search('>=?[^(,|#)]*', line).group(0)
            if version:
                version = re.sub('>=?', '==', version)
                version = re.sub('[\'"",]', '', version)
                requirement += version
            versions.append(requirement)
    elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
        started = True"
pretrained-models.pytorch,https://github.com/Cadene/pretrained-models.pytorch/tree/master/pretrainedmodels/datasets/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretrained-models.pytorch/pretrainedmodels/datasets/utils.py,AveragePrecisionMeter,"def value(self):
    """"""Returns the model's average precision for each class
        Return:
            ap (FloatTensor): 1xK tensor, with avg precision for each class k
        """"""
    if self.scores.numel() == 0:
        return 0
    ap = torch.zeros(self.scores.size(1))
    rg = torch.arange(1, self.scores.size(0)).float()
    for k in range(self.scores.size(1)):
        scores = self.scores[:, k]
        targets = self.targets[:, k]
        ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)
    return ap","for k in range(self.scores.size(1)):
    scores = self.scores[:, k]
    targets = self.targets[:, k]
    ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)","for k, _ in enumerate(range(self.scores.size(1))):
    scores = self.scores[:, k]
    targets = self.targets[:, k]
    ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)"
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/map_detection/grid_predictor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/map_detection/grid_predictor.py,GridPredictor,"def predict_enemy_genre(self):
    image_dic = {}
    scaling_dic = self.config.MAP_ENEMY_GENRE_DETECTION_SCALING
    for (name, template) in self.template_enemy_genre.items():
        if template is None:
            logger.warning(f'Enemy detection template not found: {name}')
            logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
            raise ScriptError(f'Enemy detection template not found: {name}')
        short_name = name[6:] if name.startswith('Siren_') else name
        scaling = scaling_dic.get(short_name, 1)
        scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
        for scale in scaling:
            if scale not in image_dic:
                shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
                image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
            if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
                return name
    return None","for scale in scaling:
    if scale not in image_dic:
        shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
        image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
    if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
        return name","for i,scale in enumerate(scaling):
    if scale not in image_dic:
        shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
        image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
    if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
        return name"
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/outputs/base_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/outputs/base_output.py,AbstractOutput,"def check_triggers(self, output_id, amount=None, output_channel=0):
    """"""
        This function is executed whenever an output is turned on or off
        It is responsible for executing Output Triggers
        """"""
    output_channel_dev = db_retrieve_table_daemon(OutputChannel).filter(and_(OutputChannel.output_id == output_id, OutputChannel.channel == output_channel)).first()
    if output_channel_dev is None:
        self.logger.error('Could not find channel in database')
        return
    trigger_output = db_retrieve_table_daemon(Trigger)
    trigger_output = trigger_output.filter(Trigger.trigger_type == 'trigger_output')
    trigger_output = trigger_output.filter(Trigger.unique_id_1 == output_id)
    trigger_output = trigger_output.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output = trigger_output.filter(Trigger.is_activated.is_(True))
    if self.is_on(output_channel):
        trigger_output = trigger_output.filter(or_(Trigger.output_state == 'on_duration_none', Trigger.output_state == 'on_duration_any', Trigger.output_state == 'on_duration_none_any', Trigger.output_state == 'on_duration_equal', Trigger.output_state == 'on_duration_greater_than', Trigger.output_state == 'on_duration_equal_greater_than', Trigger.output_state == 'on_duration_less_than', Trigger.output_state == 'on_duration_equal_less_than'))
        on_duration_none = and_(Trigger.output_state == 'on_duration_none', amount == 0.0)
        on_duration_any = and_(Trigger.output_state == 'on_duration_any', bool(amount))
        on_duration_none_any = Trigger.output_state == 'on_duration_none_any'
        on_duration_equal = and_(Trigger.output_state == 'on_duration_equal', Trigger.output_duration == amount)
        on_duration_greater_than = and_(Trigger.output_state == 'on_duration_greater_than', amount > Trigger.output_duration)
        on_duration_equal_greater_than = and_(Trigger.output_state == 'on_duration_equal_greater_than', amount >= Trigger.output_duration)
        on_duration_less_than = and_(Trigger.output_state == 'on_duration_less_than', amount < Trigger.output_duration)
        on_duration_equal_less_than = and_(Trigger.output_state == 'on_duration_equal_less_than', amount <= Trigger.output_duration)
        trigger_output = trigger_output.filter(or_(on_duration_none, on_duration_any, on_duration_none_any, on_duration_equal, on_duration_greater_than, on_duration_equal_greater_than, on_duration_less_than, on_duration_equal_less_than))
    else:
        trigger_output = trigger_output.filter(Trigger.output_state == 'off')
    for each_trigger in trigger_output.all():
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)
    trigger_output_pwm = db_retrieve_table_daemon(Trigger)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.trigger_type == 'trigger_output_pwm')
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_1 == output_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.is_activated.is_(True))
    for each_trigger in trigger_output_pwm.all():
        trigger_trigger = False
        duty_cycle = self.output_state(output_channel)
        if duty_cycle == 'off':
            if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
                trigger_trigger = True
        elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
            trigger_trigger = True
        if not trigger_trigger:
            continue
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for each_trigger in trigger_output.all():
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for i, each_trigger in enumerate(trigger_output.all()):
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)"
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/outputs/base_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/outputs/base_output.py,AbstractOutput,"def check_triggers(self, output_id, amount=None, output_channel=0):
    """"""
        This function is executed whenever an output is turned on or off
        It is responsible for executing Output Triggers
        """"""
    output_channel_dev = db_retrieve_table_daemon(OutputChannel).filter(and_(OutputChannel.output_id == output_id, OutputChannel.channel == output_channel)).first()
    if output_channel_dev is None:
        self.logger.error('Could not find channel in database')
        return
    trigger_output = db_retrieve_table_daemon(Trigger)
    trigger_output = trigger_output.filter(Trigger.trigger_type == 'trigger_output')
    trigger_output = trigger_output.filter(Trigger.unique_id_1 == output_id)
    trigger_output = trigger_output.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output = trigger_output.filter(Trigger.is_activated.is_(True))
    if self.is_on(output_channel):
        trigger_output = trigger_output.filter(or_(Trigger.output_state == 'on_duration_none', Trigger.output_state == 'on_duration_any', Trigger.output_state == 'on_duration_none_any', Trigger.output_state == 'on_duration_equal', Trigger.output_state == 'on_duration_greater_than', Trigger.output_state == 'on_duration_equal_greater_than', Trigger.output_state == 'on_duration_less_than', Trigger.output_state == 'on_duration_equal_less_than'))
        on_duration_none = and_(Trigger.output_state == 'on_duration_none', amount == 0.0)
        on_duration_any = and_(Trigger.output_state == 'on_duration_any', bool(amount))
        on_duration_none_any = Trigger.output_state == 'on_duration_none_any'
        on_duration_equal = and_(Trigger.output_state == 'on_duration_equal', Trigger.output_duration == amount)
        on_duration_greater_than = and_(Trigger.output_state == 'on_duration_greater_than', amount > Trigger.output_duration)
        on_duration_equal_greater_than = and_(Trigger.output_state == 'on_duration_equal_greater_than', amount >= Trigger.output_duration)
        on_duration_less_than = and_(Trigger.output_state == 'on_duration_less_than', amount < Trigger.output_duration)
        on_duration_equal_less_than = and_(Trigger.output_state == 'on_duration_equal_less_than', amount <= Trigger.output_duration)
        trigger_output = trigger_output.filter(or_(on_duration_none, on_duration_any, on_duration_none_any, on_duration_equal, on_duration_greater_than, on_duration_equal_greater_than, on_duration_less_than, on_duration_equal_less_than))
    else:
        trigger_output = trigger_output.filter(Trigger.output_state == 'off')
    for each_trigger in trigger_output.all():
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)
    trigger_output_pwm = db_retrieve_table_daemon(Trigger)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.trigger_type == 'trigger_output_pwm')
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_1 == output_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.is_activated.is_(True))
    for each_trigger in trigger_output_pwm.all():
        trigger_trigger = False
        duty_cycle = self.output_state(output_channel)
        if duty_cycle == 'off':
            if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
                trigger_trigger = True
        elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
            trigger_trigger = True
        if not trigger_trigger:
            continue
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for each_trigger in trigger_output_pwm.all():
    trigger_trigger = False
    duty_cycle = self.output_state(output_channel)
    if duty_cycle == 'off':
        if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
            trigger_trigger = True
    elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
        trigger_trigger = True
    if not trigger_trigger:
        continue
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)","for i, each_trigger in enumerate(trigger_output_pwm.all()):
    trigger_trigger = False
    duty_cycle = self.output_state(output_channel)
    if duty_cycle == 'off':
        if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
            trigger_trigger = True
    elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
        trigger_trigger = True
    if not trigger_trigger:
        continue
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
    self.control.trigger_all_actions(each_trigger.unique_id, message=message)"
curator,https://github.com/elastic/curator/tree/master/curator/indexlist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/curator/curator/indexlist.py,IndexList,"def filter_by_size(self, size_threshold=None, threshold_behavior='greater_than', exclude=False, size_behavior='primary'):
    """"""
        Remove indices from the actionable list based on index size.

        `threshold_behavior`, when set to `greater_than` (default), includes if it the index
        tests to be larger than `size_threshold`. When set to `less_than`, it includes if
        the index is smaller than `size_threshold`

        :arg size_threshold: Filter indices over *n* gigabytes
        :arg threshold_behavior: Size to filter, either ``greater_than`` or ``less_than``. Defaults
            to ``greater_than`` to preserve backwards compatability.
        :arg size_behavior: Size that used to filter, either ``primary`` or ``total``. Defaults to ``primary``
        :arg exclude: If `exclude` is `True`, this filter will remove matching
            indices from `indices`. If `exclude` is `False`, then only matching
            indices will be kept in `indices`.
            Default is `False`
        """"""
    self.loggit.debug('Filtering indices by index size')
    if not size_threshold:
        raise exceptions.MissingArgument('No value for ""size_threshold"" provided')
    if size_behavior not in ['primary', 'total']:
        raise ValueError('Invalid value for ""size_behavior"": {0}'.format(size_behavior))
    if threshold_behavior not in ['greater_than', 'less_than']:
        raise ValueError('Invalid value for ""threshold_behavior"": {0}'.format(threshold_behavior))
    index_size_limit = float(size_threshold) * 2 ** 30
    self.loggit.debug('Cannot get disk usage info from closed indices.  Omitting any closed indices.')
    self.filter_closed()
    working_list = self.working_list()
    for index in working_list:
        if size_behavior == 'primary':
            index_size = self.index_info[index]['primary_size_in_bytes']
        else:
            index_size = self.index_info[index]['size_in_bytes']
        msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
        if threshold_behavior == 'greater_than':
            self.__excludify(index_size > index_size_limit, exclude, index, msg)
        elif threshold_behavior == 'less_than':
            self.__excludify(index_size < index_size_limit, exclude, index, msg)","for index in working_list:
    if size_behavior == 'primary':
        index_size = self.index_info[index]['primary_size_in_bytes']
    else:
        index_size = self.index_info[index]['size_in_bytes']
    msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
    if threshold_behavior == 'greater_than':
        self.__excludify(index_size > index_size_limit, exclude, index, msg)
    elif threshold_behavior == 'less_than':
        self.__excludify(index_size < index_size_limit, exclude, index, msg)","for i,index in enumerate(working_list):
    if size_behavior == 'primary':
        index_size = self.index_info[index]['primary_size_in_bytes']
    else:
        index_size = self.index_info[index]['size_in_bytes']
    msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
    if threshold_behavior == 'greater_than':
        self.__excludify(index_size > index_size_limit, exclude, index, msg)
    elif threshold_behavior == 'less_than':
        self.__excludify(index_size < index_size_limit, exclude, index, msg)"
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for item in arguments:
    if item.startswith('choice'):
        if arguments[item][0] != '':
            uuidsplit = item.split('uuid-')
            if len(uuidsplit) > 1:
                choiceitems[uuidsplit[1]] = arguments[item][0]
            else:
                for flagoption in arguments[item]:
                    if len(flagoption) > 0:
                        FlagChoice.create_choice(flag, decode(flagoption))","for i,item in enumerate(arguments):
    if item.startswith('choice'):
        if arguments[item][0] != '':
            uuidsplit = item.split('uuid-')
            if len(uuidsplit) > 1:
                choiceitems[uuidsplit[1]] = arguments[item][0]
            else:
                for flagoption in arguments[item]:
                    if len(flagoption) > 0:
                        FlagChoice.create_choice(flag, decode(flagoption))"
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for choice in currentchoices:
    if not choice['uuid'] in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice['uuid'])
        self.dbsession.delete(flagchoice)","for i, choice in enumerate(currentchoices):
    if not choice['uuid'] in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice['uuid'])
        self.dbsession.delete(flagchoice)"
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for choice in choiceitems:
    flagchoice = FlagChoice.by_uuid(choice)
    if choiceitems[choice] != flagchoice.choice:
        flagchoice.choice = decode(choiceitems[choice])
        self.dbsession.add(flagchoice)","for i, choice in enumerate(choiceitems):
    flagchoice = FlagChoice.by_uuid(choice)
    if choiceitems[choice] != flagchoice.choice:
        flagchoice.choice = decode(choiceitems[choice])
        self.dbsession.add(flagchoice)"
RootTheBox,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminEditHandler,"def edit_choices(self, flag, arguments):
    """"""Edit flag multiple choice items""""""
    choiceitems = {}
    currentchoices = json.loads(flag.choices())
    for item in arguments:
        if item.startswith('choice'):
            if arguments[item][0] != '':
                uuidsplit = item.split('uuid-')
                if len(uuidsplit) > 1:
                    choiceitems[uuidsplit[1]] = arguments[item][0]
                else:
                    for flagoption in arguments[item]:
                        if len(flagoption) > 0:
                            FlagChoice.create_choice(flag, decode(flagoption))
    for choice in currentchoices:
        if not choice['uuid'] in choiceitems:
            flagchoice = FlagChoice.by_uuid(choice['uuid'])
            self.dbsession.delete(flagchoice)
    for choice in choiceitems:
        flagchoice = FlagChoice.by_uuid(choice)
        if choiceitems[choice] != flagchoice.choice:
            flagchoice.choice = decode(choiceitems[choice])
            self.dbsession.add(flagchoice)
    self.dbsession.commit()","for flagoption in arguments[item]:
    if len(flagoption) > 0:
        FlagChoice.create_choice(flag, decode(flagoption))","for i, flagoption in enumerate(arguments[item]):
    if len(flagoption) > 0:
        FlagChoice.create_choice(flag, decode(flagoption))"
django-rest-framework,https://github.com/encode/django-rest-framework/tree/master/rest_framework/viewsets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rest-framework/rest_framework/viewsets.py,ViewSetMixin,"def get_extra_action_url_map(self):
    """"""
        Build a map of {names: urls} for the extra actions.

        This method will noop if `detail` was not provided as a view initkwarg.
        """"""
    action_urls = OrderedDict()
    if self.detail is None:
        return action_urls
    actions = [action for action in self.get_extra_actions() if action.detail == self.detail]
    for action in actions:
        try:
            url_name = '%s-%s' % (self.basename, action.url_name)
            url = reverse(url_name, self.args, self.kwargs, request=self.request)
            view = self.__class__(**action.kwargs)
            action_urls[view.get_view_name()] = url
        except NoReverseMatch:
            pass
    return action_urls","for action in actions:
    try:
        url_name = '%s-%s' % (self.basename, action.url_name)
        url = reverse(url_name, self.args, self.kwargs, request=self.request)
        view = self.__class__(**action.kwargs)
        action_urls[view.get_view_name()] = url
    except NoReverseMatch:
        pass","for i, action in enumerate(actions):
    try:
        url_name = '%s-%s' % (self.basename, action.url_name)
        url = reverse(url_name, self.args, self.kwargs, request=self.request)
        view = self.__class__(**action.kwargs)
        action_urls[view.get_view_name()] = url
    except NoReverseMatch:
        pass"
grover,https://github.com/rowanz/grover/tree/master/realnews/prepare_lm_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grover/realnews/prepare_lm_data.py,,"def _stream_from_buffer(buffer, current_desired_size, pad_token=0, add_articles_to_end=False):
    """""" Combines short articles that are in a buffer """"""
    random.shuffle(buffer)
    i = 0
    while i < len(buffer):
        article = buffer[i]
        if add_articles_to_end:
            for article2add in buffer[i + 1:]:
                i += 1
                article['input_ids'].append(encoder.padding)
                article['input_ids'].append(encoder.reset_context)
                article['input_ids'].extend(article2add['input_ids'])
                if len(article['input_ids']) >= current_desired_size:
                    article['input_ids'] = article['input_ids'][:current_desired_size]
                    break
        amount_to_pad = current_desired_size - len(article['input_ids'])
        article['input_ids'].extend([pad_token] * amount_to_pad)
        article['sub_index'] = 0
        yield article
        i += 1","for article2add in buffer[i + 1:]:
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break","for j, article2add in enumerate(buffer[i + 1:]):
    i += 1
    article['input_ids'].append(encoder.padding)
    article['input_ids'].append(encoder.reset_context)
    article['input_ids'].extend(article2add['input_ids'])
    if len(article['input_ids']) >= current_desired_size:
        article['input_ids'] = article['input_ids'][:current_desired_size]
        break"
xalpha,https://github.com/refraction-ray/xalpha/tree/master/xalpha/indicator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xalpha/xalpha/indicator.py,indicator,"def max_drawdown(self, date=yesterdayobj()):
    """"""
        

        :param date: date obj or string
        :returns: three elements tuple, the first two are the date obj of
            start and end of the time window, the third one is the drawdown amplitude in unit 1.
        """"""
    li = [(row['date'], row['netvalue']) for (i, row) in self.price[self.price['date'] <= date].iterrows()]
    res = []
    for (i, _) in enumerate(li):
        for j in range(i + 1, len(li)):
            res.append((li[i][0], li[j][0], (li[j][1] - li[i][1]) / li[i][1]))
    return min(res, key=lambda x: x[2])","for j in range(i + 1, len(li)):
    res.append((li[i][0], li[j][0], (li[j][1] - li[i][1]) / li[i][1]))","for i, (item1, value1) in enumerate(li):
    for j, (item2, value2) in enumerate(li[i+1:], i+1):
        res.append((item1, item2, (value2 - value1) / value1))"
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for n1 in tempList:
    testnum = n1 % radix
    testnum = testnum // (radix / 10)
    for n2 in range(10):
        if testnum == n2:
            newArr[n2].append(n1)","for i,n1 in enumerate(tempList):
    testnum = n1 % radix
    testnum = testnum // (radix / 10)
    for n2 in range(10):
        if testnum == n2:
            newArr[n2].append(n1)"
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for i in range(len(newArr)):
    for j in range(len(newArr[i])):
        tempList.append(newArr[i][j])","for i, arr in enumerate(newArr):
    for j, val in enumerate(arr):
        tempList.append(val)"
AlgorithmsByPython,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master//RadixSort.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython//RadixSort.py,,"def radixSortLSD(alist):
    if len(alist) == 0:
        return
    if len(alist) == 1:
        return alist
    tempList = alist
    maxNum = max(alist)
    radix = 10
    while maxNum * 10 > radix:
        newArr = [[], [], [], [], [], [], [], [], [], []]
        for n1 in tempList:
            testnum = n1 % radix
            testnum = testnum // (radix / 10)
            for n2 in range(10):
                if testnum == n2:
                    newArr[n2].append(n1)
        tempList = []
        for i in range(len(newArr)):
            for j in range(len(newArr[i])):
                tempList.append(newArr[i][j])
        radix *= 10
    return tempList","for j in range(len(newArr[i])):
    tempList.append(newArr[i][j])","for j, val in enumerate(newArr[i]):
    tempList.append(val)"
second.pytorch,https://github.com/traveller59/second.pytorch/tree/master/second/kittiviewer/viewer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/second.pytorch/second/kittiviewer/viewer.py,,"def _riou3d_shapely(rbboxes1, rbboxes2):
    (N, K) = (rbboxes1.shape[0], rbboxes2.shape[0])
    corners1 = box_np_ops.center_to_corner_box2d(rbboxes1[:, :2], rbboxes1[:, 3:5], rbboxes1[:, 6])
    corners2 = box_np_ops.center_to_corner_box2d(rbboxes2[:, :2], rbboxes2[:, 3:5], rbboxes2[:, 6])
    iou = np.zeros([N, K], dtype=np.float32)
    for i in range(N):
        for j in range(K):
            iw = min(rbboxes1[i, 2] + rbboxes1[i, 5], rbboxes2[j, 2] + rbboxes2[j, 5]) - max(rbboxes1[i, 2], rbboxes2[j, 2])
            if iw > 0:
                p1 = Polygon(corners1[i])
                p2 = Polygon(corners2[j])
                inc = p1.intersection(p2).area * iw
                if inc > 0:
                    iou[i, j] = inc / (p1.area * rbboxes1[i, 5] + p2.area * rbboxes2[j, 5] - inc)
    return iou","for i in range(N):
    for j in range(K):
        iw = min(rbboxes1[i, 2] + rbboxes1[i, 5], rbboxes2[j, 2] + rbboxes2[j, 5]) - max(rbboxes1[i, 2], rbboxes2[j, 2])
        if iw > 0:
            p1 = Polygon(corners1[i])
            p2 = Polygon(corners2[j])
            inc = p1.intersection(p2).area * iw
            if inc > 0:
                iou[i, j] = inc / (p1.area * rbboxes1[i, 5] + p2.area * rbboxes2[j, 5] - inc)","for i, _ in enumerate(range(N)):
    for j in range(K):
        iw = min(rbboxes1[i, 2] + rbboxes1[i, 5], rbboxes2[j, 2] + rbboxes2[j, 5]) - max(rbboxes1[i, 2], rbboxes2[j, 2])
        if iw > 0:
            p1 = Polygon(corners1[i])
            p2 = Polygon(corners2[j])
            inc = p1.intersection(p2).area * iw
            if inc > 0:
                iou[i, j] = inc / (p1.area * rbboxes1[i, 5] + p2.area * rbboxes2[j, 5] - inc)"
deepdrive,https://github.com/deepdrive/deepdrive/tree/master/vendor/tensorflow/models/research/slim/datasets/download_and_convert_mnist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deepdrive/vendor/tensorflow/models/research/slim/datasets/download_and_convert_mnist.py,,"def _clean_up_temporary_files(dataset_dir):
    """"""Removes temporary files used to create the dataset.

  Args:
    dataset_dir: The directory where the temporary files are stored.
  """"""
    for filename in [_TRAIN_DATA_FILENAME, _TRAIN_LABELS_FILENAME, _TEST_DATA_FILENAME, _TEST_LABELS_FILENAME]:
        filepath = os.path.join(dataset_dir, filename)
        tf.gfile.Remove(filepath)","for filename in [_TRAIN_DATA_FILENAME, _TRAIN_LABELS_FILENAME, _TEST_DATA_FILENAME, _TEST_LABELS_FILENAME]:
    filepath = os.path.join(dataset_dir, filename)
    tf.gfile.Remove(filepath)","for i,filename in enumerate([_TRAIN_DATA_FILENAME, _TRAIN_LABELS_FILENAME, _TEST_DATA_FILENAME, _TEST_LABELS_FILENAME]):
    filepath = os.path.join(dataset_dir, filename)
    tf.gfile.Remove(filepath)"
pre-commit,https://github.com/pre-commit/pre-commit/tree/master/pre_commit/languages/pygrep.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pre-commit/pre_commit/languages/pygrep.py,,"def main(argv: Sequence[str] | None=None) -> int:
    parser = argparse.ArgumentParser(description='grep-like finder using python regexes.  Unlike grep, this tool returns nonzero when it finds a match and zero otherwise.  The idea here being that matches are ""problems"".')
    parser.add_argument('-i', '--ignore-case', action='store_true')
    parser.add_argument('--multiline', action='store_true')
    parser.add_argument('--negate', action='store_true')
    parser.add_argument('pattern', help='python regex pattern.')
    parser.add_argument('filenames', nargs='*')
    args = parser.parse_args(argv)
    flags = re.IGNORECASE if args.ignore_case else 0
    if args.multiline:
        flags |= re.MULTILINE | re.DOTALL
    pattern = re.compile(args.pattern.encode(), flags)
    retv = 0
    process_fn = FNS[Choice(multiline=args.multiline, negate=args.negate)]
    for filename in args.filenames:
        retv |= process_fn(pattern, filename)
    return retv","for filename in args.filenames:
    retv |= process_fn(pattern, filename)","for i, filename in enumerate(args.filenames):
    retv |= process_fn(pattern, filename)"
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,,"def test_compute_log_manager(mock_s3_bucket):

    @op
    def easy(context):
        context.log.info('easy')
        print(HELLO_WORLD)
        return 'easy'

    @job
    def simple():
        easy()
    with tempfile.TemporaryDirectory() as temp_dir:
        with environ({'DAGSTER_HOME': temp_dir}):
            run_store = SqliteRunStorage.from_local(temp_dir)
            event_store = SqliteEventLogStorage(temp_dir)
            manager = S3ComputeLogManager(bucket=mock_s3_bucket.name, prefix='my_prefix', local_dir=temp_dir)
            instance = DagsterInstance(instance_type=InstanceType.PERSISTENT, local_artifact_storage=LocalArtifactStorage(temp_dir), run_storage=run_store, event_storage=event_store, compute_log_manager=manager, run_coordinator=DefaultRunCoordinator(), run_launcher=DefaultRunLauncher(), ref=InstanceRef.from_dir(temp_dir))
            result = simple.execute_in_process(instance=instance)
            capture_events = [event for event in result.all_events if event.event_type == DagsterEventType.LOGS_CAPTURED]
            assert len(capture_events) == 1
            event = capture_events[0]
            file_key = event.logs_captured_data.file_key
            log_key = manager.build_log_key_for_run(result.run_id, file_key)
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr
            s3_object = mock_s3_bucket.Object(key=manager._s3_key(log_key, ComputeIOType.STDERR))
            stderr_s3 = s3_object.get()['Body'].read().decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr_s3
            local_dir = os.path.dirname(manager._local_manager.get_captured_local_path(log_key, IO_TYPE_EXTENSION[ComputeIOType.STDOUT]))
            for filename in os.listdir(local_dir):
                os.unlink(os.path.join(local_dir, filename))
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr","for expected in EXPECTED_LOGS:
    assert expected in stderr","for i, expected in enumerate(EXPECTED_LOGS):
    assert expected in stderr"
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,,"def test_compute_log_manager(mock_s3_bucket):

    @op
    def easy(context):
        context.log.info('easy')
        print(HELLO_WORLD)
        return 'easy'

    @job
    def simple():
        easy()
    with tempfile.TemporaryDirectory() as temp_dir:
        with environ({'DAGSTER_HOME': temp_dir}):
            run_store = SqliteRunStorage.from_local(temp_dir)
            event_store = SqliteEventLogStorage(temp_dir)
            manager = S3ComputeLogManager(bucket=mock_s3_bucket.name, prefix='my_prefix', local_dir=temp_dir)
            instance = DagsterInstance(instance_type=InstanceType.PERSISTENT, local_artifact_storage=LocalArtifactStorage(temp_dir), run_storage=run_store, event_storage=event_store, compute_log_manager=manager, run_coordinator=DefaultRunCoordinator(), run_launcher=DefaultRunLauncher(), ref=InstanceRef.from_dir(temp_dir))
            result = simple.execute_in_process(instance=instance)
            capture_events = [event for event in result.all_events if event.event_type == DagsterEventType.LOGS_CAPTURED]
            assert len(capture_events) == 1
            event = capture_events[0]
            file_key = event.logs_captured_data.file_key
            log_key = manager.build_log_key_for_run(result.run_id, file_key)
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr
            s3_object = mock_s3_bucket.Object(key=manager._s3_key(log_key, ComputeIOType.STDERR))
            stderr_s3 = s3_object.get()['Body'].read().decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr_s3
            local_dir = os.path.dirname(manager._local_manager.get_captured_local_path(log_key, IO_TYPE_EXTENSION[ComputeIOType.STDOUT]))
            for filename in os.listdir(local_dir):
                os.unlink(os.path.join(local_dir, filename))
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr","for expected in EXPECTED_LOGS:
    assert expected in stderr_s3","for i, expected in enumerate(EXPECTED_LOGS):
    assert expected in stderr_s3"
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,,"def test_compute_log_manager(mock_s3_bucket):

    @op
    def easy(context):
        context.log.info('easy')
        print(HELLO_WORLD)
        return 'easy'

    @job
    def simple():
        easy()
    with tempfile.TemporaryDirectory() as temp_dir:
        with environ({'DAGSTER_HOME': temp_dir}):
            run_store = SqliteRunStorage.from_local(temp_dir)
            event_store = SqliteEventLogStorage(temp_dir)
            manager = S3ComputeLogManager(bucket=mock_s3_bucket.name, prefix='my_prefix', local_dir=temp_dir)
            instance = DagsterInstance(instance_type=InstanceType.PERSISTENT, local_artifact_storage=LocalArtifactStorage(temp_dir), run_storage=run_store, event_storage=event_store, compute_log_manager=manager, run_coordinator=DefaultRunCoordinator(), run_launcher=DefaultRunLauncher(), ref=InstanceRef.from_dir(temp_dir))
            result = simple.execute_in_process(instance=instance)
            capture_events = [event for event in result.all_events if event.event_type == DagsterEventType.LOGS_CAPTURED]
            assert len(capture_events) == 1
            event = capture_events[0]
            file_key = event.logs_captured_data.file_key
            log_key = manager.build_log_key_for_run(result.run_id, file_key)
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr
            s3_object = mock_s3_bucket.Object(key=manager._s3_key(log_key, ComputeIOType.STDERR))
            stderr_s3 = s3_object.get()['Body'].read().decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr_s3
            local_dir = os.path.dirname(manager._local_manager.get_captured_local_path(log_key, IO_TYPE_EXTENSION[ComputeIOType.STDOUT]))
            for filename in os.listdir(local_dir):
                os.unlink(os.path.join(local_dir, filename))
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr","for filename in os.listdir(local_dir):
    os.unlink(os.path.join(local_dir, filename))","for i,filename in enumerate(os.listdir(local_dir)):
    os.unlink(os.path.join(local_dir, filename))"
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/libraries/dagster-aws/dagster_aws_tests/s3_tests/test_compute_log_manager.py,,"def test_compute_log_manager(mock_s3_bucket):

    @op
    def easy(context):
        context.log.info('easy')
        print(HELLO_WORLD)
        return 'easy'

    @job
    def simple():
        easy()
    with tempfile.TemporaryDirectory() as temp_dir:
        with environ({'DAGSTER_HOME': temp_dir}):
            run_store = SqliteRunStorage.from_local(temp_dir)
            event_store = SqliteEventLogStorage(temp_dir)
            manager = S3ComputeLogManager(bucket=mock_s3_bucket.name, prefix='my_prefix', local_dir=temp_dir)
            instance = DagsterInstance(instance_type=InstanceType.PERSISTENT, local_artifact_storage=LocalArtifactStorage(temp_dir), run_storage=run_store, event_storage=event_store, compute_log_manager=manager, run_coordinator=DefaultRunCoordinator(), run_launcher=DefaultRunLauncher(), ref=InstanceRef.from_dir(temp_dir))
            result = simple.execute_in_process(instance=instance)
            capture_events = [event for event in result.all_events if event.event_type == DagsterEventType.LOGS_CAPTURED]
            assert len(capture_events) == 1
            event = capture_events[0]
            file_key = event.logs_captured_data.file_key
            log_key = manager.build_log_key_for_run(result.run_id, file_key)
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr
            s3_object = mock_s3_bucket.Object(key=manager._s3_key(log_key, ComputeIOType.STDERR))
            stderr_s3 = s3_object.get()['Body'].read().decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr_s3
            local_dir = os.path.dirname(manager._local_manager.get_captured_local_path(log_key, IO_TYPE_EXTENSION[ComputeIOType.STDOUT]))
            for filename in os.listdir(local_dir):
                os.unlink(os.path.join(local_dir, filename))
            log_data = manager.get_log_data(log_key)
            stdout = log_data.stdout.decode('utf-8')
            assert stdout == HELLO_WORLD + SEPARATOR
            stderr = log_data.stderr.decode('utf-8')
            for expected in EXPECTED_LOGS:
                assert expected in stderr","for expected in EXPECTED_LOGS:
    assert expected in stderr","for i, expected in enumerate(EXPECTED_LOGS):
    assert expected in stderr"
s3fs,https://github.com/fsspec/s3fs/tree/master/s3fs/tests/test_s3fs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3fs/s3fs/tests/test_s3fs.py,,"def test_s3_big_ls(s3):
    for x in range(1200):
        s3.touch(test_bucket_name + '/thousand/%i.part' % x)
    assert len(s3.find(test_bucket_name)) > 1200
    s3.rm(test_bucket_name + '/thousand/', recursive=True)
    assert len(s3.find(test_bucket_name + '/thousand/')) == 0","for x in range(1200):
    s3.touch(test_bucket_name + '/thousand/%i.part' % x)","for i in range(1200):
    s3.touch(test_bucket_name + '/thousand/%i.part' % i)"
espresso,https://github.com/freewym/espresso/tree/master/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,KLDivergenceRerankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    sample_size = sample['id'].numel()
    assert sample_size % self.task.cfg.mt_beam == 0, f'sample_size ({sample_size}) cannot be divided by beam size ({self.task.cfg.mt_beam}).Please set --required-batch-size-multiple={self.task.cfg.mt_beam}.'
    batch_out = []
    for i in range(0, sample_size, self.forward_batch_size):
        j = min(i + self.forward_batch_size, sample_size)
        out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
        batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))
    batch_out = torch.cat(batch_out, dim=0).view(self.task.cfg.mt_beam, sample_size // self.task.cfg.mt_beam, -1)
    if model.joint_classification == 'sent':
        batch_out = model.joint_forward(batch_out)
    scores = model.classification_forward(batch_out.view(sample_size, 1, -1)).view(-1, self.task.cfg.mt_beam)
    loss = self.compute_kl_loss(scores, sample['target'][:, 0].view(-1, self.task.cfg.mt_beam))
    sample_size = sample_size // self.task.cfg.mt_beam
    logging_output = {'loss': loss.detach(), 'ntokens': sample['ntokens'], 'nsentences': sample_size * self.task.cfg.mt_beam, 'sample_size': sample_size, 'scores': scores.detach()}
    return (loss, sample_size, logging_output)","for i in range(0, sample_size, self.forward_batch_size):
    j = min(i + self.forward_batch_size, sample_size)
    out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
    batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))","for i in range(0, sample_size, self.forward_batch_size):
    j = min(i + self.forward_batch_size, sample_size)
    out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
    batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))"
napalm,https://github.com/napalm-automation/napalm/tree/master/napalm/iosxr_netconf/iosxr_netconf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/iosxr_netconf/iosxr_netconf.py,IOSXRNETCONFDriver,"def get_snmp_information(self):
    """"""Return the SNMP configuration.""""""
    snmp_information = {}
    rpc_reply = self.device.get_config(source='running', filter=('subtree', C.SNMP_RPC_REQ_FILTER)).xml
    snmp_result_tree = ETREE.fromstring(rpc_reply)
    _PRIVILEGE_MODE_MAP_ = {'read-only': 'ro', 'read-write': 'rw'}
    snmp_information = {'chassis_id': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:chassis-id', default='', namespaces=C.NS), 'contact': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:contact', default='', namespaces=C.NS), 'location': self._find_txt(snmp_result_tree, './/snmp:snmp/snmp:system/snmp:location', default='', namespaces=C.NS), 'community': {}}
    for community in snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS):
        name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
        privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
        acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
        snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}
    return snmp_information","for community in snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS):
    name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
    privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
    acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
    snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}","for i, community in enumerate(snmp_result_tree.xpath('.//snmp:snmp/snmp:administration/             snmp:default-communities/snmp:default-community', namespaces=C.NS)):
    name = self._find_txt(community, './snmp:community-name', default='', namespaces=C.NS)
    privilege = self._find_txt(community, './snmp:priviledge', default='', namespaces=C.NS)
    acl = self._find_txt(community, './snmp:v6-access-list', default='', namespaces=C.NS) or self._find_txt(community, './snmp:v4-access-list', default='', namespaces=C.NS)
    snmp_information['community'][name] = {'mode': _PRIVILEGE_MODE_MAP_.get(privilege, ''), 'acl': acl}"
EasyTransfer,https://github.com/alibaba/EasyTransfer/tree/master/scripts/fashion_bert/image_feature_extract.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/scripts/fashion_bert/image_feature_extract.py,PredictorImpl,"def search_pb(self, directory):
    """"""
    search pb file recursively, if multiple pb files exist, exception will be
    raised

    Returns:
      directory contain pb file
    """"""
    dir_list = []
    for (root, dirs, files) in tf.gfile.Walk(directory):
        for f in files:
            (_, ext) = os.path.splitext(f)
            if ext == '.pb':
                dir_list.append(root)
    if len(dir_list) == 0:
        raise ValueError('savedmodel is not found in directory %s' % directory)
    elif len(dir_list) > 1:
        raise ValueError('multiple saved model found in directory %s' % directory)
    return dir_list[0]","for f in files:
    (_, ext) = os.path.splitext(f)
    if ext == '.pb':
        dir_list.append(root)","for i,f in enumerate(files):
    (_, ext) = os.path.splitext(f)
    if ext == '.pb':
        dir_list.append(root)"
pulsar,https://github.com/quantmind/pulsar/tree/master/docs/_ext/sphinxtogithub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pulsar/docs/_ext/sphinxtogithub.py,FileHandler,"def process(self):
    text = self.opener(self.name).read()
    for replacer in self.replacers:
        text = replacer.process(text)
    self.opener(self.name, 'w').write(text)","for replacer in self.replacers:
    text = replacer.process(text)","for i,replacer in enumerate(self.replacers):
    text = replacer.process(text)"
binaryalert,https://github.com/airbnb/binaryalert/tree/master/rules/clone_rules.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/binaryalert/rules/clone_rules.py,,"def _clone_repo(url: str, include: Optional[List[str]], exclude: Optional[List[str]]) -> int:
    """"""Clone the given repo and copy only the YARA files from the specified paths.

    Returns:
        Number of files copied.
    """"""
    cloned_repo_root = os.path.join(tempfile.gettempdir(), os.path.basename(url))
    if os.path.exists(cloned_repo_root):
        shutil.rmtree(cloned_repo_root)
    subprocess.check_call(['git', 'clone', '--quiet', '--depth', '1', url, cloned_repo_root])
    if '//' in url:
        target_repo_root = os.path.join(RULES_DIR, url.split('//')[1])
    else:
        target_repo_root = os.path.join(RULES_DIR, url.split('@')[1].replace(':', '/', 1))
    if os.path.exists(target_repo_root):
        shutil.rmtree(target_repo_root)
    files_copied = 0
    for relative_path in _files_to_copy(cloned_repo_root, include, exclude):
        os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
        src = os.path.join(cloned_repo_root, relative_path)
        dst = os.path.join(target_repo_root, relative_path)
        shutil.copy(src, dst)
        files_copied += 1
    shutil.rmtree(cloned_repo_root)
    return files_copied","for relative_path in _files_to_copy(cloned_repo_root, include, exclude):
    os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
    src = os.path.join(cloned_repo_root, relative_path)
    dst = os.path.join(target_repo_root, relative_path)
    shutil.copy(src, dst)
    files_copied += 1","for i, relative_path in enumerate(_files_to_copy(cloned_repo_root, include, exclude)):
    os.makedirs(os.path.join(target_repo_root, os.path.dirname(relative_path)), exist_ok=True)
    src = os.path.join(cloned_repo_root, relative_path)
    dst = os.path.join(target_repo_root, relative_path)
    shutil.copy(src, dst)
    files_copied += 1"
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/pypower/polycost.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/pypower/polycost.py,,"def polycost(gencost, Pg, der=0):
    """"""Evaluates polynomial generator cost & derivatives.

    C{f = polycost(gencost, Pg)} returns the vector of costs evaluated at C{Pg}

    C{df = polycost(gencost, Pg, 1)} returns the vector of first derivatives
    of costs evaluated at C{Pg}

    C{d2f = polycost(gencost, Pg, 2)} returns the vector of second derivatives
    of costs evaluated at C{Pg}

    C{gencost} must contain only polynomial costs
    C{Pg} is in MW, not p.u. (works for C{Qg} too)

    @author: Ray Zimmerman (PSERC Cornell)
    """"""
    if any(gencost[:, MODEL] == PW_LINEAR):
        sys.stderr.write('polycost: all costs must be polynomial\n')
    ng = len(Pg)
    maxN = max(gencost[:, NCOST].astype(int))
    minN = min(gencost[:, NCOST].astype(int))
    c = zeros((ng, maxN))
    for n in arange(minN, maxN + 1):
        k = find(gencost[:, NCOST] == n)
        c[k, :n] = gencost[k, COST + n - 1:COST - 1:-1]
    for d in range(1, der + 1):
        if c.shape[1] >= 2:
            c = c[:, 1:maxN - d + 1]
        else:
            c = zeros((ng, 1))
            break
        for k in range(2, maxN - d + 1):
            c[:, k - 1] = c[:, k - 1] * k
    if len(c) == 0:
        f = zeros(Pg.shape)
    else:
        f = c[:, :1].flatten()
        for k in range(1, c.shape[1]):
            f = f + c[:, k] * Pg ** k
    return f","for k in range(2, maxN - d + 1):
    c[:, k - 1] = c[:, k - 1] * k","for i,k in enumerate(range(2, maxN - d + 1)):
    c[:, i] = c[:, i] * k"
KILT,https://github.com/facebookresearch/KILT/tree/master/kilt/readers/t5/base_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KILT/kilt/readers/t5/base_transformer.py,LoggingCallback,"def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
    logger.info('***** Validation results *****')
    if pl_module.is_logger():
        metrics = trainer.callback_metrics
        for key in sorted(metrics):
            if key not in ['log', 'progress_bar']:
                logger.info('{} = {}\n'.format(key, str(metrics[key])))","for key in sorted(metrics):
    if key not in ['log', 'progress_bar']:
        logger.info('{} = {}\n'.format(key, str(metrics[key])))","for i,key in enumerate(sorted(metrics)):
    if key not in ['log', 'progress_bar']:
        logger.info('{} = {}\n'.format(key, str(metrics[key])))"
pdpipe,https://github.com/pdpipe/pdpipe/tree/master/pdpipe/sklearn_stages.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pdpipe/pdpipe/sklearn_stages.py,Encode,"def _transform(self, df, verbose):
    inter_df = df
    for colname in self.encoders:
        lbl_enc = self.encoders[colname]
        source_col = df[colname]
        loc = df.columns.get_loc(colname) + 1
        new_name = colname + '_enc'
        if self._drop:
            inter_df = inter_df.drop(colname, axis=1)
            new_name = colname
            loc -= 1
        inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)
    return inter_df","for colname in self.encoders:
    lbl_enc = self.encoders[colname]
    source_col = df[colname]
    loc = df.columns.get_loc(colname) + 1
    new_name = colname + '_enc'
    if self._drop:
        inter_df = inter_df.drop(colname, axis=1)
        new_name = colname
        loc -= 1
    inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)","for i, colname in enumerate(self.encoders):
    lbl_enc = self.encoders[colname]
    source_col = df[colname]
    loc = df.columns.get_loc(colname) + 1
    new_name = colname + '_enc'
    if self._drop:
        inter_df = inter_df.drop(colname, axis=1)
        new_name = colname
        loc -= 1
    inter_df = out_of_place_col_insert(df=inter_df, series=lbl_enc.transform(source_col), loc=loc, column_name=new_name)"
text_classification,https://github.com/brightmart/text_classification/tree/master/aa1_data_util/data_util_zhihu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/text_classification/aa1_data_util/data_util_zhihu.py,,"def create_voabulary_label(voabulary_label='train-zhihu4-only-title-all.txt', name_scope='', use_seq2seq=False):
    print('create_voabulary_label_sorted.started.traning_data_path:', voabulary_label)
    cache_path = 'cache_vocabulary_label_pik/' + name_scope + '_label_voabulary.pik'
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as data_f:
            (vocabulary_word2index_label, vocabulary_index2word_label) = pickle.load(data_f)
            return (vocabulary_word2index_label, vocabulary_index2word_label)
    else:
        zhihu_f_train = codecs.open(voabulary_label, 'r', 'utf8')
        lines = zhihu_f_train.readlines()
        count = 0
        vocabulary_word2index_label = {}
        vocabulary_index2word_label = {}
        vocabulary_label_count_dict = {}
        for (i, line) in enumerate(lines):
            if '__label__' in line:
                label = line[line.index('__label__') + len('__label__'):].strip().replace('\n', '')
                if vocabulary_label_count_dict.get(label, None) is not None:
                    vocabulary_label_count_dict[label] = vocabulary_label_count_dict[label] + 1
                else:
                    vocabulary_label_count_dict[label] = 1
        list_label = sort_by_value(vocabulary_label_count_dict)
        print('length of list_label:', len(list_label))
        countt = 0
        if use_seq2seq:
            i_list = [0, 1, 2]
            label_special_list = [_GO, _END, _PAD]
            for (i, label) in zip(i_list, label_special_list):
                vocabulary_word2index_label[label] = i
                vocabulary_index2word_label[i] = label
        for (i, label) in enumerate(list_label):
            if i < 10:
                count_value = vocabulary_label_count_dict[label]
                print('label:', label, 'count_value:', count_value)
                countt = countt + count_value
            indexx = i + 3 if use_seq2seq else i
            vocabulary_word2index_label[label] = indexx
            vocabulary_index2word_label[indexx] = label
        print('count top10:', countt)
        if not os.path.exists(cache_path):
            with open(cache_path, 'a') as data_f:
                pickle.dump((vocabulary_word2index_label, vocabulary_index2word_label), data_f)
    print('create_voabulary_label_sorted.ended.len of vocabulary_label:', len(vocabulary_index2word_label))
    return (vocabulary_word2index_label, vocabulary_index2word_label)","for (i, label) in zip(i_list, label_special_list):
    vocabulary_word2index_label[label] = i
    vocabulary_index2word_label[i] = label","for i, label in enumerate(label_special_list):
    vocabulary_word2index_label[label] = i_list[i]
    vocabulary_index2word_label[i_list[i]] = label"
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines","for check in valid_checks:
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')","for i, check in enumerate(valid_checks):
    if has_process_signature(check):
        status = 'X'
        checks_with_ps += 1
    else:
        status = ' '
    lines.append(f'    - [{status}] {check}')"
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","for i in range(self.FILE_HEADER.NumberOfSections):
    if i >= MAX_SECTIONS:
        self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
        break
    simultaneous_errors = 0
    section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
    if not section:
        break
    section_offset = offset + section.sizeof() * i
    section.set_file_offset(section_offset)
    section_data = self.__data__[section_offset:section_offset + section.sizeof()]
    if count_zeroes(section_data) == section.sizeof():
        self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
        break
    if not section_data:
        self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
        break
    section.__unpack__(section_data)
    self.__structures__.append(section)
    if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
    if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
    if section.Misc_VirtualSize > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
    if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
    if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
    if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
        self.__warnings.append('Too many warnings parsing section. Aborting.')
        break
    section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
    set_flags(section, section.Characteristics, section_flags)
    if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
        if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
            pass
        else:
            self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
    self.sections.append(section)","for i in range(self.FILE_HEADER.NumberOfSections):
    if i >= MAX_SECTIONS:
        self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
        break
    simultaneous_errors = 0
    section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
    if not section:
        break
    section_offset = offset + section.sizeof() * i
    section.set_file_offset(section_offset)
    section_data = self.__data__[section_offset:section_offset + section.sizeof()]
    if count_zeroes(section_data) == section.sizeof():
        self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
        break
    if not section_data:
        self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
        break
    section.__unpack__(section_data)
    self.__structures__.append(section)
    if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
    if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
    if section.Misc_VirtualSize > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
    if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
        simultaneous_errors += 1
        self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
    if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
        simultaneous_errors += 1
        self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
    if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
        self.__warnings.append('Too many warnings parsing section. Aborting.')
        break
    section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
    set_flags(section, section.Characteristics, section_flags)
    if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
        if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
            pass
        else:
            self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
    self.sections.append(section)"
faust,https://github.com/robinhood/faust/tree/master/t/stress/producer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/stress/producer.py,,"def install_produce_command(app) -> None:

    @app.command(option('--max-latency', type=float, default=0.5, envvar='PRODUCE_LATENCY', help='Add delay of (at most) n seconds between publishing.'), option('--max-messages', type=int, default=None, help='Send at most N messages or 0 for infinity.'))
    async def produce(self, max_latency: float, max_messages: int):
        """"""Produce example events.""""""
        prods = {aiter(p(max_messages)) for p in app.stress_producers}
        i = 0
        while not app.should_stop:
            to_remove: Set[Any] = set()
            for producer in prods:
                i += 1
                try:
                    await anext(producer)
                except StopAsyncIteration:
                    to_remove.add(producer)
                if not max_latency:
                    if not i % 10000:
                        self.say(f'+SEND {i}')
                elif not i % 10:
                    self.say(f'+SEND {i}')
            if not prods:
                await asyncio.sleep(1.0)
            if max_latency:
                await asyncio.sleep(random.uniform(0, max_latency))
            for producer in to_remove:
                prods.discard(producer)
        print('No more producers - exiting', file=sys.stderr)","for producer in prods:
    i += 1
    try:
        await anext(producer)
    except StopAsyncIteration:
        to_remove.add(producer)
    if not max_latency:
        if not i % 10000:
            self.say(f'+SEND {i}')
    elif not i % 10:
        self.say(f'+SEND {i}')","to_remove = set()
for i, producer in enumerate(prods):
    try:
        await anext(producer)
    except StopAsyncIteration:
        to_remove.add(producer)
    if not max_latency:
        if not (i+1) % 10000:
            self.say(f'+SEND {i+1}')
    elif not (i+1) % 10:
        self.say(f'+SEND {i+1}')"
faust,https://github.com/robinhood/faust/tree/master/t/stress/producer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/stress/producer.py,,"def install_produce_command(app) -> None:

    @app.command(option('--max-latency', type=float, default=0.5, envvar='PRODUCE_LATENCY', help='Add delay of (at most) n seconds between publishing.'), option('--max-messages', type=int, default=None, help='Send at most N messages or 0 for infinity.'))
    async def produce(self, max_latency: float, max_messages: int):
        """"""Produce example events.""""""
        prods = {aiter(p(max_messages)) for p in app.stress_producers}
        i = 0
        while not app.should_stop:
            to_remove: Set[Any] = set()
            for producer in prods:
                i += 1
                try:
                    await anext(producer)
                except StopAsyncIteration:
                    to_remove.add(producer)
                if not max_latency:
                    if not i % 10000:
                        self.say(f'+SEND {i}')
                elif not i % 10:
                    self.say(f'+SEND {i}')
            if not prods:
                await asyncio.sleep(1.0)
            if max_latency:
                await asyncio.sleep(random.uniform(0, max_latency))
            for producer in to_remove:
                prods.discard(producer)
        print('No more producers - exiting', file=sys.stderr)","for producer in to_remove:
    prods.discard(producer)","for i, producer in enumerate(to_remove):
    prods.discard(producer)"
oppia,https://github.com/oppia/oppia/tree/master/core/domain/user_services_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/user_services_test.py,UserServicesUnitTests,"def test_create_new_user_with_invalid_emails_raises_exception(self) -> None:
    bad_email_addresses_with_expected_error_message = [('@', 'Invalid email address: @'), ('@@', 'Invalid email address: @@'), ('abc', 'Invalid email address: abc'), ('', 'No user email specified.'), (None, 'Expected email to be a string, received None'), (['a', '@', 'b.com'], re.escape(""Expected email to be a string, received ['a', '@', 'b.com']""))]
    for (email, error_msg) in bad_email_addresses_with_expected_error_message:
        with self.assertRaisesRegex(utils.ValidationError, error_msg):
            user_services.create_new_user('auth_id', email)","for (email, error_msg) in bad_email_addresses_with_expected_error_message:
    with self.assertRaisesRegex(utils.ValidationError, error_msg):
        user_services.create_new_user('auth_id', email)","for i, (email, error_msg) in enumerate(bad_email_addresses_with_expected_error_message):
    with self.assertRaisesRegex(utils.ValidationError, error_msg):
        user_services.create_new_user('auth_id', email)"
chia-rosechain,https://github.com/snight1983/chia-rosechain/tree/master/chia/daemon/server.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chia-rosechain/chia/daemon/server.py,WebSocketServer,"def remove_connection(self, websocket: WebSocketServerProtocol):
    service_name = None
    if websocket in self.remote_address_map:
        service_name = self.remote_address_map[websocket]
        self.remote_address_map.pop(websocket)
    if service_name in self.connections:
        after_removal = []
        for connection in self.connections[service_name]:
            if connection == websocket:
                continue
            else:
                after_removal.append(connection)
        self.connections[service_name] = after_removal","for connection in self.connections[service_name]:
    if connection == websocket:
        continue
    else:
        after_removal.append(connection)","for i, connection in enumerate(self.connections[service_name]):
    if connection == websocket:
        continue
    else:
        after_removal.append(connection)"
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i,_ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate"
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i,_ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate"
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])","for i in range(N):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate","for i,_ in enumerate(range(N)):
    self.addLayer(nChannels, growthRate, do_dropout)
    nChannels += growthRate"
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/hapi/dynamic_flops.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/hapi/dynamic_flops.py,,"def count_parameters(m, x, y):
    total_params = 0
    for p in m.parameters():
        total_params += p.numel()
    m.total_params[0] = abs(int(total_params))","for p in m.parameters():
    total_params += p.numel()","for i,p in enumerate(m.parameters()):
    total_params += p.numel()"
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/transpiler/passes/basis/basis_translator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/transpiler/passes/basis/basis_translator.py,BasisTranslator,"def _extract_basis_target(self, dag, qarg_indices, source_basis=None, qargs_local_source_basis=None):
    if source_basis is None:
        source_basis = set()
    if qargs_local_source_basis is None:
        qargs_local_source_basis = defaultdict(set)
    for node in dag.op_nodes():
        qargs = tuple((qarg_indices[bit] for bit in node.qargs))
        if dag.has_calibration_for(node):
            continue
        if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
            qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
        else:
            source_basis.add((node.name, node.op.num_qubits))
        if isinstance(node.op, ControlFlowOp):
            for block in node.op.blocks:
                block_dag = circuit_to_dag(block)
                (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)
    return (source_basis, qargs_local_source_basis)","for node in dag.op_nodes():
    qargs = tuple((qarg_indices[bit] for bit in node.qargs))
    if dag.has_calibration_for(node):
        continue
    if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
        qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
    else:
        source_basis.add((node.name, node.op.num_qubits))
    if isinstance(node.op, ControlFlowOp):
        for block in node.op.blocks:
            block_dag = circuit_to_dag(block)
            (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)","for i,node in enumerate(dag.op_nodes()):
    qargs = tuple((qarg_indices[bit] for bit in node.qargs))
    if dag.has_calibration_for(node):
        continue
    if qargs in self._qargs_with_non_global_operation or any((frozenset(qargs).issuperset(incomplete_qargs) for incomplete_qargs in self._qargs_with_non_global_operation)):
        qargs_local_source_basis[frozenset(qargs)].add((node.name, node.op.num_qubits))
    else:
        source_basis.add((node.name, node.op.num_qubits))
    if isinstance(node.op, ControlFlowOp):
        for block in node.op.blocks:
            block_dag = circuit_to_dag(block)
            (source_basis, qargs_local_source_basis) = self._extract_basis_target(block_dag, {inner: qarg_indices[outer] for (inner, outer) in zip(block.qubits, node.qargs)}, source_basis=source_basis, qargs_local_source_basis=qargs_local_source_basis)"
rally,https://github.com/elastic/rally/tree/master/it/tracker_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/it/tracker_test.py,,"def test_create_track(cfg, tmp_path, test_cluster):
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'
    assert it.race(cfg, cmd) == 0
    track_name = f'test-track-{uuid.uuid4()}'
    track_path = tmp_path / track_name
    assert it.esrally(cfg, f'create-track --target-hosts=127.0.0.1:{test_cluster.http_port} --indices=geonames --track={track_name} --output-path={tmp_path}') == 0
    base_generated_corpora = 'geonames-documents'
    expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']
    for f in expected_files:
        full_path = track_path / f
        assert full_path.exists(), f'Expected file to exist at path [{full_path}]'
    with open(track_path / f'{base_generated_corpora}-1k.json', 'rt') as f:
        num_lines = sum((1 for line in f))
    assert num_lines == 1000, f'Corpora [{base_generated_corpora}-1k.json] used by test-mode is [{num_lines}] lines but should be 1000 lines'
    cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0","for f in expected_files:
    full_path = track_path / f
    assert full_path.exists(), f'Expected file to exist at path [{full_path}]'","for i,f in enumerate(expected_files):
    full_path = track_path / f
    assert full_path.exists(), f'Expected file to exist at path [{full_path}]'"
hivemind,https://github.com/learning-at-home/hivemind/tree/master/hivemind/optim/experimental/grad_averager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hivemind/hivemind/optim/experimental/grad_averager.py,GradientAverager,"def reset_accumulated_grads_(self):
    """"""reset averager-internal gradient accumulators and the denominator""""""
    self._accumulators_used_in_step = False
    self.local_samples_accumulated = self.local_times_accumulated = 0
    self._anchor_batch_size = None
    for grad_buf in self._grad_accumulators():
        grad_buf.zero_()","for grad_buf in self._grad_accumulators():
    grad_buf.zero_()","for i, grad_buf in enumerate(self._grad_accumulators()):
    grad_buf.zero_()"
